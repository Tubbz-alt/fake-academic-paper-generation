%%%% ijcai18.tex

\typeout{IJCAI-18 Instructions for Authors}

% These are the instructions for authors for IJCAI-18.
% They are the same as the ones for IJCAI-11 with superficial wording
%   changes only.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
% The file ijcai18.sty is the style file for IJCAI-18 (same as ijcai08.sty).
\usepackage{ijcai18}

% Use the postscript times font!
\usepackage{times}
\usepackage{xcolor}
\usepackage{soul}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}

\usepackage{url}

\usepackage[ruled]{algorithm2e}
\usepackage[caption=false]{subfig}
\usepackage{amssymb}
\usepackage[cmex10]{amsmath}
\usepackage{booktabs}
\usepackage[figuresright]{rotating}

\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{comment}

\title{Unifying and Merging Well-trained Deep Neural Networks for Inference Stage}
%\title{Unifying and Merging Deep Neural Networks in Inference Stage}

%% Single author syntax
%\author{Jêröme Lang\\
%Laboratoire d'Analyse et Modélisation des Systèmes pour l'Aide à la Décision (LAMSADE)  \\
%pcchair@ijcai-18.org}

%% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
%\iffalse
\author{
	Yi-Min Chou$^{1,2}$,
	Yi-Ming Chan$^{1,2}$,
	Jia-Hong Lee$^{1,2}$,
	Chih-Yi Chiu$^3$,
	Chu-Song Chen$^{1,2}$
	\\
	$^1$ Institute of Information Science, Academia Sinica, Nankang, Taipei, Taiwan \\
	$^2$ MOST Joint Research Center for AI Technology and All Vista Healthcare\\
	$^3$ National Chiayi University,    Chiayi City, Taiwan  \\
	%%
	\tt\small \{chou, yiming, honghenry.lee, song\}@iis.sinica.edu.tw,
	\tt\small chihyi.chiu@gmail.com,
}
%% If your authors do not fit in the default space, you can increase it
%% by uncommenting the following (adjust the "2.5in" size to make it fit
%% properly)
%% \setlength\titlebox{2.5in}
%\fi

\begin{document}
	
	\maketitle
	
	\begin{abstract}
		%In this paper,
		We propose a novel method to merge convolutional neural-nets for the inference stage.
		Given two well-trained networks that may have different architectures that handle different tasks, our method aligns the layers of the original networks and merges them into a unified model by sharing the representative codes of weights.
		The shared weights are further re-trained to fine-tune the performance of the merged model.
		The proposed method effectively produces a compact model that may run original tasks simultaneously on resource-limited devices.
		As it preserves the general architectures and leverages the co-used weights of well-trained networks, a substantial training overhead can be reduced to shorten the system development time.
		Experimental results demonstrate a satisfactory performance and validate the effectiveness of the method.
	\end{abstract}
	
	\section{Introduction}
	
	The research on deep neural networks has gotten a rapid progress and achievement recently.
	It is successfully applied in a wide range of artificial intelligence (AI) applications, including computer vision, speech processing, natural language processing, bioinformatics, etc.
	To handle various tasks, we usually design different network models and train them with particular datasets separately, so they can behave well for specific purposes.
	However, in practical AI applications, it is common to handle multiple tasks simultaneously, leading to a high demand for the computation resource in both training and inference stages.
	Therefore, how to effectively integrate multiple network models in a system is a crucial problem towards successful AI applications.
	
	This paper tackles the problems of merging multiple well-trained (known-weights) feed-forward networks and unifying them into a single but compact one.
	The original networks, whose architectures may not be identical, can be either single or multiple source input.
	After unification, the merged network should be capable of handling the original tasks but is more condensed than the whole original models.
	Our approach (NeuralMerger) contains two phases:
	
	\noindent \textbf{Alignment and encoding phase}: First, we align the architectures of neural network models and encode the weights
	such that they are shared among the networks.
	The purpose is to unify the weights so that the filters and weights of different neural networks can be co-used.
	
	\noindent \textbf{Fine-tuning phase}: Second, we fine-tune the merged model with partial or all training data (calibration data). A method following the concept of distilling dark knowledge of neural networks in \cite{HintonDistilling14} is employed in this phase.
	
	\begin{figure}[t]
		\begin{center}
			%\includegraphics[width=0.6\textwidth]{./figures/Fig1_new.pdf}
			\includegraphics[width=0.75\columnwidth]{./figures/Fig1_new_2.pdf}
			%      \vspace{-0.05cm}
			%            \\(a)
		\end{center}
		%        \begin{center}
		%            \includegraphics[width=\columnwidth]{./figures/Fig2.pdf}
		%         \vspace{-0.5cm}
		%            \\(b)
		%        \end{center}
		\vspace{-0.1cm}
		\caption{Two tasks accomplished by feed-forward networks, where model A (or B) consists of $c_A$ (or $c_B$) convolution and $f_A$ (or $f_B$) fully-connected layers, respectively. Our NeuralMerger unifies the two models into a single one consisting of $max(c_A, c_B)$ convolution and $max(f_A, f_B)$ fully-connected layers for the model inference; \textbf{E}-Conv and \textbf{E}-FC are referred to as the \emph{Jointly-\textbf{E}ncoded} \emph{convolution} and \emph{fully-connected} layers, respectively.%(b) Example of merging three models, ZF, VGG, and LeNet into a single one in the inference stage via our approach.
		}
		\label{fig:concept}
	\end{figure}
	
	Neural network models may have very different topologies.
	%For example, a feed-forward network contains only the layers in a cascade, whereas a recurrent neural network (RNN) has loops among the layers.
	Currently, this study focuses on merging feed-forward networks, while merging networks with loops remains a future work.
	A modern feed-forward network consists of several kinds of layers, including convolution, pooling, and full-connection, which is generally referred to as a convolutional network (CNN).
	When merging two CNNs, our approach aligns the same-type layers (convolution; full-connection) into pairs.
	The layers in a pair are merged into a single layer that shares a common weight codebook through the proposed encoding scheme.
	The codebooks in the merged single model can be further trained via back-propagation algorithm; it thus can be fine-tuned to seek for performance improvement.
	
	\begin{figure}[t]
		%    \begin{center}
		%\includegraphics[width=0.6\textwidth]{./figures/Fig1_new.pdf}
		%   \includegraphics[width=\columnwidth]{./figures/Fig1_new.pdf}
		%    \\(a)\\
		%    \end{center}
		\begin{center}
			\includegraphics[width=\columnwidth]{./figures/Fig2.pdf}
			%   \\(b)
		\end{center}
		\vspace{-0.35cm}
		\caption{Example of merging three models, ZF, VGG-avg, and LeNet into a single one for the inference stage via our NeuralMerger.}
		\label{fig:mergemore}
	\end{figure}
	
	
	\vspace{0.2cm}
	\noindent \textbf{Motivation of Our Study}:
	Merging existing neural networks has a great potential for real-world applications.
	%For example, in intelligent agents or robots, many recognition tasks based on same or different signal sources (eg., image, sound) are often required.
	%Even when using a single source only (eg.~image), visual classification tasks of different types such as object recognition, face identification, hand gesture prediction and scene-text classification could also be involved in an intelligent robot system.
	%When those individual functionalities are available, model merging on the inference stage is helpful to build a system toward strong AI.
	%    
	To tackle multiple recognition tasks in a single system based on either unique or various signal sources, a typical approach is to design a new model and train the model on the union datasets of these tasks, eg., \cite{DBLP:journals/corr/KaiserGSVPJU17,DBLP:journals/corr/AytarVT17}.
	Such ``learn-them-all" approaches train a single complex model to handle multiple tasks simultaneously.
	However, two issues may arise.
	First, it is hard to choose a suitable neural-net architecture for learning all the tasks well in advance;
	hence, a trial-and-error process is required to conduct suitable architectures.
	Second, learning from a random initial with large training data of different types could be demanding.
	To tackle these issues, the networks with bridging layers among the original models are conducted as a joint model for multi-task learning in \cite{RN274_multitask}.
	However, increased complexity and size of the joint model hinder their availability on resource-limited or edge devices in the inference stage.
	
	As many models trained for various tasks are available now, a practical way to integrate different functionalities in a system would be leveraging on these individual-task models.
	In this paper, we introduce an approach that merges different neural networks by removing the co-redundancy among their filters or weights.
	The proposed NeuralMerger can take advantage of existing well-trained models.
	%Given two neural network models with their weights provided,
	Our approach merges them %into a single model
	via finding and sharing the representative codes of weights;
	%via finding and sharing the representative weights;
	the shared codes can still be refined by learning.
	To our knowledge, this is the first study on merging known-weights neural-nets into a more compact model. % that handles the original tasks simultaneously.
	Because our approach compresses the networks for weight sharing and redundancy removal, it is useful %for merging different models
	for the deep-learning embedded system or edge computing in the inference stage.
	
	
	%For example, in computer vision, different models are learned for object recognition on image classification datasets (such as MNIST, CIFAR10/100) to large (such as ImageNet) and face recognition on identity-labeled face datasets (such as ...).
	%However, in an application system such as a home robot, we would like to have only a `single' model for both the tasks of object recognition and face recognition.
	
	\vspace{0.15cm}
	\noindent \textbf{Overview of Our Approach}:
	%An overview of our approach is shown in Figure~\ref{fig:concept}.
	%When merging two different feed-forward CNN models $C_A$ and $C_B$, the output is a CNN model consisting of co-compressed convolutional (\textbf{C}-Conv) and fully-connected (\textbf{C}-FC) layers.
	%We will show that the gradients of \textbf{C}-Conv and \textbf{C}-FC layers are still computable; thus the entire network remains trainable via end-to-end back-propagation.
	When merging two different CNN models $C_A$ and $C_B$, the output is a CNN model consisting of jointly encoded convolution (\textbf{E}-Conv) and fully-connected (\textbf{E}-FC) layers.
	An overview of our approach is illustrated in Fig.~\ref{fig:concept} and an example of merging three models via our approach is given in Fig.~\ref{fig:mergemore}.
	%We will show that the gradients of the \textbf{E}-Conv and \textbf{E}-FC layers are still computable; thus the entire network remains trainable via end-to-end back-propagation.
	%We can, therefore, fine-tune the merged model through the distilling principle~\cite{HintonDistilling14} by using only few calibration data from the two tasks.
	
	Contributions of this paper are summarized as follows:
	
	\noindent (1) Given well-trained CNN models, the introduced NeuralMerger can merge them for multi-tasks even the models are different. The merging process preserves the general architectures of the well-trained networks and removes their redundancy. It avoids the cumbersome-design and trial-and-error process raised by the learn-them-all approaches.
	
	\noindent (2) The proposed method produces a more compact model to handle the original tasks simultaneously. The compact model consumes less computational time and storage than the compound model of the original networks. It has a great potential to be fitted in low-end systems.
	%\item The experiments evaluate various CNN models and datasets, and the result demonstrates a satisfactory performance. Overall, the proposed method achieves xxx times speed-up and xxx times compression, while the accuracy only degrades within xxx percentage.
	
	%This paper is organized as follows.
	
	
	\section{Related Work}
	%In this section, we briefly review multi-task deep learning, network compression, and model calibration.
	%\noindent \textbf{Multi-task Deep Models}:
	To simultaneously achieve various tasks via a single neural-net model, a typical way is to increase the output nodes (for multi-tasks) of a pre-chosen neural-net structure and train it from an initialization.
	%In~\cite{levi2015age}, a joint facial age estimation and expression classification model is proposed.
	%Learn-them-all approaches have been proposed to solve multi-tasks across various domains.
	In ~\cite{DBLP:journals/corr/KaiserGSVPJU17}, MultiModel architecture is introduced to allow input data to be images, sound waves, and text of different dimensions, and then converts them into a unified representation.
	%The convolution, attention, and sparsely-gated mixture-of-experts layers are incorporated to get good performance on various problems.
	In~\cite{DBLP:journals/corr/AytarVT17}, a deep CNN leverages massive synchronized data (sound and sentences paired with images) to learn an aligned representation.
	%The aligned representation can be shared across modalities for multi-modal tasks such as cross-modal retrieval and classification.
	Nevertheless, as mentioned earlier, applying the learn-them-all approaches has to pay cumbersome training effort and intensive inference computation.
	
	%\noindent \textbf{Neural-Net Compression}:
	Compressing a neural-net is an active direction to deploy the compact model on resource-limited embedded systems.
	To reduce the representation, binary weights and bit-wise operations are used in % binarized neural networks
	\cite{hubara2016binarized} and % XNOR-networks
	\cite{rastegari2016xnor}.
	Han et al.~\cite{Han16} introduce a three-stage pipeline: pruning redundant network connections, quantizing weights with a codebook, and Huffman encoding weights and index, to reduce the storage required by CNN. % without loss of accuracy.
	%However, it induces irregular sparsity in the pruned networks and requires special libraries or hardware for speedup.
	Quantized CNN (Q-CNN)~\cite{Wu16} is proposed to address both the speed and compression issues, which splits the input layer space and applies vector quantization to each subspace.
	%For each subspace, the inner products between the input and codewords can be pre-computed and stored in a lookup table; it effectively reduces the computation and storage for the layer.
	%Therefore,
	Researchers also try to prune filters and feature maps to directly reduce the computational cost~\cite{molchanov2016pruning}\cite{li2016pruning}\cite{he2017channel}.
	%In~\cite{molchanov2016pruning}, it models the pruning problem as a filter combinatorial problem and solves through a greedy approach that removes the least important neuron from the network.
	%Similarly, ~\cite{li2016pruning} prunes the filters with small L1-norm values.
	%\noindent \textbf{Network Distilling and Model Calibration}
	Transferring the learned knowledge from a large network to a small network is another important direction towards effective network compression.
	In~\cite{HintonDistilling14}, distilling the knowledge of an ensemble model into a smaller model is introduced. %, where 3\% data is enough to train a small model in some cases.
	%The class probabilities of the ensemble model are distilled with a high temperature of the final softmax as soft targets for training.
	%Since the soft targets provide more information and less gradient variance, the small model can be trained with fewer data.
	
	%In~\cite{he2017channel}, only 1/10 iterations in fine-tune stage is enough to recover the performance of pruned models.
	
	Instead of compressing a single network, the goal of this study is to merge multiple networks simultaneously.
	%This works inspire us
	Besides, our method can restore the performance of the jointly compressed models by fine-tuning it with the training samples.
	
	
	\section{Deep Model Integration}
	Assume that model A (or B) consists of $c_A$ (or $c_B$) convolution (Conv) followed by $f_A$ (or $f_B$) fully connected (FC) layers.
	Let $c_{min}=min(c_A,c_B)$.
	In our approach, a correspondence $(c_{A(i)},c_{B(i)})$ is established between the Conv layers for the alignment of the two models, $i \in \{1:c_{min}\}$; $A(\cdot)$ is a strictly increasing mapping from $\{1:c_{min}\}$ to $\{1:c_A\}$, and $B(\cdot)$ is a strictly increasing mappings from $\{1:c_{min}\}$ to $\{1:c_B\}$.
	Likewise, a correspondence $(f_{A(i)},f_{B(i)})$ is also established between the FC layers for $i \in \{1:f_{min}\}$.
	%\subsection{Channel Alignment Permutation Optimization}
	
	In our method, the merged layers have to be of the same type (Conv or FC).
	Given two layers, one in model A and the other in model B, the principle to merge them is finding a set of (fewer) exemplar codewords that represent the weights of the layers with small quantization errors.
	%Vector quantization (or clustering) is used to find the representative codewords.
	%In our approach, to reduce the quantization errors better, product quantization (PQ) \cite{} is employed.
	The layers are thus jointly compressed for redundancy removal.
	Below, we first consider unifying the Conv layers, and then the FC layers.
	
	\subsection{Merging Convolution Layers}
	
	%Hence, merging and compression are achieved simultaneously in our approach.
	Assume that some Conv layer in model A and some other in model B are to be merged.
	The layer of model A has the input volume size $N_A \times M_A \times d_A$, where $N_A\times M_A$ is the spatial size and $d_A$ is the depth (number of channels).
	The input volume is convolved with $p_A$ convolution kernels, where the size of each kernel is $n_A \times m_A \times d_A$.
	The output of the Conv layer in model A is thus a volume of $N_A \times M_A \times p_A$ (without loss of generality, assume that padded convolutions are used.)
	
	Likewise, similar notations apply to the respective layer in model B.
	An input volume of the size $N_B \times M_B \times d_B$ are convolved by $p_B$ convolution kernels of $n_B \times m_B \times d_B$.
	The output volume of that layer is of the size $N_B \times M_B \times p_B$.
	
	We aim to jointly encode the convolution coefficients.
	As there are $p_A$ (or $p_B$) convolution kernels in the layers of A (or B), we hope to find a new set of fewer (than $p_A+p_B$) exemplars to express the original ones so that the models are fused and the redundancy between them is removed.
	To this end, a viable way is to perform vector quantization (such as k-means clustering) on the convolution kernels and find a smaller number of codewords ($p<p_A+p_B$) to jointly represent the kernels compactly.
	However, it is demanding to make this method practicable because the kernel dimensions could be inconsistent (i.e., $n_A \neq m_A$ or $n_B \neq m_B$).
	%Eg., when merging a layer consisting of $3\times 3$ kernels in VGG network~\cite{simonyan2014very} and another layer consisting of $5\times 5$ kernels in AlexNet~\cite{krizhevsky2012imagenet}, a primary difficulty would lie in the dimension mismatch. % caused by different kernel sizes.
	
	To address this issue, we unify the different convolution kernels by using spatially  $1\times 1$ convolutions,
	%We then separate the kernel along with the depth direction,
	so that merging CNNs with convolution kernels of different sizes is attainable.
	In the following, we review the operations in a convolution layer at first and then show how to separate the dimensions so that different layers are unified and jointly encoded.
	
	%However, this idea is feasible only when the dimension of kernels is consistent, $(n_A, m_A, d_A)=(n_B,m_B,d_B)$; otherwise, clustered them is demanding due to dimension mismatch.
	%Hence, a primary difficulty to jointly encoding the convolution kernels lies in the kernel size inconsistency.
	
	\subsubsection{Operations in Convolution Layer}
	The operations in a Conv layer of CNNs are reviewed as follows.
	Suppose $x\in R^{N\times M \times d}$ is the input volume (a.k.a.~3D tensor) to a Conv layer and $y\in R^{N\times M \times p}$ is the output volume.
	Assume that $p$ convolution kernels of size $n\times m \times d$ are applied to the layer, denoted as
	\begin{equation}
	\label{eq0}
	\{g^{(t)}\in R^{n\times m \times d} | t=1\cdots p\}.
	\end{equation}
	Then, the $t$-th channel output is obtained as $y_t=x \star g^{(t)}$, the volume convolution of $x$ and $g^{(t)}$,
	%    \begin{equation}
	%    \label{eq1}
	%    y_t = x\star g^{(t)},
	%    \end{equation}
	and the output $y$ is the concatenation of $y_t$,
	\begin{equation}
	\label{eq1.1}
	y =[y_{1}~y_{2}~\cdots~y_{p}].
	\end{equation}
	Let $x_u\in R^{N\times M}$ and $g_u^{(t)}\in R^{n\times m}$ respectively be the $u$-th channel of $x$ and $g^{(t)}$.  % acting as a 2D FIR filter~\cite{}.
	The volume convolution is formed by summing the 2D-convolution results of the $d$ channels:
	\begin{equation}
	\label{eq1.5}
	x\star g^{(t)}=\sum_{u=1}^{d} x_u\ast g_u^{(t)},
	\end{equation}
	where $\ast$ denotes the 2D convolution operator.
	
	In CNNs, various $n$ and $m$ (eg., $n=m=3, 5, 7 \cdots$) are used in existing networks.
	Particularly, when $n=m=1$, the volume convolution of size $1\times 1 \times d$ is often referred to as a $1\times 1$ convolution in CNNs for all $d$.
	
	\subsubsection{Kernel Decomposition in Spatial Directions}
	In the above, the volume convolution is computed as a spatially sliding operation (2D convolution) followed by a channel-wise summation along the depth direction.
	In this section, we show that, no matter what $n$ and $m$ are, it can be equivalently represented by $1\times 1$ convolutions via decomposing the kernel along with the spatial directions as follows.
	
	Given the kernel $g^{(t)}$, let $g^{(t)}_{[i_0,j_0],u}$ specify its entry at the spatial location ($i_0,j_0$) of the $u$-th channel. In particular, let $g_{[i_0,j_0]}^{(t)}\in R^d$ stand for the $1\times 1 \times d$ volume convolution at $(i_0,j_0)$; % of the kernel $g^{(t)}$;
	e.g., a kernel of spatial size $5\times 5$ consists of 25 kernels of spatial size $1\times 1$, $(i_0, j_0) \in \{1 : 5\} \times \{1 : 5\}$, $\forall d$.
	
	Following the notation, we decompose an $n\times m \times d$ volume convolution into multiple $1\times 1 \times d$ convolutions and combine them with shift operators:
	Without loss of generality, we assume that the spatial sizes $n, m$ of the kernel are odd numbers and replace them with $w=(n-1)/2$ and $h=(m-1)/2$.
	The $t$-th channel output $y_t$ can be equivalently represented as
	\begin{equation}
	\label{eq2}
	y_t =\sum_{i_0=-w}^{w}\sum_{j_0=-h}^{h} S_{-i_0, -j_0}[x \star g_{[i_0,j_0]}^{(t)}],
	\end{equation}
	where $g_{[i_0,j_0]}^{(t)}$ ($-w\leq i_0 \leq w, -h\leq j_0 \leq h$) are the $1\times 1 \times d $ convolutions depicted above, and %; in Eq.~\ref{eq2},
	$S$ is the shift operator, % satisfying that
	\begin{equation}
	S_{i_0,j_0}[x](i,j,u)= x(i-i_0,j-j_0,u), \forall i, j, u,
	\end{equation}
	with $i,j$ the spatial location and $u$ $(1\leq u \leq d)$ the channel index.
	Hence, for all $n,m$, the $t$-th channel output of the volume convolution can be decomposed as the shifted sum of $nm$ $1\times 1$ convolutions via Eq.~\ref{eq2}.
	Then, the output $y$ is obtained via the concatenation in Eq.~\ref{eq1.1}.
	
	%(Eg., $S_{-1,-1}[x]$ means moving the volume $x$ one step toward the upper and left directions spatially.)
	
	%and $g_u^{(t)}\in R^{n\times m}$ is the $u$-th channel of $g^{(t)}$ (as used in Eq.~(\ref{eq1.5})).
	
	% and the ... can be extended to the even-number case.
	
	%Regarding that the spatial sizes of the convolutions could be different.
	
	To address the issue caused by dimension mismatch in merging two convolution layers, we then propose to take the representation of $1 \times 1$ convolutions for both layers. %, which takes advantage of the above property.
	Hence, a kernel in model A is decomposed into $C_A=n_A m_A$ convolutions of size $1\times 1 \times d_A$ and that in model B is decomposed into $C_B=n_B m_B$ convolutions of size $1\times 1 \times d_B$.
	The kernel is then unified into $1 \times 1$ in the spatial domain no matter whether $n_A$ (or $m_A$) equals to $n_B$ (or $m_B$).
	%Merging CNNs with different spatial kernels is thus feasible.
	
	\subsubsection{Kernel Separation along Depth Direction}
	Then, we seek to jointly express the $C_{AB}$ $1 \times 1$ convolutions by a compact representation so that the two layers are co-compressed, where $C_{AB}=p_A C_A+p_B C_B$ and $p_A, p_B$ are the numbers of kernels of the layers in A and B, respectively.
	Though the subspace dimensions in the spatial domain are consistent $(1\times 1)$ now, they are still inconsistent in the depth direction ($d_A$ vs.~$d_B$) and thus crucial to be jointly clustered.
	To address this problem, we simply separate the $1\times 1\times d$ kernel $g_{[i_0,j_0]}^{(t)}$ into non-overlapping $1\times 1\times r$ kernels along the depth direction ($r<d$).
	As the convolution in CNNs are summation-based in the depth direction, we divide the kernel $g_{[i_0,j_0]}^{(t)}\in R^d$ into $\lceil d/r \rceil$ vectors of dimension $r$,
	\begin{equation}
	\label{eq7}
	g_{[i_0,j_0];\langle v \rangle}^{(t)}\in R^r, v=1, \cdots, \lceil d/r \rceil,
	\end{equation}
	where $g_{[i_0,j_0];\langle v \rangle}^{(t)}$ (of size $1\times 1\times r$) is the $v$-th segment of the original kernel.
	The output $y_t$ in Eq.~\ref{eq2} then becomes
	\begin{equation}
	\label{eq8}
	y_t =\sum_{v=1}^{\lceil d/r \rceil}\sum_{i_0=-w}^{w}\sum_{j_0=-h}^{h} S_{-i_0, -j_0}[x_{\langle v \rangle} \star g_{[i_0,j_0];\langle v \rangle}^{(t)}],
	\end{equation}
	where $x_{\langle v \rangle} \in R^{N\times M \times r}$ is the $v$-th %length-$r$
	sub-volume of the input $x$ for  $d=d_A$ or $d_B$.
	%By dividing the space into subspaces of an equal dimension, the kernel weights in models A and B can then be clustered in a unified space.
	Specifically, a spatially $1\times 1$ kernel is respectively segmented into $\rho_A=\lceil d_A/r \rceil$ (or $\rho_B=\lceil d_B/r \rceil$) kernels of dimension $r$ in model A (or B), where the last segment is padded with zero if necessary.
	
	Let $\rho=min(\rho_A, \rho_B)$.
	There are then $C_{AB}$ kernels of size $1\times 1\times r$ for the segment $1\leq v \leq \rho$.
	To jointly represent the kernels of both layers, we use $C$ codewords ($C<C_{AB})$ in the dim-$r$ space to encode the convolution coefficients compactly.
	We run the k-means algorithm with various initials for the $C_{AB}$ vectors and then select the results yielding the least representation error to produce the $C$ codewords (i.e., cluster centers of k-means), for $v\in \{1,\cdots, \rho\}$.
	\footnote{For those remaining segments, $\rho+1\leq v \leq max(\rho_A,\rho_B)$, we also use $C$ codewords to encode the $p_AC_A$ (or $p_BC_B$) dim-$r$ vectors in the respective subspaces if $d_A>d_B$ (or $d_A<d_B$).}
	
	%Our method follows the product quantization (PQ)~\cite{jegou2011product} principle that divides the dim-$d$ space into smaller subspaces ( dim-$r$) for clustering; the original space is the Cartesian product of the subspaces, and the total number of codewords is the product of that of the subspaces. % PQ often yields smaller quantization errors than vector quantization for the whole space.
	%Besides, all subspaces have the same dimension in our setting, which resolves the mismatch between $d_A$ and $d_B$.
	
	\begin{figure}[t]
		\begin{center}
			\includegraphics[width=0.95\columnwidth]{./figures/Fig3.pdf}
		\end{center}
		\vspace{-0.3cm}
		\caption{Illustration of merging two models' Conv layers having the kernels of spatial size $3\times3$ and $2\times 2$, respectively; each layer is divided into 2 segments. They are decomposed into spatially $1\times 1$ kernels and the kernels in every segment is clustered via k-means clustering to build a coodbook. The convolutions are pre-computed on the codebook, and a lookup table is built to index the results.}
		\label{fig:decomposition}
	\end{figure}
	
	\subsubsection{E-Conv Layer and Weights Co-use}
	\label{sec:merge}
	The merged convolution layer (called the \textbf{E}-Conv layer), is a newly-formed layer where the weights are co-used among the convolution kernels:
	Denote the $C$ codewords in the $v$-th subspace to be $\{b_{c,v}\in R^r|c=1:C\}$.
	We then replace each dim-$r$ kernel at the spatial site $(i_0,j_0)$ in the subspace $v$ (namely, $g_{[i_0,j_0];\langle v \rangle}^{(t)}\in R^r$) with $b_{\pi(i_0,j_0,v,t);v}$, the closest codeword in the dim-$r$ space, where $\pi(i_0,j_0,v,t)\in\{1:C\}$ is the code-assignment mapping.
	Eq.~\ref{eq8} is then simplified as
	\begin{equation}
	\label{eq9}
	y_t =\sum_{v=1}^{\lceil d/r \rceil}\sum_{i_0=-w}^{w}\sum_{j_0=-h}^{h} S_{-i_0, -j_0}[x_{\langle v \rangle} \star b_{\pi(i_0,j_0,v,t);v}].
	\end{equation}
	Because the number of codewords $C$ is fewer than that of the total kernel vectors $C_{AB}$, Eq.~\ref{eq9} can be executed more efficiently via computing the $1\times1$ convolutions of the $C$ codewords at first:
	\begin{equation}
	\label{eq10}
	x_{\langle v \rangle} \star b_{c,v}~~~(\forall c=1 \cdots C, v=1 \cdots \rho),
	\end{equation}
	and then storing the results in a lookup table at run-time.
	The run-time operation of $1\times1$ convolution is thus replaced by table indexing.
	Hence, the convolution kernels of the two models A and B are representationally shared in a compact codebook, $\{b_{c,v}|v=1:C\}$, and the computation time is saved.
	An illustration of the \textbf{E}-Conv layer is given in Fig.~\ref{fig:decomposition}.
	
	
	When choosing the codewords $C$ fewer, the amount of convolution coefficients is reduced to $C/C_{AB}$.
	The merged model is thus co-compressed as it consumes less storage than the total required for the two convolution layers.
	As for the computational speed, each $x_{i,j;\langle v\rangle}\in R^r$ is replaced with an index and there are $NMd/r$ entries for indexing in $x_{\langle v \rangle}$, where $i,j$ are the spatial location.
	Let $\tau_x$ and be the time unit for a table-indexing operation and $\tau_r$ be the time unit for a $1\times 1\times r$ convolution.
	The speedup ratio is then $(C\tau_r+NMd\tau_x/r)/(C_{AB}\tau_r$) in terms of the complexity.
	Hence, when the codewords $C$ is fewer or the subspace dimension $r$ is larger, the speedup is getting higher.
	
	%$r$ is usually set to be far smaller than $min(d_A,d_B)$, which compensates the inconsistency of $d_A$ and $d_B$:
	
	%The smaller is the subspace dimension $r$, the more codewords are used to encode the entire space of dim-$d$,
	
	%Merging the two layers results in a new structure of the layer, referred to as the compressed convolution layer (C-Conv layer) in this study.
	
	\subsubsection{Derivatives of the Merged Layer}
	Besides condensing and unifying the convolution operations, the \textbf{E}-Conv layer is also differentiable and so end-to-end back-propagation learning still remains realizable.
	However, evaluating the derivatives would be hard based on the table-lookup structure as the indices are not continuous.
	Hence, the table is used for the inference stage in our approach.
	While for learning, we slightly change the form of Eq.~\ref{eq9} to conduct the derivatives of $y_{i,j;t}$ (the output at the spatial location $i,j$ of channel $t$) to $\{b_{c,v}\}$ (the codewords).
	From Eq.~\ref{eq9}, % $y_{i,j;t}$ can be written as
	\begin{equation}
	\label{eq11}
	y_{i,j;t} =\sum_{v=1}^{\lceil d/r \rceil}\sum_{i_0=-w}^{w}\sum_{j_0=-h}^{h} \langle x_{i+i_0,j+j_0,\langle v \rangle}, b_{\pi(i_0,j_0,v,t);v} \rangle,
	\end{equation}
	where $\langle\cdot,\cdot\rangle$ is the inner product.
	Let $\Phi=[b_{c,v}]\in R^{r\times C}$ be the matrix whose columns are the dim-$r$ codewords of the $v$-th segment.
	Let $\beta_{i_0,j_0,v,t}\in R^C$ be the one-hot vector where the $c$-th entry in $\beta_{i_0,j_0,v,t}$ is $1$ if $c=\pi(i_0,j_0,v,t)$; otherwise the entry is $0$.
	Then, the codeword mapping $b_{\pi(i_0,j_0,v,t);v}$ in Eq.~\ref{eq9} can be replaced by $b_{\pi(i_0,j_0,v,t);v}=\Phi\beta_{i_0,j_0,v,t}$.
	Hence, the derivative of the \textbf{E}-Conv layer is conducted as
	\begin{equation}
	\label{eq12}
	\frac{\partial y_{i,j;t}}{\partial \Phi} = \sum_{v=1}^{\lceil d/r \rceil}\sum_{i_0=-w}^{w}\sum_{j_0=-h}^{h} x_{i+i_0,j+j_0,\langle v \rangle}\beta_{i_0,j_0,v,t}^T.
	\end{equation}
	As $\Phi$ is the matrix consisting of the codewords $\{b_{c,v}\}$, they can then be fine-tuned via the gradients for learning. % once the assignment $\pi{}$ is determined.    
	
	%To reflect the real-world speedup
	%
	%We report the theoretical speed-up for more consistent results, since the realistic speed-up may be affected by various factors, e.g. CPU, cache, and RAM.
	%
	%analyze the acceleration effect of adopting the OpenBLAS library.
	
	
	%Since PQ separates the original space into multiple subspaces, the convolution kernels are fused piecewisely in the space.
	%How to divide the subspaces in PQ becomes an important issue for model merging.
	%
	%As the convolution operation in CNNs are sliding-based along the spatial direction but summation-based along the depth direction, this property does not hold if the convolution kernels are decomposed along the depth direction.
	
	%More specifically, given the convolution kernel that is a volume of the dimension $n_A \times m_A \times d_A$, we decompose it into subspaces of dimension $1 \times 1 \times r$, with $1\leq r<min(d_A,d_B)$.
	%There are then totally $ \kappa_A \times \lceil d_A/r \rceil$ subspaces per convolution kernel, where $\kappa_A=n_A \times m_A$.
	%Similarly, each convolution kernel in model B is decomposed into $\kappa_B \times \lceil d_B/r \rceil$ subspaces with the dimension of each subspace $1 \times 1 \times r$ too, where $\kappa_B=n_B \times m_B$.
	
	%Jointly considering both models A and B, the subspaces are unified as the same size, $1\times 1\times r$ in this way.
	%For each subspace, there are thus totally $\kappa_A p_A + \kappa_B p_B$ vectors from the two models.
	%We then perform k-means clustering for each $r$-dimensional subspace to encode the vectors together; the number of codewords, $p$, is chosen such that $p<\kappa_A p_A + \kappa_B p_B$.
	
	
	
	
	%\subsubsection{Channel Alignment}
	
	%In our study, $r$ is chosen such that it will not cause
	%
	%Besides, uniformly dividing the depth direction into length-$r$ channels is helpful to resolve the number mismatch of input channels, $d_A$ and $d_B$, of the two models too.
	%Hence, we decompose the convolution kernels into $1\times 1\times r$ subspaces to ease the model merging process.
	%
	%In practice, $r$ is selected such that $1\leq r <<min(c_A,c_B)$.
	
	%Let $r_A=n_j\times m_j\times D_j$ and $r_B=n_l\times m_l\times D_l$.
	%The weights of a the FIR filters in model A and model B can thus be represented as $P_j$ vectors in the $r_A$-dimensional space and and $P_l$ vectors in the $r_B$-dimensional space, respectively.
	%
	%Let us denote them as $\mathbb{W}^A=\{W^A_1, \cdots  W^A_{P_j}\}$, where $W^A_k \in \mathbf{R}^{r_A}$ for $k=1 \cdots P_j$.
	%Smilarly, the filter weights of model B are denoted as $\mathbb{W}^B=\{W^B_1, \cdots  W^B_{P_l}\}$, where $W^A_k \in \mathbf{R}^{r_B}$ for $k=1 \cdots P_l$, where $r_B=n_l\times m_l\times D_l$
	%
	% and the weights of model B lie in $(n_l \times m_l\times D_l)$-dimensional space.
	%We intend to find the
	%
	%are the spatial sizes of the filters applied to the two layers.
	%They are
	
	
	\subsection{Merging Fully-connected Layers}
	The volume input to an FC layer is re-shaped to a vector in general.
	Let $x_F \in R^{N_I}$ be the input vector of an FC layer and $y_F \in R^{N_O}$ be its output.
	Then $y_F=\mathbf{W}x_F$, where $\mathbf{W} \in R^{N_O\times N_I}$ are the weights of the FC layer.
	
	Unlike Conv layers that have sliding operations, all the operations in FC are summation-based.
	Thus, given the two weight matrices of models A and B, namely, $\mathbf{W}_A$ and $\mathbf{W}_B$, we simply divide them into length-$r$ segments along the row direction.
	The dim-$r$ weight vectors in the same segment are then clustered via k-means algorithm and $C$ codewords are found.
	%Analogous to splitting along the depth direction of the Conv layer,
	In this way an \textbf{E}-FC layer is built as well, and
	It is easy to show that the \textbf{E}-FC layer is also differentiable. % like the \textbf{E}-Conv layer.
	
	\subsection{End-to-end Fine Tuning} %with Partial Data}
	As both \textbf{E}-Conv and \textbf{E}-FC layers are differentiable, once their codebooks are constructed, we can then fine-tune the entire model from partial or all training data through end-to-end back-propagation learning.
	%As our method is used in the inference stage, we do not want to re-train the merged model through all the training data.
	%Inspired by the network-distilling work \cite{HintonDistilling14}, we only use a very limited set of data to fine-tune the model.
	The training data are referred to as calibration data and the fine-tuning process is called the \emph{calibration training} in our work.
	Codebooks are also used for the weights quantization in the single-model compression approach~\cite{Wu16}.
	However, unlike our work that the codebooks in all layers are tunable in an end-to-end manner, only the codebook in a single layer can be tuned per time (with the other layers fixed) in~\cite{Wu16}, making its learning process inefficient and demanding to seek better solutions.
	Besides, our approach merges and jointly compresses multiple models, instead of only a single model.
	
	Two error terms are combined for the minimization in our calibration training.
	One is the classification (or regression) loss utilized in the original models A and B.
	The other is the layer-wise output mismatch error; % inspired by the network-distilling work \cite{HintonDistilling14}:
	when applying the input $x_I$ to the model A (or B), the output of every layer in the merged model should be close to the output of the associated layer in A (or B), and $L_1$ norm is used to measure this error.
	%In our experiments, only $1000$ random samples per class are used as the calibration data and satisfactory results can be obtained.
	%As only few calibration data are required, our approach suits for the inference stage well.
	We use a framework (TensorFlow~\cite{abadi2016tensorflow}) to implement the calibration training. % in our work.
	
	In the inference stage, to make the approach generalizable to edge devices that may not contain GPUs, we use CPU and OpenBLAS library~\cite{xianyi2012model,wang2013augem} to implement the merged model with the associated codebooks.
	To make fair comparisons, the Conv layers of the individual models compared with ours are realized via the unrolling convolution~\cite{chellapilla2006high,anwar2017structured} that converts the volume convolution into a single matrix product, which is commonly used as an efficient implementation for the Conv layer. Our code will be publicly available at GitHub\footnote{https://github.com/ivclab/NeuralMerger}.
	
	\section{Experiments}
	%In this paper, we conduct several experiments to verify our approach.
	
	In this section, we present the results of several experiments conducted to verify the effectiveness of our approach.
	
	\subsection*{Merging Sound and Image Recognition CNNs}
	
	The first experiment is to merge two CNNs of heterogeneous signal sources: image and sound.
	Although the sources are different, the same network (LeNet~\cite{LeCun98}) is used.
	It is applied to the datasets of two tasks:
	
	\noindent \textbf{Sound20}\footnote{https://github.com/ivclab/Sound20}: This dataset contains 20 classes of sounds recorded from animal and instrument with 16636 training samples and 3727 testing samples. It is constructed using Animal Sound Data~\cite{keoghmonitoring} and Instrument Data~\cite{Juliani16}.
	The raw signals are converted to 2D spectrograms and then resized to $32\times32$ images for classification. %~\footnote{Note that unlike the Sound dataset used in~\cite{chou2018CVPRW}, the Sound20 dataset adopted in this study is larger and more suitable for evaluation.}
	
	%\noindent \textbf{Sound dataset}~\cite{Juliani16} contains 13 classes of sounds recorded from the drum kit and guitar.
	%The raw signals are converted to 2D spectrograms and then resized to $32\times32$ images for classification.
	%It has more than 480 examples per class, split into 70\% training, 20\% validation and 10\% testing sets.
	
	\noindent \textbf{Fashion-MNIST dataset}~\cite{xiao2017/online}: This dataset contains a collection of 60,000 training and 10,000 testing images of size $28\times28$ in 10 classes of dressing styles.
	
	\begin{table}[t]
		\centering
		\small
		\caption{The parameters of $r/C$ for 'ACCU' and 'LIGHT' settings in merging two LeNet models (while the classification layers are not co-compressed).}
		\label{Experimentone_param}
		\begin{tabular}{lccc}
			\toprule
			Para.       & Conv1       & Conv2      & Fc1           \\ \hline
			ACCU        & 1/64        & 8/128      & 8/128       \\
			LIGHT       & 1/64        & 32/128     & 8/64      \\
			\bottomrule
		\end{tabular}
	\end{table}
	
	
	\begin{table}[t]
		\centering
		\small
		\caption{Compression ratio, Speedup, and Accuracy drop of sound and image merged model. The speedup ratio is tested on the CPU of Intel(R) Xeon(R) CPU E5-2640 v4, in the single-thread model. %, where ACCU is the setting of $r/C=8/128$ for the 2nd Conv and 1st FC layers, and LIGHT is that of $r/C=32/128$ for both the 2nd Conv and $8/64$ for the 1st FC layers of LeNet, respectively; the 1-st Conv layer is $1/64$ for both, while the classification layers are not co-compressed.
		}
		\label{LeNetMerge}
		%\vspace{-0.25cm}
		\begin{tabular}{lcccc}
			\toprule
			Para.   & Compr.      & Speedup                 & Sound $\downarrow$            & Image $\downarrow$  \\ \hline
			ACCU       & 10.4$\times$   & 1.3$\times$     & -0.06\%                   & 0.68\%      \\
			LIGHT       & 15.3$\times$   & 1.8$\times$    & 0.76\%                    & 1.40\%     \\
			\bottomrule
		\end{tabular}
	\end{table}
	
	\begin{comment}
	\begin{table}[t]
	\centering
	\small
	\caption{Compression ratio, Speedup, and Accuracy drop of sound and image merged model. The speedup ratio is tested on both Intel and ARM device. One is the CPU of Intel(R) Xeon(R) CPU E5-2640 v4, and the other is CPU of NVIDIA Jetson TX1. ``$a/b$" in the Speedup field denotes the speedup ratio using different CPUs.%, where ACCU is the setting of $r/C=8/128$ for the 2nd Conv and 1st FC layers, and LIGHT is that of $r/C=32/128$ for both the 2nd Conv and $8/64$ for the 1st FC layers of LeNet, respectively; the 1-st Conv layer is $1/64$ for both, while the classification layers are not co-compressed.
	}
	\label{LeNetMerge}
	%\vspace{-0.25cm}
	\begin{tabular}{lcccc}
	\toprule
	Para.   & Compr.      & Speedup                 & Sound $\downarrow$            & Image $\downarrow$  \\ \hline
	ACCU       & 10.4 $\times$   & 1.3$/$4.2 $\times$     & -0.06\%                   & 0.68\%      \\
	LIGHT       & 15.3 $\times$   & 1.8$/$6.2 $\times$    & 0.76\%                    & 1.40\%     \\
	\bottomrule
	\end{tabular}
	\end{table}
	\end{comment}
	
	LeNet consists of 2 Conv layers with 32 and 64 kernels of size $5 \times 5 \times 1$ and $5 \times 5 \times 32$, respectively, each followed by a $2\times 2$ max-pooling, and then an FC layer of 1024 units and a $\gamma$-way output ($\gamma$ is the number of classes).
	Before merged, LeNet can achieve 78.08\% and 91.57\% accuracy on the above sound (Sound20) and image recognition (Fashion-MNIST) tasks, respectively.
	The models are simply aligned and merged layer by layer since they are identical.
	
	There are two main parameters, $r$ and $C$, in our method, where $r$ is the length of the segment and $C$ is the number of codewords.
	As the input layer's depth of LeNet is 1, we set $r=1$ for the first Conv layer of both models and choose $C=64$ for it.
	Then, we alter the parameters of $r \in \{8, 16, 32\}$ and $C \in \{64, 128, 256\}$ for the remaining Conv- and FC-layers (except for the classification layer) and find the parameters per layer according to the performance when merging this layer only.
	By doing so, we conducted two settings, namely `ACCU' and `LIGHT', as shown in Table.~\ref{Experimentone_param}, where `ACCU' enforces the accuracy and `LIGHT' enforces the model size and inference speed, respectively.
	%Since the speedup ratio may vary under different hardware, 
	We test the inference speed of our model on %two CPUs (in the single-thread mode), 
	the Intel Xeon CPU E5-2640 v4 (single thread). % and the CPU of NVIDIA Jetson TX1.
	The C++ codes from~\cite{Wu16}, which adopts OpenBLAS library and realizes unrolling convolution based on Caffe's~\cite{jia2014caffe} CPU-mode implementation, is used and enhanced to realize the inference stage of our work.
	
	The overall performance (after calibration training) and accuracy drop (denoted by $\downarrow$) are shown in Table~\ref{LeNetMerge}.
	With the setting of `ACCU', over 10 times of the compression ratio can be obtained on the joint model size, while only a negligible accuracy drop is imposed.
	%and 1.3 times speedup per task are achieved
	As can be seen, for the image task, the accuracy of our merged model drops only $0.68\%$; for the sound task, the merged model even achieves a better accuracy than the original model ($-0.06\%$ accuracy drop).
	Under the `LIGHT' setting, our approach can achieve over 15 times of joint model-size compression
	%and 1.8 times speedup
	with less than 1.1\% accuracy drop on average.
	Fig.~\ref{fig:CalibrationDataRatio} further shows the influence calibration data amount.
	It can be seen that no matter for the `ACCU' or `LIGHT' setting, more training data yield a lower accuracy drop in overall.
	
	%When using only $20\%$ data for the calibration training, both the sound and image recognition tasks have only a limited accuracy drop (of about $1.5\%$) on the `ACCU' setting.
	
	% \begin{table}[ht]
	% \centering
	% \caption{Increased Error Rate and Speed-Up of LeNet Models under Different Layer CONV2 Settings}
	% \label{LeNet_Conv2_Settings}
	% \begin{tabular}{ccccc}
	% \toprule
	% Para.  &Sound $\uparrow$        & Image $\uparrow$       & Avg. Err. $\uparrow$ & Speed-Up \\ \hline
	% 32/128 & 5.38\%                 & 6.54\%                 & 5.96\%               & 4.01 $\times$    \\
	% 16/128 & 3.32\%                 & 4.39\%                 & 3.86\%               & 2.69 $\times$    \\
	% 8/128  & 1.58\%                 & 2.03\%                 & 1.81\%               & 1.61 $\times$    \\
	% 4/128  & 0.95\%                 & 0.67\%                 & 0.81\%               & 0.89 $\times$    \\ \hline
	% 8/32   & 7.59\%                 & 6.39\%                 & 6.99\%               & xxx  $\times$     \\
	% 8/64   & 4.43\%                 & 3.39\%                 & 3.91\%               & 1.78 $\times$    \\
	% 8/128  & 1.58\%                 & 2.03\%                 & 1.81\%               & 1.61 $\times$    \\
	% 8/256  & 0.63\%                 & 1.88\%                 & 1.26\%               & xxx  $\times$     \\
	% \bottomrule
	% \end{tabular}
	% \end{table}
	
	%\begin{table}[t]
	%    \centering
	%    \caption{Performance of (a) varying the parameters of the 2nd Conv layer with the FC layers fixed and (b) varying the parameters of the 1st FC layer with the Conv layers fixed in the LeNet models. Note that the compression and speedup ratios are respectively evaluated for the Conv and FC layers only. }
	%    \vspace{-0.25cm}
	%    \label{LeNet_Settings}
	%    \small
	%    \begin{tabular}{cccc}
	%        \multicolumn{4}{c}{(a) Convolution Layer Settings}                                           \\
	%        \toprule
	%        $r$/$C$     & Compression             & Speedup                  & Average Error $\uparrow$     \\ \hline
	%        32/128      & \textbf{20.41 $\times$} & \textbf{4.01 $\times$}   & 5.96\%                       \\
	%        16/128      & 17.60 $\times$          & 2.69 $\times$            & 3.86\%                       \\
	%        8/128       & 13.81 $\times$          & 1.61 $\times$            & \textbf{1.81\%}              \\
	%        \toprule
	%        \multicolumn{4}{c}{(b) FC Layer Settings}                                                      \\
	%        \toprule
	%        $r$/$C$  & Compression               & Speedup                  & Average Error $\uparrow$    \\ \hline
	%        8/64     & \textbf{15.94 $\times$ }  & \textbf{1.80 $\times$}   & 0.47\%                  \\
	%        8/128    & 10.64 $\times$            & 1.71 $\times$            & \textbf{0.19\%}         \\
	%        8/256    &  6.39 $\times$            & 1.52 $\times$            & 0.43\%                  \\
	%        \toprule
	%    \end{tabular}
	%\end{table}
	
	%\begin{table}[pt]
	%    \small
	%    \centering
	%    \caption{Increased Error Rate of Merge and Single Models}
	%    \label{LeNetMergeOrSingle}
	%    \vspace{-0.25cm}
	%    \begin{tabular}{lcccc}
	%        \toprule
	%        Para.           & Merge           & Image $\uparrow$ & Sound $\uparrow$ & avg.~Err. $\uparrow$       \\ \hline
	%        Accu.           & \checkmark      & 2.06\%           & 0.44\%           & \textbf{1.25\%}  \\
	%        &                 & 1.81\%           & N/A              & 1.81\%           \\ \hline
	%        Light           & \checkmark      & 4.73\%           & 1.54\%           & \textbf{3.14\%}  \\
	%        &                 & 4.84\%           & N/A              & 4.84\%           \\
	%        \bottomrule
	%    \end{tabular}
	%\end{table}
	\begin{figure}[t]
		\centering
		\includegraphics[width=2.5in]{./figures/Sound.pdf}
		\includegraphics[width=2.5in]{./figures/Image.pdf}
		\caption{Accuracy drop vs. data ratio in calibration stage}	\label{fig:CalibrationDataRatio}
	\end{figure}
	
	\begin{table}[pt]
		\small
		\centering
		\caption{Comparison of the merged model (ACCU) with the individual sound and image models compressed via our approach. Each individual model and the merged model use the same parameters of $r/C$ and have identical model size and execution speed, but more tasks can be done in the merged model.}
		\label{LeNetMergeOrSingle}
		%\vspace{-0.25cm}
		\begin{tabular}{lcccc}
			\toprule
			Parameter            & Image $\downarrow$            & Sound $\downarrow$       & Average Accuracy $\downarrow$      \\ \hline
			ACCU                &  0.68\%           & -0.06\%     & 0.31\%    \\ \hline
			Sound only           & N/A              & 0.40\%      & N/A                \\     
			Image only           & 0.51\%           & N/A         & N/A                \\
			\bottomrule
		\end{tabular}
	\end{table}
	
	% \begin{table}[t]
	% \centering
	% \caption{Increased Error Rate and Speed-Up of LeNet Models under Different Layer FC1 Settings}
	% \label{LeNet_FC1_Settings}
	% \begin{tabular}{ccccc}
	% \toprule
	% Para.  & Sound $\uparrow$      & Image $\uparrow$       & Avg. Err. $\uparrow$ & Compress.         \\ \hline
	% 32/128 & 1.58\%                & 2.21\%                 & 1.90\%               & 14.18 $\times$    \\
	% 16/128 & 0.95\%                & 1.45\%                 & 1.20\%               & 12.80 $\times$    \\
	% 8/128  & -0.16\%               & 0.54\%                 & 0.19\%               & 10.67 $\times$    \\
	% 4/128  & -0.32\%               & 0.16\%                 & -0.08\%              & 8.00  $\times$    \\ \hline
	% 8/32   & 1.58\%                & 1.30\%                 & 1.44\%               & 21.34 $\times$    \\
	% 8/64   & 0.32\%                & 0.62\%                 & 0.47\%               & 16.00 $\times$    \\
	% 8/128  & -0.16\%               & 0.54\%                 & 0.19\%               & 10.67 $\times$    \\
	% 8/256  & 0.47\%                & 0.38\%                 & 0.43\%               &  6.40 $\times$    \\
	% \bottomrule
	% \end{tabular}
	% \end{table}
	% \begin{table}[t]
	% \centering
	% \caption{Increased Error Rate and Speed-Up of LeNet Models under Different Layer FC1 Settings}
	% \label{LeNet_FC1_Settings}
	% \begin{tabular}{cccc}
	% \toprule
	% Para.  & Avg. Err. $\uparrow$ & Speed-Up                & Compress.         \\ \hline
	% 8/64   & 0.47\%               & \textbf{1.80 $\times$}  & \textbf{16.00 $\times$ }   \\
	% 8/128  & \textbf{0.19\%}      & 1.71 $\times$           & 10.67 $\times$    \\
	% 8/256  & 0.43\%               & 1.52 $\times$           &  6.40 $\times$    \\
	% \bottomrule
	% \end{tabular}
	% \end{table}
	
	%In the following, we detail the parameter settings of different layers for performance analysis.
	%First, we fix the FC weights and jointly encode only the Conv layers.
	%With the 1st Conv layer's $r/C$ chosen as above, the 2nd layer's are varying and three settings (under $C=128$) are selected to be shown in Table~\ref{LeNet_Settings}(a) after calibration training (with the FC weights fixed).
	%As expected, the speedup is increased with $r$ because the table indexing time is reduced; the accuracy is higher when $r$ is lower because a finer subspace is divided. % by PQ.
	%The best $r/C$, $8/128$ for the speedup and $32/128$ for the compression, are chosen as the Conv-layer settings of `Accu' and `Light' in Table~\ref{LeNetMerge} for the performance report, respectively.
	%Likewise, we also vary the parameters of the 1st FC layer with the Conv coefficients fixed and select $r/C=8/128$ and $8/64$ respectively for the `Accu' and `Light' settings of the FC layer.
	%Note that the speedup and compression in this table are evaluated based on the Conv or FC layers only; thus they cannot be compared with that in Table~\ref{LeNetMerge} directly.
	
	%In the following, we detail the influence of the number of data used in calibration stage.
	%In calibration stage, the number of training data is closely related to the performance.
	%Researchers would like to use less data as possible to boost their training process while maintaining high performance.
	%To evaluate our merged model with various data number, we use different data ratio(from 0.1 to 1.0) in calibration stage.
	%The result is reported in Fig.~\ref{fig:mergemore} It can be seen that our merged model can achieve great result even when using 10\% of all training data.
	%As data number increases, the performance increases gradually.
	
	As our approach finds several codewords per layer and then performs end-to-end calibration training to refine the codewords, it is also applicable to an individual model.
	One may wonder how the individual models perform when they are compressed via our approach.
	For an even comparison, we use the same $r/C$ settings to encode the single sound (or image) model and also fine-tune it using all training data.
	Hence, the compressed single model consumes the same resource as our merged one (i.e.,~they have exactly the same model size and execution time).
	As shown in Table~\ref{LeNetMergeOrSingle}, when the sound-only model is compressed, the accuracy is slightly dropped by 0.4\%; that is, our merged model can achieve even better accuracy while an additional functionality (image recognition) is added.
	On the other hand, when the image-only model is compressed, the accuracy-drop only changes little (from 0.51\% to 0.68\%) but an extra functionality (sound recognition) is supplemented.
	We owe that the two models have co-redundancy in between, and thus jointly encoding the models can take advantage of the additional inter-redundancy to achieve a better representation.
	
	
	
	%Likewise, Table~\ref{LeNet_Settings}(b) shows the performance when fixing the Conv layers under several settings ($r=8$) of the 1st FC layer.
	%As can be seen, the accuracy drops smaller than that in (a).
	%This is probably because the Conv layers hold more critical features than the FC layer for classification.
	%The compression ratio is getting better when $C$ is decreased, reflecting that fewer codewords imply a stronger compression.
	%However, the speedup and accuracy drop is not monotonic with $C$.
	%We owe this to the reason that, as the results are obtained after fine-tuning with calibration data, which outpasses the influence of representation error.
	
	%To verify the performance drop and speed up using different merging parameters, we examine the settings of convolutional layers with fully-connected layers fixed at first. Then we adjust the fully connected layer settings to check the performance and compression rate. The details are shown in the table \ref{LeNet_Settings}. It shows the greater the $r$, the higher the speed in convolutional layers. Normally, the more codewords we choose, the less the quantization error is. Nevertheless, it is worthy to mention that, 128 codewords is enough for the fully connected layer in LeNet architecture.
	%
	%To show the effectiveness of the merging, we also compare our merged model with single models. Since different parameter settings result in different performance, we choose two different settings namely, Accu., and Light, for high accuracy and high compression rate, respectively. Under same settings, the performance of single models is less than 0.25\% than our merge models. Nevertheless, merge models can provide extra function, in this case, the sound classifier. The performance of the extra function is good enough since only less than 1.54\% error increased. The details are shown in the Table \ref{LeNetMergeOrSingle}.
	
	% \begin{table}[ht]
	% \centering
	% \caption{Increased Error Rate, Speed-Up, and Compression Rate of Merge and Single Models}
	% \label{LeNetMerge}
	% \begin{tabular}{lcccc}
	% \toprule
	% Para.             & Image $\uparrow$ & Sound $\uparrow$ & Speed            & Compr.            \\ \hline
	% Merge$_{acc}$     & 2.06\%           & 0.44\%           & 1.31 $\times$    & 10.38 $\times$    \\
	% Single$_{acc}$    & 1.81\%           & N/A              & 1.31 $\times$    & 6.32 $\times$     \\
	% Merge$_{light}$   & 4.73\%           & 1.54\%           & xxx  $\times$    & 15.33 $\times$    \\
	% Single$_{light}$  & 4.84\%           & N/A              & xxx  $\times$    & 10.40 $\times$    \\
	
	% \bottomrule
	% \end{tabular}
	% \end{table}
	
	%The Table \ref{LeNetMerge} shows the overall speed-up and compression rate of our merged models under accurate or light settings. 15.34 times compression rate can be achieved while only 3.14\% average error rate increased for sound and image classification. With accurate settings, the model can achieve only 1.25\% error rate increased while providing 10.38 times model compression.
	\begin{table}[t]
		\centering
		\small
		\caption{The parameters of $r/C$ for 'ACCU' and 'LIGHT' settings in the clothing and gender merged model. Each group is a stack of two or three 3 $\times$ 3 Conv layers defined in VGG-16 model.  }
		\label{Experimenttwo_param}
		\begin{tabular}{lccccc}
			\toprule
			Para.       & Group1,2    & Group3   & Group 4,5  & Fc1    & Fc2             \\ \hline
			ACCU        & 32/256      & 16/256   & 8/256      & 4/128  & 4/64          \\
			LIGHT       & 32/128      & 32/128   & 16/128     & 8/128  & 8/64    \\
			\bottomrule
		\end{tabular}
	\end{table}
	
	
	\begin{table}[t]
		\centering
		\small
		\caption{Overall performance of the merged model for image classification of clothing and gender.}
		\label{TwoMerge}
		\begin{tabular}{lccccc}
			\toprule
			\multirow{2}{*}{Para.} & \multirow{2}{*}{Compr.} & \multicolumn{2}{c}{Clothing} & \multicolumn{2}{c}{Gender} \\
			&                              & Speedup      & Acc. $\downarrow$      & Speedup     & Acc. $\downarrow$     \\ \hline
			ACCU                      & 12.1$\times$                        & 1.5$\times$         & -0.83\%         & 1.8$\times$        & 0.27\%
			\\
			LIGHT                      & 20.1$\times$                        & 2.8$\times$         & 0.50\%         & 2.5$\times$         & 1.58\%          \\
			
			\bottomrule
		\end{tabular}
	\end{table}
	
	
	\begin{table}[pt]
		\small
		\centering
		\caption{Comparison of the merged model (ACCU setting) with the individual sound and image models compressed via our approach. Each individual model and the merged model use the same parameters of $r/C$ and have identical model size and execution speed, but more tasks can be done in the merged model.}
		\label{Ex2MergeOrSingle}
		%\vspace{-0.25cm}
		\begin{tabular}{lcccc}
			\toprule
			Parameter            & Clothing  & Gender  & Average Accuracy $\downarrow$    \\ \hline
			ACCU                &  -0.83\%           & 0.27\%     & -0.28\%             \\ \hline
			Clothing only           & N/A              & 0.53\%         & N/A                          \\     
			Gender only           & -0.73\%           & N/A              & N/A                          \\
			\bottomrule
		\end{tabular}
	\end{table}
	%The second experiment is to merge two large CNNs model of the same signal source but structurally different network: ZF-Net~\cite{ZFNet} and VGG~\cite{simonyan2014very}. It is applied to datasets of two tasks:
	%
	%\noindent \textbf{Adience Dataset}~\cite{eidinger2014age} contains age and gender classes of face pictures. We utilize the 11,136 training and 3,000 testing images, which resized to $227 \times 227$ by the in-plane aligned for gender classification.
	%
	%\noindent \textbf{Multi-View Clothing Dataset}~\cite{DBLP:conf/mir/LiuCC16} composed of pictures with four different views of clothes.We utilize the 37.485 training and 3,000 testing images of size $224 \times 224$ in $17$ attributes of second-layer clothing styles for clothing classification, as defined in~\cite{DBLP:conf/mir/LiuCC16}.
	%
	%In VGG network structure, we change the max-pooling layer, which is located between final convolution layer and first fully connected layer, into the average-pooling layer. Before merged, ZF can achieve 83.4\% on the gender classification task and VGG can achieve 87.4\% on the clothing classification task. Since the number of convolution layers in VGG and ZF are inconsistent, we align their layer from low level to low level, high level to high level as shown in Figure 2. Each convolution layers in ZF is allocated to one group of VGG convolution layers. On the other hand, the numbers of fully-connected layers in two model are identical, we simply align them layer by layer.
	
	%Following the approach developed in experiment one, a finer subspace is applied on 'Accu.' setting, while larger subspace on 'Light' setting. For further considering the parameter size vary in different layers, we use more codewords on layers with a larger size. The final overall performance is shown in Table 4. As can be seen, under 'Accu.' setting, the compression ratio is over 12 times and the speedup exceeds x times and y times in two models, while only 0.42\% average error rate increased. Compared to experiment one, this result obtains higher compression ratio, speedup, and accuracy, which give credence to the idea that larger models have more redundancy can be removed and weights can be co-used, which emphasize the importance of our approach.
	
	%Likewise, we compare our merged model (Accu.) to individual model. When the clothing-only model is encoded via our approach, the increased error rate is 0.67\%. Our merged model can achieve approximately the same performance (0.69\%) while an additional functionality is added.
	
	
	\subsection*{Merging Clothing and Gender CNN Classifiers}
	In the second experiment, we merge two image classifiers (one for clothing and the other for gender recognition) into a single model holding double functionalities.
	The clothing classifier adopted is VGG-avg~\cite{yang2018supervised} which substitutes the first FC layer in the VGG-16 model~\cite{simonyan2014very} with average-pooling; it has been shown that VGG-avg achieves similar performance to VGG-16 while consuming far less storage.
	The gender classifier adopted is ZF-Net~\cite{ZFNet} that has fewer layers than VGG-avg.
	The overall structures and alignment between them can be found in Fig.~\ref{fig:mergemore}.
	
	The clothing and gender datasets employed are depicted as follows.
	The Adience Dataset~\cite{eidinger2014age} contains face pictures of different genders and ages, where 11,136 images (each resized to $227 \times 227$) are used for training and the other 3,000 are for testing.
	The Multi-View-Clothing dataset~\cite{DBLP:conf/mir/LiuCC16} contains clothing images of different views with over 250 attributes organized in a tree, where the frontal images (37,485 for training and 3,000 for testing) and the 13 attributes of the first two layers are used to construct a classifier of 13 classes.
	Before merged, ZF-Net and VGG-avg achieve 83.43\% and 89.80\% accuracies on the gender and clothing classification tasks, respectively.
	
	Because the kernel sizes in the two models are not always consistent, they are merged via the decomposition into $1\times 1 \times r$ convolutions as described in Section~\ref{sec:merge}.
	Similar to the process in the first experiment, we choose the parameters $r/C$ and conduct two settings `ACCU' and `LIGHT', where the former stresses accuracy and the latter emphasizes compression/speed. The detailed parameter settings are reported in Table~\ref{Experimenttwo_param}, and the overall performance is reported in Table~\ref{TwoMerge}.
	As the model sizes of both VGG-avg and ZF-Net are larger than that of LeNet, there are more redundancies between the models.
	Hence, an even better performance is achieved than that of experiment 1.
	In the `ACCU' setting, the compression ratio exceeds 12 times with even an increase of the average accuracy of the two tasks (negative average accuracy drop).
	The compression ratio is enhanced to more than 20 times with acceptable accuracy drops on the `LIGHT' setting.
	
	Besides, Table~\ref{Ex2MergeOrSingle} shows the results when compressing the individual models by using our approach under the same setting.
	It can be seen that similar (or even slightly better) accuracies can be obtained, with an additional merit that the merged model holds an extra functionality than the individual models.
	The results reveal that our approach can exploit both the intra- the inter-models redundancy and demonstrate satisfactory performance on merging deep learning models.
	
	
	% \begin{table}[t]
	%         \centering
	%         \scriptsize
	%         \caption{Overall performance of the merged model for image classification of clothing and gender.}
	%         \label{TwoMerge}
	%         \begin{tabular}{lccccc}
	%             \toprule
	%             \multirow{2}{*}{Para.} & \multirow{2}{*}{Compres.} & \multicolumn{2}{c}{Clothing} & \multicolumn{2}{c}{Gender} \\
	%             &                              & Speedup      & Acc. $\downarrow$      & Speedup     & Acc. $\downarrow$     \\ \hline
	%             ACCU                      & 12.10$\times$                        & 1.52/1.52$\times$         & -0.83\%         & 1.82/1.52$\times$        & 0.27\%
	%              \\
	%             LIGHT                      & 20.10$\times$                        & 2.83/1.52$\times$         & 0.50\%         & 2.51/1.52$\times$         & 1.58\%          \\
	
	%             \bottomrule
	%         \end{tabular}
	% \end{table}
	
	%\begin{table}[t]
	%    \centering
	%    \small
	%    \caption{Overall Performance of the merged model for image classification of clothing and gender. %where Accu.~is the setting of $r/C=8/128$ for $1\sim2$ Conv groups, $r/C=8/256$ for the $3\sim5$ Conv groups, $r/C=4/128$ for 1st FC layers, $r/C=4/64$ for 2nd FC layers, and Light is that of $r/C=32/128$ for $1\sim2$ Conv groups, $r/C=32/256$ for the $3\sim5$ Conv groups, $r/C=8/128$ for 1st FC layers, $r/C=8/64$ for 2nd FC layers, respectively; the classification layers are not co-compressed.
	%    }
	%    \label{TwoMerge}
	%    \vspace{-0.25cm}
	%    \begin{tabular}{lc|cc|c}
	%        \toprule
	%        &           &\multicolumn{2}{c|}{Speedup} & \\
	%        Parameter       & Compression  & Clothing  & Gender  & Avg.~Err. $\uparrow$\\
	%        \hline
	%        Accu       & 12.03 $\times$  & 1.20 $\times$ & 1.61 $\times$     & 0.42\%  \\
	%        Light       & 18.71 $\times$  & 3.17 $\times$ & 2.70 $\times$     & 3.95\%  \\
	%        \toprule
	%    \end{tabular}
	%\end{table}
	
	
	%\subsubsection{Adience Dataset}
	%The Adience dataset~\cite{eidinger2014age} composed of pictures taken by a camera from smartphone or tablets.
	%The images of Adience dataset capture extreme variations, including extreme blur (low-resolution), occlusions, out-of-plane pose variations, expressions.
	%The entire Adience dataset includes 26,580 unconstrained images of 2,284 subjects. Unlike other datasets (such as Morph II) where the face images are taken in a controlled environment, the Adience dataset is a challenging \emph{in-the-wild} benchmark for age and gender estimation. When training ZF-Net \cite{ZFNet} model using Adience dataset for gender classification, we use a standard five-fold cross-validation and utilize the in-plane aligned version of the faces as defined in~\cite{eidinger2014age}.
	%
	%\subsubsection{Multi-View Clothing Dataset}
	%The Multi-view clothing dataset~\cite{DBLP:conf/mir/LiuCC16} composed of pictures with four different views (front, back, left, and right views) of clothes. The entire Multi-view clothing dataset includes 161,638 clothing images of 37,499 items, which most items have at least four views. Most of the clothing image resolution is $1920 \times 2240$ pixels, which can offer sufficient details for clothing classification. The entire items are divided into $264$ ground truth attributes, which are organized into a three-layer hierarchy. In this experiment, we only utilize the second layer for clothing classification. The second layer have $17$ attributes, as defined in ~\cite{DBLP:conf/mir/LiuCC16}.
	%
	%\subsubsection{ZF-Net Network Training and Testing}
	%In the initialization of training strategy, the weights in convolution layers and final fully connected layer are initialized with random values from a zero-mean Gaussian with a standard deviation of $0.01$ and in first and second fully connected layer are initialized with random values from a zero-mean Gaussian with the standard deviation of $0.005$. After initializing all weights, we utilize the standard ZF-Net network structure\cite{ZFNet} without pre-trained model to train in the first-fold of Adience dataset, including 11,136 training images and 3,879 testing images. In order to further limit the risk of overfitting, we apply dropout layer with a dropout ratio of $0.5$ in first and second fully connected layers and utilize data augmentation by taking a random crop of $227 \times 227$ pixels from the $256 \times 256$ input image and randomly mirror it in each forward-backward training pass. The training itself is performed using stochastic gradient descent with image batch size of $32$ and the initial learning rate is $e^{-3}$, reduced to $e^{-6}$ after 16K iterations.
	%
	%In the prediction of testing strategy, we apply over-sampling method to extract five $227 \times 227$ regions, four from the corners of the $256 \times 256$ face image, and an additional crop region from the center of the face and its final prediction is taken to be the average prediction value across these five regions. After deciding the method of prediction, we evaluate the accuracy of ZF-Net model in the testing images of the first-fold for gender classification. Finally, the accuracy of ZF-Net model is $0.834$.
	%
	%\subsubsection{VGG-Avg Network Training and Testing}
	%In this experiment, we choose a popular deep CNN model, 16-layer VGG~\cite{simonyan2014very}, for clothing classification. In the 16-layer VGG network structure, we change the max-pooling layer, which is located between final convolution layer and first fully connected layer, into the average-pooling layer. In the training strategy,we utilize the multi-label 16-layer VGG model generated by ~\cite{DBLP:conf/mir/LiuCC16} to fine-tuned on the multi-view clothing dataset, which we only use $17$ attributes for single-label clothing classification. Fine-tuning itself is performed using stochastic gradient descent with image batch size of $35$ and the initial learning rate is $e^{-5}$, reduced to $5 \times e^{-6}$ after 20 epochs.
	%
	%\subsubsection{Analysis Layer Quantization}
	%To begin with, the number of convolution layers in VGG and ZF model are inconsistent, we align their layer from low level to low level, high level to high level as shown in Figure 3. Each convolution layers in ZF model is allocated to one group of VGG convolution layers. For further investigating the influence between convolution groups, we separately quantize different group of layers without fine-tuning and study their effects on accuracy. As presented in Figure 2, the result indicate that deeper convolution layers require more codewords to maintain high performance, which show good agreement with their parameter size.
	%
	%In fully-connected layers, we align VGG and ZF models according to layer orders and investigate the effect of quantization in each layer. From Figure 2, it appears that quantizing fully-connected layers will not significantly increased the error rate, while convolution layers are more sensitive to output performance.
	%
	%\subsubsection{Quantizing the Whole Network}
	%From the present results obtained, we select two parameter setting as high performance model and light weight model in table X.
	
	%\begin{figure}
	%    \begin{center}
	%        \includegraphics[scale=0.28]{./figures/Experiment2_final.pdf}
	%    \end{center}
	%    \caption{this figure shows that the average increased error rate of Conv group and FC layer}
	%    \label{fig:exp2}
	%\end{figure}
	
	%\begin{figure}
	%    \begin{center}
	%        \includegraphics[scale=0.35]{./figures/Fig2.pdf}
	%    \end{center}
	%    \caption{this figure shows how to merge Lenet, ZFnet and VGG together}
	%    \label{fig:exp3_graph}
	%\end{figure}
	
	\subsection*{Merging More Models}
	Finally, we show that our method can merge more models incrementally.
	The clothing-gender model in the above is added with another one, LeNet, for sound recognition (Fig.~\ref{fig:mergemore}).
	In Table~\ref{MoreMerge}, the accuracy-drop changes are negligibly small %only a negligible error rate change %increment is imposed
	but one more usage is added, which further confirms that our method can employ the redundancy among the models to enforce an efficient representation.
	
	%we further merge the Sound model into our previous Clothing and Gender classification model (Accu.). This result further confirms that our NeuralMerger can incrementally merge models even with different structure and signal source, and exploit the redundancy and co-used weight among different models.
	
	
	\section{Conclusions}
	In this paper, we present a method that can unify multiple feed-forward models into a single but more compact one.
	To our knowledge, this is the first study on merging well-trained deep models for the inference stage. In our method, overall architectures of the original models are preserved.
	The unified model is still differentiable and can be fine-tuned to restore or enhance the performance. Experimental results show that the merged model can be extensively compressed under very limited accuracy drops, which makes our method potentially useful for deep model inference on low-end devices.
	
	%Our approach finds a set of compact codewords among the layers aligned in advance, and jointly compresses their weights to form a merged model.
	%In our method, overall architectures of the original models are preserved, and the merged model may serve as a base model for multi-task learning.
	%The performance could be further boosted when all training data (instead of only few calibration data) are used, which remains our future study.
	
	
	\section*{Acknowledgement}
	This work was supported in part by the MOST under the grant MOST 107-2634-F-001-004.
	% \subsubsection*{Acknowledgments}
	
	
	\begin{table}[t]
		\centering
		\small
		\caption{Accuracy Drop of merging three models(ACCU parameter)}
		\label{MoreMerge}
		%\vspace{-0.25cm}
		\begin{tabular}{lcccc}
			\toprule
			Merged model             & Clothing $\downarrow$ & Gender $\downarrow$  & Sound $\downarrow$                \\ \hline
			Clothing + Gender     & -0.83\%    & 0.27\%   & N/A          \\
			Clothing + Gender + Sound    & -0.38\%    & 1.11\%   & 0.25\%     \\
			\bottomrule
		\end{tabular}
	\end{table}    
	
	
	% Use unnumbered third level headings for the acknowledgments. All
	% acknowledgments, including those to funding agencies, go at the end of the paper.
	
	%\bibliography{iclr2018_conference}
	%\bibliographystyle{iclr2018_conference}
	
	%% The file named.bst is a bibliography style file for BibTeX 0.99c
	\bibliographystyle{named}
	\bibliography{ijcai18_conference}
	
\end{document}



