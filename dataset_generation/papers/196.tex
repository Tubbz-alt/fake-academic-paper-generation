\documentclass[10pt,twocolumn,letterpaper]{article}
\pdfoutput=1
\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
%%% Added by Thomas:
%% Not sure this is compatible with the style format, but I like the sort and compress settings:
\usepackage[noadjust]{cite}
%%%

\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{subfig}
\usepackage{booktabs}
\usepackage{sidecap}
% \usepackage{pgfplots} 
% \setlength{\mywidth}{16cm}
% \setlength{\myheight}{8cm}%
      
% \pgfplotsset{compat=newest} 
% \pgfplotsset{plot coordinates/math parser=false} 
% \newlength\mywidth 
% \newlength\myheight

\usepackage[normalem]{ulem}
% \usepackage{commath}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\ws}[1]{\textcolor{WildStrawberry}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\tm}[1]{\textcolor{blue}{[\textbf{TM}: #1]}}
\newcommand{\cgm}[1]{\textcolor{purple}{\textbf{CS}: #1}}
% \newcommand{\argmin}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{min}}\;}
% \newtheorem{theorem}{Theorem}[section]

\newtheorem{cond}{Condition}

% \newcommand{\argmin}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{min}}\;}
\def\etal{\emph{et al}\onedot}
\def\ie{\emph{i.e.}\xspace}
\DeclareMathOperator*{\argmax}{arg\; max}
\DeclareMathOperator*{\argmin}{arg\; min}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\hinge}[1]{\left[#1\right]_{+}}
\newcommand{\snorm}[1]{\norm{#1}^2}
\newcommand{\mb}[1]{\bm #1}
\newcommand{\Ree}{\mathbb{R}}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newcommand{\eqr}[1]{Eq. \eqref{#1}}
\newcommand{\figurecorrection}{\vspace{-5mm}}

\iccvfinalcopy % *** Uncomment this line for the final submission

% \def\iccvPaperID{119} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\
\pagestyle{empty}
\begin{document}

%%%%%%%%% TITLE
%\title{Old Data Die Hard: Reusing Past Datasets for Novel Tasks}
\title{\emph{Active Transfer Learning with Zero-Shot Priors}:\\ Reusing Past Datasets for Future Tasks}

\author{E. Gavves\thanks{This work has been done while working at ESAT-PSI, KU Leuven.}\\
QUVA Lab, University of Amsterdam\\
%{\tt\small egavves@uva.nl}
%
\and
T. Mensink\\
University of Amsterdam\\
%{\tt\small Thomas.Mensink@uva.nl}
%
\and
T. Tommasi\footnotemark[1]\\
UNC Chapel Hill\\
%{\tt\small ttommasi@cs.unc.edu}
%
\and
C.G.M. Snoek\\
QUVA Lab, University of Amsterdam\\
%{\tt\small cgmsnoek@uva.nl}
%
\and
T. Tuytelaars\\
KU Leuven, ESAT-PSI, iMinds\\
%{\tt\small Tinne.Tuytelaars@esat.kuleuven.be}
}

\maketitle
%\thispagestyle{empty}


%%%%%%%%% ABSTRACT
\begin{abstract}
% !TEX root =  main.tex
How can we reuse existing knowledge, in the form of available  datasets, when solving a new and apparently unrelated target task from a set of unlabeled data? 
In this work we make a first contribution to answer this question in the context of image classification. 
We frame this quest as an active learning problem and use zero-shot 
classifiers to guide the learning process by linking the new task to the
existing classifiers.
By revisiting the dual formulation of adaptive SVM, we reveal two basic conditions to choose greedily only the most relevant samples to be annotated. 
On this basis we propose an effective active learning algorithm which learns the best possible target classification model with minimum human labeling effort. 
Extensive experiments on two challenging datasets show the value of our approach compared to the state-of-the-art active learning methodologies, as well as its potential to reuse past datasets with minimal effort for future tasks.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
% \cgm{Intro can use a few more citations here and there.}\thispagestyle{empty}

Modern visual learning algorithms are founded on data. Given a set of images annotated with the desired object categories, the algorithm learns models able to recognize, detect or describe unseen images. Despite their importance, image datasets are assumed to be single-use only: whenever an object of interest is not previously annotated we urge for a new collection containing the precious label. Isn't this a wasteful approach? Take \emph{ImageNet}\cite{imagenet_cvpr09} for example. With about $15M$ images this huge collection contains much more information than the officially listed $22K$ synsets. 
However, most often such existing knowledge resources are ignored, because of no label overlap with the future tasks at hand, be it for classification~\cite{Freytag14, KovashkaVG11, Tommasi_BMVC_2012} or localization~\cite{VijayanarasimhanJG10, VezhnevetsBF12, LiECCV2014}.

\begin{figure}[t!]
    \centering
    \includegraphics[width=.9\columnwidth]{sustainable_new_intro.pdf}
    \caption{Imagine building a new classifier for \emph{``BMX''} bicycles, a category absent from \emph{ImageNet}. Rather than annotating the category from scratch, we propose to leverage already available annotations for learning the \emph{``BMX''} class. Through a theoretical analysis of active learning within a max-margin framework, we present the optimal conditions for sampling new data to label using zero-shot priors. \vspace{-5mm}}
\label{fig:intro}
\end{figure}% 

We postulate that \emph{no past knowledge is useless for future learning}, even if it appears to be so.
The fundamental question is how to reuse existing labels from given datasets, without the need for new annotations.
Previous transfer learning methods proved to be helpful when at least few annotated samples of the new target task are available~\cite{Pan_survey}. However, they were never challenged with the more difficult active learning setting, where no initial labeled data is available and the existing knowledge from the source datasets is most probably irrelevant for the new task.


We are inspired by advances in zero-shot classification~\cite{MensinkCVPR2014, lampert13pami, akata13cvpr}. Zero-shot learning was originally proposed~\cite{lampert13pami,socher_NIPS2013} as a strategy to obtain classifiers for arbitrary, novel categories, when no annotated data is available. Differently from~\cite{lampert13pami, akata13cvpr} who depend on human-provided attribute annotations, recent works demonstrate that label co-occurrences from image datasets~\cite{MensinkCVPR2014} and textual embeddings~\cite{devise, conse} can be used for reliable zero-shot classification models. Here \emph{we re-purpose zero-shot learning as priors} for a faster, more accurate and more economic active learning of novel categories. The zero-shot models provide us with some notion on the label distribution over the new unlabeled data in the feature space. By using this knowledge, and focusing on the dual formulation of SVM, we present and discuss the optimal conditions for active query sampling which we coin as the \emph{maximum conflict} and \emph{label equality} conditions. Based on these conditions we present a practical algorithm for optimal active learning sampling.

% Overall with this work we make three contributions. First, we revisit the concept of leveraging knowledge from past datasets for future learning and generalize, such that future learning can benefit even from semantically unrelated existing, past datasets. We believe this is an interesting task both  for its research potential, as well as for its relevance for practical purposes and applications, and we make two main contributions. Second, we propose a new angle to the problem by re-purposing zero-shot classifiers as zero-shot priors. Third, by combining zero-shot and transfer learning we describe an SVM-based theoretical framework that describes the two conditions for optimal active learning and present a practical algorithm to exploit them.

We make three contributions. First, we revisit the concept of leveraging knowledge from past datasets and generalize it: future learning benefits even from semantically unrelated existing datasets. We believe this is an interesting task both for its research potential, as well as for its relevance for practical purposes and applications. Second, we re-purpose zero-shot classifiers as zero-shot priors to guide the learning. Third, by combining zero-shot and active learning in an SVM-based framework we derive the two conditions for optimal query selection in an active learning setting. The proposed methodology is supported by an extensive evaluation on two recent datasets. Before detailing our active learning approach we first discuss related work.

%OLD_STUFF%Visual classification has mainly been viewed as a temporally static procedure: given a dataset, learn a function that maps images (the input) to visual object classes (the output). What is more, the cornerstone of high accuracies in modern visual datasets is the large dataset sizes.%%This paradigm makes a very basic assumption, namely the dataset and its annotations are pre-existing. Here we challenge this basic assumption. In the times of large datasets and large number of classes, while one can easily acquire millions of visual data, their annotation is still linear to their size. Therefore the data become largely unaccessible, since building a new classifier optimal for the particular data is bound to be overly expensive.%We make two key observations. First, based on recent advances,  Second, for every new classifier built in a particular dataset, the practictioner typically ignores all the existing, and potentially irrelevant (in terms of classes) datasets. While zero-shot classifiers have mainly been used for traditional classification, we argue that they can be also very relevant for supervised learning, when dropping the assumption of a static, pre-annotated dataset. Moreover, we argue that existing datasets can be successfully employed to build the same as accurate visual classifiers at a fraction of the cost (up to 99\% more cost effective classifiers with only 1\% loss in mAP accuracy in our experiments). We refer to the use of zero-shot classifiers and existing, annotated datasets for the purpose of learning novel classifiers as \textit{sustainable learning}.%%In the following we start from two datasets, a pre-existing $\mathcal{A}=\{x^A_i, y^A_i\}$ with some annotations, and a novel dataset $\mathcal{B}=\{x^B_i\}$ with no annotations. Assuming a novel class $u$ we want to build a new classifier $w^B_u$ optimal for dataset $\mathcal{B}$, while minimizing the annotation cost $\mathcal{C}$ associated with this task.%%Semi-supervised learning and domain adaptation

\section{Related Work}
Optimizing the efficiency of artificial learning is one of the long standing goals of
computer vision. Several research directions have been proposed towards this target, with 
\emph{active}, \emph{transfer} and \emph{zero-shot learning} among the most studied and adopted 
strategies.

\textbf{Active learning.}
The objective of \emph{active learning} is to optimize a model within a limited time and annotation budget by selecting the most informative training instances~\cite{settles2009active}. A core assumption in active learning is that there is at least one positive and one negative sample for the novel category at time $t=0$.
% , mostly either in a stream-based or in a pool-based setting. In the first case, given a set of samples provided in sequence, the learner decides at each step% whether or not to request the corresponding data label. In the second setting, given a large % collection of unlabeled data, the instances are selected and queried by the learner according to an % informativeness measure. 

In the context of image classification a popular pool-based active learning paradigm consists of
estimating the expected model change, and querying the samples which will likely influence the model most. 
Sampling the data closest to the support vector hyperplane~\cite{Schohn, Vijayanarasimhan2011} has 
been shown to reduce the version space of the learnt classifier~\cite{Tong2002}. 
%In~\cite{GuoS07}, the expected score is computed by integrating out the label variable for all %unlabeled samples. 
In~\cite{VijayanarasimhanJG10} the unlabeled samples are mirrored to obtain ghost positive and negative labels.
%%\cgm{Don't understand this sentence so removed}%and the samples with the highest influence on the current primal SVM objective. %
In~\cite{Freytag14, VezhnevetsBF12} the expected model or accuracy changes are exploited to build Gaussian process classifiers.
Similarly, in~\cite{KovashkaVG11} the total entropy on the predictions is measured over all the labeled and unlabeled data, and the samples leading to the maximum entropy reduction are selected.
Other active learning methods exploit the cluster structure in the data, either by imposing a hierarchy \cite{Dasgupta} or a neighborhood graph on the unlabeled data \cite{Zhu03combiningactive} before locally propagating the labels.

% Active learning strategies can also be grouped with respect to the number of queries sampled at % each iteration. While the majority of the methods~\cite{settles2009active} assume the most % flexible setting of querying a single instance at a time, several approaches~\cite{HoiJZL06, % GuoS07, VijayanarasimhanJG10} postulated the advantage of batch active sampling. \\\textbf{Transfer learning.}\emph{Transfer learning} aims at boosting the learning process of a new target task by transferring knowledge from previous and related source task experiences \cite{Pan_survey}. Different knowledge sources have been considered in the literature: instances \cite{lst_nips11}, models \cite{tommasi_PAMI14,Aytar12} and features \cite{Luo_ICCV11,daume}.
In the first case one exploits the availability of extra source data to enrich a poorly populated target training set.
Leveraging over models instead, allows to initialize the target learning process without the need to store the source data.
Finally, feature transfer learning relies on the source knowledge to define a representation that simplifies the target task.

Some transfer learning solutions have been proposed in a dynamic setting for online learning \cite{Tommasi_BMVC_2012}
and iterative self-labeling \cite{dasvm}. The first demonstrates the advantage of using related source knowledge
as a warm start for online target learning. The second proposes to actively annotate the most uncertain target samples with the label predicted by a known source model.

\textbf{Zero-shot learning.} Both active and transfer learning approaches suppose either the availability of at least a few labeled target training samples or an overlap with existing labels from other datasets. In contrast, \emph{zero-shot learning} exploits known relations between source and target tasks to define a target learning model without expecting annotated samples~\cite{lampert13pami,socher_NIPS2013}. This problem setting has recently attracted a lot of attention~\cite{lampert13pami, akata13cvpr, MensinkCVPR2014, devise, conse, LiECCV2014}.
Binary attributes are commonly used to encode presence or absence of visual characteristics for the object categories~\cite{lampert13pami, akata13cvpr}. They provide a description for each class and work as a natural interface for zero-shot learning. However, attribute-to-class relations are usually defined by human labelers.
Less expensive class-to-class relations can also be mined from external sources, even from textual data only~\cite{devise,conse,rohrbach10cvpr}. To avoid any labeling effort~\cite{MensinkCVPR2014} extract tag statistics from external sources like Flickr to use as class-to-class relations, whereas~\cite{Chen_2013_ICCV} directly learn novel categories by querying search engines.

%the co-occurence statistics between object class labels~\cite{MensinkCVPR2014}%or the output of neural network semantic embeddings which leverage %. For instance, the weights $\beta_{zk}$ are a function of the co-occurence statistics between %object class labels in~\cite{MensinkCVPR2014}, or the output of a neural network semantic embedding%which leverages textual data in~\cite{devise,conse}.%\vspace{1mm}
All the aforementioned paradigms are closely related. With the aim of minimizing time and annotation cost, the learning process should actively select, label and use only the most relevant new data from an unlabeled set, while exploiting pre-existing knowledge. We base our approach on a max-margin formulation, which transfers knowledge from zero-shot learning models used as priors. We also present the conditions for active learning and optimal query sampling in this challenging setting.

%We make a brief summary of active learning and zero-shot classification works in the recent literature.% \subsection{Active learning}% The field of active learning is too vast to cover for us completely, the interested reader is referred to~\cite{settles2009active, wang2012, Freytag14}. Here we present representative works in the context of image classification.% % A popular active learning paradigm is to to estimate the expected model change, namely which samples will likely influence the model update the most. Sampling the data closest to the support vector hyperplane~\cite{Schohn, Vijayanarasimhan2011} has been shown to reduce the version space of the learnt classifier~\cite{Tong2002}. In~\cite{GuoS07}, the model output change is also adopted as a strategy for selecting new samples. More specifically, the expected score is computed by integrating out the label variable for all unlabeled samples. In~\cite{VijayanarasimhanJG10} the unlabelled samples are mirrored so that to obtain ghost positive and negative labels. Then, the samples that influence the most the current primal SVM objective are selected via relaxed linear programming. Starting from Gaussian process classifiers, in~\cite{Freytag14} the model output change is shown that in an upper bound for loss reduction on the training set. In general, the above methods attempt to strike the right balance between \emph{exploration}, namely which part of the feature space to investigate next, and \emph{exploitation}, namely which part of the feature space is the most useful to process. % % Contrast to the above methods that compute the expected model change, in~\cite{VezhnevetsBF12} the expected change in the final label accuracy is measured. For these computations the label variables are once more integrated out for all unlabelled samples. Similarly, in~\cite{KovashkaVG11} the total entropy in outputs is measured over all data, labelled and unlabelled. Then, the samples that lead to the maximum reduction of the total entropy are selected. % % Active learning strategies can also be grouped with respect to the number of queries sampled at each iteration. While the majority of the methods~\cite{settles2009active} assume the most flexible setting of querying a single instance at a time, several methods~\cite{HoiJZL06, GuoS07, VijayanarasimhanJG10} postulate that batch active sampling is also beneficial. % % In this work we investigate what are the optimal exploration conditions for active learning within a max-margin framework. Based on this conditions, our model samples the data likely to maximally improve the solution state of our model from the previous round. % Our proposed model supports both single query mode, as well as batch mode, active learning. % % - Active learning with support vector machines   % - expected model output%    - Far-Sighted Active Learning on a Budget for Image and Video Recognition ??%    - Selecting Influential Examples: Active Learning with Expected Model Output Changes%    - Actively Selecting Annotations Among Objects and Attributes (?)%    - Large-Scale Live Active Learning: Training Object Detectors with Crawled Data and Crowds%    - Support vector machine active learning with applications to text classification%    - Active learning with support vector machines%    - Less is more: Active learning with support vector machines% - optimal classification performance%    - Discriminative Batch Mode Active Learning%    - Active Learning of an Action Detector from Untrimmed Videos%    - Active learning for semantic segmentation with expected change% \item Cost-Sensitive Active Visual Category Learning% \item A reinforced active learning formulation for object class recognition% \item Active Frame Selection for Label Propagation in Videos% \subsection{Zero-shot classifiers}% % Zero-shot classification has been a research branch that has gained some increased attention recently~\cite{lampert13pami, akata13cvpr, MensinkCVPR2014, devise, conse, LiECCV2014}, mainly for its ability to classify images with labels for which no annotated examples are provided. In general a zero-shot prediction is expressed as% %% \begin{equation}% p(z | x) = \sum_{k \in \mathcal{K}} \beta_{zk} p(z | x, w_k),% \label{zeroshot}% \end{equation}% %% where $p(z | x)$ is the likelihood of image $x$ being classified as the unseen label $z$, and $w_k$ is the $k$-th classifier from the set of known concepts $\mathcal{K}$. The $\beta_{zk}$ are weights that should ideally minimize the discrepancy between the optimal prediction $p(z | x, w_z^*)$ and $p(z | x)$. As for the optimal prediction we would need to have annotated samples, the weights $\beta_{zk}$ need to conform to a surrogate criterion. Such criteria could either be attribute-to-class relations~\cite{lampert13pami, akata13cvpr} defined by human labelers, or more generally class-to-class relations~\cite{MensinkCVPR2014, devise,conse} mined from external, possibly not even image related, corpora. \cgm{As our goal is sustainability, namely minimizing the annotation labor, we opt for the latter zero-shot classifiers.}% % In~\cite{MensinkCVPR2014} the weights $\beta_{zk}$ are a function of the co-occurence statistics between labels. In~\cite{devise} the weights $\beta_{zk}$ are the output of a word embedding neural network. For~\cite{MensinkCVPR2014, devise} the zero-shot prediction is obtained through linear transformations (\ie, $w_k, \forall k$ is a matrix), hence eq.~\eqref{zeroshot} is rewritten as% %% \begin{equation}% w_z = \sum_{k \in \mathcal{K}} \beta_{zk} w_k.% \label{zeroshot-linear}% \end{equation}% %% Similarly to~\cite{devise},~\cite{conse} projects the label names to a semantic, word embedding space. Then the model makes zero-shot predictions based on eq.~\eqref{zeroshot}, using the final softmax layer of a convolutional neural network as a classifier. Given that the softmax classifier $\sigma(Wx)$ is a nonlinear, yet monotonic, function of a linear transformation of the input $Wx$, we can drop the sigmoid $\sigma(\cdot)$ and once again use eq.~\eqref{zeroshot-linear}.% % Our analysis and models are agnostic to the type of the zero-shot prior classifiers used, as long as they can be easily integrated in a max-margin framework.% % % % % \subsection{Semi-supervised learning and domain adaptation}% % Latent variables, clustering% % % Active learning methodologies can also be divided according to the learning framework they adopt. Inarguably, one of the most popular choices is active learning~\cite{Tong2002} within the max-margin framework, where the hinge loss and support vector classifiers are employed. Other choices involve logistic regression models~\cite{GuoS07} and gaussian processes~\cite{Freytag14}. Our analysis also focuses on the max-margin framework, since it has been generally proven to return accurate and robust classifiers even when the number of data is small.% % %Applications of active learning are widespread. Within the machine vision community, active learning has been used in image~\cite{VijayanarasimhanJG10, Freytag14} and medical image~\cite{HoiJZL06} classification, video-based object and action segmentation and classification~\cite{VijayanarasimhanG12, Bandla2013}, attribute~\cite{KovashkaVG11} prediction, object detection~\cite{Vijayanarasimhan2011, Bandla2013, VezhnevetsBF12}, as well as general efficient classifier learning~\cite{Tong2002, GuoS07}.

%\section{Sustainable learning}
%----------------------------------------------------------\section{Auxiliary zero-shot active learning}\label{sub:adaptivesvm}%----------------------------------------------------------
Let us consider a pool of unlabeled samples $\{\mb{x}_i\}_{i=0}^N \in \mathbb{R}^d$  belonging to $\mathcal{C}$ classes.
We would like to learn a classification model for each class within a limited time and annotation
budget by querying an oracle only for the labels of the most informative instances.
We consider learning a binary classifier for one of the classes with $y_i\in\{-1,+1\}$, which are not known until queried from the active learner.
We focus on standard linear classification models with the learning parameters coded into the vector $\mb{w} \in \mathbb{R}^d$.
The prediction score for class $c$ will be expressed by $f(\mb{x}) = \mb{w}\cdot\mb{x}~$ and the final annotation is obtained by $y = sign(f(\mb{x}))$.

%----------------------------------------------------------\subsection{Maximum Conflict - Label Equality}%----------------------------------------------------------

We can formulate a greedy learning algorithm starting from standard SVM and adding a binary selection 
variable $\gamma_i^t\in \{0, 1\}$ which indicates whether at time 
step $t$ the label $y_i$ has been queried, and therefore it is known to the classifier.
The dual objective function at time $t$ is
\begin{align}
\max_{\alpha^t, \gamma^t}&\sum_i \gamma_i^t\lambda_i^t\alpha_i^t 
                        - \frac{1}{2} \sum_{i, j} \alpha_i^t \alpha_j^t \gamma_i^t \gamma_j^t y_i y_j\mb{x}_i\cdot \mb{x}_j \label{eq:sustlearn}\\
\textrm{s.t.} \quad & \sum \gamma_i^t \alpha^t_i y_i  = 0 \label{eq:sustlearn_equilibrium}\\
                    & 0 \leq \alpha^t_i \leq C, \; \forall i ~,\label{eq:sustlearn_positivealpha}\\
                    &\gamma_i^t \geq \gamma_i^{t-1}, \; \forall i ~,\label{eq:gammacond1}\\
		    &\sum_i \gamma_i^t = \sum_i \gamma_i^{t-1} +B~ \label{eq:gammacond2}~.
\end{align}
The last two constraints define the annotation procedure over time.
\eqr{eq:gammacond1} indicates that once a sample is selected, it enters and remains in the training 
set for all the subsequent iterations. \eqr{eq:gammacond2} specifies that the number of samples 
increases in time with a budget $B$, which is the maximum annotation budget per iteration. 
According to the proposed formulation, we can define two main conditions for the optimal active
query sampling procedure. 

\begin{proposition}[\textbf{Maximum Conflict}]
To maximize the objective \eqr{eq:sustlearn} at time $t$, we should query the sample $i^*$ such that \emph{(a)} 
its label $y_{i^*}$ has an opposite sign from its classification score at $(t-1)$, 
while \emph{(b))} the classifier score is as high as possible. 
\label{prop1}
\end{proposition}
This condition follows from the role of the Lagrange multipliers $\mathbf{\alpha}^t$ in choosing the support vectors.
Intuitively, if the current model misclassifies $\mb{x}_i$, the output of the new 
classifier at time $t$ must deviate from the previous one such that it can predict $y_i$ correctly. 
This is realized by introducing in the model a new support vector $\mb{x}_i$ with a large weight $\alpha_i$.
Alternatively, if the current classifier correctly labels $\mb{x}_i$, the value of $(\mb{w}^{t}\cdot\mb{x}_i)$ 
does not need to differ from $\left(\mb{w}^{t-1}\cdot\mb{x}_i\right)$ so the weight $\alpha_i$ 
can be small or even zero, and it is less likely that $\mb{x}_i$ will be introduced in the support vector set.

\begin{proposition}[\textbf{Label Equality}]
To respect the constraint \eqr{eq:sustlearn_equilibrium} the number of positive and negative 
examples in the training set should be balanced, 
\ie $\sum_i \gamma_{i}^t  [y_i=1] = \sum_i \gamma_{i}^t  [y_i=-1]$.
\label{prop2}
\end{proposition}

By focusing only on the selected samples, from \eqr{eq:sustlearn_equilibrium} we can write:
\begin{equation}
\Big(\sum_{\forall i: \gamma^t_i=1} \alpha_i^t y_i \Big)^2=0 \Rightarrow \sum_i (\alpha_i^t)^2 + \sum_{i} \sum_{j \neq i} \alpha_i^t \alpha_j^t y_i y_j = 0.
\label{eq:equilibrium}
\end{equation}
Since for any non degenerate classifier (\ie$\mb{w} \neq \vec{0}$), holds that $\sum_i \alpha_i^2 > 0$, this implies that
\begin{equation}
\sum_{i} \sum_{j \neq i} \alpha_i \alpha_j y_i y_j <0.
\label{eq:flex}
\end{equation}
Due to the positiveness of the Lagrange multipliers, \eqr{eq:sustlearn_positivealpha}, it is easy to show that \eqr{eq:flex} attains the smallest values when $\sum_i [y_i=1] = \sum_i [y_i=-1]$, where $[\cdot]$ is the Iverson bracket. Having balanced training sets has also been experimentally certified to be beneficial in large scale supervised learning~\cite{good_practices}.
%%\cgm{The sentence below is vague, I don't get the point. So removed.}%%\red{Thus we ensure that in every new round of active learning the number of positives is as close as possible samples to the number of negative samples, which in turn guarantees that the core constraint of \cgm{correct?} eq.(2) is almost satisfied and we are close to the next global optimum. This conclusion has been also experimentally shown to be beneficial for large scale supervised learning~\cite{good_practices}.} % Here $[\cdot]$ indicates the Iverson bracket and this condition follows from the positiveness of the % Lagrange multipliers (see constraint in \eqr{eq:sustlearn_positivealpha}).%----------------------------------------------------------\subsection{Zero-shot priors}%----------------------------------------------------------
In zero-shot learning the likelihood of an image $\mb{x}$ being classified as the unseen label $c$
is generally expressed as a linear combination of conditional probability distributions over a 
set of known concepts $\mathcal{K}$~\cite{akata13cvpr,MensinkCVPR2014,conse}:
%prediction can be expressed as a convex combination of known classes~\cite{akata13cvpr,MensinkCVPR2014,conse}:%In general, a zero-shot prediction is expressed as \begin{equation}
p(c | \mb{x}) = \sum_{k \in \mathcal{K}} \beta_{ck}  \,  p(k | \mb{x}, \mb{w}_k),
\label{zeroshot}
\end{equation}
where $\mb{w}_k$ is the classifier for the $k$-th concept and the weights $\beta_{ck}$ indicate the relation between the known concepts and the new class. 

% Zero-shot learning methods define the likelihood of an image $\mb{x}$ to belong to the unseen class % $c$ as a linear combination of conditional probability distributions over a % set of known concepts $\mathcal{K}$~\cite{akata13cvpr,MensinkCVPR2014,conse}:% \begin{equation}% p(c | \mb{x}) = \sum_{k \in \mathcal{K}} \beta_{ck}  \,  p(k | \mb{x}, \mb{w}_k),% \label{zeroshot}% \end{equation}% where $\mb{w}_k$ is the classifier for the $k$-th concept and% the weights $\beta_{zk}$ indicate the relation between the known concepts and the new class.

By using linear models, the zero-shot prediction can be written as 
\begin{equation}
 f^{zs}(\mb{x}) = \sum_{k \in \mathcal{K}} \beta_{ck} \, \mb{w}_k\cdot\mb{x}_i~,
\end{equation}
and exploited as an auxiliary source of knowledge while performing active learning.
Specifically we modify the prediction score of active learning to
\begin{equation}
f^{t}(\mb{x}) = \eta^{t}f^{zs}(\mb{x}) + \mb{w}^{t}\cdot\mb{x}.
\end{equation}
In this way, zero-shot learning is used as initialization at $t=0$ and supports the active learning process at each following step both in the training sample selection and in the test prediction.

%==========================================================\section{Query sampling Procedure}\label{sub:sustsampling}%==========================================================\begin{figure}[t]
    \centering
    \includegraphics[width=.85\columnwidth]{sampling2.eps}
    \caption{Sampling regions for SVM classifiers. Left: in a fully supervised SVM we expect that the (gray) margin-hyperplane zone $\mathcal{F}_0$  contains roughly as many positive as negatives, that are also the most confusing samples for the classifier. Right: as the active SVM classifier is uncertain in the beginning, sampling exclusively from $\mathcal{F}_0$ will likely result in negative labels during the early iterations, thus delaying the learning. We suggest sampling adaptively from the (green) positive outer margin zone $\mathcal{F}_+$ and $\mathcal{F}_0$ to maintain a good label distribution during active learning.}
    \label{fig:samplingfig}
\end{figure}

In the iterative process the current classifier divides the feature space into three zones:
the negative outer margin zone $\mathcal{F}_-$, the margin-hyperplane zone $\mathcal{F}_0$, and the positive outer margin
zone $\mathcal{F}_+$, see Fig. \ref{fig:samplingfig}. The samples will therefore be queried from one of the three zones. 

%----------------------------------------------------------\subsection{Sampling from different feature space zones}%----------------------------------------------------------
First, for balanced multiclass image collections as most often in computer vision, the prior class probabilities 
are roughly bounded: \ie$p(c) - p(c') < \delta$ for all categories $c, c'$ and an arbitrary small number $\delta$. 
As a result, by focusing on binary problems, we can safely expect that 
\begin{equation}
p(c) \ll p(\neg c),
\label{class-inequality}
\end{equation}
and thus most of the data will have negative labels.

Second, the performance of the defined active learning algorithm should be decent enough at the beginning due to the used zero-shot prior and it will progressively improve in time. 
We denote the random variable for a correct prediction, be it correctly \emph{positive} or correctly \emph{negative}, with $l$ and with $\neg l$ otherwise. 
Hence, we can expect the likelihood of a correct prediction to  be relatively higher than that of a wrong prediction
\begin{equation}
\frac{p(l]| \mb{w}^{t-1}, \mb{x})}{p(\neg l|\mb{w}^{t-1}, \mb{x})} > 1+\delta^{t-1}, 0 < \delta^{t-1} < \delta^{t}, \forall t\geq0.
\label{classifier-reliability}
\end{equation}
We consider the three zones and their influence of the sampling.

\textbf{Sampling from the negative outer margin zone $\mathcal{F}_-$.} 
The negative outer margin zone is defined by the region where $f^{t-1}(\mb{x}) < -1$. %$\mb{w}^{t-1}\cdot \mb{x} < -1$.
It is the largest zone,  according to \eqr{class-inequality}, and with a high probability of sampling real negatives $y_i=-1$, according to \eqr{classifier-reliability}.
Thus, even when sampling randomly in this region we are likely to repeatedly sample correctly classified negative examples, violating the label equality condition. Hence, we expect sampling from the negative outer margin zone to be suboptimal.

\textbf{Sampling from the hyperplane zone $\mathcal{F}_0$.} This region is defined by $-1 \leq f^{t-1}(\mb{x})  \leq +1$. It is the region where the classifier is maximally confused, and therefore is generally considered to be the best sampling zone~\cite{Schohn, Vijayanarasimhan2011}.
.
%
However, the samples close to the hyperplane have by definition low classification scores. Therefore, we expect that these data will have negligible effect, positive or negative, on the maximum conflict condition. Moreover, sampling from this region implicitly puts a lot of faith on the current hyperplane, an over-optimistic assumption. As a result, considering \eqr{class-inequality}, it is likely that many more negatives than positives will be sampled, which clashes with the label equality condition.

\textbf{Sampling from the positive outer margin zone $\mathcal{F}_+$.} This zone is defined by $f^{t-1}(\mb{x}) > 1$. 
In the early rounds our classifier is decent, yet not fully reliable. The likelihood of sampling a positive label will be only slightly higher than sampling a negative, but certainly higher than sampling a positive label from either $\mathcal{F}_-$ or $\mathcal{F}_0$.
If the new label is negative, $y_i=-1$, given that $f^{t-1}(\mb{x}_i) \gg 1$, and the signs are opposite, the maximum 
conflict condition is fully satisfied. If the label is positive, $y_i=+1$, the label equality condition gets closer to be satisfied over time with the abundance of negative samples.
% , see \eqr{class-inequality}, which initially work as main attractors of the query sampling procedure.
Overall we conclude that sampling from the positive outer margin zone is beneficial for both the maximum conflict condition as well as the label equality condition. Hence we expect fastest learning in the first rounds when sampling from this zone.

%----------------------------------------------------------\subsection{Maximum conflict-label equality sampling}%----------------------------------------------------------
Sampling only from the $\mathcal{F}_+$ is beneficial at the early stages of active learning, when the reliability of the classifier can be low. In order to meet the maximum conflict and label equality conditions on later iterations, however, we propose a novel sampling strategy.

We consider the likelihoods for sampling positive and negative labels from $\mathcal{F}_+$ and $\mathcal{F}_0$, namely $p(l|\mathcal{F}_+, t)$, $p(\neg l|\mathcal{F}_+, t)$, $p(l|\mathcal{F}_0, t)$, $p(\neg l|\mathcal{F}_0, t)$. We compute these likelihoods at each iteration $t$, based on the previously sampled labels and the zones they were sampled from.

Assuming for clarity an annotation budget $B=1$, at iteration $t$ we have that
%\begin{equation}
p(l|\mathcal{F}_+, t) \sim p(l|\mathcal{F}_+, t-1) + \frac{\sum_{r=1}^{t-1} [l_r=1]\cdot[\mb{x}^r \in \mathcal{F}_+^r]}{t-1},
\end{equation}%
where $[\mb{x}^r \in \mathcal{F}_+^r]=1$ means that at iteration $r$ the sample $\mb{x}^r$ was sampled from the $\mathcal{F}_+$. 
The rest of the probabilities are computed in a similar manner. 
The term $p(l|\mathcal{F}_+, t-1)$ is included such that the likelihoods do not fluctuate violently, 
especially during the early iterations. Given that we have no information at the first round, for $t=1$ we set $p(l|\mathcal{F}_+, t-1)=0.5$ and $p(l|\mathcal{F}_0, t-1)=0.1$, although different 
initializations did not have any significant effect. At each iteration we normalize the probabilities to sum up to one, and we 
measure the label equality for the sampled data up to that point as
%\begin{equation}
\rho^{t-1}=\frac{\sum_{r=1}^{t-1} [l_r=1]}{\sum_{r=1}^{t-1} [l_r=1]+[l_r=-1]}~.
\end{equation}%
We then sample such that
%\begin{equation}
\rho^{t-1} < \rho' \Rightarrow \mb{x}_t \sim \mathcal{F}_+^{t-1}~,
\end{equation}%
namely we sample from $\mathcal{F}_+^{t-1}$ if we have too many negatives, otherwise we sample from $\mathcal{F}_0^{t-1}$. 
Selecting data from other regions, \eg negatives $\mathcal{F}_-^{t-1}$, did not seem to perform well in practice. Next, we present and discuss the empirical results of our adaptive active learning  approach.\\

\noindent\textbf{Empirical observations.}
The threshold $\rho'$ can be cross validated in a separate dataset, although a $\rho'=0.5$ seemed to work well in practice. Similar to MCMC Gibbs samplers~\cite{meyn1993markov} a burn-in period, where we sample only from $\mathcal{F}_+$, was shown to result in higher robustness. After enough samples have been queried, \eg about 150, the priors are not needed anymore, as we are in a discriminative setting, and thus they are dropped. Futhermore, we observe that in the first rounds most of the highest scoring samples are located in the edge between $\mathcal{F}_+$ and $\mathcal{F}_0$. This is because the training set is still small and also, because of the well-known shift for positive and negative sample scores, when moving from the train to the test set. Still the MCLE sampling strategy remains consistent, as we query samples after ranking, namely querying from $\mathcal{F}_+$ equals to querying the most confidently positive sample and querying from $\mathcal{F}_0$ equals to querying the most confused new sample.

%%%%%%%%% OLD  %%%%%%%%%%%\section{Query Sampling Procedure}%\label{sub:sustsampling}%%Two factors influence the query sampling procedure in the defined setting.%%First, for balanced multiclass image collections, the majority of image datasets in computer vision, the prior class probabilities %are roughly bounded: \ie $p(c) - p(c') < \delta$ for all categories $c, c'$%and an arbitrary small number $\delta$. As a result, by focusing on binary problems,%we can safely expect that %\begin{equation}%p(c) \ll p(\neg c)%\label{class-inequality}%\end{equation}%with the most part of the data having negative labels.%%Second, the performance of the defined active learning algorithm should be decent %enough at the beginning due to the used zero-shot prior and it will progressively %improve in time. Hence, we can expect the likelihood of a correct prediction, positive %or negative, to  be relatively higher than that of a wrong prediction%\begin{equation}%\frac{p(l, \hat{l} | \mb{w}^{t-1}, \mb{x})}{p(\neg l, \hat{l} |\mb{w}^{t-1}, \mb{x})} > 1+\delta^{t-1}, 0 < \delta^{t-1} < \delta^{t}, \forall t\geq0,%\label{classifier-reliability}%\end{equation}%where $l$ is the true label of the sample $x$ and $\hat{l}$ is the prediction of our classifier $w$. %%\begin{figure}[t]%    \centering%    \includegraphics[width=.9\columnwidth]{images/sampling2}%    %\caption{Sampling regions for SVM classifiers: the margin-hyperplane zone $\mathcal{F}_0$ (depicted blue-ish)  is usually used for sampling, we suggest to use the positive outer margin zone $\mathcal{F}_+$ (green-ish) for sampling, the negative outer margin zone $\mathcal{F}_-$ is depicted brownish.}%    \caption{Sampling regions for SVM classifiers. In a fully supervised SVM (left) we expect that in the (gray) margin-hyperplane zone $\mathcal{F}_0$  there are roughly the same number of positive and negative labelled samples, the ones that are most confusing for the classifier. Hence, $\mathcal{F}_0$ is the default sampling in active learning. As the active SVM classifier is uncertain in the beginning (right), sampling strictly from $\mathcal{F}_0$ will result mostly in negative labels for the early iterations, thus delaying the learning. We suggest to sample adaptively from the (green) positive outer margin zone $\mathcal{F}_+$ and $\mathcal{F}_0$ to maintain a good label distribution during the active learning.}    %%    Sampling regions for SVM classifiers: the margin-hyperplane zone $\mathcal{F}_0$, where $-1 \leq \mb{w}^{t-1} \cdot \mb{x} \leq 1$ (red), is usually used for sampling. We propose to sample from the positive outer margin zone $\mathcal{F}_+$, where $\mb{w}^{t-1} \cdot \mb{x} > 1$ (green), the negative outer margin zone $\mathcal{F}_-$, where  $\mb{w}^{t-1} \cdot \mb{x} < -1$, is depicted in blue.    %% }    %    \label{fig:samplingfig}%\end{figure}%%In the iterative process the current classifier divides the feature space into three zones (see Figure \ref{fig:samplingfig}):%the negative outer margin zone $\mathcal{F}_-$, the margin-hyperplane zone $\mathcal{F}_0$, and the positive outer margin%zone $\mathcal{F}_+$.\\%%\noindent\textbf{Sampling from the negative outer margin zone $\mathcal{F}_-$.} %In this region we have \ie $\mb{w}^{t-1}\cdot \mb{x}_{\mathcal{F}_-} \ll -1$. %Furthermore, due to eq.~\eqref{class-inequality} and~\eqref{classifier-reliability}, and to the Bayes rule, %most of the samples are real negatives $y_i=-1$. Thus, even when sampling randomly in this region we are likely to %sample category negatives which are correctly recognized by the current training model, violating the %maximum conflict condition.%What is more, sampling repeatedly from the negative tail distribution, will most likely result in a %heavily unbalanced label distribution, which violates the label equality condition. %Therefore, we expect that sampling from the negative outer margin zone is suboptimal.\\%%\noindent\textbf{Sampling from the hyperplane zone $\mathcal{F}_0$}%This is generally indicated as the region of maximal uncertainty for a classifier and%considered as the zone from where relevant samples should be selected to improve the%current model in active learning \cite{}. However, the samples which falls close to the%hyperplane  have by definition very low absolute classification scores, %namely $\mb{w}^{t-1}\cdot\mb{x} \approx 0$, therefore, we expect that these data%will have no effect, positive or negative, on the maximum conflict condition.%Moreover, extracting queries from this region means putting a lot of faith on the current %hyperplane in an over-optimistic assumption. As a result, considering \eqref{class-inequality}, %it is more likely that we will end up sampling many more negatives than positives, which clashes %with the label equality condition.\\%%%All in all, sampling from the hyperplane zone can be viewed as a safe strategy. Most likely in the early rounds we %%obtain a similar solution to the one of the previous round, waiting for the dataset to grow big enough %%for a more confident learning. In practice, this is a conservative strategy that delays the learning in %%the beginning when the classifier is unreliable, but yields good results later.\\%%\noindent\textbf{Sampling from the positive outer margin zone $\mathcal{F}_+$.} %When sampling from the top of the positive outer margin, we obtain samples with largest positive scores. %As in the early rounds our classifier is decent, yet not fully reliable, the likelihood of sampling %a positive label will be only slightly higher than sampling a negative, but certainly higher than sampling a %positive label from either $\mathcal{F}_-$ or $\mathcal{F}_0$. If the new label is negative, $y_i=-1$, %given that $\mb{w}^{t-1}\cdot \mb{x}_{\mathcal{F}_+}$ is large, and the signs are opposite, the maximum %conflict condition is satisfied. %In the subsequent iterations we can expect to query more and more positive labelled samples in %this region. However, considering the abundance of negative samples which initially work as main %attractors of the query sampling procedure, adding positive samples in the training set helps%satisfying the label equality condition over time as desired.%%Overall we conclude that, when querying from the positive outer margin zone, %we arrive at a better solution by satisfying both the Maximum Conflict and Label Equality%conditions. Hence, we are guaranteed to observe a faster learning than when sampling from %the negative outer margin zone or the hyperplane zone.%%%%with the aim of defining a simple and effective strategy for our %%adaptive active learning approach, the safest bet consists in querying from the positive%%outer margin zone. This choice allows to satisfy both the conditions mentioned in the previous%%section, providing some basic guarantee on the efficiency (fast learning, minimum annotation effort)%%of the learning process.%%Of course, this holds for as long as the active classifiers are decent, but not too good.

\section{Experiments}
% !TEX root =  main.tex% We start by describing the used experimental setup (section \ref{sec:EXPsetup}), then we focus on % evaluating the importance of the zero-shot auxiliary source model (section \ref{sec:EXPzeroshot}), and testing % the chosen query sample procedure (section \ref{sec:EXPmin-slack}). Finally we present a comparison against% several state-of-the-art active learning algorithms (section \ref{sec:EXPsota}) and we conclude showing the % performance our method ``in the wild'': when the class-to-class relation weights  % for the zero-shot models are obtained on a dataset different from the one used for testing.%------------------------------------------% Figure priors%------------------------------------------\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{evaluating_priors_hsun.pdf}\\
    \includegraphics[width=0.95\linewidth]{evaluating_priors_coco.pdf}
    % \includegraphics[width=\linewidth]{images/evaluating_priors_hsun.png}
    \caption{Various sources of prior information for active learning on HSUN (\emph{top}) and MSCOCO. The reason for the lower accuracy in MSCOCO is that we use only 300 out of the total $80,000$ training images (for HSUN there are $4,000$ in total).
    Active learning with zero-shot priors considerably outperforms a non-informative random prior, the default in the active learning literature.}
    \label{fig:priors}
    \figurecorrection
\end{figure}%------------------------------------------\subsection{Experimental setup}\label{sec:EXPsetup} 
We run \footnote{We implemented the MCLE zero-shot active learning in MATLAB. The code is available at http://www.egavves.com/list-of-publications.} our experiments on two challenging image datasets. \textbf{Hierarchical SUN (HSUN) dataset~\cite{ChoiCVPR2010}.}  This is a generic, multi-class and multi-label image classification dataset, covering both object as well as scene categories. There are in total 107 classes and 8,634 images, split into a training and a test set composed of 4,367 and 4,317 images respectively. \textbf{Microsoft COCO (MSCOCO) dataset~\cite{LinECCV2014}.} MSCOCO is a multi-class and multi-label dataset containing 80 object categories. The MSCOCO dataset contains 123,287 images in total, split into a training and a test set composed of $82,783$ and $40,504$ images respectively. Each image is annotated with respect to the presence or absence of a particular object. 

Following the common procedure in the zero-shot recognition literature~\cite{akata13cvpr, MensinkCVPR2014}, we divide 
the classes randomly into two sets: 75\% of known and 25\% of unknown classes. Also, all training and active 
sample querying is performed strictly on the training set. The evaluations, as well as the reported results
(Mean Average Precision: Mean AP), are computed using the completely independent test sets.
%For experiments up to Sec.~\ref{sec:active-transfer-zero}, when we use zero-shot priors~\cite{MensinkCVPR2014}, %we extract statistics from the same dataset, but from the independent training split. For a more realistic %showcase, we conclude with an experiment, where we transfer zero-shot statistics from completely unrelated %sources (Flickr tag co-occurrences), while using zero-shot classifiers trained on a different dataset. %By doing so, we exploit the dormant knowledge from independent, already annotated datasets, for categorization with minimal labor.\\
For both datasets and all experiments we rely on deep learning features~\cite{KrizhevskySH12}, trained on 
the separate \textit{ImageNet} dataset~\cite{imagenet_cvpr09}. We fix the SVM learning parameter $C$
to $1$ and we run our method over 300 iterations (\ie the maximum number of queried training samples is 300).

% % For the zero-shot priors we use % the dice coefficients from~\cite{MensinkCVPR2014} and for the attribute-like zero-shot priors we % use the binary coefficients from~\cite{MensinkCVPR2014}. For the classifiers trained on the known % categories of each dataset we use a fixed $C=1$. In the absence of data during the active learning rounds, % we cannot cross-validate the value of the $\epsilon, \epsilon_0$ for the different flavors of zero-shot prior % SVM. Therefore, we simply set them to 1. When sampling from $\mathcal{F}_{+}$ or $\mathcal{F}_{-}$, we sample % the most positive and negative samples respectively, whereas when sampling $\mathcal{F}_{0}$ we sample the % data closest to the hyperplane.\begin{figure}[t!]
    \centering
    \includegraphics[width=0.95\linewidth]{evaluating_prior_forms_hsun.pdf}
    \caption{Different ways to update the zero-shot prior for active learning. A constant prior works best. Moreover, zero-shot learning acts as a good initialization and accelerates the active learning.}
    \label{fig:prior-forms}
    \figurecorrection
\end{figure}%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\subsection{Zero-shot priors for active learning}\label{sec:EXPzeroshot}%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
We first establish the value of using a zero-shot model as an auxiliary source for active learning. To that end, we compare the effect of the zero-shot warm initialization against a basic random initialization which is a standard choice in active learning. Regarding sampling we opt for simplicity and sample from $\mathcal{F}_+$.
We compare three zero-shot models: 
\textbf{COSTA}~\cite{MensinkCVPR2014} using co-occurrences to express the class-to-class relation;
\textbf{Attribute-like} models using binarized class-to-class relations mined from other sources~\cite{lampert13pami,rohrbach10cvpr}; and \textbf{Image search} model based using the first 12 images returned by Google image search as positive examples, similar to~\cite{Chen_2013_ICCV}.
These zero-shot models are used as auxiliary model $f^{a}(\cdot)$ only in the first step of the active learning process to sample the first example and initiate SVM learning, \ie the classifier is $f(\mb{x})^t = \eta^t f^{a}(\mb{x}) + \mb{w}^t\cdot\mb{x}$ with $\eta^0=1$ and $\eta^t=0$ for $t>0$.

The obtained results are presented in Fig.~\ref{fig:priors}, including the fully supervised results as upper bound.
We observe that using a zero-shot warm initialization always improves the learning considerably, both at $t=0$ and at $t=300$. For the HSUN dataset the COSTA prior is best from start to end. For the object-oriented MSCOCO dataset the image search zero-shot prior is better in the first iterations, whereas COSTA is better afterwards.
%The different amount of available samples per category naturally influences the relative performance obtained with active learning when using a maximum of 300 queries (gap between active and full learning).

In a second experiment, we evaluate different ways to exploit the zero-shot prior during the active learning process.
We compare 4 strategies: \textbf{vanilla prior} where we use the prior only at $t=0$ as above, \textbf{constant prior} where $\eta^t=1~, \forall t$, \textbf{inverse decay prior} where $\eta^t=1/(t+1)$, and finally \textbf{linear decay prior} where $\tfrac{t_0}{t+t_0} f^{a}(\mb{x}) + \tfrac{t}{t+t_0}\mb{w}^t\cdot\mb{x}$ with a relatively high $t_0=20$.
% \begin{itemize}\setlength{\itemsep}{-1mm}% 	\item \textbf{vanilla prior}, using the prior only at $t=0$ (as above);% 	\item \textbf{constant prior}, using $\eta^t=1~, \forall t$;% 	\item \textbf{inverse decay prior}, using $\eta^t=1/(t+1)$;  % 	\item \textbf{linear decay prior}, using $\tfrac{t_0}{t+t_0} f^{a}(\mb{x}) + \tfrac{t}{t+t_0}\mb{w}^t\cdot\mb{x}$, with a relatively high $t_0=20$.% \end{itemize}%
The results are presented in Fig.~\ref{fig:prior-forms}. 
The constant prior appears to be the fastest learner, proving that zero-shot learning works not only as a good initialization but can also consistently accelerate the active learning. As expected, all the considered variants converge when a larger number of training samples is available.
%
On the basis of these results, we use in the remaining experiments the constant prior, using COSTA source models for HSUN and the Image search source models for MSCOCO.
% !TEX root =  main.tex\begin{figure}[t!]
    \centering
    \includegraphics[width=0.95\linewidth]{evaluating_sampling_hsun.pdf}\\
    \includegraphics[width=0.95\linewidth]{evaluating_sampling_coco.pdf}
    \caption{Different active sampling strategies for HSUN (\emph{top}) and MSCOCO. The adaptive MCLE sampling works as good as $\mathcal{F}_+$ for HSUN and considerably better for MSCOCO.}
    \label{fig:active-sampling}
    \figurecorrection
\end{figure}%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\subsection{Maximum conflict-label equality}\label{sec:EXPmin-slack}%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In this experiment we evaluate different sampling strategies as detailed in Sec.~\ref{sub:sustsampling}. 
Note that sampling from $\mathcal{F}_0$ is equivalent to selecting the data points on which the current
classifier presents the maximal uncertainty, as repeatedly proposed in the literature~\cite{Tong2002, Schohn, Vijayanarasimhan2011}. 
We present results for the HSUN and the MSCOCO datasets in Fig.~\ref{fig:active-sampling}.

For the HSUN dataset the zero-shot priors might be weak (COSTA co-occurrence classifiers),
while for MSCOCO the priors are more reliable (image search supervised classifiers). Moreover,
in HSUN the images are harder to classify than in MSCOCO as can be appreciated by comparing the accuracy
of the fully supervised case. These observations explain the different behaviours visible in the
two plots. For HSUN, $\mathcal{F}_+$ contains both positive and negative samples with a roughly 
balanced distribition, and MCLE mainly selects samples from this region. For MSCOCO, $\mathcal{F}_+$ contains mainly positive
samples and, to have a balanced set, MCLE chooses samples also from $\mathcal{F}_0$. The results of the vanilla prior
baseline confirms that it is useful to sample from $\mathcal{F}_0$: an analysis on the 
accuracies of the individual class experiments reveales that the high increase in performance after 125 iterations
coincides with the optimal label equality.

Overall we can state that our method automatically adapts to the different conditions providing always higher 
or equal results than selecting only from $\mathcal{F}_+$ or $\mathcal{F}_0$. We conclude that MCLE sampling 
is a reliable strategy when using zero-shot priors for active learning.

\begin{table*}[t!]
  \centering
  \setlength{\tabcolsep}{6pt}
 \scalebox{0.8}{
  \begin{tabular}{l c c c c c c c c c c c c c c c}
  \toprule
  & \multicolumn{7}{c}{\textit{HSUN (All samples: 0.383 mAP)}} & & \multicolumn{7}{c}{\textit{Small COCO (All samples: 0.460 mAP)}} \\
  \cmidrule{2-8} \cmidrule{10-16} 
  No. of queries & 0 & 50 & 100 & 150 & 200 & 250 & 300 & & 0 & 50 & 100 & 150 & 200 & 250 & 300 \\
  \midrule
  MCLE (Dataset prior) & 0.255 & 0.315 & 0.337 & 0.348 & 0.346 & 0.355 & 0.361 & & 0.250 & 0.350 & 0.381 & 0.383 & 0.444 & 0.448 & 0.460 \\
  MCLE (External prior) & 0.270 & 0.289 & 0.327 & 0.341 & 0.336 & 0.348 & 0.358 & & 0.197 & 0.293 & 0.391 & 0.427 & 0.436 & 0.442 & 0.457 \\
  \midrule
  BBAL~\cite{VijayanarasimhanJG10} & 0.158 & 0.241 & 0.276 & 0.309 & 0.322 & 0.328 & 0.325 & & 0.168 & 0.283 & 0.335 & 0.364 & 0.380 & 0.395 & 0.408 \\
%   C-balanced BBAL~\cite{VijayanarasimhanJG10} & 0.158 & 0.241 & 0.276 & 0.309 & 0.322 & 0.328 & 0.325 & & 0.154 & 0.272 & 0.342 & 0.375 & 0.396 & 0.405 & 0.403 \\
  Hiearchical Sampling~\cite{Dasgupta} & 0.089 & 0.156 & 0.199 & 0.221 & 0.234 & 0.230 & 0.246 & & 0.076 & 0.182 & 0.250 & 0.287 & 0.309 & 0.331 & 0.365 \\
  GP Mean~\cite{Kapoor} & 0.154 & 0.282 & 0.319 & 0.340 & 0.350 & 0.361 & 0.365 & & 0.186 & 0.344 & 0.394 & 0.412 & 0.421 & 0.431 & 0.438 \\
  GP Variance~\cite{Kapoor} & 0.154 & 0.201 & 0.206 & 0.216 & 0.226 & 0.240 & 0.244 & & 0.186 & 0.233 & 0.263 & 0.284 & 0.291 & 0.309 & 0.326 \\
  GP Impact Bayes~\cite{Freytag14} & 0.154 & 0.251 & 0.286 & 0.305 & 0.316 & 0.327 & 0.345 & & 0.186 & 0.298 & 0.346 & 0.393 & 0.417 & 0.430 & 0.436 \\
  GP EMOC Bayes~\cite{Freytag14} & 0.154 & 0.277 & 0.310 & 0.320 & 0.328 & 0.332 & 0.337 & & 0.186 & 0.336 & 0.375 & 0.388 & 0.397 & 0.399 & 0.405 \\
  \bottomrule
  \end{tabular}
  }
  \caption{Comparison with state-of-the-art active learning methods. For HSUN the external prior comes from COSTA learned on MSCOCO, whereas for MSCOCO the external COSTA prior is learned on HSUN. For both datasets, the MCLE sampling outperforms all baselines almost always, especially in the early rounds. In fact for Small MSCOCO MCLE reaches the full mAP within 300 samples, even with external priors. Hence, \emph{we effectively reuse past datasets}.} \label{tab:compare_al}
\vspace{5mm}
\end{table*}% It is, therefore, interesting % to look at the evolution of the accuracies and the label distributions for individual classes over time, see % Fig.~\ref{fig:active-sampling-mscoco}. Indeed, the sudden increase in accuracy for the various methods % coincides with optimally satisfying the label equilibrium condition.%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\subsection{State-of-the-art comparisons}\label{sec:EXPsota}%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\begin{table}[t!]
  \centering
  \setlength{\tabcolsep}{6pt}
  \scalebox{0.65}{
  
  \begin{tabular}{l l c c c c c c c c}
  \toprule
    & Class  & 0 & 50 & 100 & 150 & 200 & 250 & 300 \\
  \midrule
  MCLE & \multirow{2}{*}{\textit{Bicycle}} & 0.134 & 0.244 & 0.259 & 0.258 & 0.279 & 0.286 & 0.296 \\
  BBAL~\cite{VijayanarasimhanJG10} &  & 0.046 & 0.173 & 0.190 & 0.250 & 0.268 & 0.272 & 0.257 \\
  \midrule
  MCLE & \multirow{2}{*}{\textit{Stop sign}} & 0.048 & 0.307 & 0.313 & 0.316 & 0.369 & 0.369 & 0.375 \\
  BBAL~\cite{VijayanarasimhanJG10} & & 0.021 & 0.031 & 0.032 & 0.033 & 0.150 & 0.320 & 0.312 \\
  \midrule
  MCLE & \multirow{2}{*}{\textit{Cow}} & 0.019 & 0.046 & 0.127 & 0.269 & 0.275 & 0.283 & 0.352 \\
  BBAL~\cite{VijayanarasimhanJG10} &  & 0.166 & 0.206 & 0.204 & 0.258 & 0.281 & 0.299 & 0.300 \\
  \midrule
  MCLE & \multirow{2}{*}{\textit{Refrigerator}} & 0.365 & 0.379 & 0.419 & 0.453 & 0.474 & 0.452 & 0.479 \\
  BBAL~\cite{VijayanarasimhanJG10} &  & 0.319 & 0.291 & 0.407 & 0.341 & 0.375 & 0.381 & 0.384 \\
  \bottomrule
  \end{tabular}
  }
  \caption{Accuracies for specific categories on Small COCO. For MCLE we use external prior from HSUN COSTA. We compare with BBAL~\cite{VijayanarasimhanJG10} the best SVM based active learning method from Tab.~\ref{tab:compare_al}. We observe that the MCLE sampling learns faster and better even when the zero-shot priors are not good enough. Note that~\cite{VijayanarasimhanJG10} assumes there is at least one positive sample available at the beginning, a strong assumption for practical use.}
\label{tab:soa-cat}
\vspace{-5mm}
\end{table}% \begin{figure}[t!]%     \centering%     \includegraphics[width=1.0\linewidth]{images/plotly/evaluating_stateoftheart_hsun_plotly.png}%     % \includegraphics[width=\linewidth]{images/different_samplings_rounds300_nbeta1.png}%     \caption{Comparison to state-of-the-art active learning methods.     %     Active learning with zero-shot priors and MCLE sampling outperforms other methods, especially in the early rounds of the active learning, where the classifier is most uncertain. %Note that unlike the other methods the Gaussian Process based active learning methods make the strong assumption that one positive and one negative sample is available in the beginning.%     }%     \label{fig:stateoftheart}%     \figurecorrection% \end{figure}\begin{figure}[t!]
    \centering
    \includegraphics[width=0.95\linewidth]{plot_full_hsun_tm.pdf}\\
    \includegraphics[width=0.95\linewidth]{plot_full_coco_tm.pdf}
    \caption{Active learning progress with MCLE when all training samples are used. MCLE sampling reaches the full accuracy rather early in the process. With only 300 samples, which is about 95\% of the labels MCLE obtains 95-100\% of the mAP for both datasets.}
\label{tab:soa}
    \label{fig:mcle-full}
    \figurecorrection
\end{figure}

Next, we present comparisons to state-of-the-art active learning methods. For computational reasons we perform all experiments on a random subset of MSCOCO containing 4,000 training and 4,000 test images, which we call `Small MSCOCO'. We consider the  \emph{far-sighted} active learning from~\cite{VijayanarasimhanJG10}, the hierarchical sampling approach~\cite{Dasgupta}, as well as the Gaussian Process based active learning methods from~\cite{Freytag14} and~\cite{Kapoor}. For these baselines we use the publicly available code and the recommended settings.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{sustainable_examples.eps}
    \caption{Images sampled during the first seven iterations of active learning for \emph{``boat''} and \emph{``traffic light''} on MSCOCO with HSUN zero-shot priors. Positive samples are denoted with green, negative ones with red. Observe the balanced label distribution, while the visual similarity indeed reveals the maximum conflict during the selection of the next sample.}
    \label{fig:examples}
    \vspace{5mm}
\end{figure*}

For MCLE sampling we use two types of priors: one from the training set of the same dataset which can be over-optimistic, and a second one from external data.
For HSUN we use the COSTA priors computed on MSCOCO, while for MSCOCO we use the image search classifiers. 
%
To build the label-to-label co-occurrence matrix for the zero-shot COSTA priors we 
compute the Flickr tag statistics as in~\cite{MensinkCVPR2014}. For sake of compatible comparisons, we perform the experiment on the same unknown categories (and not the full dataset) as in the previous experiments. 

% \cgm{Are all seven paragraphs below needed, can be cut a bit to win space.}

All baselines require one positive and one negative sample 
at time $t = 0$, while they are not needed for MCLE
sampling with zero-shot priors. This provides a significant
advantage for the baselines: selecting a positive data out of
an unbalanced distribution with a large number of negatives
is challenging and surely a strong assumption. Still, we believe that useful conclusions can
be drawn. We present results in Tab.~\ref{tab:soa}.

% Note that MCLE sampling only relies on zero-shot priors for initialization.% In contrast, the common assumption in almost all active learning methods is that at least one positive \emph{and} one negative sample is provided at time $t=0$. This is analogous to having the best possible prior information, and quite a strong one, especially regarding the positive sample, thus favoring the baseline methods. % Still, useful conclusions can be drawn. We present results in Tab.~\ref{tab:soa}.

MCLE sampling with zero-shot priors outperforms all other methods, especially in the early rounds where classifiers are most uncertain.
For Small MSCOCO MCLE sampling allows for obtaining even the full mAP with only 300 samples.
Furthermore, MCLE sampling with external priors is as good as when using priors from the own dataset.

In Tab.~\ref{tab:soa-cat} we present results for separate categories in Small MSCOCO, using the MCLE with external prior and the best SVM-based active learning method from the results above.  MCLE sampling learns faster and better even for poor zero-shot priors.
Furthermore, with active learning on HSUN using MSCOCO zero-shot priors, 10 out of the 26 unknown labels are common between the two datasets (for the reverse 4 labels are common). Hence, results remain good even in the simultaneous presence of known and unknown labels. To rule out negative effects of severe label imbalance, we repeat the experiments for BBAL~\cite{VijayanarasimhanJG10} tuning the misclassification penalty $C$ per class. We observed no benefit. Further, the hierarchical sampling~\cite{Dasgupta} intrinsically deals with label imbalance, still giving inferior results. We conclude that low accuracy is due to the query samples chosen rather than label imbalance.
Finally, we plot in Fig.~\ref{fig:mcle-full} the full evolution of the active learning until all training images are used. Full accuracy is obtained quite fast, thus allowing for a considerable saving in annotation requirements: with 300 samples, namely about 5-10\% of the labels, we obtain about 95-100\% of the full mAP. 

Based on the \emph{maximum conflict-label equality} conditions we can exploit past datasets and existing human annotations for the active learning of future tasks on unknown categories. We conclude that zero-shot priors and MCLE sampling allows for a faster, more robust and evidently a more economical active learning. To the best of our knowledge, \emph{we are the first to show how to exploit existing but unrelated annotations for active learning of novel image categories.}% Another interesting observation stems from taking a look at the combination of different priors and sampling from the hyperplane $\mathcal{F}_0$. More specifically, it appears that for both datasets sampling from the hyperplane behaves no matter whether we inject some good prior knowledge, vanilla priors or random priors. This phenomenon could explain why, surprisingly enough, zero-shot priors have not been explored earlier in active learning.% We witness one more interesting observation in the \emph{MSCOCO} dataset. More specifically, it appears that after about 170 samples sampling from the hyperplane $\mathcal{F}_0$ with vanilla or random priors starts benefiting the active learning more. This can be quite nicely explained if we take a look at individual classes, where we plot the accuracies, as well as the label distributions over time. For example we plot the illustrate the case for the category ``motorcycle'' in Fig.~\ref{fig:active-sampling-mscoco-motorcycle}. For as long as sampling from $\mathcal{F}_+$ was retaining a reasonable label distribution the accuracy of the classifier was increasing. However, at about 170 samples sampling from $\mathcal{F}_+$ led to highly biased datasets in favor of positive samples. At the same time sampling $\mathcal{F}_0$ led to a better label equilibrium as compared to the earlier rounds, where mainly only negatives were sampled.% \begin{figure*}[t!]%     \centering%     \includegraphics[width=0.23\linewidth]{images/evaluating_querying_strategies_coco_bird.png}%     \includegraphics[width=0.23\linewidth]{images/evaluating_querying_strategies_coco_chair.png}%     \includegraphics[width=0.23\linewidth]{images/evaluating_querying_strategies_coco_motorcycle.png}%     \includegraphics[width=0.23\linewidth]{images/evaluating_querying_strategies_coco_tennisracket.png}%     \includegraphics[width=0.23\linewidth]{images/evaluating_querying_strategies_coco_label_distribution_bird.png}%     \includegraphics[width=0.23\linewidth]{images/evaluating_querying_strategies_coco_label_distribution_chair.png}%     \includegraphics[width=0.23\linewidth]{images/evaluating_querying_strategies_coco_label_distribution_motorcycle.png}%     \includegraphics[width=0.23\linewidth]{images/evaluating_querying_strategies_coco_label_distribution_tennisracket.png}%     \caption{\red{Needs to be updated.} Different active sampling strategies for the \emph{MSCOCO} categories \emph{bird}, \emph{chair}, \emph{motorcycle}, \emph{tennis racket}, displayed columnwise. In the top row we plot the accuracy over time, in the bottom row we plot the label distribution for each class (sampled positive labels over positive+negative labels) over time. We observe that different active sampling strategies start outperforming the others exactly when they maintain a reasonable label equilibrium between $0.3-0.7$. Also, the combination of the constant prior and sampling from $\mathcal{F}_0$ seems to return unchanged accuracies, thus indicating the dominance of the prior. When the label distribution is uneven, either overly positive or negative, this stalls learning. The plots for the rest of the categories are provided in the supplementary material.}%     \label{fig:active-sampling-mscoco}% \end{figure*}

\section{Conclusion}
In this work we attempt a first answer to the question of how to reuse past datasets for 
faster and more accurate learning of new and seemingly unrelated future tasks. We start from 
zero-shot classifiers and re-purpose them as priors to warm up active learning. 
Focusing on the dual formulation of SVM, we reveal two conditions for optimal sampling, 
identifying the most important samples to be annotated next. We then propose an 
effective active learning approach that learn the best possible target classification model 
with minimum human labeling effort. 

Up to our knowledge no previous work combined zero-shot with active learning.
As demonstrated by the MCLE optimal sampling conditions, this setting is different from standard
active learning without auxiliary knowledge, in that the positive outer margin zone $\mathcal{F}_+$
gains importance with respect to the traditional hyperplane zone $\mathcal{F}_0$.

Extensive experiments on two challenging datasets show the value of our approach compared to the state-of-the-art, and outline the potential of reusing past datasets with minimal effort for future recognition tasks.

\vspace{2mm}{
\small
\textbf{Acknowledgments}
This research is supported by the iMinds strategic project HiViz, the STW STORY project and the Dutch national program COMMIT.
}

\clearpage{\small
\bibliographystyle{ieee}
\bibliography{sustainable_learning}
}

\end{document}

% \begin{figure}[t]
% \begin{center}
% \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    %\includegraphics[width=0.8\linewidth]{egfigure.eps}
% \end{center}
%    \caption{Example of caption.  It is set in Roman so that mathematics
%    (always set in Roman: $B \sin A = A \sin B$) may be included without an
%    ugly clash.}
% \label{fig:long}
% \label{fig:onecol}
% \end{figure}
