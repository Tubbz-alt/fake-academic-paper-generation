
%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}

\usepackage{multirow}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{color}
\pdfminorversion=6
\usepackage[switch]{lineno}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.



% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.


% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at:
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and
% Axel Sommerfeldt. This package may be useful when used in conjunction with
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Deep Feature Aggregation and Image Re-ranking with Heat Diffusion for Image Retrieval}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Shanmin~Pang,
        Jin~Ma,
        Jianru~Xue,~\IEEEmembership{Member,~IEEE,}
        Jihua~Zhu,~\IEEEmembership{Member,~IEEE,}
        Vicente~Ordonez% <-this % stops a space
\thanks{S. Pang, J. Ma and J. Zhu are with the School of Software Engineering,
Xi'an Jiaotong University, Xi'an 710049, China.\protect
~E-mail: pangsm@xjtu.edu.cn; m799133891@stu.xjtu.edu.cn;  zhujh@xjtu.edu.cn.}% <-this % stops a space
\thanks{J. Xue is with the Institute of Artificial Intelligence and Robotics (IAIR), Xi'an Jiaotong University, Xi'an 710049, China.\protect
~E-mail: jrxue@xjtu.edu.cn.}
\thanks{V. Ordonez is with the  Dept. of Computer Science, University of Virginia, VA 22904, USA.\protect
~E-mail: vicente@virginia.edu.}}
%\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}

% note the % following the last \IEEEmembership and also \thanks -
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
%
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~**, No.~**, August~2018}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
%
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
Image retrieval based on deep convolutional features has demonstrated state-of-the-art performance in popular benchmarks. In this paper, we present a unified solution to address deep convolutional feature aggregation and image re-ranking by simulating the dynamics of heat diffusion.
A distinctive problem in image retrieval is that repetitive or \emph{bursty} features tend to dominate final image representations, resulting in representations less distinguishable.
We show that by considering each deep feature as a heat source, our unsupervised aggregation method is able to avoid over-representation of \emph{bursty} features. We additionally provide a practical solution for the proposed aggregation method and further show the efficiency of our method in  experimental evaluation.
Inspired by the aforementioned deep feature aggregation method, we also propose a method to re-rank a number of top ranked images for a given query image by considering the query as the heat source.
Finally, we extensively evaluate the proposed approach with  pre-trained and fine-tuned deep networks
on common public benchmarks and show superior performance compared to previous work.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Heat equation, Deep feature aggregation, Re-ranking, Image retrieval
\end{IEEEkeywords}



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


%\linenumbers
\section{Introduction}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
%
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
%
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
%
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
%
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
\IEEEPARstart{I}mage retrieval has always been an attractive research topic in the field of computer vision.
By allowing users to search similar images from a large database of digital images,
it provides a natural and flexible interface for image archiving and browsing. Convolutional Neural  Networks (CNNs) have shown remarkable accuracy in tasks such as image classification, and object detection. Recent research has also shown positive results of using CNNs
on image retrieval~\cite{babenko2015aggregating,kalantidis2016cross,hoang2017selective, chadha2017voronoi, zhang2018query}. However, unlike image classification approaches which often use global feature vectors produced by fully connected layers, %and are trained end-to-end
these methods extract local features depicting image patches from the outputs of convolutional layers and aggregate these features into \emph{compact} (a few hundred dimensions) image-level descriptors.
Once meaningful and representative image-level descriptors are defined, visually similar images are retrieved by computing similarities between pre-computed database feature representations and query representations.

Thus, a key step contained in these image retrieval methods is to compute global representations.
In order to generate distinguishable image-level descriptors, one has to avoid over-representing bursty (or repetitive) features during the aggregation process.
Inspired by an observation of similar phenomena in textual data, Jegou \textit{et al.} \cite{jegou2009burstiness} identified \emph{intra-image burstiness} as the phenomenon that numerous feature descriptors are almost identical within the same image\footnote{In the original publication \cite{jegou2009burstiness}, the authors define two kinds of burstiness: intra- and inter-image burstiness. Here,  we only study intra-image burstiness, and in what follows it is simplified as burstiness for short.}.
This phenomenon, which appears due to repetitive patterns (e.g., window patches in an image of a building's facade), is widely existed in images containing man-made objects.
Since bursty descriptors are often in large numbers, they may contribute much to the final image representations through inappropriate aggregation strategies such as sum pooling.
However, this is undesirable as such bursty descriptors may correspond to cluttered regions and consequently result in less distinguishable representations.
To address visual burstiness, Jegou \textit{et al.} \cite{jegou2009burstiness} proposed several re-weighting strategies that penalised
descriptors assigned to the same visual word within an image and penalised descriptors matched to many images.
Despite effectiveness, these strategies are only customized for the Bag-of-words (BoW) representation model that considers descriptors individually, and cannot be
combined with \emph{compact} representation models that aggregate local features into a global vector (\emph{i.e.}, do not consider descriptors individually).

In this manuscript, we develop a method to reduce the contribution of bursty features to final image representations in the aggregation stage.
We do so by revealing the relationship among features in an image.
Based on the property of burstiness, the sum of the similarity score between a bursty feature and the whole feature set would be large as there exist many other features identical (or nearly identical) to the bursty feature.
Whereas, the sum of the similarity score between a distinctive feature and the whole feature set tends to be small as it should be  dissimilar to other features.
To formulate this idea, we emulate deep features of an image as a heat system, where the sum of the similarity score is measured as system temperature. Specifically, for a certain feature, we consider it as the unique heat source, and compute the temperature of any other feature with the partial differential equation induced by the heat equation.
Consequently, we define system temperature obtained with this certain feature by summing temperatures of all features.
It is simple to understand that features leading to high system temperatures tend to be bursty ones, while features resulting in low system temperatures are distinctive ones.
Thus, in order to balance the contributions of bursty features and distinctive ones to the final image-level descriptor,
we compel the system temperatures derived from all features (heat sources) in one image to be a constant by introducing a set of weighting coefficients.


Heat diffusion, and more specifically anisotropic diffusion, has been used successfully in various image processing and computer vision tasks. Ranging from the classical work of Perona and Malik~\cite{perona1990scale} to further applications in image co-segmentation, image denoising, and keypoint detection~\cite{zhang2010diffusion,kim2011distributed,karpushin2016keypoint, cho2017geodesic}.
Here, we employ the system temperature induced by this well-known theory for measuring the similarity score between a certain deep convolutional feature and others due to the following two reasons.
First, as well-known, diffusing the similarity information in a weighted graph can measure similarities between different deep features more accurately compared to the pairwise cosine distances.
Second and more importantly, it inspires our second contribution and allows us to obtain considerable performance gains in the image re-ranking stage. Specifically, by considering global similarities that also take into  account the relation among the database representations, we propose a method to re-rank a number of top ranked images for a given query image using the query as the heat source.


Our contributions can be summarized as follows:

\begin{itemize}
  \item Feature weighting: By greedily considering each deep feature as a heat source and enforcing the temperature of the system be a constant within each heat source, we propose a novel efficient feature weighting approach to reduce the undesirable influence of \emph{bursty} features (Section \ref{sec:feature_weighting}).
  \item Image re-ranking: By considering each query image as a heat source, we also produce an image re-ranking technique that leads to significant gains in performance (Section \ref{sec:image_re-ranking}).
  \item We conduct extensive quantitative evaluations on commonly used image retrieval benchmarks, and demonstrate substantial performance improvement over existing unsupervised methods for feature aggregation (Section~\ref{sec:experiments}).
\end{itemize}


The remainder of this manuscript is organized as follows: We briefly overview representative works that close to us in Section~\ref{sec:related_work}.
The details of the proposed feature weighting strategy are described in Section~\ref{sec:feature_weighting}, while the proposed image re-ranking method is presented in Section~\ref{sec:image_re-ranking}. Experimental results are described and discussed in Section~\ref{sec:experiments}, and conclusions are drawn in Section~\ref{sec:conclusions}.

\section{Related Work}\label{sec:related_work}
Since both our deep feature aggregation and image-ranking methods are built on heat diffusion, we therefore first review classical diffusion methods in the computer vision field.
Second, we review representative deep learning based image retrieval methods as this paper also aims to address the image retrieval problem with convolutional neural networks.

\subsection{Diffusion for Computer Vision}
Anisotropic diffusion has been applied to  many computer vision
problems, such as image segmentation~\cite{zhang2010diffusion,kim2011distributed}, saliency detection~\cite{lu2014learning, chen2016discriminative}, and clustering~\cite{donoser2013replicator, pang2018large}.
In these applications, diffusion is used for finding central points by capturing the intrinsic manifold structure of the data.
Our deep feature aggregation method models the problem in the opposite direction, and we differentiate them by weakening deep features that are densely connected to other features with high similarities.

Diffusion is also popular in the context of retrieval~\cite{egozi2010improving, yang2009locally, donoser2013diffusion, furuya2015diffusion, iscen2017efficient}. Among them, the approaches~\cite{egozi2010improving, yang2009locally, furuya2015diffusion} addressed the shape retrieval problem, and performed diffusion on image level. While we focus on the instance-level retrieval problem, and we carry out heat diffusion on deep convolutional features.
Donoser and Bischof~\cite{donoser2013diffusion} reviewed a number of diffusion
mechanisms for retrieval. They focused on iterative solutions
arguing that closed form solutions, when existing, were impractical
due to inversion of large matrices.
However, we rather focus on a closed form solution without iteration as in our case the number of features and the number of re-ranking images is small.
Recently, Iscen \textit{et al.}~\cite{iscen2017efficient} introduced a regional diffusion mechanism on image
regions for better measuring similarities between images. Compared with this method, our re-ranking method is much efficient as we re-rank images based on global level image vectors.

Democratic Diffusion Aggregation (DDA)~\cite{gao2016democratic} is probably the most closest to our feature aggregation method  as it also handled the bursts problem by diffusion.
However, there exists at least a distinctive difference between our method and DDA.
Specifically, we start from the heat equation, and balance the
influence between rare features and frequent ones by enforcing
the system temperatures obtained with different heat sources be a constant.
While DDA inherited from Generalized Max-Pooling (GMP)~\cite{murray2017interferences}, which equalized the contribution of a single descriptor to the aggregated vector.
Furthermore, we provide a unified solution to feature aggregation and image re-ranking, which is otherwise not possible by \cite{gao2016democratic}.

\subsection{Deep Learning for Image Retreival}
Early attempts to use deep learning for image retrieval considered the use of the activations of fully connected layers as image-level descriptors~\cite{babenko2014neural,razavian2014cnn,gong2014multi}.
In~\cite{babenko2014neural}, a global representation was derived from the output of the penultimate layer.
This work was among the first to show better performance than traditional methods based on SIFT-like features at the same dimensionality.
Concurrently, Gong \textit{et al.}~\cite{gong2014multi} extracted multiple fully-connected activations by partitioning images into fragments, and then used VLAD-embeddings~\cite{jegou2010aggregating} to aggregate the activations into a single image vector.
The work~\cite{razavian2014cnn} reported promising results using sets of a few dozen
features from the fully-connected layers of a CNN, without aggregating them into a global descriptor. However, observing that neural activations of the lower layers of a CNN capture more spatial details, later works advocated using the outputs of convolutional layers as features~\cite{babenko2015aggregating,kalantidis2016cross,hoang2017selective,tolias2016particular,azizpour2015generic,razavian2016visual,noh2017largescale,xu2018unsupervised, pang2018building}.
These convolutional features were subsequently used for similarity computation either with individual feature matching~\cite{noh2017largescale} or with further aggregation steps~\cite{babenko2015aggregating,kalantidis2016cross,hoang2017selective,tolias2016particular}.
In this work, we consider convolutional features as local features, and
aggregate them into a global image descriptor.

Considerable effort has been dedicated to aggregating the activations of convolutional layers into a distinctive global image vector.
For instance, \cite{azizpour2015generic,razavian2016visual} evaluated image-level descriptors obtained using max-pooling over the last convolutional layer, while Babenko and Lempitsky~\cite{babenko2015aggregating} showed that sum-pooling leads to better performance.
Kalantidis \textit{et al.}~\cite{kalantidis2016cross} further proposed a non-parametric method to learn weights for both spatial locations and feature channels.
Related to that, Hoang \textit{et al.}~\cite{hoang2017selective} proposed several masking schemes to select a representative subset of local features before aggregation,
and achieved satisfactory results by taking advantage of the triangulation embedding~\cite{jegou2014triangulation}.
Similarly, we proposed a deep feature selection and weighting method using the replicator equation in our very recent work~\cite{pang2018building}.
In another work, Tolias \textit{et al.}~\cite{tolias2016particular} computed a collection of region vectors with max-pooling on the final convolutional layer, and then combined all region vectors into a final global representation.
More recently, Xu \textit{et al.}~\cite{xu2018unsupervised} independently employed selected part detectors to generate regional representations with weighted sum-pooling~\cite{kalantidis2016cross}, and then concatenated regional vectors as the global descriptor. In this paper, we instead propose heat diffusion to weight and then aggregate deep feature descriptors.

Fine-tuning an off-the-shelf network is also popular for improving retrieval quality.
For instance, there are a number of approaches that learn features for the specific task of landmark retrieval~\cite{radenovic2016cnn,gordo2016deep,arandjelovic2016netvlad,cao2016quartet}.
While fine-tuning a pre-trained model is usually preceded by extensive manual annotation,
Radenovic \textit{et al.}~\cite{radenovic2016cnn} introduced an unsupervised fine-tuning of CNN for image retrieval from a large collection of unordered images in a fully automated manner.
Similar to this work, the methods presented in~\cite{arandjelovic2016netvlad,cao2016quartet} overcame laborious annotation, and collected training data in a weakly supervised manner.
More specifically, Arandjelovic \textit{et al.}~\cite{arandjelovic2016netvlad} proposed a new network architecture, NetVLAD, that was trained for place recognition in an end-to-end manner from weakly supervised Google Street View Time Machine images. Cao \textit{et al.}~\cite{cao2016quartet} trained a special architecture Quartet-net by harvesting data automatically from GeoPair~\cite{thomee2016yfcc100m}. We show that our feature weighting, and image re-ranking approach, while not requiring extra supervision, performs favorably compared to these previous methods.

In a couple of very recent works~\cite{iscen2017efficient,iscen2017fast},
images were represented by multiple high-dimensional regional vectors.
These two approaches achieve great performance on common benchmarks, they are however computationally demanding, both in terms of
memory and computational usage. In contrast, our work uses a single vector representation while achieving similar performance.

\section{Feature Weighting with the Heat Equation}\label{sec:feature_weighting}
Given an input image $I$ that is fed through a pre-trained or a fine-tuned CNN,
the activations (responses) of a convolutional layer form a 3D tensor
$\boldsymbol{X} \in \mathbb{R}^{W\times H \times K}$, where $W \times H$ is the spatial resolution of the feature maps,
and $K$ is the number of feature maps (channels).
We denote $\mathcal {V}= \left\{\boldsymbol{f}_{l}\right\}$ as a set of $W \times H$ local features,
where $\boldsymbol{f}_{l}$ is a $K$-dimensional vector  at spatial location $(i,j)$  in $\boldsymbol{X}$.
That is to say,~$l = i+(j-1)\times W$, where $1 \leq i \leq W$ and $1 \leq j \leq H$.
We assume that Rectified Linear Units (ReLU) are applied as a last step,
guaranteeing that all elements of $\boldsymbol{f}_{l}$ are non-negative.

\subsection{Problem Formulation}
We utilize the theory of anisotropic diffusion~\cite{weickert1998anisotropic}
to compute weights for each feature in $\mathcal {V}$  based on their distinctiveness, thus avoiding
the burstiness issue. Let us assume that the deep feature point set $\mathcal {V} = \{\boldsymbol{f}_l\}$ constitute an undirected graph. By assigning $\boldsymbol{f}_l$ as the unique heat source, we assume that the graph constitutes a  heat transfer system, and the \emph{linear heat equation} for this system is defined as follows~\cite{weickert1998anisotropic}:
\begin{equation}
\partial_t\boldsymbol{\mu}_l = \text{div}(\boldsymbol{P}\nabla \boldsymbol{\mu}_l),
\end{equation}
where $\boldsymbol{\mu}_l = (\mu_l(1,t), \mu_l(2,t),\ldots, \mu_l(|\mathcal {V}|,t))^{T}$, and its $m$-th entry $\mu_l(m,t)$ represents the temperature at node $\boldsymbol{f}_m$ at time $t$.
$\boldsymbol{P} = (P(m,n))_{|\mathcal {V}|\times |\mathcal {V}|}$ is a positive definite symmetric matrix
called the \emph{diffusion tensor},  and $P(m,n)$ denotes the $(m,n)$-th diffusion coefficient reflecting the interactions between the feature pair $\boldsymbol{f}_m$ and $\boldsymbol{f}_n$.

Our problem is to compute the temperature at each node $\boldsymbol{f}_m$.
That is,
\begin{equation}\label{heat_equation1}
\left\{
\begin{array}{ll}
\frac{\partial \mu_{l}(m,t)}{\partial t}= \text{div}(\boldsymbol{P}\nabla \mu_l(m,t)),\\
s.t.~\mu_l(g)=0,~\mu_l(l)=1,
\end{array}
\right.
\end{equation}
where we use the \emph{Dirichlet} boundary conditions~\cite{weickert1998anisotropic},
and assume that the temperature of the environment node (outside of the system) and the source node
is always zero (i.e., $\mu_l(g)=0$) and one (i.e., $\mu_l(l)=1$), respectively.


In practice, we compute the temperature at each node with the following simplified assumptions.
Specifically, we let $t\rightarrow+\infty$ and consequently drop $t$ in our method
as we are interested in the steady state, and define the diffusion tensor $\boldsymbol{P}$
by the \emph{cosine} similarity between deep feature vectors:
\begin{equation}
 P(m,n)= \left\{
\begin{array}{ll}
0, & m=n,\\
\frac{\boldsymbol{f}_m^{T}\boldsymbol{f}_n}{\|\boldsymbol{f}_m\|\|\boldsymbol{f}_n\|}, & m\neq n.
\end{array}
\right.
\end{equation}
Furthermore, we assume that the dissipation heat loss at a node $\boldsymbol{f}_m$ is $\lambda_m$, which is constant in time.
In other words, each node $\boldsymbol{f}_m \in \mathcal {V} \setminus f_l$ is connected to an environment node $g$ with diffusivity of $\lambda_m$.
With these assumptions, the heat diffusion Eq.(\ref{heat_equation1}) reduces to the simplified version~\cite{weickert1998anisotropic,grady2006random}:
\begin{equation}\label{heat_equation2}
\left\{
\begin{array}{ll}
\mu_l(m)= \frac{1}{a_m}\sum P(m,n)\mu_l(n),\\
s.t.~\mu_l(g)=0,~\mu_l(l)=1,
\end{array}
\right.
\end{equation}
where $a_m = \sum P(m,n) + \lambda_m,~m \neq g, l$. Without loss of generality, we assume $\boldsymbol{P} = (0, \boldsymbol{P}_1^{T}; \boldsymbol{P}_1, \boldsymbol{P}_2)$,
where $\boldsymbol{P}_1 \in \mathbb{R}^{(|\mathcal {V}|-1)\times 1}$ stores the similarities between points in $\mathcal {V} \setminus \boldsymbol{f}_l$ and $\boldsymbol{f}_l$,  and $\boldsymbol{P}_2 \in \mathbb{R}^{(|\mathcal {V}|-1)\times (|\mathcal {V}|-1)}$
stores the similarities between any two pair points in $\mathcal {V} \setminus \boldsymbol{f}_l$.
Then, Eq.(\ref{heat_equation2}) can be rewritten as
\begin{equation}\label{heat_equation3}
\boldsymbol{\mu}_l = (\boldsymbol{I}_{|\mathcal {V}|-1}-\boldsymbol{\Lambda}^{-1}_{l}\boldsymbol{P}_2)^{-1}(\boldsymbol{\Lambda}^{-1}_{l}\boldsymbol{P}_1),
\end{equation}
where $\boldsymbol{I}_{|\mathcal {V}|-1}$ is the identity matrix of size $(|\mathcal {V}|-1) \times (|\mathcal {V}|-1)$, and $\boldsymbol{\Lambda}_l = \text{diag}(a_1,\ldots, a_{l-1}, a_{l+1}, \ldots, a_{|\mathcal {V}|})$ is the diagonal matrix.
Thus, the temperature of the system induced by the heat source $\boldsymbol{f}_l$ is defined as
\begin{equation}\label{psi_l}
\psi_l = \sum_{m=1}^{|\mathcal {V}|}\mu_l(m).
\end{equation}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.7in]{Fig/ox21_0.pdf}
    \includegraphics[width=1.72in]{Fig/ox21_w.pdf}
    \includegraphics[width=1.7in]{Fig/ox34_0.pdf}
    \includegraphics[width=1.73in]{Fig/ox34_w.pdf}
    \caption{Two example visualization results of the proposed method with deep features extracted using the SiaMAC$\dagger$~\cite{radenovic2016cnn} CNN model. Left: the original image; Right: relative weights (warmer colors indicate larger weights) for deep convolutional features. For instance, in the top image,
    our feature weighting method assigns larger weights to features corresponding to the distinctive area of tower, and smaller weights to ones corresponding to the repetitive areas of grass and sky.}\label{fig:visualizing_results}
\end{figure}

We greedily consider each point $\boldsymbol{f}_l \in \mathcal {V}$ as a heat source, and compute the corresponding system temperature $\psi_l$ under the linear anisotropic diffusion equation.
The value of $\psi_l$ can indicate whether $\boldsymbol{f}_l$ is a bursty feature or a distinctive one.
As described earlier, a bursty feature tends to be identical (or nearly identical) to many other features, whereas a distinctive feature is prone to be dissimilar to other features.
This means a bursty feature is densely connected to other features with high similarities, which consequently rises the temperature of the system.
In contrast, a distinctive feature connects other features sparsely and therefore causes the system temperature to be low.
Thus, in order to balance the influence between bursty features and distinctive ones,
we compel the system temperatures derived from all features (heat sources) in one image to be a constant by introducing a set of weighting coefficients $w_l$.
That is,
\begin{equation}
w_l\times \psi_l = 1, ~\forall \boldsymbol{f}_l \in \mathcal {V}.
\end{equation}

As a result, $w_l$ is used to reduce the burstiness effects,
and we accordingly compute the final image representation $\boldsymbol{v}_I$ of each image $I$ by
\begin{equation}\label{representation_equation}
\boldsymbol{v}_I = \frac{\left(\sum_{l=1}^{|\mathcal {V}|}w_l\boldsymbol{f}_l\right)^{\alpha}}{\left\|\left(\sum_{l=1}^{\mathcal {V}}
w_l\boldsymbol{f}_l\right)^{\alpha}\right\|},
\end{equation}
where $0 < \alpha \leq 1$ is a constant, and we typically set $\alpha = 0.5$.~$\alpha$ plays the same role as the exponent parameter in the Power-law Normalization (PN) formula~\cite{perronnin2010improving}.
However, it is worth noting that, we apply $\alpha$-normalisation on the image vector \emph{before} PCA whitening,
while PN is integrated in the retrieval frameworks~\cite{murray2017interferences,do2017embedding}
\emph{after} rotating the image representation with a PCA rotation matrix. Fig.~\ref{fig:visualizing_results} visualizes the weights computed for two sample input images, larger weights are shown in warmer colors. As shown, our feature weighting method assigns larger weights to distinctive areas, and smaller weights to repetitive ones.

For convenience, in the following we denote our heat equation based feature weighting method presented in this section as HeW.


\subsection{Computing Weights in Practice}
It seems that we have to solve Eq.(\ref{heat_equation3}) $|\mathcal {V}|$ times to get the image representation of $I$, and each time
we need to solve a linear equation of size $(|\mathcal {V}|-1)\times (|\mathcal {V}|-1)$.
Thus, the total time cost of HeW is about in $O(|\mathcal {V}|^4)$.
This might be computationally intensive if the selected feature set cardinality $|\mathcal {V}|$ is large.

However, the actual time complexity can be reduced to $O(|\mathcal {V}|^3)$,
and we can compute all $\psi_l$ by inverting the matrix  $(\boldsymbol{I}_{|\mathcal {V}|}-\boldsymbol{\Lambda^{-1} P})$ only once, where $\boldsymbol{\Lambda} = \text{diag}(a_1,\ldots,  a_{|\mathcal {V}|})$.
Specifically, we take computing $\psi_{1}$ as an example to illustrate the practical computational process.
We leverage the block structure of $(\boldsymbol{I}_{|\mathcal {V}|}-\boldsymbol{\Lambda^{-1} P})$, i.e.,
\begin{equation}
\boldsymbol{I}_{|\mathcal {V}|}-\boldsymbol{\Lambda^{-1} P} =
{\left[ \begin{array}{cc}
1 & -\boldsymbol{x}^{T}\\
-\boldsymbol{y} & \boldsymbol{Q}
\end{array}
\right]},
\end{equation}
where $\boldsymbol{x}$ and $\boldsymbol{y}$ are vectors of size $|\mathcal {V}|-1$, and $\boldsymbol{Q}$ is the matrix of size
$(|\mathcal {V}|-1)\times (|\mathcal {V}|-1)$. According to Eqs. (\ref{heat_equation3}) and (\ref{psi_l}),
it is simple to know that
\begin{equation}
\psi_1 = \sum\boldsymbol{Q}^{-1}\boldsymbol{y} +1.
\end{equation}
By leveraging the property of the block matrix, we can derive that  $(\boldsymbol{I}_{|\mathcal {V}|}-\boldsymbol{\Lambda^{-1} P})^{-1} =$
\begin{equation}{\left[ \begin{array}{cc}
1+\boldsymbol{x}^{T}(\boldsymbol{Q}-\boldsymbol{yx}^{T})^{-1}\boldsymbol{y}~&~ \boldsymbol{x}^{T}(\boldsymbol{Q}-\boldsymbol{yx}^{T})^{-1}\\
(\boldsymbol{Q}-\boldsymbol{yx}^{T})^{-1}\boldsymbol{y} ~&~ (\boldsymbol{Q}-\boldsymbol{yx}^{T})^{-1}
\end{array}
\right]}.
\end{equation}
Furthermore, one can prove that
\begin{equation}
\boldsymbol{Q}^{-1}\boldsymbol{y} = \frac{(\boldsymbol{Q}-\boldsymbol{yx}^{T})^{-1}\boldsymbol{y}}{1+\boldsymbol{x}^{T}(\boldsymbol{Q}-\boldsymbol{yx}^{T})^{-1}\boldsymbol{y}}.
\end{equation}
The above three equations
demonstrate that we can derive $\psi_1$ by using the first column of $(\boldsymbol{I}_{|\mathcal {V}|}-\boldsymbol{\Lambda^{-1} P})^{-1}$.
Similarly, we can get $\psi_l$ using the $l$-th column of $(\boldsymbol{I}_{|\mathcal {V}|}-\boldsymbol{\Lambda^{-1} P})^{-1}$:
\begin{equation}
\psi_l = \frac{\sum_{m=1,m\neq l}^{|\mathcal {V}|}(\boldsymbol{I}_{|\mathcal {V}|}-\boldsymbol{\Lambda^{-1} P})^{-1}(m,l)}{(\boldsymbol{I}_{|\mathcal {V}|}-\boldsymbol{\Lambda^{-1} P})^{-1}(l,l)}+1.
\end{equation}
Thus, the conclusion that the computational cost is in $O(|\mathcal {V}|^3)$ holds.

For very large-scale image retrieval, the time complexity in  $O(|\mathcal {V}|^3)$ might still be computationally intensive as we need to process too many images. However, it is worth noting that database representations are \emph{pre-computed} in a retrieval system, and therefore image representation efficiency is not so important.
Furthermore, as each database image can be dealt with independently, one can take advantage of the parallel processing technique (with a multi-core computer or with multiple computers, or even with both) for fast computing image-level descriptors of database images.

\section{Image Re-ranking with the Heat Equation}\label{sec:image_re-ranking}
Inspired by our deep feature aggregation method HeW, we propose a heat equation based re-ranking approach HeR in this section.
Given a query image $q$, we denote its image vector produced by HeW or other potential image representation methods as $\boldsymbol{v}_q$. After querying the database, we get a ranked list of images $I_1, I_2, \ldots, I_k, \ldots$ for the query $q$, where $I_k$ is the $k$-th top ranked image. Similarly, we denote the global image vector of $I_k$ as $\boldsymbol{v}_k$.

We consider the query as the heat source, and re-rank top-ranked $k$ images by computing their temperatures with the linear anisotropic diffusion equation Eq.(\ref{heat_equation1}).
For simplicity, as performed in the previous section, we also use the following more specific equation:
\begin{equation}\label{eq:re-ranking}
\boldsymbol{\mu}_q = (\boldsymbol{I}_{k}-\boldsymbol{\Lambda}^{-1}_{q}\boldsymbol{Q}_2)^{-1}(\boldsymbol{\Lambda}^{-1}_{q}\boldsymbol{Q}_1),
\end{equation}
to compute temperature gains of each image, where $\boldsymbol{Q}_2 \in \mathbb{R}^{k\times k}$ contains the cosine similarity scores among image vectors $\boldsymbol{v}_1, \ldots, \boldsymbol{v}_k$,
and $\boldsymbol{Q}_1 \in \mathbb{R}^{k\times 1}$ represents the similarity vector with the $m$-th entry storing the similarity between $\boldsymbol{v}_q$ and $\boldsymbol{v}_m$ ($m=1, 2, \ldots, k$).\footnote{In practice, before computing the similarity between any two different vectors, we first center the set of image vectors. That is, $\boldsymbol{v}_q := \boldsymbol{v}_q- \frac{1}{k+1}(\boldsymbol{v}_q+\sum_{i=1}^{k}\boldsymbol{v}_i)$, $\boldsymbol{v}_m := \boldsymbol{v}_m- \frac{1}{k+1}(\boldsymbol{v}_q+\sum_{i=1}^{k}\boldsymbol{v}_i)$. }
Additionally,  $\boldsymbol{\Lambda}_{q}$ is the diagonal matrix that is similar to $\boldsymbol{\Lambda}_{l}$ appeared in Eq.(\ref{heat_equation3}), and $\boldsymbol{\mu}_q \in \mathbb{R}^{k\times 1}$ with the $m$-th entry $\mu_q(m)$  denoting the temperature gain of $I_m$.
Apparently, we re-rank $I_1, I_2, \ldots, I_k$ based on $\boldsymbol{\mu}_q$, and images with larger temperature gains are ranked higher.

The additional computational burden of both memory usage and running time introduced by HeR is negligible. In fact, HeR is computed on image-level descriptors, and it only refines a shortlist of the top $k$ best results.
This means we only need to store $k+1$ image vectors (query as well as $k$ results), and matrices of size $k \times k$ contained in Eq.(\ref{eq:re-ranking}). In practice, both image vector dimensions and $k$ are relatively small (a few hundreds), showing the low memory usage of HeR. Furthermore, computing $\boldsymbol{\mu}_q$ is very fast with $k=800$ (adopted in our experiments), and we observe that the actual computing time on our platform is less than 30ms.

\section{Experiments and Results}\label{sec:experiments}
This section describes the implementation of our method, and reports results on public image retrieval benchmarks\footnote{Our code is available at https://github.com/MaJinWakeUp/HeWR.}.
Throughout the section, we normalize the final image vector to have unit Euclidean norm.

\subsection{Datasets and Evaluation Protocol}
We evaluate our method on three public datasets: \textbf{Oxford Buildings} (Oxford5k)~\cite{philbin2007object}, \textbf{Paris} (Paris6k)~\cite{Philbin08lost}, and \textbf{INRIA Holidays} (Holidays)~\cite{jegou2010improving}.

\vspace{0.01in}
\textbf{Oxford5k} contains a set of 5,062 photographs comprising 11 different Oxford landmarks.
There are 55 query images with each 5 queries corresponding to a landmark.
The ground truth similar images with respect to each query is provided by the dataset.
Following the standard protocol, we crop the query images based on the provided bounding boxes before retrieval.
The performance is measured using mean average precision (mAP) over the 55 queries,
where \emph{junk} images are removed from the ranking.

\vspace{0.01in}
\textbf{Paris6k} consists of 6,392 high resolution images of the city Paris.
Similar to Oxford5k, it was collected images from Flickr by querying the associated
text tags for famous Paris landmarks.
Additionally, this dataset also provides 55 query images and their corresponding ground truth relevant images.
We also use cropped query images to perform retrieval, and measure the overall retrieval performance using mAP.

\vspace{0.01in}
\textbf{Holidays} includes 1,491 images in total,
and selects 500 images as queries associated with the 500 partitioning groups of the image set. To be directly comparable with recent works~\cite{babenko2015aggregating,kalantidis2016cross,hoang2017selective},
we manually fix images in the wrong orientation by rotating them by $\pm 90$ degrees.
The retrieval quality is also measured using mAP over 500 queries, with the query removed from the ranked list.

\vspace{0.01in}
\textbf{Flickr100k}~\cite{philbin2007object} was crawled from Flickr's 145 most popular tags and consists of 11,071 images.
We combine these 100k distractor images with Oxford5k and Paris6k, and produce Oxford105k and Paris106k datasets respectively.
In this way, we evaluate the behavior of our method at a larger scale.

\begin{table*}[t]
\centering
\caption{Computational cost for the considered combinations on Oxford105k. We measure image representation time with images of size $1,024 \times 768$ (the number of features is typically $3,072$). For image representation time, we do not include time for feature extraction.}\label{times}
\begin{tabular}{|l|c|c c c c c|}
  \hline
  \multirow{2}{*}{Method}&\multirow{2}{*}{Representation time}   & \multicolumn{5}{c|}{Query time with varied image vector dimensions}\\
  \cline{3-7}
  &  &32  &64  &128 &256  &512  \\
  \hline
  SumA&13ms  &\multirow{2}{*}{8ms}  &\multirow{2}{*}{9ms}  &\multirow{2}{*}{11ms} &\multirow{2}{*}{13ms}  &\multirow{2}{*}{15ms}  \\
  HeW &173ms   & & & & & \\
  \hline
  SumA+QE+HeR&13ms &\multirow{2}{*}{45ms} &\multirow{2}{*}{47ms} &\multirow{2}{*}{48ms} &\multirow{2}{*}{50ms}  &\multirow{2}{*}{52ms}  \\
  HeW+QE+HeR&173ms & & & & & \\
  \hline
\end{tabular}
\end{table*}
\subsection{Implementation Notes}

\textbf{Deep convolutional features.} In order to extensively evaluate our method,
we use two pre-trained and a fine-tuned deep neural networks to extract multiple deep convolutional features for each image.
The adopted pre-trained networks are VGG16~\cite{simonyan2014very} and ResNet50~\cite{he2016deep}, which are widely used in the literature.
The fine-tuned network is siaMAC$\dagger$~\cite{radenovic2016cnn}, a popular fine-tuned model of VGG16.

Following the practice of previous works~\cite{kalantidis2016cross,radenovic2016cnn},
we choose the last convolutional layer of each network to separately extract patch-level image features.
We use public available trained models. Specifically, we use the MatConvNet toolbox~\cite{vedaldi2015matconvnet} for VGG16 and ResNet50,
and use the model provided in~\cite{arandjelovic2016netvlad}  for siaMAC$\dagger$. In addition, in order to accelerate feature extraction,
we resize the longest side of all images to 1,024 pixels while preserving aspect ratios before feeding them  into each deep network.

\vspace{0.01in}
\textbf{PCA whitening} is widely used in many image retrieval systems~\cite{jegou2012negative,kalantidis2016cross,hoang2017selective,babenko2015aggregating} as it can effectively improve the discriminative ability of image vectors.
In order to avoid over-fitting,
the PCA matrix is usually learned with the set of aggregated image vectors produced from a held-out dataset.
To be directly comparable with related works, we learn PCA parameters on Paris6k for Oxford5k and Oxford105k,
and on Oxford5k for Paris6k and Paris106k.
As for Holidays, we randomly select a subset of 5,000 images from Flickr100k to learn parameters.

\vspace{0.01in}
\textbf{Query Expansion (QE)}~\cite{chum2007total} is an effective post-processing technique to increase retrieval performance.
Given the ranked list of database images over a query image, we simply calculate the average vector of the 10 top-ranked image vectors and the query vector, and we then use the $L_2$ normalized average vector to re-query again.
After QE, we then apply our re-ranking algorithm HeR to further improve retrieval performance.
We will show the combination of QE and HeR is beneficial in practice.

\subsection{Impact of the Parameters} 
We investigate the impact of the parameter $k$ as well as the impact of the final image vector dimensionality
to different retrieval frameworks with deep convolutional features extracted by the network siaMAC$\dagger$ on the datasets of Oxford5k and Oxford105k.
Specifically, we evaluate their impact to
the following four combinations: SumA, HeW, SumA+QE+HeR and HeW+QE+HeR.
SumA means we obtain image representations by simply setting $w_l \equiv 1$ in Eq.(\ref{representation_equation}), and perform image search by linearly scanning database vectors.
It justifies the contribution of our deep feature aggregation method HeW.
SumA+QE+HeR and HeW+QE+HeR indicate that we perform re-ranking QE+HeR with image vectors produced by SumA and HeW respectively.
We use them to determine the parameter $k$ that is introduced in HeR.
Meanwhile, by setting $k=0$, they can also be used for evaluating the effect of our image re-ranking strategy HeR.


\begin{figure}[t]
\centering
\includegraphics[width=1.72in]{Fig/ox5k_k.pdf}
\includegraphics[width=1.72in]{Fig/ox105k_k.pdf}
\caption{Impact of the number of re-ranking images to retrieval quality. The image vector dimensions $D=512$, and it is worth noting that  HeW+QE+HeR (SumA+QE+HeR) degenerates to  HeW+QE (SumA+QE) when $k=0$.}
\label{fig:k}
\end{figure}

\begin{table*}[t]
\begin{center}
\caption{Impact of the networks to the considered methods. We do not perform re-ranking on Holidays as as it is not a standard practice.}\label{comparison_with_baseline}
\begin{tabular}{|l|c|c| c c c c c|}
  \hline
  Network                    &Method             &~Dim.~  &Oxford5k  &Oxford105k  &Paris6k  &Paris106k &Holidays \\
  \hline
  \hline
  \multirow{10}{*}{ResNet50} &SumA               &2,048    &71.7  &65.9    &83.0     &76.8          &89.4\\
                             &HeW                &2,048    &72.1  &66.2    &84.5     &78.6          &\textbf{90.1}  \\
                             &SumA+QE            &2,048    &76.2  &71.9    &87.8     &81.2          &-- \\
                             &HeW+QE             &2,048    &77.2  &73.4    &89.7     &83.5          &--  \\
                             &HeW+QE+HeR         &2,048    &80.1  &75.4    &91.0     &85.1          &--  \\
                             \cline{2-8}
                             &SumA               &512     &59.6  &52.8    &81.1     &72.8          &88.1 \\
                             &HeW                &512     &61.1  &54.4    &82.9     &75.0          &88.3  \\
                             &SumA+QE            &512     &61.5  &55.3    &84.2     &77.8          &-- \\
                             &HeW+QE             &512     &63.2  &57.5    &86.5     &80.2          &--  \\
                             &HeW+QE+HeR         &512     &67.7  &62.0    &90.1     &82.3          &-- \\
  \hline
  \multirow{5}{*}{VGG16}     &SumA               &512     &71.0  &66.0        &80.6     &73.6      &87.8  \\
                             &HeW                &512     &72.8  &68.0        &81.5     &74.4      &88.4  \\
                             &SumA+QE            &512     &76.5  &74.3        &85.5     &81.5      &--  \\
                             &HeW+QE             &512     &77.8  &75.3        &87.0     &82.7      &--  \\
                             &HeW+QE+HeR         &512     &82.0  &78.7        &91.2     &86.1      &--  \\
  \hline
  \multirow{5}{*}{SiaMAC$\dagger$} &SumA         &512    &80.8  &76.5        &86.3     &80.6      &86.3 \\
                             &HeW                &512    &82.6  &78.8        &87.0     &81.3      &87.1  \\
                             &SumA+QE            &512    &87.3  &85.2        &89.4     &85.2      &--  \\
                             &HeW+QE             &512    &88.8  &87.0        &90.7     &86.6      &--  \\
                             &HeW+QE+HeR         &512    &\textbf{92.0}  &\textbf{90.3}        &\textbf{94.3}     &\textbf{90.2}   &--\\
  \hline
\end{tabular}
\end{center}
\end{table*}

\vspace{0.01in}
\textbf{Impact of the number of re-ranking images.} We first use the retrieval frameworks SumA+QE+HeR and HeW+QE+HeR to evaluate the impact of $k$ to retrieval quality with full image vector dimensions $D=512$.
The mAP performance for the considered two frameworks on Oxford5k and Oxford105k under different $k$ values is shown in Fig.~\ref{fig:k}, where $k=0$ means HeR is not applied.
As we see,  HeR consistently improves the retrieval quality, and the margin is around 3\% for the  two largest $k$ values.
The best results for HeW+QE+HeR and SumA+QE+HeR on Oxford5k and Oxford105k are achieved at $k=600$ and $k=800$,
respectively. Since $k=800$ gives overall better results than $k=600$, therefore it is used in the following experiments.

\vspace{0.01in}
\textbf{Impact of the final image vector dimensionality.} With $k=800$, we illustrate mAP curves when varying the dimensionality of the final image vectors from 32 to 512 in Fig.~\ref{fig:dimensions}.
Dimensionality reduction is achieved by keeping only the first $D$ components of 512 dimensional image vectors after PCA whitening.
As illustrated, the gain of HeW over SumA is nearly 2\% in mAP on both Oxford5k and Oxford105k at $D= 512$.
Furthermore, the performance gain is increasing with the reduction of the number of dimensions, and the gain is around 4\% at $D= 32$.
This means our image representation method HeW affects less by dimensionality reduction than the baseline SumA.

Both HeW and SumA are significantly benefited with image re-ranking, especially with image vectors of higher dimensions.
As shown, when $D=512$, the increased mAP values of HeW+QE+HeR (SumA+QE+HeR) over HeW (SumA) on Oxford5k and Oxford105k are 9.4\% (9.7\%) and 11.5\% (12.1\%), respectively.
After incorporating re-ranking, the performance advantage of HeW over SumA is enlarged at low dimensions.
For instance, the increased mAP values of HeW+QE+HeR over SumA+QE+HeR on Oxford5k and Oxford105k at $D= 32$  are 4.7\% and 4.6\%, respectively.

\begin{figure}[t]
\centering
\includegraphics[width=1.72in]{Fig/ox5k.pdf}
\includegraphics[width=1.72in]{Fig/ox105k.pdf}
\caption{Impact of the the final image vector dimensionality to the considered combinations. Short representations are achieved by keeping only the first components of 512 dimensional image vectors after PCA whitening.}
\label{fig:dimensions}
\end{figure}

\vspace{0.01in}
\textbf{Computational cost.}
We now turn to present running time for the considered combinations on Oxford105k. Table~\ref{times} reports timings (excluding time for feature extraction) measured to compute image representations and to perform image querying. We implement both SumA and HeW in Matlab, and benchmarks are obtained on an Intel Xeon E5-2630/2.20GHz with 20 cores.
As the table shows, although the baseline method SumA is  faster than HeW by about an order of magnitude, HeW is still fast in practice.
For an image of high resolution $1,024 \times 768$, the number of deep features is $3,072$,  and the time to derive image vector with HeW is typically 173ms. It is worth noting that, the number of features extracted by siaMAC$\dagger$ is four times of that extracted by VGG16 and ResNet50.
This means aggregating features produced VGG16 and ResNet50 is much faster.
In practice, we observe that, we can accomplish feature aggregation in less than 10ms with features extracted by both VGG16 and ResNet50.

It is simple to know that HeW does not affect search efficiency, and it has the same searching time as SumA. Thus, the online query time for HeW is about 188ms at $D=512$.
The increased time caused by incorporating image re-ranking into the retrieval system is very limited. As shown, the total online processing time for HeW+QE+HeR with $D=512$ is only about 225ms. This means searching with our method is efficient in practice.

\begin{table*}[t]
\begin{center}
\caption{Performance (in mAP) comparison with methods using SIFT and off-the-shelf available networks.
As many previous works, we do not perform re-ranking on Holidays as each query only has a few ground truth similar images.
}\label{comparison_with_unsupervised}
\begin{tabular}{|c|l|r|c c c c c|}
  \hline
  Feature &Method                                 &~Dim.~  &Oxford5k  &Oxford105k  &Paris6k  &Paris106k &Holidays \\
  \hline
  \hline
  \multirow{2}{*}{{SIFT}}
   &F-FAemb~\cite{do2017embedding}         &7,245 &66.1      &64.3        &--       &--        &75.5     \\
   &Temb~\cite{murray2017interferences}    &8,064 &70.0      &64.4        &--       &--        &71.6     \\
  \hline
  Fully&MOP-CNN~\cite{gong2014multi}           &2,048   &--        &--          &--       &--        &80.8      \\
  connected &Neural codes~\cite{babenko2014neural}  &4,096   &54.5      &51.2        &--       &--        &79.3        \\
  layer&CNNaug-ss~\cite{razavian2014cnn}       &4,096 &68.0      &--          &79.5     &--        &84.3   \\
  \hline
  \multirow{13}{*}{\rotatebox{90}{Deep Conv. layer of VGG16}} %&SPoC~\cite{babenko2015aggregating}     &256   &53.1      &--          &50.1     &--        &80.2      \\
  &R-MAC~\cite{tolias2016particular}      &512   &66.9      &61.6        &\textbf{83.0}     &75.7 &-- \\
  &CroW~\cite{kalantidis2016cross}        &512   &70.8      &65.3        &79.7     &72.2      &85.1      \\
  &SUM-mask~\cite{hoang2017selective}     &512   &64.0      &58.8        &78.6     &70.4      &86.4      \\
  &MAX-mask~\cite{hoang2017selective}     &512   &65.7      &60.5        &81.6     &72.4      &85.0      \\
  &PWA~\cite{xu2018unsupervised}          &512   &72.0      &66.2        &82.3     &\textbf{75.8}  &--  \\
  &ReSW~\cite{pang2018building}           &512   &72.6      &67.5        &82.4     &73.0      &85.3   \\
  &HeW                                    &512   &\textbf{72.8}  &\textbf{68.0}        &81.5     &74.4  &\textbf{88.4} \\
  \cline{2-8}
  &R-MAC+AML+QE~\cite{tolias2016particular}&512   &77.3 &73.2   &86.5     &79.8 &-- \\
  &CroW+QE~\cite{kalantidis2016cross}     &512   &74.9      &70.6        &84.8     &79.4       &--\\
  &PWA+QE~\cite{xu2018unsupervised}       &512   &74.8      &72.5        &86.0     &80.7      &--       \\
  &ReSW+QE~\cite{pang2018building}        &512   &76.3      &73.5        &86.3     &80.2      &--   \\
  &HeW+QE                                 &512   &77.8      &75.3        &87.0     &82.7      &-- \\
  &HeW+QE+HeR                             &512   &\textbf{82.0}   &\textbf{78.7}    &\textbf{91.2}           &\textbf{86.1}   &--\\
  \hline
\end{tabular}
\end{center}
\end{table*}

\subsection{Impact of the Networks}
Table~\ref{comparison_with_baseline}  illustrates the impact of the evaluated networks to the baseline and our methods.
Although ResNet50 has demonstrated much superior performance than VGG16 on the ILSVRC classification task~\cite{he2016deep,vedaldi2015matconvnet}, it does not actually produce better performance than the latter on the image retrieval benchmarks. As shown, while relying on much higher image representation dimensions $D=2,048$, ResNet50-based results are still inferior to VGG16-based results in many cases.
Even worse, when reducing the dimensionality to 512 components using PCA, ResNet50-based results fall behind the corresponding VGG16-based results by large margins except on the dataset of Holidays.
As is expected, SiaMAC$\dagger$-based results outperform VGG16-based results on the datasets of Oxford and Paris as the network SiaMAC$\dagger$ is fine-tuned with a large number of landmark building photos.
However, it is worth noting that, fine-tuning may result in over-fitting.
As shown, after fine-tuning, the performance for both SumA and HeW is slightly decreased on the Holidays.

Both the proposed feature aggregation method HeW and image re-ranking method HeR give boost in performance.
As shown in Table~\ref{comparison_with_baseline}, although HeW outperforms SumA by a little margin in some cases, it outperforms the latter by over 1\% in mAP in most cases after incorporating QE. Furthermore, we obtain additional 3\% mAP gains with our re-ranking method HeR. Accordingly, the proposed complete method HeW+QE+HeR typically outperforms the baseline method SumA+QE by 4\% with the adopted networks.


\begin{table*}[t]
\begin{center}
\caption{Performance (in mAP) comparison with the state-of-the-art methods using unsupervised fine-tuned networks. }\label{comparison_with_fine_tuning}
\begin{tabular}{|l|c| c c c c c|}
  \hline
  Method                                 &~Dim.~  &Oxford5k  &Oxford105k  &Paris6k  &Paris106k &Holidays \\
  \hline
  \hline
  NetVLAD~\cite{arandjelovic2016netvlad} &512 &67.6  &--          &74.9     &--        &86.1\\
  siaMAC$\dagger$+MAX-mask~\cite{hoang2017selective}&512   &77.7      &72.7        &83.2     &76.5      &86.3\\
  Fisher Vector~\cite{ong2017siamese}               &512   &81.5  &76.6      &82.4        &--       &--   \\
  siaMAC$\dagger$+MAC~\cite{radenovic2016cnn}     &512   &79.0      &73.9        &82.4     &74.6      &79.5\\
  siaMAC$\dagger$+R-MAC~\cite{radenovic2016cnn}     &512   &77.0      &69.2        &83.8     &76.4      &82.5\\
  siaMAC$\dagger$+ReSW~\cite{pang2018building}     &512   &\textbf{83.4}      &\textbf{79.3}        &86.5     &80.1      &85.5\\
  siaMAC$\dagger$+HeW                              &512   &82.6 &78.8 &\textbf{87.0} &\textbf{81.3} &\textbf{87.1}    \\
  \hline
  siaMAC$\dagger$+MAC~\cite{radenovic2016cnn}       &256   &77.4      &70.7       &80.8     &72.2      &77.3\\
  siaMAC$\dagger$+R-MAC~\cite{radenovic2016cnn}     &256   &74.9      &67.5       &82.3     &74.1      &81.4\\
  siaMAC$\dagger$+HeW                              &256    &80.5      &76.5       &85.7     &79.9      &85.9\\
  \hline
  siaMAC$\dagger$+MAC~\cite{radenovic2016cnn}       &128   &75.8      &68.6        &77.6     &68.0      &73.2\\
  siaMAC$\dagger$+R-MAC~\cite{radenovic2016cnn}     &128   &72.5      &64.3        &78.5     &69.3      &79.3\\
  siaMAC$\dagger$+HeW                              &128    &75.7      &70.9        &81.7     &75.0      &85.0\\
  \hline
  \hline
  siaMAC$\dagger$+MAC+R+QE~\cite{radenovic2016cnn}       &512   &85.0  &81.8   &86.5    &78.8    &--\\
  siaMAC$\dagger$+R-MAC+R+QE~\cite{radenovic2016cnn}     &512   &82.9  &77.9   &85.6    &78.3    &--\\
  siaMAC$\dagger$+HeW+QE                                &512   &\textbf{88.8}  &\textbf{87.0}   &\textbf{90.7}  &\textbf{86.6 }  &-- \\
  siaMAC$\dagger$+HeW+QE                                &256   &85.8  &83.9   &89.6  &85.5  &-- \\
  siaMAC$\dagger$+HeW+QE                                &128   &80.7  &78.2   &86.4  &81.6  &-- \\
  \hline
  \hline
  siaMAC$\dagger$+HeW+QE+HeR                   &512    &\textbf{92.0}  &\textbf{90.3}     &\textbf{94.3}     &\textbf{90.2}     &--\\
  siaMAC$\dagger$+HeW+QE+HeR                   &256    &88.7   &85.3     &93.0     &88.2     &--\\
  siaMAC$\dagger$+HeW+QE+HeR                   &128    &84.0   &80.2     &88.4     &84.3    &--\\
  \hline
\end{tabular}
\end{center}
\end{table*}

\begin{figure*}[t]
\centering
\includegraphics[width=5.5in]{Fig/example.pdf}
\caption{A failure example (top) and a successful example (bottom) from the test set Oxford105k for our complete approach HeW+QE+HeR.
There are 27 false positive images ranked between 21 and 54 (the number of ground truth similar images is 54) for the failure case.
For this example, its average precision is only 58.4\% and its top six false positive results are displayed for illustration.
For the successful example, there are 24 ground truth similar images, and there is only one false positive image ranked higher than 24.
As illustrated, we show its retrieval results ranked from 19th to 24th. Note, the blue/red/green border represent query region/false positive result/true positive result, respectively.}
\label{fig:ox105k_qer}
\end{figure*}


\subsection{Comparison with the State-of-the-art}
We below show the comparison results of the proposed approach with related
unsupervised methods that use off-the-shelf  and fine-tuned networks separately.

\vspace{0.01in}
\textbf{Comparison with methods using SIFT and pre-trained networks.}
In Table~\ref{comparison_with_unsupervised}, we present comparisons of our approach using VGG16 with methods using SIFT and off-the-shelf available networks, which utilize global representations of images. The comparison results are summarized as follows:
\begin{itemize}
  \item Our approach HeW significantly outperforms two state-of-the-art methods~\cite{do2017embedding,murray2017interferences} using weaker SIFT features, although their dimensions are more than 10 times higher than ours.
  Furthermore, it also shows clear advantages over~\cite{gong2014multi,babenko2014neural,razavian2014cnn} that utilize fully connected layers to derive image representations.
  \item Compared with~\cite{tolias2016particular,kalantidis2016cross,hoang2017selective,xu2018unsupervised,pang2018building} which are also using the VGG16 model, our method HeW achieves the best results on the datasets of Oxford5k, Oxford105k and Holidays. Additionally, it is worth noting that, the gain on Holidays is over 3\% at the same dimensionality.
  \item When combined with query expansion, our approach outperforms the compared methods~\cite{tolias2016particular,kalantidis2016cross,xu2018unsupervised,pang2018building} on all evaluated datasets. It should be noted that~\cite{tolias2016particular} includes a nontrivial and computationally intensive spatial verification process.
  \item Applying HeR after QE results in significant performance gains. As shown, our re-ranking strategy HeR gives a boost of around 3.5\% in mAP on all evaluated datasets. Consequently, we outperform the compared methods on two large-scale datasets Oxford105k and Paris106k by more than 5\% in mAP.
\end{itemize}

\begin{table*}[t]
\begin{center}
\caption{Comparison with the best results reported in the literature. }\label{comparison_with_best}
\begin{tabular}{|l|r| c c c c c|}
  \hline
  Method                                          &~~Dim.  &Oxford5k  &Oxford105k  &Paris6k  &Paris106k &Holidays \\
  \hline
  \hline
  Mikulik \textit{et al.}~\cite{mikulik2013learning}       &16M        &84.9    &82.4     &79.5     &77.3   &75.8\\
  Tolias \textit{et al.}~\cite{tolias2013aggregate}        &8M        &87.9    &--       &85.4     &--     &85.0\\
  Tolias and J{\'e}gou~\cite{tolias2015visual}    &8M        &89.4    &84.0     &82.8     &--     &--  \\
  Arandjelovic \textit{et al.}~\cite{arandjelovic2016netvlad}&4,096 &71.6    &--       &79.7     &--     &87.5\\
  Hoang \textit{et al.}~\cite{hoang2017selective}          &4,096   &83.8    &80.6     &88.3     &83.1   &\textbf{92.2}\\
  Iscen \textit{et al.}~\cite{iscen2017efficient}          &5$\times$512   &91.5    &84.7     &95.6     &\textbf{93.0}    &--\\
  Iscen \textit{et al.}~\cite{iscen2017fast}               &5$\times$512   &91.6    &86.5     &95.6    &92.4 &--\\
  Noh \textit{et al.}~\cite{noh2017largescale}             &--             &90.0    &88.5     &\textbf{95.7}    &92.8  &--\\
  Gordo \textit{et al.}~\cite{gordo2016deep}                &512     &89.1    &87.3     &91.2     &86.8    &89.1\\
  \hline
  This paper                                        &512    &\textbf{92.0}  &\textbf{90.3}     &94.3     &90.2     &88.4\\
  \hline
\end{tabular}
\end{center}
\end{table*}

\vspace{0.01in}
\textbf{Comparison with methods using  fine-tuned networks.}
We perform comparisons with recent unsupervised fine-tuned methods~\cite{arandjelovic2016netvlad,radenovic2016cnn,hoang2017selective,ong2017siamese,pang2018building} in Table~\ref{comparison_with_fine_tuning}.
The table again demonstrates the superior performance of our approach over related baselines at the same dimensionality:
\begin{itemize}
  \item Although HeW slightly falls behind our very recent work~\cite{pang2018building} on Oxford5k and Oxford105k, we establish new state-of-the-art results on the other three evaluated datasets at dimensionality of 512, and the improved mAP values over~\cite{arandjelovic2016netvlad,radenovic2016cnn,hoang2017selective,ong2017siamese} are
      not negligible. For example, the gain on Paris106k is at least 4.7\%.
  \item With the same siaMAC$\dagger$ features, our approach improves two related baselines R-MAC and MAC presented in~\cite{radenovic2016cnn} without and with dimensionality reduction, and the improvement is more significant on two large datasets Oxford105k and Paris106k.
  \item Our method, HeW, also produces better results than~\cite{radenovic2016cnn} after query expansion,
  although~\cite{radenovic2016cnn} uses more sophisticated post-processing than us.
  Additionally, HeW outperforms ~\cite{radenovic2016cnn} at $D=256$, and keeps competitive even at $D=128$.
  \item Finally, we further enlarge the mAP gains over the compared methods by applying HeR after QE. As shown, at  $D=512$,
  we outperform~\cite{radenovic2016cnn} by 7.0\%, 8.5\%, 7.8\%, 11.4\% on Oxford5k, Oxford105k, Paris6k, Paris106k, respectively.
\end{itemize}


To better understand our complete method HeW+QE+HeR, we visualize two example query images in Fig.~\ref{fig:ox105k_qer}.
The top query example can be considered as a failure case for our method as its average precision is only 58.8\%, falling far behind the mAP value of 92.0\%. As displayed, although the landmark contained in the 24-th ranked image is exactly not the same as the one contained in the query, it is visually similar to the query.
For the bottom query example, there are 24 ground truth similar images, and there is only one false positive image ranked at 22-th.
Its average precision is 97.0\%, and thus it can be seen as a successful example.
As shown, the unique false positive image contains several window patches, and therefore it is understandable that it has a large similarity score with the query region.

\vspace{0.01in}
\textbf{Comparison with costly methods.}
Table~\ref{comparison_with_best} compares our best results
with costly methods that focus on spatial verification or matching kernel.
Some of them~\cite{iscen2017efficient,iscen2017fast,noh2017largescale} do not necessarily rely on a
global representation, and some others~\cite{mikulik2013learning,tolias2013aggregate,tolias2015visual, arandjelovic2016netvlad,hoang2017selective} represent images with much higher dimensional vectors, and are thus not directly comparable.
There is no doubt that these methods use a larger memory footprint than our approach.
Additionally, their search efficiency is obviously  much lower than our method.
For instance, the method of~\cite{tolias2015visual} requires a slow spatial verification taking over 1
second per query (excluding descriptor extraction time). This means these best results are  hardly scalable as they require a lot of storage memory and searching time.
Compared with these methods, we still produce the best performance on Oxford5k and Oxford105k.
Similar to our approach, the supervised fine-tuned method~\cite{gordo2016deep} also represents images with 512 dimensional vectors.
Compared with this method, we produce slightly inferior result on Holidays, and achieve much better performance on the other datasets.

\section{Conclusions}\label{sec:conclusions}
We proposed an efficient aggregation approach for building compact but powerful image representations by utilizing the heat equation in this manuscript.
We utilized the theory of anisotropic diffusion, and assumed that graph defined by a set of deep features constitutes a heat transfer system.
By considering each deep feature as a heat source, our approach avoided over-representation of bursty features by enforcing the system temperatures  derived from all features be a constant. We provided a practical solution to derive image vectors, and demonstrated the effectiveness of our method on the task of instance-level retrieval.
Inspired by our aggregation method, we also presented a heat equation based image re-ranking method to further increase retrieval performance.
Both of feature aggregation and image re-ranking methods are unsupervised, and can be compatible with different CNNs, including pre-trained and fine-tuned networks.
Experimental results showed that we have established new state-of-the-art results on public image retrieval benchmarks using 512-dimensional vector representations.


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex,
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively.
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.


% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%

% use section* for acknowledgment
\section*{Acknowledgments}
This work was supported by National Key Research and Development Plan 2016YFB1001004,
National Natural Science Foundation of China Grant 61603289, China Postdoctoral
Science Foundation Grant 2016M602823, and Fundamental Research Funds for the Central Universities xjj2017118.
%This work was done when the first author was a visiting scholar at University of Virginia.


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
\bibliography{HeWR}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
%\begin{thebibliography}{1}
%
%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.
%
%\end{thebibliography}

% biography section
%
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\begin{IEEEbiography}[{\includegraphics[width=1.0in, height=1.25in, clip,keepaspectratio]{Fig/Shanmin}}]{Shanmin Pang} received the BS, MS, and Ph.D degrees from Shaanxi Normal University, Capital Normal University, and Xi'an Jiaotong University
in 2006, 2009 and 2015, respectively.
Prof. Pang is now an assistant professor in the School of Software Engineering at Xi'an Jiaotong University. His research interests include pattern recognition, computer vision and image processing. He won the best application paper award at the ACCV 2012 conference.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1.0in, height=1.25in, clip,keepaspectratio]{Fig/Jin}}]{Jin Ma}
received his BS degree  from Xi'an Jiaotong University, Xi'an, China, in 2016.
He is pursuing his ME degree in the School of Software Engineering at Xi'an Jiaotong University.
His research interest is computer vision, with a focus on content based image retrieval.
\end{IEEEbiography}


\begin{IEEEbiography}[{\includegraphics[width=1.0in, height=1.25in, clip,keepaspectratio]{Fig/Jianru}}]{Jianru Xue} (M'06)
got his BS degree from Xi'an University of Technology in 1994, and both MS and PhD degrees from Xi'an Jiaotong University in 1999
and 2003, respectively.
He is currently a full professor of the Institute of Artificial Intelligence and Robotics at Xi'an Jiaotong University, Xi'an,
China. He worked in FujiXerox, Tokyo, Japan, from 2002 to 2003, and visited University
of California, Los Angeles, from 2008 to 2009. His research field includes computer vision, visual localization and navigation, and video
coding based on analysis.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1.0in, height=1.25in, clip,keepaspectratio]{Fig/Jihua}}]{Jihua Zhu}
received the BE degree in automation from Central South University, China and the Ph.D. degree in pattern recognition and intelligence systems from Xi'an Jiaotong University, China, in 2004 and 2011, respectively.
He is an associate professor in the School of Software Engineering at Xi'an Jiaotong University.
His research interests include computer vision and machine learning.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1.0in, height=1.25in, clip,keepaspectratio]{Fig/Vicente}}]{Vicente Ordonez} is an assistant professor in the Department of Computer Science at the University of Virginia. His research interests lie at the intersection of computer vision, natural language processing and machine learning, with a focus on exploiting the natural connections that occur in vision and language using large amounts of data. He was a recipient of the Best Long Paper Award at EMNLP 2017, the IEEE David Marr Prize in Computer Vision in 2013. He obtained a PhD in Computer Science at the University of North Carolina at Chapel Hill, and an MS in Computer Science at Stony Brook University in New York State.
\end{IEEEbiography}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


