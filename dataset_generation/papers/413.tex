\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithmic}
\usepackage{float}
\usepackage{subcaption}
\usepackage{color}
\usepackage{xcolor}
\newcommand\myworries[1]{\textcolor{red}{#1}}
\usepackage{algorithm}
\usepackage[square,numbers,sort]{natbib}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\floatstyle{plain}
\newfloat{myalgo}{tbhp}{mya}
\newenvironment{Algorithm}[1][th]%
{\begin{myalgo}[#1]
\centering
\begin{minipage}{8cm}
\begin{algorithm}[H]}%
{\end{algorithm}
\end{minipage}
\end{myalgo}}

%
\def\sharedaffiliation{%
\end{tabular}
\begin{tabular}{c}}
%

\def\mA{\mathcal{A}}\def\mB{\mathcal{B}}\def\mC{\mathcal{C}}\def\mD{\mathcal{D}}\def\mE{\mathcal{E}}\def\mF{\mathcal{F}}\def\mG{\mathcal{G}}\def\mH{\mathcal{H}}\def\mI{\mathcal{I}}\def\mJ{\mathcal{J}}\def\mK{\mathcal{K}}\def\mL{\mathcal{L}}\def\mM{\mathcal{M}}\def\mN{\mathcal{N}}\def\mO{\mathcal{O}}\def\mP{\mathcal{P}}\def\mQ{\mathcal{Q}}\def\mR{\mathcal{R}}\def\mS{\mathcal{S}}\def\mT{\mathcal{T}}\def\mU{\mathcal{U}}\def\mV{\mathcal{V}}\def\mW{\mathcal{W}}\def\mX{\mathcal{X}}\def\mY{\mathcal{Y}}\def\mZ{\mathcal{Z}}\def\1n{\mathbf{1}_n}\def\0{\mathbf{0}}\def\1{\mathbf{1}}\def\A{{\bf A}}\def\B{{\bf B}}\def\C{{\bf C}}\def\D{{\bf D}}\def\E{{\bf E}}\def\F{{\bf F}}\def\G{{\bf G}}\def\H{{\bf H}}\def\I{{\bf I}}\def\J{{\bf J}}\def\K{{\bf K}}\def\L{{\bf L}}\def\M{{\bf M}}\def\N{{\bf N}}\def\O{{\bf O}}\def\P{{\bf P}}\def\Q{{\bf Q}}\def\R{{\bf R}}\def\S{{\bf S}}\def\T{{\bf T}}\def\U{{\bf U}}\def\V{{\bf V}}\def\W{{\bf W}}\def\X{{\bf X}}\def\Y{{\bf Y}}\def\Z{{\bf Z}}\def\a{{\bf a}}\def\b{{\bf b}}\def\c{{\bf c}}\def\d{{\bf d}}\def\e{{\bf e}}\def\f{{\bf f}}\def\g{{\bf g}}\def\h{{\bf h}}\def\i{{\bf i}}\def\j{{\bf j}}\def\k{{\bf k}}\def\l{{\bf l}}\def\m{{\bf m}}\def\n{{\bf n}}\def\o{{\bf o}}\def\p{{\bf p}}\def\q{{\bf q}}\def\r{{\bf r}}\def\s{{\bf s}}\def\t{{\bf t}}\def\u{{\bf u}}\def\v{{\bf v}}\def\w{{\bf w}}\def\x{{\bf x}}\def\y{{\bf y}}\def\z{{\bf z}}\def\balpha{\mbox{\boldmath{$\alpha$}}}\def\bbeta{\mbox{\boldmath{$\beta$}}}\def\bdelta{\mbox{\boldmath{$\delta$}}}\def\bgamma{\mbox{\boldmath{$\gamma$}}}\def\blambda{\mbox{\boldmath{$\lambda$}}}\def\bsigma{\mbox{\boldmath{$\sigma$}}}\def\btheta{\mbox{\boldmath{$\theta$}}}\def\bTheta{\mbox{\boldmath{$\Theta$}}}\def\bomega{\mbox{\boldmath{$\omega$}}}\def\bxi{\mbox{\boldmath{$\xi$}}}\def\bmu{\mbox{\boldmath{$\mu$}}}\def\bepsilon{\mbox{\boldmath{$\epsilon$}}}\def\bDelta{\mbox{\boldmath{$\Delta$}}}\def\bOmega{\mbox{\boldmath{$\Omega$}}}\def\bPhi{\mbox{\boldmath{$\Phi$}}}\def\bLambda{\mbox{\boldmath{$\Lambda$}}}\def\bSigma{\mbox{\boldmath{$\Sigma$}}}\def\bGamma{\mbox{\boldmath{$\Gamma$}}}\def\tt{\mbox{\tiny $T$}}\newcommand{\mymin}[1]{\mathop{\textrm{minimize}}_{#1}}\newcommand{\mymax}[1]{\mathop{\textrm{maximize}}_{#1}}\newcommand{\mymins}[1]{\mathop{\textrm{min}}_{#1}}\newcommand{\mymaxs}[1]{\mathop{\textrm{max}}_{#1}}\newcommand{\myargmin}[1]{\mathop{\textrm{argmin}}_{#1}}\newcommand{\myargmax}[1]{\mathop{\textrm{argmax}}_{#1}}\newcommand{\myst}{\textrm{s.t. }}\newcommand{\denselist}{\itemsep -1pt}\newcommand{\sparselist}{\itemsep 1pt}%\newcommand{\denselist}{\itemsep -3pt}\DeclareMathOperator{\vect}{vec}\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}\let\endchangemargin=\endlist%%%%% Optimization -Satya %%%%%%%%%%%%% Optimization\newcommand{\objective}[2]{\ensuremath{\underset{#1}\min & & #2\\[5pt]}}\newcommand{\objectivemax}[2]{\ensuremath{\underset{#1}\max & & #2\\[5pt]}}\newcommand{\st}{\mathrm{s.t.}}\newcommand{\constraint}[2][]{& & #2 & #1}\newcommand{\argmin}[1]{\ensuremath{\underset{#1}{\mathrm{arg\,min\,\,}}}\xspace}\newcommand{\mytodo}[1]{\textcolor{red}{TODO: #1}}\newcommand{\minisection}[1]{\noindent \textbf{#1}}%\def\posemachine{\emph{Pose Machine}}\def\posemachine{pose machine}\newenvironment{optimize}{\begin{equation}
\begin{array}{rlll}}{\end{array}
\end{equation}}% Example%\begin{optimize}%\objective{x \in \mathbb{R}^n}{\mathbb{E}_{\omega} [f(x,\omega)]}%\st \constraint{g(x,\omega) \leq 0}{}\\%\constraint{x_l \leq x \leq x_u,}{}%\label{eq:gen_problem}%\end{optimize}
%\usepackage{breqn}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}


\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{1074} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\renewcommand\baselinestretch{.97}
\setlength{\belowcaptionskip}{-10pt}
% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\renewcommand{\baselinestretch}{0.95}
\begin{document}
%%%%%%%%% TITLEp
\title{Convolutional Pose Machines}

%\title{ Deep Pose Machines: A Deep Convolutional Architecture with Local Intermediate Supervision }
%\title{ Deep Pose Machines: A Deep Convolutional Architecture with Local Intermediate Supervision for Articulated Pose Estimation}

\author{
Shih-En Wei \\ \texttt{\small shihenw@cmu.edu}
\and
Varun Ramakrishna \\ \texttt{\small vramakri@cs.cmu.edu}
\and
Takeo Kanade \\ \texttt{\small Takeo.Kanade@cs.cmu.edu}
\and
Yaser Sheikh \\ \texttt{\small yaser@cs.cmu.edu}
\sharedaffiliation
 The Robotics Institute \\
 Carnegie Mellon University
}
%\affiliation{The Robotics Institute \\ Carnegie Mellon University}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%Robotics Institute\\
%Carnegie Mellon University\\
%\texttt{\small \{shihenw,aaa,bbb,ccc\}@cmu.edu}

%\author{Robotics Institute}
%Carnegie Mellon University}

\maketitle
% \thispagestyle{empty}
%\abstract{This is an abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

Pose Machines provide a 
%powerful modular framework for articulated pose estimation. The 
sequential prediction framework 
%allows 
for 
%the 
learning 
%of 
rich implicit spatial models.
%, but currently relies on manually designed features for representing image and spatial context. 
%In this work, we incorporate a convolutional network architecture into the pose machine framework allowing the learning of representations for both image and spatial context directly from data. 
%
%The contribution of this paper is to model long-range spatial dependencies through learning convolutional networks directly applied on the intermediate confidence maps which preserve expressive spatial uncertainty. 
In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation.
%We propose a systematic approach to composing staged convolutional networks to refine the results 
%%with large receptive fields 
%of structured prediction tasks without the need for any graphical model style inference. 
The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference.
%
Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.


% below are v2.
%Pose Machines provide a powerful modular framework for articulated pose estimation. The sequential prediction framework allows for the learning of rich implicit spatial models, but currently relies on manually designed features for representing image and spatial context. In this work, we incorporate a convolutional network architecture into the pose machine framework allowing the learning of representations for both image and spatial context directly from data. The contribution of this paper is a systematic approach to composing convolutional networks with large receptive fields for pose estimation tasks. Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing backpropagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP and FLIC datasets.
%
\end{abstract}%Pose Machines provide a powerful modular framework for articulated pose estimation but  currently rely on manually designed features for representing image and spatial context information.  The contribution of this paper is a systematic approach to composing convolutional networks with large receptive fields for pose estimation tasks in the architecture of a pose machine. The sequential prediction framework of a pose machine which addresses the vanishing gradients problem by enforcing intermediate supervision. Convolutional pose machines inherit the benefits of the original pose machines framework, including the tight integration of multi-scale, multi-part cues and the ability to learn dependencies between image-based appearance and spatial context. In addition, the differentiability of the network allows joint training of all stages and levels through back propagation. We demonstrate that Convolutional Pose Machines outperform all state-of-the-art approaches in human pose detection on the standard LSP and FLIC benchmarks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}%Improvements in pose estimation of articulated objects have been mainly due to two reasons. The first is due to advances in developing feature representations which make the maximal use of image evidence \cite{jain2014learning,pishchulin13iccv} and the second is due to methods that are capable of sophisticated spatial modeling \cite{tompson2014joint,Ramakrishna2014posemachines,pishchulin2013poselet}.%Recent work \cite{Ramakrishna2014posemachines} introduced the modular \emph{\posemachine} framework which allows for the implicit learning  of complicated dependencies between part appearance and location, but relies on classical hand-crafted features for representing both image and spatial context information. In this work, we combine the rich spatial modeling afforded by the \posemachine~framework with recent advances in deep convolutional architectures to design a \emph{Convolutional Pose Machine}. Convolutional Pose Machines learn the image and spatial context representations directly from data, and the differentiability of the network allows joint training of all stages and levels through back propagation

We introduce \emph{Convolutional Pose Machines (CPMs)} for the task of articulated pose estimation. CPMs inherit the benefits of the \emph{\posemachine}\cite{Ramakrishna2014posemachines} architecture---the implicit learning of long-range dependencies between image and multi-part cues, tight integration between learning and inference, a modular sequential design---and combine them with the advantages afforded by convolutional architectures: the ability to learn feature representations for both image and spatial context directly from data; a differentiable architecture that allows for globally joint training with backpropagation; and the ability to efficiently handle large training datasets. 


CPMs consist of a sequence of convolutional networks that repeatedly produce 2D belief maps~\footnote{We use the term \emph{belief} in a slightly loose sense, however the belief maps described are closely related to beliefs produced in message passing inference in graphical models. The overall architecture can be viewed as an unrolled mean-field message passing inference algorithm~\cite{ross2011} that is learned end-to-end using backpropagation.} 
for the location of each part. At each stage in a CPM, image features and the belief maps produced by the previous stage are used as input. The belief maps provide the subsequent stage an expressive non-parametric encoding of the spatial uncertainty of location for each part, allowing the CPM to learn rich image-dependent spatial models of the relationships between parts. Instead of explicitly parsing such belief maps either using graphical models~\cite{tompson2014joint,tompson2015cvpr,pishchulin2015deepcut} or specialized post-processing steps~\cite{toshev2013deeppose,tompson2015cvpr}, we learn convolutional networks that directly operate on intermediate belief maps and learn implicit image-dependent spatial models of the relationships between parts. The overall proposed multi-stage architecture is fully differentiable and therefore can be trained in an end-to-end fashion using backpropagation.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\columnwidth]{figures/teaser_half-crop}
    \caption{A \textbf{Convolutional Pose Machine} consists of a sequence of predictors trained to make dense predictions at each image location. Here we show the increasingly refined estimates for the location of the \emph{right elbow} in each stage of the sequence.
(a) Predicting from local evidence often causes confusion. (b) Multi-part context helps resolve ambiguity. (c) Additional iterations help converge to a certain solution.}
    \label{fig:teaser}
\end{figure}

At a particular stage in the CPM, the spatial context of part beliefs provide strong disambiguating cues to a subsequent stage. As a result, each stage of a CPM produces belief maps with increasingly refined estimates for the locations of each part (see Figure~\ref{fig:teaser}). 
In order to capture long-range interactions between parts, the design of the network in each stage of our sequential prediction framework is motivated by the goal of achieving a large receptive field on both the image and the belief maps. We find, through experiments, that large receptive fields on the belief maps are crucial for learning long range spatial relationships and result in improved accuracy.
%To solve the ambiguity caused by predicting from local appearance, the design of the network in the subsequent stages of our sequential prediction framework is motivated by the goal of achieving a large receptive field on both the image and the confidence maps. We find, through experiments, that large receptive fields on the confidence maps are crucial for learning long range spatial relationships and result in improved accuracy.%We build Convolutional Pose Machines with a sequence of prediction networks trained to repeatedly predict confidences maps for the locations of each part, with each stage of the sequence improving the prediction (see Figure \ref{fig:teaser}). %The design of the network in each stage of our sequential prediction framework is motivated by the goal of achieving a large receptive field on both the image and, more importantly, on the belief maps outputted by the preceding stage which serve as priors of the current prediction task. We find, through experiments, that large receptive fields on the semantic belief maps are crucial for learning long range spatial relationships and result in improved accuracy.%We use intermediate loss layers to provide supervision to guide the network to repeatedly produce the aforementioned confidence maps. For such a network composed with multiple stages and therefore many layers, it is usually at risk of the problem of 
Composing multiple convolutional networks in a CPM results in an overall network with many layers that is at risk of the problem of
\emph{vanishing gradients}~\cite{hochreiter2001gradient,bradley2010learning,glorot2010understanding,bengio1994learning} during learning. 
This problem can occur because back-propagated gradients diminish in strength as they are propagated through the many layers of the network. 
While there exists recent work~\footnote{New results have shown that using skip connections with identity mappings~\cite{he15arxiv} in so-called residual units also aids in addressing vanishing gradients in ``very deep'' networks. We view this method as complementary and it can be noted that our modular architecture easily allows us to replace each stage with the appropriate residual network equivalent.} which shows that supervising very deep networks at intermediate layers aids in learning~\cite{lee2014deeply,szegedy2014going}, they have mostly been restricted to classification problems.
In this work, we show how for a structured prediction problem such as pose estimation, CPMs naturally suggest a systematic framework that replenishes gradients and guides the network to produce increasingly accurate belief maps  by enforcing intermediate supervision periodically through the network. We also discuss different training schemes of such a sequential prediction architecture. 
%The use of intermediate supervision in CPMs naturally replenishes gradients by enforcing intermediate supervision periodically through the network. While there exists recent work which shows that supervising very deep networks at intermediate layers aids in learning~\cite{lee2014deeply,szegedy2014going}, they have mostly been restricted to image classification problems. In this work, we show how for a structured prediction problem such as pose estimation, CPMs suggest a systematic framework that resolves the problem of vanishing gradients by enforcing intermediate supervision. We also discuss different training schemes of such a staged architecture.%%such a convolutional architecture that achieves a large receptive field allowing for encoding of long-range spatial dependencies with intermediate supervision.%The design of the network in each stage of our sequential prediction framework is motivated by the goal of achieving a large receptive field on both the image and, more importantly, on the output predictions of the preceding stage which serves as a prior of the current prediction task. We find, through experiment, that large receptive fields on the semantic feature maps are crucial for learning long range spatial relationships and result in improved accuracy.%Using convolutional networks to implicitly capture the long distance spatial context, our approach avoids using graphical models that requires prior knowledge of the prediction task, and make both image feature and spatial model completely driven by data.%%Large receptive fields could be achieved by different designs: increasing pooling; increasing the number of layers; or increasing the kernel size of each convolutional filter. Each of these design choices come with trade-offs in either precision of localization, increase in network depth, or increase in parameters. %In this paper, we discuss the design and training schemes of such a convolutional architecture that achieves a large receptive field allowing for the encoding of long range spatial dependencies with intermediate supervision.%Our main contributions are (a) learning implicit spatial models through convolutions networks directly operated on the previously predicted spatial uncertainty in a sequential prediction scheme, and (b) a systematic approach to composing convolutional networks that learn both image feature and image-dependent spatial models for structured prediction tasks, without the need for any graphical model style inference. We achieve state-of-the-art results on standard benchmarks including the MPII, LSP, and FLIC datasets, and analyze the effects of jointly training a multi-staged architecture with repeated intermediate supervision.
Our main contributions are (a) learning implicit spatial models via a sequential composition of convolutional architectures and (b) a systematic approach to designing and training such an architecture  to learn both image features and image-dependent spatial models for structured prediction tasks, without the need for any graphical model style inference. We achieve state-of-the-art results on standard benchmarks including the MPII, LSP, and FLIC datasets, and analyze the effects of jointly training a multi-staged architecture with repeated intermediate supervision.

%Our main contributions are (a) a sequential convolutional architecture that directly learns both image and image-dependent spatial context from data and %(b) %%building on the pose machine framework, we provide %a systematic approach to composing convolutional networks with large receptive fields for pose estimation tasks without the need for any graphical model style inference. We achieve state-of-the-art results on standard benchmarks including FLIC, LSP and MPII datasets, and analyze the effects of jointly training an staged architecture with repeated intermediate supervision, providing further evidence that networks with large depths can be effectively learned via intermediate supervision for structured prediction tasks.%Each stage in the sequential prediction framework uses as features information from both local image evidence as well as spatial context information from confidence map predictions of the preceding stage. In order to model long range spatial relationships we require networks with large receptive fields. Large receptive fields can be achieved either by pooling, at the expense of precision, or by increasing the number of layers, increasing the susceptibility to vanishing gradients. The contribution of this paper is a systematic approach to composing convolutional networks with large receptive fields for pose estimation tasks, which elegantly addresses the vanishing gradients problem by enforcing intermediate supervision. We demonstrate state-of-the-art performance of our system on standard benchmarks and outperform competing methods. We make the following technical contributions:%%%%%% Recent work \cite{Ramakrishna2014posemachines} introduced the modular \emph{\posemachine} framework which allows for the implicit learning of complicated dependencies between part appearance and location. However, the model relies on classical hand-crafted features for representing both image and spatial context information. In this work, we combine the rich spatial modeling afforded by the \posemachine~framework with recent advances in deep convolutional architectures for supervised feature learning to directly learn, without manual design, feature representations for both image and spatial context information.% The pose machine architecture consists of a sequence of predictors trained to predict confidences for locations of the parts, improving their predictions in each stage (Figure \ref{fig:teaser}). When convolutional networks are composed in the framework of a pose machine, the resulting deepening of the network can be susceptible to the problem of \emph{vanishing gradients}. Networks with a large number of layers can be challenging to train as the backpropagated gradients diminish in strength as they are propagated through the network \cite{bradley2010learning,glorot2010understanding}. We show how the sequential prediction framework of a \posemachine~provides a natural learning objective that enforces intermediate supervision periodically through the network thus replenishing backpropagated gradients at each stage.% \begin{figure}[t!]%     \centering%     \includegraphics[width=1\columnwidth]{figures/teaser_half}%     \caption{A \textbf{Convolutional Pose Machine} consists of a sequence of predictors trained to make a dense predictions at each image location. Here we show the increasingly refined estimates for the location of the \emph{right wrist} in each stage of the sequence.}%     \label{fig:teaser}% \end{figure}% Each stage in the sequential prediction framework uses as features information from both local image evidence as well as spatial contextual information from predictions of the preceding stage of the sequence. To model the spatial relationships between body parts, large receptive fields are needed because long range spatial relationships are informative. Large receptive fields can be achieved either by pooling, at the expense of precision, or by increasing the number of layers, increasing the susceptibility to vanishing gradients. We demonstrate state-of-the-art performance of our system on standard benchmarks and outperform all competing methods on the LSP and FLIC benchmarks. Our contributions are:% \begin{itemize}% \setlength{\itemsep}{1pt}% \setlength{\parskip}{1pt}% \setlength{\parsep}{1pt}% 	\item[---] A systematic approach to composing convolutional networks with large receptive fields for pose estimation tasks, which  addresses the vanishing gradients problem by enforcing intermediate supervision to replenish backpropagated gradients.% 	\item[---] We show how a \emph{Convolutional Pose Machine} with many layers can be trained by using a learning objective that enforces local supervision after every stage.% \end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%!TEX root = ../ICCV2015DeepPose.tex\section{Related Work}%\vspace{-3pt}
The classical approach to articulated pose estimation is the \textbf{pictorial structures} model \cite{fh2005pictorial,ramanan2005strike,Andriluka2010,Andriluka2009,pishchulin13iccv,pishchulin2013poselet,yang2011articulated,johnson2010clustered} in which spatial correlations between parts of the body are expressed as a tree-structured graphical model with kinematic priors that couple connected limbs. These methods have been successful on images where all the limbs of the person are visible, but are prone to characteristic errors such as double-counting image evidence, which occur because of correlations between variables that are not captured by a tree-structured model. The work of Kiefel et al. \cite{kiefel2014human} is based on the pictorial structures model but differs in the underlying graph representation. \textbf{Hierarchical models}\cite{tian2012exploring,sun2011articulated} represent the relationships between parts at different scales and sizes in a hierarchical tree structure. The underlying assumption of these models is that larger parts (that correspond to full limbs instead of joints) can often have discriminative image structure that can be easier to detect and consequently help reason about the location of smaller, harder-to-detect parts. 
\textbf{Non-tree models}\cite{wang2008multiple,sigal2006measure,lan2005beyond,karlinsky2012using,Dantone2013} incorporate interactions that introduce loops to augment the tree structure with additional edges that capture symmetry, occlusion and long-range relationships. These methods usually have to rely on approximate inference during both learning and at test time, and therefore have to trade off accurate modeling of spatial relationships with models that allow efficient inference, often with a simple parametric form to allow for fast inference. In contrast, methods based on a \textbf{sequential prediction} framework \cite{Ramakrishna2014posemachines} learn an \emph{implicit} spatial model with potentially complex interactions between variables by directly training an inference procedure, as in \cite{munoz2010,ross2011,tu2010PAMI,Pinheiro14recurrentconvolutional}. 
%

There has been a recent surge of interest in models that employ \textbf{convolutional architectures} for the task of articulated pose estimation \cite{ouyang2014multi,tompson2015cvpr,tompson2014joint,Chen_NIPS14,
carreia2015human,pishchulin2015deepcut,pfister2015flowing}. 
Toshev et al. \cite{toshev2013deeppose} take the approach of directly regressing the Cartesian coordinates using a standard convolutional architecture \cite{krizhevsky2012imagenet}.
%%Tompson et al. \cite{tompson2014joint,tompson2015cvpr} use a deep architecture for body part detection with a graphical model network whose parameters are learned jointly.%%Pishchulin et al. \cite{pishchulin2015deepcut} focus on multi-person pose estimation by solving part-person assignment problem with deep networks generating part candidates.
Recent work regresses image to confidence maps, and resort to graphical models, which require hand-designed energy functions or heuristic initialization of spatial probability priors, to remove outliers on the regressed confidence maps. Some of them also utilize a dedicated network module for precision refinement \cite{tompson2015cvpr,pishchulin2015deepcut}.
%
In this work, we show the regressed confidence maps are suitable to be inputted to further convolutional networks with large receptive fields to learn implicit spatial dependencies without the use of hand designed priors, and achieve state-of-the-art performance over all precision region without careful initialization and dedicated precision refinement. Pfister et al. \cite{pfister2015flowing} also used a network module with large receptive field to capture implicit spatial models.
Due to the differentiable nature of convolutions, our model can be globally trained, where Tompson et al. \cite{tompson2014joint} and Steward et al. \cite{steward2015endtoend} also discussed the benefit of joint training.

%In this work, we design our convolutional network in a sequential prediction framework, inspired by the work of \posemachine~\cite{Ramakrishna2014posemachines}.%with recent advances in image feature learning using deep convolutional architectures, to build a globally trainable convolutional network.%%The idea of building an end-to-end trainable model is also discussed for object detection tasks~\cite{steward2015endtoend} where a post-processing heuristic combining step is often required.%%We also leverage advances in developing fully convolutional deep architectures \cite{long_shelhamer_fcn,tompson2014joint} to achieve efficient dense pixel-wise predictions. %Our resulting architecture can be viewed as a network with a large number of layers, %Networks with many layers %which have been shown to be prone to the problem of \emph{vanishing gradients} \cite{hochreiter2001gradient,bradley2010learning,glorot2010understanding,bengio1994learning} during training. While there has been some work examining architectures that bypass this problem \cite{lee2014deeply,szegedy2014going,simonyan2014very} they have mostly been limited to classification problems. %Most related is very recent work \cite{lee2014deeply} where supervision is enforced after every layer of a deep network, which results in improved classification performance.

Carreira et al.~\cite{carreia2015human} train a deep network that iteratively improves part detections using error feedback but use a cartesian representation as in~\cite{toshev2013deeppose} which does not preserve spatial uncertainty and results in lower accuracy in the high-precision regime.
%Carreira et al. \cite{carreia2015human} train a deep network to iteratively predict error feedback, which is also in a sequential prediction framework. While they showed the benefit of predicting error instead of direct prediction, they are regressing into Cartesian errors like \cite{toshev2013deeppose}, which loses uncertainty and suffers from lower accuracy in high-precision region. %mapping \cite{tompson2014joint}.%with a set of convolutional kernels applied on both raw images and heatmaps.%
In this work, we show how the sequential prediction framework takes advantage of the preserved uncertainty in the confidence maps to encode the rich spatial context, with enforcing the intermediate local supervisions to address the problem of vanishing gradients.
%enforces intermediate local supervision. %The intermediate supervision replenishes back-propagated gradients during training, %Benefited by joint end-to-end training, our results achieve state of the art performance over all precision region on the structured prediction task of articulated pose estimation.%\myworries{Where to cite People detection (Steward) that reviewer asked?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Method}\begin{figure*}[t!]
    \centering
    \includegraphics[width=1\textwidth]{figures/effective_receptive_field-crop.pdf}
    \caption{\textbf{Architecture and receptive fields of CPMs.} We show a convolutional architecture and receptive fields across layers for a CPM with any $T$ stages. The \posemachine~\cite{Ramakrishna2014posemachines} is shown in insets (a) and (b), and the corresponding convolutional networks are shown in insets (c) and (d). Insets (a) and (c) show the architecture that operates only on image evidence in the first stage. Insets (b) and (d) shows the architecture for subsequent stages, which operate both on image evidence as well as belief maps from preceding stages. The architectures in (b) and (d) are repeated for all subsequent stages (2 to $T$). The network is locally supervised after each stage using an intermediate loss layer that prevents vanishing gradients during training. Below in inset (e) we show the effective receptive field on an image (centered at left knee) of the architecture, where the large receptive field enables the model to capture long-range spatial dependencies such as those between head and knees. (Best viewed in color.)}
    \label{fig:arch_half}
\end{figure*}\subsection{Pose Machines}
We denote the pixel location of the $p$-th anatomical landmark (which we refer to as a part),
%in the image, $Y_p \in \mathcal{Z} \subset \mathbb{R}^2$, where $\mathcal{Z}$ is the set of all $(u, v)$ locations in an image. 
%Our model consists of parts at different scales arranged in a hierarchy with \emph{levels} $l \in \{1, \ldots, L\}$. 
Our goal is to predict the image locations 
%$Y=(Y_1,\ldots,Y_{P_l})$ $Y=(Y_1,\ldots,Y_{P})$%for all $P_l$ parts
for all $P$ parts. 
%in each of the $L$ levels of the hierarchy.%\begin{figure}[t!]%    \centering%    \includegraphics[width=1\linewidth]{figures/pose_machine_hier1_new-crop.pdf}%    \caption{\textbf{Pose Machines.} A pose machine consists of a sequence of predictors that improve their predictions in each stage of the sequence using spatial context cues from the previous stage's predictions.}%    \label{fig:pose_machine}%\end{figure}
A \posemachine~\cite{Ramakrishna2014posemachines} (see Figure \ref{fig:arch_half}a and \ref{fig:arch_half}b) consists of a sequence of multi-class predictors, 
%${}^lg_t(\cdot)$, $g_t(\cdot)$,
that are trained to predict the location of each part in each level of the hierarchy. In each \emph{stage}$t \in \{ 1 \ldots T\}$, the classifiers 
%$ {}^lg_t$% $g_t$
predict beliefs for assigning a location to each part $Y_p = z, ~\forall z \in \mathcal{Z},$ based on features extracted from the image at the location $z$ denoted by $\mathbf{x}_z \in \mathbb{R}^d$ and contextual information from the preceding classifier in the neighborhood around each $Y_p$ in stage $t$. 
A classifier 
%for the $l^{\mathrm{th}}$ level of the hierarchy% 
in the first stage $t = 1$, therefore produces the following belief values:
%\begin{equation}%^{l}g_{1}(\mathbf{x}^{l}_z) \rightarrow \left\{ ^l b_1^p( Y_p = z)\right\}_{p \in \{0 \ldots P_l\}},%\end{equation}\begin{equation}
g_{1}(\mathbf{x}_z) \rightarrow \left\{ b_1^p( Y_p = z)\right\}_{p \in \{0 \ldots P\}},
\end{equation}\noindent where 
%${}^l b^{p}_1(Y_p = z)$% $b^{p}_1(Y_p = z)$
is the score predicted by the classifier 
%$^{l}g_1$% $g_1$
for assigning the $p^{\mathrm{th}}$ part 
%in the $l^{\mathrm{th}}$ level of the hierarchy% 
in the first stage at image location $z$. We represent all the beliefs of part $p$%of level $l$% 
evaluated at every location $z=(u,v)^T$ in the image as 
%$^l \mathbf{b}^p_t \in \mathbb{R}^{w \times h}$, %$\mathbf{b}^p_t \in \mathbb{R}^{w \times h}$, 
where $w$ and $h$ are the width and height of the image, respectively. That is,
%\begin{equation}%^l \mathbf{b}^p_t[u,v] = {}^l b^{p}_t(Y_p = z).%\end{equation}\begin{equation}
\mathbf{b}^p_t[u,v] = b^{p}_t(Y_p = z).
\end{equation}
For convenience, we denote the collection of belief maps for all the parts 
%belonging to level $l$% 
as 
%${}^l \mathbf{b}_t \in \mathbb{R}^{w \times h \times (P_l+1)}$% $\mathbf{b}_t \in \mathbb{R}^{w \times h \times (P+1)}$ ($P$ parts plus one for background).
%(see Figure \ref{fig:arch}).

In subsequent stages, the classifier predicts a belief for assigning a location to each part $Y_p = z, ~\forall z \in \mathcal{Z},$ based on (1) features of the image data $\mathbf{x}^t_z \in \mathbb{R}^d$ again, and (2) contextual information from the preceeding classifier in the neighborhood around each $Y_p$:
%%%%%\begin{equation}%    {}^lg_t \left(\mathbf{x}^l_z, \underset{l \in \{1 \ldots L\}}{\bigoplus} \psi( z, {}^l \mathbf{b}_{t-1}) \right) \rightarrow \left\{ ^l b_t^p( Y_p = z)\right\}_{p \in \{0 \ldots P_l\}},%    \label{eqn:predoutput}%\end{equation}\begin{equation}
    g_t \left(\mathbf{x}'_z, \psi_t(z, \mathbf{b}_{t-1}) \right) \rightarrow \left\{ b_t^p( Y_p = z)\right\}_{p \in \{0 \ldots P+1\}},
    \label{eqn:predoutput}
\end{equation}\noindent where 
%$\bigoplus$ denotes vector concatenation and %$\psi_{t>1}(\cdot)$ is a mapping from the beliefs $\mathbf{b}_{t-1}$ to context features. In each stage, the computed beliefs provide an increasingly refined estimate for the location of each part. Note that we allow image features $\mathbf{x}'_z$ for subsequent stage to be different from the image feature used in the first stage $\mathbf{x}$.
The \posemachine~proposed in~\cite{Ramakrishna2014posemachines} used boosted random forests for prediction 
%($\{{}^lg_t\}$) %
($\{g_t\}$),
fixed hand-crafted image features across all stages ($\mathbf{x}' = \mathbf{x}$), and fixed hand-crafted context feature maps ($ \psi_t(\cdot)$) to capture spatial context across all stages.

%\vspace{5pt}\subsection{Convolutional Pose Machines}
We show how the prediction and image feature computation modules of a pose machine can be replaced by a deep convolutional architecture allowing for both image and contextual feature representations to be learned directly from data. Convolutional architectures also have the advantage of being completely differentiable, thereby enabling end-to-end joint training of all stages of a CPM. 
%using an online training procedure such as stochastic gradient descent. 
We 
%systematically 
describe our design for a CPM that combines the advantages of deep convolutional architectures with the implicit spatial modeling afforded by the pose machine framework.

% \begin{figure}[t!]%    \centering%    \includegraphics[width=\linewidth]{figures/arch_local_new.pdf}%    \caption{\textbf{Convolutional Architecture for Keypoint Localization.} A deep convolutional architecture for performing keypoint localization that relies on local image evidence in a small region (receptive field) around each pixel location.}%    \label{fig:arch_local}%\end{figure}\subsubsection{Keypoint Localization Using Local Image \\ Evidence}% \vspace{-5pt}%We first describe 
The first stage of a convolutional pose machine predicts part beliefs from only local image evidence. Figure \ref{fig:arch_half}c shows the network structure used for part detection from local image evidence using a deep convolutional network. The evidence is \emph{local} because the receptive field of the first stage of the network is constrained to a 
%tightly% %cropped% 
small patch around the output pixel location.
We use a network structure composed of five convolutional layers followed by two $1 \times 1$ convolutional layers which results in a fully convolutional architecture \cite{long_shelhamer_fcn}.
%that allows inputs of an arbitrary size  $w \times h$. 
In practice, to achieve certain precision, we normalize input cropped images to size $368\times 368$ (see Section~\ref{subset:Quantitative} for details), and the receptive field of the network shown above is $160 \times 160$ pixels. The network can effectively be viewed as sliding a deep network across an image and regressing from the local image evidence in each $160\times 160$ image patch to a $P + 1$ sized output vector that represents a score for each part at that image location.

%\begin{figure}[t!]%\centering%\includegraphics[width=\linewidth]{figures/contextgood}%\caption{\textbf{Spatial context from confidence maps} can provide valuable cues for detection. The peaks in the confidence map for easier to detect landmarks, such as the shoulder, can be a strong cue for the location of difficult to detect landmarks, such as the left elbow.}%\label{fig:contextfeaturesgood}%\end{figure}\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{figures/context-crop}
\caption{\textbf{Spatial context from belief maps} of easier-to-detect parts can provide strong cues for localizing difficult-to-detect parts. The spatial contexts from shoulder, neck and head can help eliminate wrong (red) and strengthen correct (green) estimations on the belief map of \emph{right elbow} in the subsequent stages.}
\label{fig:contextfeaturesgood}
\end{figure}%\vspace{-5pt}\subsubsection{Sequential Prediction with Learned Spatial\\ Context Features}%\vspace{-5pt}
While the detection rate on landmarks with consistent appearance, such as the head and shoulders, can be favorable, the accuracies are often much lower for landmarks lower down the kinematic chain of the human skeleton due to their large variance in configuration and appearance. The landscape of the belief maps around a part location, albeit noisy, can, however, be very informative. 
%
Illustrated in Figure \ref{fig:contextfeaturesgood}, when detecting challenging parts such as right elbow, the belief map for right shoulder with a sharp peak can be used as a strong cue. 
%In the same time, the confidence map for the symmetric part (left elbow) also provides signals for resolving ambiguity.%
A predictor in subsequent stages ($g_{t>1}$) can use the spatial context ($\psi_{t>1}(\cdot)$) of the noisy belief maps in a region around the image location $z$ and improve its predictions by leveraging the fact that parts occur in consistent geometric configurations.  In the second stage of a \posemachine, the classifier 
%${}^l g_2$% $g_2$
accepts as input the image features 
%$\mathbf{x}^l_{z}$% $\mathbf{x}^2_{z}$
and features computed on the beliefs via the feature function $\psi$ for each of the parts in the previous stage. The feature function $\psi$ serves to encode the landscape of the belief maps from the previous stage in a spatial region around the location $z$ of the different parts.
For a convolutional pose machine, we do not have an explicit function that computes context features. Instead, we define $\psi$ as being the receptive field of the predictor on the beliefs from the previous stage.


The design of the network is guided by achieving a receptive field at the output layer of the second stage network that is large enough to allow the learning of potentially complex and long-range correlations between parts. By simply supplying features on the outputs of the previous stage (as opposed to specifying potential functions in a graphical model), the convolutional layers in the subsequent stage allow the classifier to freely combine contextual information by picking the most predictive features. 
%
The belief maps from the first stage are generated from a network that examined the image locally with a small receptive field.
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.49\columnwidth]{figures/pdf/result_curves/FLIC/FLIC_wri_kernel_acc_exp41}
    \includegraphics[width=0.49\columnwidth]{figures/pdf/result_curves/FLIC/FLIC_elb_kernel_acc_exp41}
    \caption{\textbf{Large receptive fields for spatial context.} We show that networks with large receptive fields are effective at modeling long-range spatial interactions between parts. Note that these experiments are operated with smaller normalized images than our best setting.}
    \label{fig:flic_kernels}
\end{figure}%with a receptive field of size $160\times 160$. 
In the second stage, we design a network that drastically increases the equivalent receptive field. 
Large receptive fields can be achieved either by pooling at the expense of precision, increasing the kernel size of the convolutional filters at the expense of increasing the number of parameters, or by increasing the number of convolutional layers at the risk of encountering vanishing gradients during training. 
Our network design and corresponding receptive field for the subsequent stages ($t \geq 2$) is shown in Figure \ref{fig:arch_half}d.
%The subsequent stage of convolutional pose machines uses a combination of image feature maps and confidence maps passed as input to a network with three convolutional layers, 
We choose to use multiple convolutional layers to achieve large receptive field on the $8\times$ downscaled heatmaps, as it allows us to be parsimonious with respect to the number of parameters of the model. We found that our stride-$8$ network performs as well as a stride-$4$ one even at high precision region, while it makes us easier to achieve larger receptive fields.
%while the risk of vanishing gradients is offset thanks to the intermediate supervision enforced during training. 
We also repeat similar structure for image feature maps to make the spatial context be image-dependent and allow error correction, following the structure of \posemachine.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=1\linewidth]{figures/pdf/gradient-crop}
    \caption{\textbf{Intermediate supervision addresses vanishing gradients.} We track the change in magnitude of gradients in layers at different depths in the network, across training epochs, for models with and without intermediate supervision. We observe that for layers closer to the output, the distribution has a large variance for both with and without intermediate supervision; however as we move from the output layer towards the input, the gradient magnitude distribution peaks tightly around zero with low variance (the gradients \emph{vanish}) for the model without intermediate supervision. For the model with intermediate supervision the distribution has a moderately large variance throughout the network. At later training epochs, the variances decrease for all layers for the model with intermediate supervision and remain tightly peaked around zero for the model without intermediate supervision. (Best viewed in color)}
    \label{fig:gradient_change}
\end{figure*}
We find that accuracy improves with the size of the receptive field. In Figure \ref{fig:flic_kernels} we show the improvement in accuracy on the FLIC dataset \cite{sappmodec} as the size of the receptive field on the original image is varied by varying the architecture without significantly changing the number of parameters,  through a series of experimental trials on input images normalized to a size of $304 \times 304$. We see that the accuracy improves as the effective receptive field increases, and starts to saturate around $250$ pixels, which also happens to be roughly the size of the normalized object. This improvement in accuracy with receptive field size suggests that the network does indeed encode long range interactions between parts and that doing so is beneficial.
%
In our best performing setting in Figure~\ref{fig:arch_half}, we normalize cropped images into a larger size of $368 \times 368$ pixels for better precision, and the receptive field of the second stage output on the belief maps of the first stage is set to $31\times 31$, which is equivalently $400\times 400$ pixels on the original image, where the radius can usually cover any pair of the parts. With more stages, the effective receptive field is even larger. In the following section we show our results from up to $6$ stages.
%The two stage convolutional pose machine uses a combination of image feature maps and confidence maps passed as input to a network with four convolutional layers.  %\subsubsection{Incorporating a Hierarchy}%The visual structure around each landmark can provide discriminative information that is useful for prediction. Oftentimes larger regions around a landmark can contain more discriminative structure than a smaller, tightly cropped region. For example, for landmarks such as the wrist joint, a patch tightly cropped around it has little distinguishable visual structure. Whereas, when we include additional visual context, we observe the consistent discriminative visual structure of the forearm. Multi-scale cues such as these can be useful for detection. Pose machines incorporate such multiscale cues by defining a hierarchy over parts. The design of a convolutional pose machine that incorporates hierarchical cues across multiple stages is shown in \ref{fig:arch}. In stages after the first stage, the image feature maps (denoted by $\mathbf{x}_{z}^{l}$) are combined with the confidence maps for the parts from both hierarchy levels from the previous stage. The confidence maps from preceding layers that are at a different resolution are correspondingly up-sampled using a deconvolutional layer \cite{long_shelhamer_fcn} or down-sampled using a max-pooling layer of the appropriate stride.\begin{figure*}[ht!]
    \begin{subfigure}{0.33\textwidth}
   	     \centering
         \includegraphics[width=\textwidth]{figures/pdf/compare_with_pm/pck-total-lsp-PC.pdf}
         \caption{}
         \label{fig:comp_with_pm}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pdf/result_curves/LEEDS_PC/self_comparison/pck-total-lsp-PC}
        \caption{}
        \label{fig:compare_training}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pdf/result_curves/LEEDS_PC/stagewise_comparison/pck-total-lsp-PC}
        \caption{}
        \label{fig:compare_stages}
    \end{subfigure}
%    \begin{subfigure}{0.32\textwidth}
%        \centering
%        \includegraphics[width=\textwidth]{figures/pdf/result_curves/LEEDS_PC/hier/pck-total-lsp-PC}
%        \caption{}
%        \label{fig:compare_image}
%    \end{subfigure}
    \caption{\textbf{Comparisons on 3-stage architectures on the LSP dataset (PC)}: (a) Improvements over Pose Machine. (b) Comparisons between the different training methods. (c) Comparisons across each number of stages using joint training from scratch with intermediate supervision.} 
    %(c) \myworries{Comparisons between pure spatial model and image-dependent spatial model. (figure to be updated)}}
\end{figure*}\subsection{Learning in Convolutional Pose Machines}
The design described above for a \posemachine~results in a deep architecture that can have a large number of layers.  Training such a network with many layers can be prone to the problem of \emph{vanishing gradients}\cite{bradley2010learning,glorot2010understanding,bengio1994learning} where, as observed 
by Bradley \cite{bradley2010learning} and Bengio et al. \cite{glorot2010understanding}, the magnitude of back-propagated gradients decreases in strength with the number of intermediate layers between the output layer and the input layer.

%\begin{figure*}[ht!]%    \centering%    \includegraphics[width=1\textwidth]{figures/full_architecture_new.pdf}%    \caption{\textbf{Architecture for a $T$-stage Convolutional Pose Machines.} We show a deep convolutional architecture for a \posemachine~with any number stages. The \posemachine~is shown in the top right with insets described below. Insets (a) shows the architecture that operates only on image evidence in the first stage. Insets (b) shows the architecture for subsequent stages, which operate both on image evidence as well as confidence maps from preceding stages. The architectures in (b) is repeated for all subsequent stages (2 to $T$). The network is locally supervised after each stage using an intermediate loss layer that prevents vanishing gradients during training. (Best viewed in color.)}%    \label{fig:arch}%\end{figure*}

Fortunately, the sequential prediction framework of the \posemachine~provides a natural approach to training our deep architecture that addresses this problem. Each stage of the \posemachine~is trained to repeatedly produce the belief maps for the locations of each of the parts. We encourage the network to repeatedly arrive at such a representation by defining a loss function at the output of each stage $t$%and hierarchy level $l$ 
that minimizes the $l_2$ distance between the predicted and ideal belief maps for each part. The ideal belief map for a part $p$ is written as 
%${}^l b^{p}_{*}(Y_p = z)$.$b^{p}_{*}(Y_p = z)$,
which are created by putting Gaussian peaks at ground truth locations of each body part $p$.
The cost function we aim to minimize at the output of each stage at each level is therefore given by:
% \begin{equation}%    {}^l f_t = \sum_{p = 1}^{P_l} \sum_{z\in \mathcal{Z}}  \|{}^l b^{p}_{t}(z) - {}^l b^{p}_{*}(z)\|^{2}_{2}.%    \label{eqn:localobjective}% \end{equation}\begin{equation}
    f_t = \sum_{p = 1}^{P+1} \sum_{z\in \mathcal{Z}}  \| b^{p}_{t}(z) -  b^{p}_{*}(z)\|^{2}_{2}.
    \label{eqn:localobjective}
 \end{equation}

The overall objective for the full architecture is obtained by adding the losses at each stage and is given by:
%\begin{equation}%    \mathcal{F} = \sum_{t = 1}^{T} \sum_{l=1}^{L} {}^lf_t .%\label{eqn:fullobjective}\begin{equation}
    \mathcal{F} = \sum_{t = 1}^{T} f_t .
\label{eqn:fullobjective}
\end{equation}
We use standard stochastic gradient descend to jointly train all the $T$ stages in the network. To share the image feature $\mathbf{x}'$ across all subsequent stages, we share the weights of corresponding convolutional layers (see Figure~\ref{fig:arch_half}) across stages $t \geq 2$.
%\vspace{5pt}%\minisection{Implementation Details}:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluation}\subsection{Analysis}\minisection{Addressing vanishing gradients.}
The objective in Equation \ref{eqn:fullobjective} describes a decomposable loss function that operates on different parts of the network (see Figure \ref{fig:arch_half}). Specifically, each term in the summation is applied to the network after each stage $t$ effectively enforcing supervision in intermediate stages through the network. Intermediate supervision has the advantage that, even though the full architecture can have many layers, it does not fall prey to the \emph{vanishing gradient} problem as the intermediate loss functions replenish the gradients at each stage.

We verify this claim by observing histograms of gradient magnitude (see Figure \ref{fig:gradient_change}) at different depths in the architecture across training epochs for models with and without intermediate supervision. In early epochs, as we move from the output layer to the input layer, we observe on the model \emph{without intermediate supervision}, the gradient distribution is tightly peaked around zero because of vanishing gradients. The model \emph{with intermediate supervision} has a much larger variance across all  layers, suggesting that learning is indeed occurring in all the layers thanks to intermediate supervision. We also notice that as training progresses, the variance in the gradient magnitude distributions decreases pointing to model convergence.

\minisection{Benefit of end-to-end learning.} 
We see in Figure \ref{fig:comp_with_pm} that replacing the modules of a pose machine with the appropriately designed convolutional architecture provides a large boost of $42.4$ percentage points over the previous approach of \cite{Ramakrishna2014posemachines} in the high precision regime (PCK@0.1) and  $30.9$ percentage points in the low precision regime (PCK@0.2).

\minisection{Comparison on training schemes.}\label{sec:compare_learning} We compare different variants of training the network in Figure \ref{fig:compare_training} on the LSP dataset with person-centric (PC) annotations. To demonstrate the benefit of intermediate supervision with joint training across stages, we train the model in four ways:
(i) training from scratch using a global loss function that enforces intermediate supervision
(ii) stage-wise; where each stage is trained in a feed-forward fashion and stacked
(iii) as same as (i) but initialized with weights from (ii), and
(iv) as same as (i) but with no intermediate supervision. We find that network (i) outperforms all other training methods, showing that intermediate supervision and joint training across stage is indeed crucial in achieving good performance. The stagewise training in (ii) saturate at sub-optimal, and the jointly fine-tuning in (iii) improves from this sub-optimal to the accuracy level closed to (i), however with effectively longer training iterations.

%\myworries{describe (ii) stagewise gets suboptimal and saturate very soon and (iii) finetuning starts from higher accuracy but however converges to the same level as (i) with longer training.}\begin{figure*}[ht!]
    \centering
    \includegraphics[width=1\linewidth]{figures/leedsqual}
    \vspace{-20pt}
    \caption{\textbf{Comparison of belief maps across stages} for the elbow and wrist joints on the LSP dataset for a 3-stage CPM.} 
    %The subsequent stages of the convolutional pose machine learn a spatial model that aids in resolving confusions between parts. The first stage predictions for the wrist joints are often ambiguous or erroneous.}
    \label{fig:leeds_qual}
\end{figure*}\begin{figure*}[ht!]
        \centering
        \includegraphics[width=0.965\textwidth]{figures/pdf/result_curves/MPI/MPI_all.pdf}
        %\vspace{-10pt}
        \vspace{-10pt}
        \caption{\textbf{Quantitative results on the MPII dataset} using the PCKh metric. We achieve state of the art performance and outperform significantly on difficult parts such as the ankle.}
        \label{fig:mpi_results}
\end{figure*}\begin{figure*}[ht!]
    \centering
    %\includegraphics[width=0.965\linewidth]{figures/pdf/result_curves/LEEDS_OC/pck-all-lsp-OC}
    \includegraphics[width=0.965\linewidth]{figures/pdf/result_curves/LEEDS_PC/pck-all-lsp-PC}
    %\vspace{-10pt}
    \vspace{-10pt}
    \caption{\textbf{Quantitative results on the LSP dataset} using the PCK metric. Our method again achieves state of the art performance and has a significant advantage on challenging parts.}
    \label{fig:leeds_oc_quant}
\end{figure*}\minisection{Performance across stages.}
We show a comparison of performance across each stage on the LSP dataset (PC) in Figure \ref{fig:compare_stages}. We show that the performance increases monotonically until 5 stages, as the predictors in subsequent stages make use of contextual information in a large receptive field on the previous stage beliefs maps to resolve confusions between parts and background. We see diminishing returns at the 6th stage, which is the number we choose for reporting our best results in this paper for LSP and MPII datasets.

%\minisection{Image-dependent model vs. pure spatial model.} \myworries {some text: image-dependent model is better blahblahblah}%\minisection{Does the hierarchy help?} \myworries{to be deleted.} We compare models with one and two levels in the hierarchy on LSP in Figure \ref{fig:compare_hierarchy}. We see a substantial improvement in performance with the two-level model when using person-centeric (PC) annotations. The difficulty in learning person-centric pose estimation lies in the fact that the front-back ambiguity needs to be resolved. The favorable performance of the two-level model over the single-level model seems to suggest that information at a coarser scale assists the model in resolving this ambiguity.%\vspace{-3pt}\subsection{Datasets and Quantitative Analysis}\label{subset:Quantitative}%\vspace{-3pt}

In this section we present our numerical results in various standard benchmarks including the MPII, LSP, and FLIC datasets. To have normalized input samples of $368 \times 368$ for training, we first resize the images to roughly make the samples into the same scale, and then
crop or pad the image according to the center positions and rough scale estimations provided in the datasets if available. In datasets such as LSP without these information, we estimate them according to joint positions or image sizes. For testing, we perform similar resizing and cropping (or padding), but estimate center position and scale only from image sizes when necessary. In addition, we merge the belief maps from different scales (perturbed around the given one) for final predictions, to handle the inaccuracy of the given scale estimation.

We define and implement our model using the \emph{Caffe}\cite{jia2014caffe} libraries for deep learning. We publicly release the source code and details on the architecture, learning parameters, design decisions and data augmentation to ensure full reproducibility.\footnote{\url{https://github.com/CMU-Perceptual-Computing-Lab/convolutional-pose-machines-release}}\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/pdf/quantitative/Quantitative.pdf}
    %\vspace{-5pt}
    \vspace{-5pt}
    \caption{\textbf{Qualitative results} of our method on the MPII, LSP and FLIC datasets respectively.  We see that the method is able to handle non-standard poses and resolve ambiguities between symmetric parts for a variety of different relative camera views.}
    \label{fig:leeds_qualitative}
\end{figure*}\begin{figure}[ht!]
    \centering
    %\includegraphics[width=0.965\linewidth]{figures/pdf/result_curves/LEEDS_OC/pck-all-lsp-OC}
    \includegraphics[width=0.965\linewidth]{figures/viewpoint-crop}
    \vspace{-10pt}
    \caption{\textbf{Comparing PCKh-$0.5$ across various viewpoints in the MPII dataset.} Our method is significantly better in all the viewpoints.}
    \label{fig:mpi_vp}
\end{figure}\minisection{MPII Human Pose Dataset}. We show in Figure \ref{fig:mpi_results} our results on the MPII Human Pose dataset \cite{andriluka14cvpr} which consists more than 28000 training samples. 
We choose to randomly augment the data with rotation degrees in $[-40^{\circ},40^{\circ}]$, scaling with factors in $[0.7,1.3]$, and horizonal flipping.
The evaluation is based on PCKh metric \cite{andriluka14cvpr} where the error tolerance is normalized with respect to head size of the target. Because there often are multiple people in the proximity of the interested person (rough center position is given in the dataset), we made two sets of ideal belief maps for training: 
one includes all the peaks for every person appearing in the proximity of the primary subject and the second type where we only place peaks for the primary subject.  We supply the first set of belief maps to the loss layers in the first stage as the initial stage only relies on local image evidence to make predictions. We supply the second type of belief maps to the loss layers of all subsequent stages.
%one is including all the peaks for every people appearing in the proximity, and the other one only contains peaks of the interested person. We feed the first set of belief map into the loss layer of the first stage, to make the first stage concentrate on recognizing local parts, and feed the second set of belief map into all the subsequent loss layers. 
We also find that supplying to all subsequent stages an additional heat-map with a Gaussian peak indicating center of the primary subject is beneficial.
%With an extra heat-map map channel with a single Gaussian peak (of larger variance) indicating the center of interested person cascaded (at the ``cascading point'' of each subsequent stage), the following stages can use this clue to suppress those responses fired on surrounding people.\vspace{-3pt}

Our total PCKh-$0.5$ score achieves state of the art at $87.95\%$ ($88.52\%$ when adding LSP training data), which is $6.11\%$ higher than the closest competitor, and it is noteworthy that on the ankle (the most challenging part), our PCKh-$0.5$ score is $78.28\%$ ($79.41\%$ when adding LSP training data), which is $10.76\%$ higher than the closest competitor. This result shows the capability of our model to capture long distance context given ankles are the farthest parts from head and other more recognizable parts. Figure~\ref{fig:mpi_vp} shows our accuracy is also consistently significantly higher than other methods across various view angles defined in~\cite{andriluka14cvpr}, especially in those challenging non-frontal views.
%
In summary, our method improves the accuracy in all parts, over all precisions, across all view angles, and is the first one achieving such high accuracy without any pre-training from other data, or post-inference parsing with hand-design priors or initialization of such a structured prediction task as in \cite{tompson2014joint,pishchulin2015deepcut}. Our methods also does not need another module dedicated to location refinement as in \cite{tompson2015cvpr} to achieve great high-precision accuracy with a stride-8 network.

%We provide \textbf{video results} in supplementary material. These results were generated on a per-frame basis with no temporal consistency enforced. We encourage the reader to note the smooth predictions and the ability of the network to accurately predict correct pose even when the subjects face away from the camera. Qualitative results for all datasets are shown in Figure \ref{fig:leeds_qualitative}.\vspace{-3pt}\minisection{Leeds Sports Pose (LSP) Dataset.} We evaluate our method on the Extended Leeds Sports Dataset \cite{Johnson11} that consists of 11000 images for training and 1000 images for testing.
%with annotations provided for the full body. 
We trained on person-centric (PC) annotations and evaluate our method using the Percentage Correct Keypoints (PCK) metric \cite{yang2013articulated}.
%
Using the same augmentation scheme as for the MPI dataset, our model again achieves state of the art at $84.32\%$ ($90.5\%$ when adding MPII training data). Note that adding MPII data here significantly boosts our performance, due to its labeling quality being much better than LSP. Because of the noisy label in the LSP dataset, Pishchulin et al. \cite{pishchulin2015deepcut} reproduced the dataset with original high resolution images and better labeling quality.
%We see that for \emph{observer-centric annotations} (see Figure \ref{fig:leeds_oc_quant}) we outperform the nearest competing method by approximately $10$ percentage points in the high precision regime (PCK@0.1) and approximately $5$ percentage points in the lower precision regime (PCK@0.2). \emph{Person-centric annotations} impose a harder problem on the pose estimation task since disambiguating the left from right limbs relies on observing the target's pose relative to the camera, in addition to the spatial relationship of parts in the image plane. Figure \ref{fig:leeds_qual} shows that our model develops a representation that is able to resolve the left-right ambiguity across the stages the with large receptive fields. Our method again outperforms all of the other methods, as shown in Figure \ref{fig:leeds_oc_quant}.\minisection{FLIC Dataset}. We evaluate our method on the FLIC Dataset \cite{sappmodec} which consists of 3987 images for training and 1016 images for testing. 
%with annotations provided for the upper body. 
We report accuracy as per the metric introduced in Sapp et al. \cite{sappmodec} for the elbow and wrist joints in Figure \ref{fig:flic_quant}. Again, we outperform all prior art at PCK@0.2 with $97.59\%$ on elbows and $95.03\%$ on wrists. In higher precision region our advantage is even more significant: $14.8$ percentage points on wrists and $12.7$ percentage points on elbows at PCK@0.05, and $8.9$ percentage points on wrists and $9.3$ percentage points on elbows at PCK@0.1.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\columnwidth]{figures/pdf/result_curves/FLIC/comparison/pck-elbow-wrist.pdf}
    \vspace{-20pt}
    \caption{\textbf{Quantitative results on the FLIC dataset} for the elbow and wrist joints with a 4-stage  CPM. We outperform all competing methods.}
    \label{fig:flic_quant}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-10pt}\section{Discussion}\vspace{-5pt}%Convolutional pose machines show how a structured prediction task such as human pose estimation is handled by a sequential prediction framework with convolutional networks. Each stage of the network fully exploits the image feature and the preserved non-parametric spatial context to refine predictions.
Convolutional pose machines provide an end-to-end architecture for tackling structured prediction problems in computer vision without the need for graphical-model style inference. We showed that a sequential architecture composed of convolutional networks is capable of implicitly learning a spatial models for pose by communicating increasingly refined uncertainty-preserving beliefs between stages. Problems with spatial dependencies between variables arise in multiple domains of computer vision such as semantic image labeling, single image depth prediction and object detection and future work will involve extending our architecture to these problems.
%The training procedure describes a natural objective for learning a deep architecture by enforcing intermediate supervision addressing the vanishing gradient problem. %
Our approach achieves state of the art accuracy on all primary benchmarks, however we do observe failure cases mainly when multiple people are in close proximity. Handling multiple people in a single end-to-end architecture is also a challenging problem and an interesting avenue for future work.
%While the accuracy of our models is great, we observe%some failure cases include examples with rare poses, multiple people in close proximity and heavy occlusions. And the quality of prediction would be degraded if there are large errors in the given center position and scale of the target.%%Our future work would explore more problem domains such as jointly person detection to avoid these limitations and make our pose estimator applicable in more scenarios.

{\small
\bibliographystyle{ieee}
\bibliography{posebib}
}


\end{document}
