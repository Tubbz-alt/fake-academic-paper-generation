
%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[10pt, a4paper, conference, compsocconf]{IEEEtran}
% Add the compsocconf option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.







% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
   \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
   \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )
\usepackage[cmex10]{amsmath}
\usepackage{multirow}
\usepackage{subfig}
\usepackage{epsfig}
\usepackage{amssymb}
\usepackage{comment}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

%For references formatting
\renewcommand{\IEEEbibitemsep}{0pt plus 2pt}
\makeatletter
\IEEEtriggercmd{\reset@font\normalfont\footnotesize}
\makeatother
\IEEEtriggeratref{1}

\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Learning Surrogate Models of Document Image Quality Metrics for \\Automated Document Image Processing}

\author{\IEEEauthorblockN{Prashant Singh, Ekta Vats and Anders Hast}
\IEEEauthorblockA{Department of Information Technology\\
Uppsala University, SE-751 05 Uppsala, Sweden\\
Email: prashant.singh@it.uu.se; ekta.vats@it.uu.se; anders.hast@it.uu.se}
}


% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}


% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}


% make the title area
\maketitle

\begin{abstract}
Computation of document image quality metrics often depends upon the availability of a ground truth image corresponding to the document. This limits the applicability of quality metrics in applications such as hyperparameter optimization of image processing algorithms that operate on-the-fly on unseen documents. This work proposes the use of surrogate models to learn the behavior of a given document quality metric on existing datasets where ground truth images are available. The trained surrogate model can later be used to predict the metric value on previously unseen document images without requiring access to ground truth images. The surrogate model is empirically evaluated on the Document Image Binarization Competition (DIBCO) and the Handwritten Document Image Binarization Competition (H-DIBCO) datasets.

\end{abstract}

\begin{IEEEkeywords}
surrogate models; document image quality metrics; hyperparameter optimization

\end{IEEEkeywords}


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
\label{sec:intro}

Document image quality metrics are objective measures that enable assessment and quantification of characteristics of a given document image. Such metrics are crucial for enabling automatic document processing applications, such as fully-automatic document image binarization. Specifically, document image processing algorithms involve hyperparameters that must be optimized to achieve the best possible resulting image. Hyperparameter optimization techniques such as Bayesian optimization \cite{snoek2012practical} require formulation of an \emph{objective function} to be maximized. Document image quality metrics are natural candidates as objective functions. 

In general, document image quality is calculated by comparing the image in question to the noise-free replica of the document image, known as the ground truth reference image. There exist several popular image quality metrics in literature \cite{ye2013document}. A vast majority of the methods considered Optical Character Recognition (OCR) results as document quality metrics \cite{hale2007human, kang2014deep, nayef2015metric}. Simple techniques to measure the image quality, such as Mean Squared Error (MSE) do not suffice due to the complex and degraded nature of images. There is a need for more sophisticated methods to assess image quality. Popular document quality evaluation measures \cite{gatos2009icdar,
pratikakis2016icfhr2016} include the F-Measure, the Peak-Signal-to-Noise Ratio (PSNR), the Distance Reciprocal Distortion metric (DRD) \cite{lu2004distance}, and the Negative Rate Metric (NRM). Computation of such metrics requires a corresponding distortion-free ground truth reference image for any given document image. 

In addition to ground truth images, human opinion scores have been used as ground truth in \cite{lu2004distance,obafemi2012character,kumar2012sharpness,alaei2015document} to automatically compute the document image quality metrics. A full reference document image quality assessment technique based on texture similarity index was introduced in \cite{alaei2016document} with promising results for OCR text images. There have been recent efforts to formulate image quality metrics that are not dependent on availability of ground truths. Xu et al. \cite{xu2016no} presented a no-reference image quality metric for document image quality assessment.

However, no-reference image quality metrics, such as \cite{xu2016no}, are typically designed for document images with OCR text, and focus on specific aspects of degradations that are mostly character level distortion (e.g., noise around a character, partial or overlapping characters), and are not suitable to quantify high levels of degradations in historical handwritten texts. Machine-printed documents have simple layouts and fonts, unlike handwritten documents that have complex layouts and variability in writing style. Handwritten documents suffer from degradations such as paper stains, ink bleed-through, missing or faded data, poor contrast, warping effects, etc. that hamper document readability and pose challenges for document image processing algorithms \cite{giotis2017survey}.

Such variability and severity of degradations is better captured using ground truth based document image quality metrics such as F-Measure, PSNR and DRD. Ground truth images offer a reference point, relative to which candidate images can be ranked. This immensely helps image processing algorithms in automatically evaluating the quality of processed images.

However, the reliance on the availability of ground truth images is also severely limiting. In fact, the target domain of automated document image processing consists of ground truth generation as one of the applications. Therefore, it is impractical to have access to ground truths corresponding to previously unseen document images to be processed on-the-fly. It is possible however, to have access to a \emph{training set} of document images and corresponding ground truth images.

This work explores a novel methodology wherein document quality metric scores computed using ground truth images as reference are used to train a model that learns the relationship between the difference in image quality represented by two images, and the corresponding metric score. Given two document images - an initial image and a processed image for which the quality metric is to be computed, the trained model can be used as a \emph{surrogate} that predicts the value of the metric. Training the surrogate model is a one-time investment, and requires access to input images with corresponding ground truth images. Post training, evaluation of the surrogate model is near-instant and does not require access to ground truth image for any given test image.

This paper is organized as follows. Section \ref{sec:prob} describes the concrete problem statement. Section \ref{sec:doc} discusses various document quality metrics available in literature. Section \ref{surrogate} explains the proposed surrogate modeling approach in detail. Section \ref{sec:experiments} demonstrated the efficacy of the proposed approach on the DIBCO and H-DIBCO datasets. Section \ref{sec:discussion} discusses an alternative deep learning formulation for surrogate model training. Section \ref{sec:conclusion} concludes the paper.

\section{Problem Statement}
\label{sec:prob}
%\subsection{Automated Document Image Processing}

The performance of document image processing tasks such as document binarization, filtering, enhancement, text or line segmentation, and high level applications such as word spotting in a document, significantly depends on the associated hyperparameter values. In general, an automated document image processing algorithm involves automatic selection of control parameters on-the-fly. This work uses document binarization as a running example throughout the text. 

Although there exist several automated document image processing methods in literature \cite{vats2017automatic, howe2013document}, a ground truth reference image is required to tune the associated hyperparameters. For example, an automatic document image binarization method is proposed in \cite{vats2017automatic}, where Bayesian optimization is used to infer the hyperparameters on-the-fly. The value of hyperparameters is chosen such that the quality metrics corresponding to the binarized image, (such as F-Measure, PSNR etc.) are maximized, or error is minimized. However, the optimization of quality metrics such as the F-Measure, PSNR, DRD and NRM is dependent upon the availability of a ground truth reference image. This limits the applicability of such methods in real world document image processing applications.


%\subsection{Problem Description}
This work explores the use of surrogate models to approximate any given document image quality metric. Let $X=\{{\bf x}_i\}_{i=1}^n$ be a set of document images comprising of $n$ images. Let $G=\{{\bf g}_i\}_{i=1}^n$ be the corresponding $n$ ground truth images. Let $P=\{{\bf p}_i\}_{i=1}^n$ be the set of processed images corresponding to $X$, obtained after processing using algorithm $A$, for example, a binarization algorithm. It is possible to compute and assign various quality metrics to ${\bf p}_i$ using ${\bf g}_i$ as a reference. Let $Y=\{y_i\}_{i=1}^n$ be a vector of values computed for any such quality metric $q$ corresponding to $P$. 

Let ${\bf x}'$ be a previously unseen test document image, with ${\bf p}'$ being the processed image obtained using a given algorithm $A$. The goal is to learn a surrogate model that can predict the value $y'$ of the metric $q$ for a given pair $({\bf x}', {\bf p}')$. Such a model will enable instant on-the-fly performance feedback for the algorithm $A$ without the availability of corresponding $g$. The model $\hat{y}$ is in effect, a \emph{surrogate} of the quality metric $q$. The following section explores popular document image quality metrics.

\section{Quality Metrics}
\label{sec:doc}
The most popularly used document image quality metrics include F-Measure, PSNR, DRD and NRM. These evaluation measures compute the image quality by comparing the document image with the corresponding ground truth reference image \cite{gatos2009icdar,
pratikakis2016icfhr2016}. 

%For details on the evaluation measures, the reader is referred to \cite{gatos2009icdar,pratikakis2016icfhr2016}.
%In general, F-Measure is the measure of accuracy, defined as the weighted harmonic mean of Precision and Recall. PSNR is a popularly used metric to measure the similarity between two images. DRD is used to measure the visual distortion for all the flipped pixels in binary document images \cite{lu2004distance}. NRM measures the pixel-wise mismatch rate between the ground truth image and the resultant binarized image. MPM measures how accurately the resultant binarized image represents the contour of text in the ground truth image. The misclassification pixels are penalized on the basis of their distance from the border of ground truth image. The binarized image quality is better with high F-Measure and PSNR values, and low DRD, MPM and NRM values. For details on the evaluation measures, the reader is referred to \cite{gatos2009icdar,pratikakis2010h,gatos2011icdar2,pratikakis2012icfhr,pratikakis2013icdar,ntirogiannis2014icfhr2014,pratikakis2016icfhr2016}.

\subsection{F-Measure}
F-Measure captures accuracy, defined as the weighted harmonic mean of Precision and Recall, 
\begin{equation} 
F-Measure = \frac{ 2 \times Recall \times Precision}{Recall + Precision },
\label{eq_fm}
\end{equation}
where $Recall=\frac{TP}{TP + FN}$ and $Precision=\frac{TP}{TP + FP}$. TP, FN and FP denote True Positives, False Negatives and False Positives, respectively.

%\subsection{pseudo F-Measure}
%Pseudo F-Measure introduced in \cite{ntirogiannis2013performance} is computed using pseudo-Precision and pseudo-Recall metrics that use distance weights with respect to the contour of the ground truth. 

\subsection{Peak Signal-to-Noise Ratio (PSNR)}
PSNR is a popularly used metric to measure how close an image is to another image. The higher the value of PSNR, the higher the similarity between two images. PSNR is defined via the mean squared error (MSE). Given a noise-free $M$$\times$$N$ image $I$ and its noisy approximation $K$, MSE is defined as,
\begin{equation} 
MSE = \frac{\sum_{i=1}^{M}\sum_{i=1}^{N}(I(i,j) - I'(i,j))^2}{MN},
\label{eq_mse}
\end{equation}
and PSNR is defined as,
\begin{equation} 
PSNR = 10 log \Big( \frac{C^2}{MSE} \Big),
\label{eq_psnr}
\end{equation}
where $C$ is the difference between foreground and background image.

\subsection{Distance Reciprocal Distortion metric (DRD)}
DRD is used to measure the visual distortion for all the $S$ flipped pixels in binary document images \cite{lu2004distance}, and is defined as,
\begin{equation} 
DRD = \frac{\sum\limits_{k=1}^{S}DRD_k}{NUBN},
\label{eq_drd}
\end{equation}
where $DRD_k$ is the distortion of the $k$-th flipped pixel, calculated using a $5 \times 5$ normalized weight matrix $W_{Nm}$ as,
\begin{equation} 
DRD_k = {\sum\limits_{i=-2}^{2} \sum\limits_{j=-2}^{2} |GT_k(i,j) - B_k(x,y)| \times W_{Nm}(i,j)}.
\label{eq_drdk}
\end{equation}

$DRD_k$ denotes the weighted sum of the pixels in the $5 \times 5$ block of the ground truth that differ from the centered $k$-th flipped pixel at $(x,y)$ in the binarized image. NUBN is the number of non-uniform (not all black/white pixels) $8 \times 8$ blocks in the ground truth image. 

\begin{figure*}[!t]
  \centering
  \subfloat[Surrogate model training framework.]{\includegraphics[width=4.1in]{Fig_model1new2.png}%
\label{fig1}}
\\
\subfloat[Prediction using the trained surrogate model on previously unseen data without access to ground truth images.]{\includegraphics[width=4.1in]{Fig_model2new2.png}%
\label{fig2}}
\caption{Surrogate modeling framework for learning document image quality metrics.}
\label{fig_model}
\end{figure*}

\subsection{Negative Rate Metric (NRM)}
NRM measures the pixel-wise mismatch rate between the ground truth image and the resultant binarized image. NRM is defined as,
\begin{equation} 
NRM = \frac{NR_{FN} + NR_{FP}}{2},
\label{eq_nrm}
\end{equation}
where $NR_{FN}=\frac{N_{FN}}{N_{FN} + N_{TP}}$, $NR_{FP}=\frac{N_{FP}}{N_{FP} + N_{TN}}$. \newline

\noindent $NR_{FN}$ denotes the false negative rate, $NR_{FP}$ denotes the false positive rate, $N_{TP}$ is the number of true positives, $N_{FP}$ is the number of false positives, $N_{TN}$ is the number of true negatives and $N_{FN}$ is the number of false negatives. The lower the value of NRM, the better is the binarized image quality. 

%\subsection{Misclassification Penalty Metric (MPM)}
%MPM measures how accurately the resultant binarized image represents the contour of text in the ground truth image. The misclassification pixels are penalized on the basis of their distance from the border of ground truth image. MPM is defined as follows:
%\begin{equation} 
%MPM = \frac{MP_{FN} + MP_{FP}}{2}
%\label{eq_mpm}
%\end{equation}
%where, $MP_{FN}=\frac{\sum\limits_{i=1}^{N_{FN}}d^i_{FN}}{D}$, $MP_{FP}=\frac{\sum\limits_{j=1}^{N_{FP}}d^j_{FP}}{D}$ \newline
%
%\noindent $d^i_{FN}$ and $d^j_{FP}$ denotes the distance of the $i^{th}$ false negative and the $j^{th}$ false positive pixel from the contour of text in the ground truth image. $D$ denotes the normalization factor that represents the sum over all the pixel-to-contour distances of the ground truth object. A low value of MPM indicates high performance of the binarization algorithm in identifying an object's boundary.

\section{Surrogate Models for Learning Document Quality Metrics}
\label{surrogate}
Surrogate modeling \cite{gorissen2010surrogate} has emerged as a popular methodology to obtain a fast-to-evaluate approximation of a computationally expensive or data-scarce function. Since the surrogate model allows fast evaluation, it can be used in applications such as optimization, parameter space exploration and sensitivity analysis where a large number of repeated calls to the target function are required. 

% @ToDo: Change this para to something more suitable
For example, complex simulation codes are often used during the design process of electronic devices such as antennae, microwave filters, etc. In order to study and test the effect of varying design parameters, repeated calls to simulation codes are made. Each of these calls may take several minutes to evaluate, and this hampers the design space exploration process. Globally accurate surrogate models offer near-instant evaluation and can be used in place of such simulation codes. Obtaining such a surrogate involves preparing training data by evaluating the simulation code on a carefully selected set of parameter combinations or points, which is chosen according to a statistical design or a sampling algorithm \cite{gorissen2010surrogate}.

Automated document image processing algorithms that make use of ground truth-based image quality metrics are an excellent use-case for surrogate models. Since ground truth images are scarce, therefore it makes sense to train an accurate surrogate model of a specified image metric using the limited quantity of available ground truth images. The surrogate model can then be used to estimate the value of image quality metric on-the-fly for any input test image, and corresponding processed image.

\subsection{Surrogate Model Types}
Numerous surrogate model types exist in literature with Artificial Neural Networks (ANN), Gaussian Processes (GP) and Support Vector Machines (SVM) being popular \cite{singh2016shape}. ANNs \cite{haykin2009neural} have shown excellent results in recent years, especially in applications involving visual data, and problems involving large training sets. GPs \cite{rasmussen2006gaussian} are very popular in design optimization applications and global surrogate modeling owing to their capability of providing the variance of prediction, in addition to the prediction itself. This aids adaptive sampling algorithms in quickly searching for optima within a mathematically principled framework. 
%However, GPs are not suited for large datasets since the standard formulation of GPs has a complexity of $\mathcal{O}(n^3)$ for $n$ training points.
SVR models \cite{cortes1995support} formulate the learning problem into an optimization problem that can be solved in a straightforward manner. SVR models have proven to be robust and stable in a variety of problems, and can deal with both small and large datasets. Consequently, SVR models are a reliable choice for general use in global modeling problems. This work uses ANN, GP and SVM regression models as surrogates for the purpose of experiments. However, the framework and methodology proposed herein is independent of any particular model type. A detailed discussion on the model types is out of scope in this work, and the reader is referred to \cite{smola2004tutorial,basak2007support} for SVR (support vector regression), \cite{rasmussen2006gaussian} for GPs and \cite{haykin2009neural} for ANNs.

%\subsection{Model Training Approach A: Images as Input}
%\label{approachA}
%Let each document image ${\bf x}_i$ and processed image ${\bf p}_i$ be represented as a $k_1 \times k_2$ matrix. To ease representation, each $k_1 \times k_2$ image is reshaped into a $1 \times k_1*k_2$ vector. The training set $\mathcal{T}$, therefore comprises of targets $Y$ and inputs $I$ obtained from concatenation of reshaped $X$ and $P$ as,
%\begin{align*}
%I &= \lbrack X_{n \times (k_1*k_2)} \quad P_{n \times (k_1*k_2)} \rbrack,\\
%\mathcal{T} &= (I,Y).
%\end{align*}

%\subsection{Model Training Approach: Relative image quality metrics as Input}
\subsection{Model Training}
\label{approachB}
Let each document image ${\bf x}_i$ and processed image ${\bf p}_i$ be represented as a $k_1 \times k_2$ matrix. The surrogate model learns the mapping $inputs \rightarrow target$. The target is the value of the document image quality metric. The metric may also be user-defined scores. Intuitively, the inputs must represent the quantity of change or transformation the image processing algorithm $A$ has brought about in the original image ${\bf x}_i \in X$ to obtain ${\bf p}_i \in P$.
%The large number of variables ($k_1 \prod k_2$) make training accurate surrogate models a challenging task. The intuition behind feeding $X$ and $P$ to the surrogate as input is to enable it to understand the transformation the document image processing algorithm was able to bring about in the original image ${\bf x}_i \in X$ to obtain ${\bf p}_i \in P$. 
The surrogate must be able to learn the value of a given image quality metric associated with the difference and nature of transformation from ${\bf x}_i$ to ${\bf p}_i$. This transformation can also be represented as a vector of metrics that represent the differences between ${\bf x}_i$ and ${\bf p}_i$. Possible candidates to measure such transformation include the metrics explained in Section \ref{sec:doc}, e.g., F-Measure, PSNR, DRD, etc.
% Check ($k_1 * k_2$) - dot product?

Let $M=\{q_j\}_{j=1}^T$ be $T$ metrics. For any given document image ${\bf x}_i$ and corresponding processed image ${\bf p}_i$, the \$1 \times T$ vector $I_i$ represents the values of $T$ metrics as,
\begin{equation}
I_i = [q_1({\bf x}_i, {\bf p}_i),\; q_2({\bf x}_i, {\bf p}_i),\; \cdots , \; q_T({\bf x}_i, {\bf p}_i)].
\end{equation}
The complete $n \times T$  matrix $I$ represents the input variables to be learned by the surrogate model. The use of quality metrics as input variables immensely simplifies the learning problem as compared to the case of using raw images as input. The target vector $Y$ simply represents the values of a specific document image quality metric $q$ computed as,
\begin{equation}
Y_i = q({\bf p}_i, {\bf g}_i).
\end{equation}
The training set for the surrogate model is then $\mathcal{T} = (I,Y)$.
%as before, albeit with a much simplified $I$. 
The framework of the proposed approach is pictorially described in Fig. \ref{fig_model}.

\section{Experiments}
\label{sec:experiments}

\subsection{Dataset}
The proposed surrogate-based approach is empirically evaluated on the images from seven well-known competition datasets: DIBCO 2009 \cite{gatos2009icdar}, H-DIBCO 2010 \cite{pratikakis2010h}, DIBCO 2011 \cite{gatos2011icdar2}, H-DIBCO 2012 \cite{pratikakis2012icfhr}, DIBCO 2013 \cite{pratikakis2013icdar}, H-DIBCO 2014 \cite{ntirogiannis2014icfhr2014} and H-DIBCO 2016 \cite{pratikakis2016icfhr2016}. These datasets contain machine-printed and handwritten historical document images suffering from various kinds of degradations including stained paper, faded ink or ink bleed through, wrinkles and unknown graphical symbols. In total there are 86 document images, out of which 63 randomly chosen images are used for training and 23 images for testing. As an example, the framework is applied to perform automatic image binarization using Bayesian optimization as proposed in \cite{vats2017automatic}. The document image quality metrics used as inputs for the surrogate models include PSNR, DRD and NRM. The target image quality metric to be approximated using surrogates is the F-Measure.

%; and few images from the Bickley diary dataset \cite{deng2010binarizationshop}. The competition datasets contain machine-printed and handwritten historical document images suffering from different kinds of degradations such as stained paper, faded ink or ink bleed through, wrinkles and unknown graphical symbols. The Bickley diary dataset is a very challenging dataset with high degree of degradations. It consists of grayscale handwritten text images of the original dairy of Ms. Anna Felton Bickley \cite{deng2010binarizationshop}.

%For the experimental set up, in total there are 93 document images, out of which 69 images are used for training and 24 images for testing. The framework is applied to image binarization method proposed in our previous work \cite{vats2017automatic}, and the resultant values from five evaluation measures (F-Measure, PSNR, DRD, NRM and MPM) are used to train the surrogate models.


\subsection{Experimental Results}
The $\varepsilon$-SVR variant \cite{basak2007support} with the Sequential Minimal Optimization (SMO) \cite{platt1998sequential} solver is used for the following experiments. The hyperparameters of the SVR model are optimized using Bayesian optimization \cite{snoek2012practical}. The GP model uses a Gaussian kernel with hyperparameters being optimized using Maximum-Likelihood Estimation (MLE) \cite{rasmussen2006gaussian}. The variant of ANN used is a feed-forward back propagation neural network \cite{haykin2009neural} trained using the Levenberg-Marquardt algorithm \cite{demuth2014neural}.

The error metrics used to test the accuracy of the surrogate models are Root Relative Square Error (RRSE), Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) \cite{graczyk2009comparative}.

% The following metrics are used to test the accuracy of the SVR regression model.
% \begin{enumerate}
% \item Root Relative Square Error (RRSE) measures the quality of model approximation with reference to the mean approximation. The ideal value of RRSE is 0. RRSE is defined as,
% \begin{equation*}
% \label{eq:rrse}
% RRSE({\bf y},{\bf \hat{y}}) = \sqrt{\frac{\sum_{i=1}^n (\hat{y}_i - y_i)^2}{\sum_{i=1}^n (y_i - {\bar{y}})^2}}
% \end{equation*}
% where $y$, ${\bf \hat{y}}$ and $\bar{y}$ are the true, predicted and mean true response values respectively.
% \item Mean Absolute Error (MAE) measures the average deviation between the true and prediction response values, and is defined as,
% \begin{equation*}
% MAE({\bf y},{\bf \hat{y}}) = \frac{\sum_{i=1}^n \mid y_i - \hat{y}_i \mid }{n}
% \end{equation*}
% \item Root Mean Square Error (RMSE) measures the sample standard deviation between the true and predicted response values. RMSE is calculated as,
% \begin{equation*}
% RMSE({\bf y},{\bf \hat{y}}) = \sqrt{\frac{\sum_{i=1}^n(y_i - \hat{y}_i)^2}{n}}.
% \end{equation*}
% \end{enumerate}

%\begin{table}[htb]
%\centering
%\caption{SVR model error estimates computed on a test dataset of $12$ document images distinct from the training set.\label{tab:error}}
%\begin{tabular}{lcc}
%\hline
%RRSE & MAE & RMSE\\ \hline
%$0.8947$ & $1.8006$ & $2.1741$\\
%\hline
%\end{tabular}
%\end{table}

%Table \ref{tab:error} lists the values of the error metrics for the trained SVR regression model corresponding to Approach A described in Section \ref{approachA}. The RRSE of 0.8947 indicates a reasonable fit of the model, given a small challenging training set of $61$ document images. The values of MAE and RMSE also point towards a usable model, but indicate room for improvement in model accuracy. Training data preprocessing techniques like dimensionality reduction can aid in simplifying the target function to be approximated by the SVR model, thus bettering model accuracy. Such approaches will be explored as future work.



\begin{table}[!t]
\centering
\caption{Error estimates for the surrogate models.\label{tab:resB}}
\begin{tabular}{p{1.5cm}p{1.5cm}p{1.5cm}p{1cm}}
\hline
Model Type & RRSE & MAE & RMSE \\ \hline
ANN & 0.8781 & 2.9980 & 4.1733 \\
SVR & {\bf 0.8053} & {\bf 2.7107} & {\bf 3.8272}\\
GP & 1.0633 & 3.3506 & 25.5363 \\
Ensemble & 0.8979 & 2.9477 & 5.0533 \\
\hline
\end{tabular}
\end{table}

\begin{table}[!t]
\centering
\caption{Model training and prediction times for a test dataset of $23$ document images distinct from the training set.\label{tab:times}}
\begin{tabular}{lll}
\hline
Model Type & Training Time (s) & Prediction time (s) \\ \hline
ANN & $0.3960$ & $0.0717$\\
SVR & $37.2203$ & ${\bf 0.0039}$\\
GP & ${\bf 0.1035}$ & $0.0075$\\
\hline
\end{tabular}
\end{table}

Table \ref{tab:resB} lists error estimates corresponding to the proposed model training approach described in Section \ref{approachB}. All three error measures indicate that ANN and SVR outperform GP surrogate for the given dataset. The table also contains error estimates corresponding to an ensemble model that simply averages the predictions of ANN, SVR and GP models. It can be observed from Table \ref{tab:resB} that SVR emerges as the single best performing model type.

Table \ref{tab:times} reports the time in seconds taken to train the surrogate models and the total time taken by the models to predict F-Measure values of $23$ unseen test document images. It can be seen that once the model is trained, predictions are made almost instantly. This makes the surrogate model assisted approach ideal for use on-the-fly in image processing algorithms. The time taken for preprocessing and model training is a one-time investment. A relatively high value of training time for SVR is due to the time taken to optimize hyperparameters using Bayesian optimization. This was to ensure that the hyperparameters are as close to optimal, given a relatively small training set. 

%\begin{table}[!t]
%\centering
%\caption{Predicted F-Measure values using the surrogate model.\label{tab:fm}}
%\begin{tabular}{p{1cm}p{1cm}p{1cm}p{1.7cm}p{1.5cm}}
%\hline
%Dataset & Image \# & Observed & Predicted (SVR model) & Predicted (ensemble)\\ \hline
% & &  & & \\
%\hline
%\end{tabular}
%\end{table}

\begin{figure}[!t]
\centering
\includegraphics[width=3in]{surrogatePredictions.png}
\caption{Predicted value of F-Measure by each surrogate model type for test images. Surrogate models are accurate in general except for test instances 2, 10, 18 and 22.}
\label{fig_surrPred}
\end{figure}

% Comment: If space is available, describe the caption in few more lines..

Figure \ref{fig_surrPred} depicts the values of F-Measure predicted by different model types following the proposed model training approach. It can be seen that there are relatively large errors made by all model types for test instances numbers 2, 10 and 18. However, all models have been able to capture the \emph{general trend} of the test images, except for test instances 2 and 22. Even though the error is large for test instances 10 and 18, the models have been able to learn the 'downward' leaning behavior of F-Measure therein. % Check this para

\begin{figure}[!t]
\centering
\includegraphics[width=3in]{Fig_testImages.png}
\caption{Test images having high prediction errors.}
\label{fig_testImages}
\end{figure}

Figure \ref{fig_testImages} shows test instances 2, 10 and 18 on which all surrogate models struggled. It can be seen that test image 2 has high variation in image contrast and intensity. Test image 10 is suffering from paper wrinkles and fold marks, in addition to pen strokes of varying intensities. Test image 18 also contains variation in pen stroke intensities. Test image 22 (not shown) includes text written with multiple inks. These characteristics are not well-represented in the training set, leading to large errors in prediction of corresponding F-Measure. Having a larger training set that captures a wide variation of paper degradations, writing styles, pen stroke intensities, etc. will improve the performance of surrogate models.

%Algorithm is not able to find optimal match of these test images. These are the most challenging images, because: 
%Test image 2: High variation in image contrast and intensity. 
%Test image 10: Paper wrinkles and fold marks, pen strokes of varying intensities. 
%Test image 18: Looks like a simple image, but if looked carefully, has variation in intensities of pen strokes. Difficult to find a good match in the trained image set. 
%If training set is further extended to include a variety of images, such cases can be better taken care of. 

Figure \ref{fig_binRes} shows a sample test document image binarized using the method \cite{vats2017automatic}. The hyperparameters of the binarization algorithm are optimized using Bayesian optimization \cite{snoek2012practical} as described in \cite{vats2017automatic}. The objective function to be maximized using Bayesian optimization is the F-Measure (as predicted by the SVR surrogate model trained above). The resultant image in Figure \ref{fig_bin} is clean and validates the accurate modeling of F-Measure by the SVR surrogate.

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command, the
% \label for the overall figure must come after \caption.
% \hfil must be used as a separator to get equal spacing.
% The subfigure.sty package works much the same way, except \subfigure is
% used instead of \subfloat.
%
%\begin{figure*}[!t]
%\centerline{\subfloat[Case I]\includegraphics[width=2.5in]{subfigcase1}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{subfigcase2}%
%\label{fig_second_case}}}
%\caption{Simulation results}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.


% An example of a floating table. Note that, for IEEE style tables, the 
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals/conferences use top floats
% exclusively. Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the \fnbelowfloat
% command of the stfloats package.

\section{Discussion: Raw Images as Input}
\label{sec:discussion}
The approach discussed herein represents the inputs as image quality metrics measuring difference between $X$ and $P$. This is done to simplify the learning problem to remain within a handful of input parameters, and allows highly efficient learning and inference.
It is also possible to consider the input and processed images themselves as input, without any post-processing to calculate quality metrics. The surrogate model will then learn the mapping $({\bf x}_i, {\bf p}_i) \rightarrow target$, where each ${\bf x}_i$ and ${\bf p}_i$ is a $k_1 \times k_2$ matrix. The representation of inputs $I: ({\bf x}_i, {\bf p}_i)$ as images is an ideal use-case of deep learning inspired surrogate models such as convolutional neural networks (CNNs) \cite{krizhevsky2012imagenet}. The caveat herein is that the training set must be sufficiently large to allow meaningful learning to proceed.

\begin{figure}[!t]
  \centering
  \subfloat[Original document image.]{\includegraphics[width=3in]{data2014_02.png}%
\label{fig_org}}
\\
\subfloat[Resultant binarized document image.]{\includegraphics[width=3in]{data2014_02_bgr_kittler.png}%
\label{fig_bin}}
\caption{Automatic document image binarization performed by the algorithm described in \cite{vats2017automatic}. Hyperparameters of the binarization algorithm are optimized to maximize the F-Measure approximated using the SVR surrogate model.}
\label{fig_binRes}
\end{figure}

\section{Conclusion}
\label{sec:conclusion}
A novel approach is presented in this paper that uses surrogate models to learn a given document image quality metric. The surrogate model is trained on a dataset comprising of inputs that quantify differences in image quality between raw input images and corresponding processed images obtained using an image processing algorithm. The target to be approximated by the surrogate model is the value of a given document image quality metric that is computed for the training set by comparing the processed candidate images to corresponding ground truth images. Post training, the surrogate can be used to quickly predict the value of the document image quality metric for any given test pair of raw and processed document images, without any need for corresponding ground truth images. The methodology is tested on well-known publicly available document image datasets. Experimental evaluation indicates that the surrogate model is able to accurately learn the relationship between differing image quality and corresponding variation in document image quality metric value. Future work includes obtaining and experimenting with larger training sets, and exploring regression convolutional neural networks as surrogate models.

% conference papers do not normally have an appendix


% use section* for acknowledgement
\section*{Acknowledgment}
This work was funded by the G\"oran Gustafsson foundation, the eSSENCE strategic collaboration on eScience, and the Riksbankens Jubileumsfond (Dnr NHS14-2068:1).


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

%\newcommand{\BIBdecl}{\setlength{\itemsep}{0.25 em}}

\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{refs}


% that's all folks
\end{document}



