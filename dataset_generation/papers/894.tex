% Template for ICASSP-2018 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{fancyhdr}
 \usepackage{epsfig,amssymb,amsmath,multirow,boldline,graphicx,makecell,hyperref,subcaption,textcomp}
\usepackage{kotex}
\usepackage{float}
\usepackage{multirow}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}


\usepackage{xcolor}
\newcommand{\sscm}[1]{\textcolor{blue}{[#1 --SS]}}
\newcommand{\ohcm}[1]{\textcolor{red}{Oh$>$ #1 --}}
\newcommand{\jg}[1]{\textcolor{orange}{[#1 --JG]}}



% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}
 
% Title.
% ------
\title{Noise-Tolerant Audio-Visual Online Person Verification\\ Using an Attention-based Neural Network Fusion\\
%\sscm{Multi-modal fusion for Online person recognition Robust against incomplete modality}
}

% 
%
% Single address.
% ---------------
\name{Suwon Shon, Tae-Hyun Oh, James Glass}

\address{MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, MA, USA\\
{\small \tt \{swshon,glass\}@mit.edu, taehyun@csail.mit.edu} }
\begin{document}
\ninept
%
\maketitle
%
\begin{abstract}
In this paper, we present a multi-modal online person verification system using both speech and visual signals. Inspired by neuroscientific findings on the association of voice and face, we propose an attention-based end-to-end neural network that learns multi-sensory associations for the task of person verification. The attention mechanism in our proposed network learns to conditionally select a salient modality between speech and facial representations that provides a balance between complementary inputs. By virtue of this capability, the network is robust to missing or corrupted data from either modality. In the VoxCeleb2 dataset, we show that our method performs favorably against competing multi-modal methods. Even for extreme cases of large corruption or an entirely missing modality, our method demonstrates robustness over other unimodal methods.
%As part of this work, we present interpretable examples where the presence of some facial appearance attribute affects the attention preference to a statistically significant extent.
\end{abstract}
%
\begin{keywords}
person verification, recognition, multi-modal, cross-modal, attention, neural network.
\end{keywords}
%
\section{Introduction}
\label{sec:intro}
% % Data availability + dnn => cross-modal applications% With the significant growth of freely available multimedia data by the online sharing platform such as YouTube, the accessibility of a large amount of multi-modal data has become much easier than before. In conjunction with deep learning method, this large amount of data has been actively studied in many areas: sound source localizing from the video[cite], event detection, speech recognition, diarization\cite{Noulas2012,Hoover2017}, cross-modal retrieval, audio-visual semantic correspondence. % % The motivation of the studies is to explore and understand the cross-modal correlation between audio and visual. % As following research on this area, we will explore the problem of online person verification using voice and face based on the multi-modal fusion approach under corrupted modality. 

From cognitive and neuroscience studies on the integration of face and voice signals in humans, it has been observed that the face-voice association is treated differently in the brain compared to other paired stimuli~\cite{von2006implicit}, and that this perceptual integration plays an important role and is actually leveraged for person recognition processing \cite{hasan2016hearing}.    
% Add if possible% Campanella, S., Belin, P.: Integrating face and voice in person perception. Trends in Cognitive Sciences 11(12), 535{543 (2007)
Inspired by these findings, computational models have been recently introduced to understand whether, and to what extent, such models can leverage associations between different modalities.
To investigate this multi-modal association, Nagrani et al.~\cite{nagrani2018seeing}, Horiguchi et al.~\cite{horiguchi2018face} and Kim et al.~\cite{kim2018learning} presented a face-voice cross-modal matching task by learning a shared representation for both modalities.
Neural network-based cross-modal learning is explored to distill common or complementary information from large-scale paired data.
In particular, Kim et al. showed that their computational model has similar behaviors to humans.
% Also, Horiguchi et al.~\cite{horiguchi2018face} proposed a similar model with the model used in the above for the face-voice matching task.

Based on these explorations of multi-modal computational learnability, 
we propose to investigate the use of multi-modal neural networks for the more specific and challenging task of person verification. 
There has been some work that investigates person verification using multi-modal biometric data~\cite{Choudhury1999,Luque2006,Thomas2006,hazen07,Sargin2009,Sell2018multimodal}.
These methods typically consist of independent face and voice unimodal recognition modules that are trained separately, with respective scores from the unimodal modules being combined with score fusion.
These methods also typically run in an off-line manner, whereby multiple frames of the face and several seconds of speech are used to maximize recognition performance, so there is an inherent latency built into the methodology.
% \ohcm{
On the other hand, feature-level fusion has been uncommon in the person verification.
% , but the score-level one with asynchronous frames of the multi-modal inputs has been typically used.
The feature-level fusion has been more commonly adoped in audio-visual speech recognition~\cite{Neti2000, Kratt2004} from a simple concatenation of the feature to end-to-end system~\cite{Sanabria2016, Petridis2018} with synchronized audio-visual feature.
% where they show the effectiveness of the feature-level fusion.% However, previous work on the multi-modal person recognition typically relies on the score fusion using asynchronous frames of the multi-modal inputs, % rather than the feature-level one. 
In this work, we shed light on the feature level fusion in the multi-modal person recognition.
% }% \sscm{% In contrast to person verification, feature-level fusion is more commonly observed in audio-visual speech recognition~\cite{Neti2000, Kratt2004} from a simple concatenation of the feature to end-to-end system~\cite{Sanabria2016, Petridis2018} with synchronized audio-visual feature. However, previous work on the multi-modal person recognition typically relies on the score fusion using asynchronous regions of the multi-modal inputs.}% \sscm{% In contrast to person verification, feature-level fusion is more commonly observed in audio-visual speech recognition~\cite{Neti2000, Kratt2004} from the simple concatenation of the feature to end-to-end system~\cite{Sanabria2016, Petridis2018}.% The feature-level fusion with synchronized modality has a great advantage to be applied directly on the single modality speech recognition framework.% However, for person verification, synchronized modality is not a critical but gathering salient information as much as possible for each modality is important to guarantee performance. Thus, the previous work on the multi-modal person recognition typically relies on the score fusion using asynchronous regions of the multi-modal inputs.% }% \sscm{ For example}, feature-level fusion can be accomplished by training classifiers using a concatenation of the audio and visual synchronized features since a front facial view video is mandatory for the task. % \sscm{However, person identification can also be done when one modality is absent. Moreover, a very breif sample makes it more difficult to recognize a person's identity. Thus, the previous work on the multi-modal person recognition typically rely on the score fusion using asynchronous regions of the multi-modal inputs.}% \ohcm{from; % However, person identification can also be done when one modality is absent.  Moreover, a very brief sample makes it more difficult to recognize a person's identity. Thus, fusion for multi-modal person recognition is usually done in an off-line manner with score-level integration using asynchronous regions of the multimedia input.}% \ohcm{changed to; % However, while the feature level fusion has a chance for the  , contrary to the score fusion, it is not applicable when one modality is absent. % Thus, the previous work on the multi-modal person recognition typically rely on the score fusion using asynchronous regions of the multimodal inputs.% }% Definition of ``online'' person verification % Person verification is to recognize a person's identity in the multi-media file and it consists of face and speaker recognition system. The ``online'' person verification means that the system used a synced short segment of face and voice from the input and gives a result. Unlike typically speaker verification that uses the entire frame of the utterance, the online person verification uses short segment which is synced with the video.% This topic has not been actively explored.% Recently, Sell et al. proposed to fuse of speaker and face embeddings~\cite{Sell2018multimodal}. They used a score-level fusion approach using logistic regression. % \ohcm{the following sentence feel like unnecessary}Although it's not directly related, cross-modality identification is also studied with the publicly available dataset. 

In this paper, we explore an online audio-visual fusion system for person verification using face and voice.
In contrast to previous work on person verification, our proposed fusion method is conducted at the feature level.
In particular, we focus on the fusion of synchronized audio-visual data based on the argument that the system should naturally emphasize the time-varying contribution of each modality according to its instantaneous quality at any point time. 
Our method exploits a single video frame of the face and a short span of speech to facilitate online processing applications, while maintaining high performance relative to prior state-of-the-art.
Motivated by the attention~\cite{corbetta2002control} and the multi-sensory association mechanism of the human brain~\cite{von2006implicit}, our fusion method is implemented by an attention mechanism, such that it can learn to evaluate the salient modality of input data.
Due to the inherent robustness of this architecture, we expect stable performance even when there is corrupted information from either face or voice, due to noise masking, or missing information from basic pre-processing failures of either modality e.g., face detection, voice activity detection (VAD), etc.
% To enable this capability, we used neural network based attention layer that calculates the contribution of each modality for robust fusion. 
We experimentally verify that this audio-visual fusion network is robust to corrupted and missing information from one modality.
We also analyze the attention layer output to see how it behaves under certain characteristics of the input.

% As a separate line of work, although Hoover et al.~\cite{Hoover2017} appears to be similar to our scope, it tackled a different problem to identify active speakers from the closed set of persons in a given video. Also, they do not learn multi-modal representations.% \sscm{Is this paragraph necessary? or should be in the intro?}% \ohcm{seems this is out of context in this context. better to move to related work to differentiate the work}% This is fully supervised representation learning method and it is differ from the joint learning of multi-modal data unsupervisedly in previous study in [cite unsupervised training methods..]. They concentrated on learning joint and correlated information between face and voice, or image and audio. Thus the exclusive information of each modality would be ignored or collapsed. However, this fusion approach is utilizing joint and exclusive information both to have robust identity representation. 


\begin{figure*}[ht]
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{fusion_layer.pdf}\vspace{-3mm}
         \caption{System $\mathcal{A}$}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{fusion_add.pdf}\vspace{-3mm}
         \caption{System $\mathcal{B}$}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{fusion_att.pdf}\vspace{-3mm}
         \caption{System $\mathcal{C}$}
     \end{subfigure}
        \caption{Neural network based fusion approaches. $\mathbf{e}_v$: speaker embedding, $\mathbf{e}_f$: face embedding}
        \label{fig:attention}
\end{figure*}


\section{Online Person verification from Video}
The verification of a person's identity is often achieved by using information from a single modality that contains the biometric signal, such as images for face identification and audio for speaker verification.  When multiple modalities are available, such as in video recordings of someone speaking, then opportunities exist to explore fusion of information from both modalities.  
Both vision and hearing must address challenges due to variation in a persons appearance or voice, or occlusion due to environmental conditions.  In the case of vision, the image of a person's face will appear differently due to physical changes in a person's appearance, emotional state, occlusion due to other objects, and will depend on position and orientation relative to the camera etc.  Likewise a person's voice can change due to health, or emotional state, and will be affected by environmental noise, reverberation and channel conditions.  

One interesting difference between face and speaker ID technologies is that high quality face ID can be obtained from a single image of a person's face.  In video data, this corresponds to a single instance in time, and can be sampled many times a second.  In contrast, to achieve the same level of performance for speaker verification tasks typically requires a much longer sample of speech from the talker (e.g., 10-30sec of speech are typical conditions, with a few seconds of speech being a much more challenging task).  This discrepancy is because, unlike images of faces, the speech signal is highly time-varying due to the nature of speech production.  A random snippet of speech can be dramatically different from another, even when spoken by the same talker, due to differences in the acoustic-phonetic sequences present in the samples.  The characteristics of a talkers voice are more reliably extracted when the duration of a speech recording contains more examples of the different sounds produced by the talker.  For person verification, there is some truth to the mantra that a picture is worth a thousand words!

%There is inherently a different time-scale for person identification between vision and speech.  %Verification of a person's identity is commonly approached by using unimodal data such as face images and voice recordings independently. For processing video, in general face data is more reliable than voice because the video can contain a face in every single frame and facial information does not change rapidly between frames. The voice is produced by the source-filter model by a combination of sound source, i.e. vocal cords, and acoustic filter, i.e. vocal tract. Although they represent the voice's identity, they have high variability in a short time since the vocal cords are very sensitive to the emotion and accent and the vocal tract are influenced by the articulators, such as tongue, teeth, and lips, to speak a language. %Both face and voice also lie in a challenging condition from various factors. For the face, occlusion or temporary alteration of a face, such as a beard, lipsticks, sunglasses, make difficult to verify between reference and test face. %For voice, environmental and channel noise both degrade Signal-to-noise ratio of speech. A noisy condition also makes the Lombard effect that the voice characteristic is changed by the speaker increasing their vocal effort in a loud place.%From these various factors, voice and face have their exclusive information. Multi-modal person verification supposed to utilize the exclusive information to verify identity, so the multi-modal fusion approach is a very important step to have maximum effectiveness. The online person verification claimed here means that the system verify the identity only using the current frame or short segment of video and we do not wait until the all speech and video are captured before verification. % Thus, the performance measured using only 1 frame of video and relevant speech frames.% \subsection{Related work}% From cognitive and neuroscience studies on face-voice integration of human being, it has been observed that face-voice association is treated differently from other paired stimuli~\cite{von2006implicit}, and those perceptual integration plays an important role and is actually leveraged for person recognition processing \cite{hasan2016hearing}.  % % Add if possible% % Campanella, S., Belin, P.: Integrating face and voice in person perception. Trends in Cognitive Sciences 11(12), 535{543 (2007)% Inspired by these findings, machine learning based computational models have been introduced to understand whether and in which extent the models can leverage the association of different modalities.% For understanding the multi-modal association, Nagrani et al.~\cite{nagrani2018seeing}, Horiguchi et al.~\cite{horiguchi2018face} and Kim et al.~\cite{kim2018learning} presented the face-voice cross-modal matching task by learning a shared representation for both modalities.% The neural network based cross-modal learning is exploited to distill common or complementary information from large-scale paired data.% Particularly, Kim et al. showed that the machine model has similar performance behaviors to that of humans.% % Also, Horiguchi et al.~\cite{horiguchi2018face} proposed a similar model with the model used in the above for the face-voice matching task.% Based on these evidence of the computational learnability, % we propose multi-modal neural networks for the person verification task which is a more challenging problem. % There have been few works that investigate the person verification with multi-modal biometric data~\cite{Choudhury1999,Luque2006,Thomas2006,Sargin2009,Sell2018multimodal}.% These methods are constituted by independent face and voice uni-modal recognition modules trained separately, and the respective scores from the uni-modal modules are combined in a late fusion on the score level.% Moreover, these works proposed methods that run in an off-line manner, where multiple frames of the face and few seconds long speech are used to maximize the recognition performance. % Distinguished from these works, our proposed fusion method is conducted on the feature level, which has not been sufficiently discussed in the previous studies.% Also, our method exploits a single frame of the face and a short length of speech for the online processing application purpose, while maintaining high performance favorable against the prior arts.% Furthermore, motivated by the attention mechanism~\cite{corbetta2002control} and multi-sensory association mechanism of brain~\cite{von2006implicit}, our fusion method is implemented by an attention mechanism, such that it can learn to evaluate the salient modality of given data.% As a separate line of work, although Hoover et al.~\cite{Hoover2017} appears to be similar to our scope,% it tackled a different problem to identify active speakers from the closed set of persons in a given video. % Also, they do not learn multi-modal representations.% \sscm{Is this paragraph necessary? or should be in the intro?}% \ohcm{seems this is out of context in this context. better to move to related work to differentiate the work}This is fully supervised representation learning method and it is  differ from the joint learning of multi-modal data unsupervisedly in previous study in [cite unsupervised training methods..]. They concentrated on learning joint and correlated information between face and voice, or image and audio. Thus the exclusive information of each modality would be ignored or collapsed. However, this fusion approach is utilizing joint and exclusive information both to have robust identity representation. % % As a seperate line of related work, Ngiam et al. \cite{ngiam2011multimodal} and Chung et al. \cite{chung2017lip} use the joint signals of face and voice and show that disambiguation between voiced and unvoiced consonants can be improved. % % In the previous studies on person recognition, all the experiment was conducted in off-line manner~\cite{Choudhury1999,Luque2006,Thomas2006,Sargin2009,Sell2018multimodal}. A multiple frame of face and few seconds long speech were used to maximize the recognition performance. The unimodal recognition systems were trained exclusively using face and voice data exclusively. For fusion method, all the previous studies adopted score-level late fusion by weighted sum of each score from a unimodal system such as face and speaker recognition. Score normalization and calibration can be done before fusion and an appropriate weights of the scores could be considered by a sample trials from training or target domain samples to maximize the fusion efficiency. Feature-level fusion was not sufficiently discussed in the previous studies. %\vspace{1mm}%\noindent\textbf{Corrupted and missing modality}\quad

When processing video data there will be situations where one modality or the other may be corrupted or altogether missing.  A corrupted modality can be caused by a false alarm of a pre-processing step such as face detection or voice activity detection (VAD).  For example, a face detector may incorrectly identify a face, or detect the wrong face or region in the video, or the VAD might be activated by background noise that does not contain a human voice. These corrupted inputs could easily confound a multi-modal network to the point where its performance could be worse than fusing separate unimodal systems.  
% \ohcm{
When one modality is completely missing, 
one easiest solution in practice would be to switch to apply an alternative backup unimodal system to the uncontaminated modal data.
We will demonstrate that our multi-modal system performs favorably against this systematic approach even in the complete missing case.
% }% \sscm{% A particular case of a corrupted modality is where one modality is completely missing.  % In these cases, it would be desirable for the multi-modal system to naturally convert to unimodal behavior for the existing modality.% However, we supposed that this missing modality is also used for the multi-modal system to see how the multi-modal network operated in this case.% }%Missing modality means that one of the modality has no information in the input. This is the case when the pre-processing algorithm operated properly and we are able to switch the recognition system from multi-modal to a single-modal system. However, we supposed that this missing modality is also used for the multi-modal system to see how the multi-modal network operated in this case.

\section{Audio-Visual Multi-modal Fusion}
In this section, we describe the proposed multi-modal fusion approach and its voice and face representation subsystems.
Our method is distinguished from previous studies by its use of a feature-level fusion approach based on neural network models. 
Given discriminative face and speaker representations extracted from each subsystem, our attention layer evaluates each contributions of the  representations.
Then, we combine them according to the estimated contributions, so that a joint representation is obtained.
We learn this whole fusion network for the person verification task without additional supervision for the attention.
In the test phase, we compute the similarity of joint representations between the query (enrollment) and test samples to verify identities.

% A discriminative face and speaker representation is first extracted from each subsystem.  % Then, we fuse the representations by projecting them into a joint subspace and then producing a joint representation.  % For person verification, we calculate the similarity of the new representation between the query (enrollment) and test samples to verify identities.

In the following sections we elaborate the proposed fusion approach and the speech and face sub-systems used in our experiment. 

\subsection{Proposed Fusion Approach}
We develop a multi-modal attention model that can pay attention to the salient modality of inputs while producing a powerful fusion representation appropriate for the person verification task.
This is inspired by the humans' multi-sensory capability. Among diverse facets of the human multi-sensory system, the presence of the selective attention \cite{corbetta2002control} allows humans to first pick salient information even from crowded sensory inputs.
The human attention mechanism dynamically brings salient features to the forefront as needed without collapsing holistic information into blurry abstraction.

The realization of this attention mechanism in deep neural networks has achieved successes in various machine learning applications.
% ~\cite{chorowski2015attention, bahdanau2014neural, ba2014multiple, xu2015show}.
Our attention network is similar to the soft attention~\cite{bahdanau2014neural} which is differentiable.
While most previous work applies spatial or temporal attention, our attention is extended to be attentive across the modality axis.
Given face and speaker embeddings $\mathbf{e}_f$ and $\mathbf{e}_v$, we define the attention score ${\hat a}_{\{f,v\}}$ through attention layer $f_\texttt{att}(\cdot)$ as 
% gives \begin{equation}{\hat a}_{\{f,v\}} = f_\texttt{att}(\left[ \mathbf{e}_f, \mathbf{e}_v \right]) = \textbf{W}^\top\left[ \mathbf{e}_f, \mathbf{e}_v \right] + \textbf{b},
\end{equation}
where $\textbf{W} \in \mathbb{R}^{m\times d}$  and $\textbf{b} \in \mathbb{R}^{m}$ are the learnable parameters of the attention layer, $m$ and $d$ denote the number of modality to fuse and the input dimension of the attention layer respectively, and $\mathbf{e}_f$ and $\mathbf{e}_v$ will be discussed in the next subsection.
% , i.e. $m=2, d=1112$ on this paper.
Then, the fused embedding $\mathbf{z}$ is constructed by the weighted sum as
\begin{equation}
\mathbf{z} = \sum_{i \in \{f,v\}} \alpha_i \tilde{\mathbf{e}}_i,\quad\textrm{where}\quad 
\alpha_i = \frac{\exp({\hat a}_i)}{\sum_{k \in \{f,v\}} \exp({\hat a}_k)}
, i\in\{f,v\},
\end{equation}
where $\tilde{\mathbf{e}}$ denotes the projected embeddings to a co-embedding space compatible with the linear combination.
% To map $\tilde{\mathbf{e}}_{\{f,v\}}$ from ${\mathbf{e}}_{\{f,v\}}$, we used 2 Fully Connected (FC) layers with 1,200 and 600 hidden nodes, i.e. $\tilde{\mathbf{e}} \in \mathbb{R}^{600}$. We used ReLUs for non-linearities in the first FC layer. 
To map $\tilde{\mathbf{e}}_{\{f,v\}}$ from ${\mathbf{e}}_{\{f,v\}}$, we used a Fully Connected (FC) layer with 600 hidden nodes, i.e. $\tilde{\mathbf{e}} \in \mathbb{R}^{600}$. We do not used non-linearity in the FC layer. 
We train the attention networks by the contrastive loss on the joint embedding $\mathbf{z}\in \mathbb{R}^{600}$.
For each training step, we used 60 positive and negative pairs, a total of 120 pairs for each mini-batch, and all pairs were sampled from the VoxCeleb2 development set.



The proposed attention networks allow us to naturally deal with corruption or missing data from either modality.
In our framework, the attention networks spontaneously learn to assess the quality of given multi-modal data implicitly.
For example, if the audio signal is largely corrupted by surrounding noise, the attention network would switch off the voice representation path and only rely on the face representation, and vice versa.
In this way, as long as at least one modality provides appropriate information for the task, this model will be able to perform person verification. 
% \ohcm{still working on it; add cognitive reference of voice-face integration mechanism in brain as a motivation of the network design} \vspace{1mm}\noindent\textbf{Relationship with Other Fusion Methods}\quad
In the context of the multi-modal person verification, the traditional score-level fusion with logistic regression has been investigated up to these days~\cite{Choudhury1999,Luque2006,Thomas2006,hazen07,Sargin2009,Sell2018multimodal}.
These score fusion methods do not leverage any large capacity deep neural networks which are capable of dealing with non-trivial fusion strategy.
One can come up with an extension based on the above approaches, where FC layers are stacked on top of the concatenated speaker and face embeddings, $\mathbf{e}_v$ and $\mathbf{e}_f$, as shown in Figure~\ref{fig:attention}-(a), i.e. System $\mathcal{A}$. We used 2 FC layers with 1,200 and 600 hidden nodes and ReLUs for non-linearities in the first FC layer. 
This can be regarded as a feature level fusion similar to Nagrani et al.~\cite{nagrani2018seeing}.
A downside of this would be the fact that the performance of the system is degraded by corrupted modal data.
% corrupted modality cannot be dealt with as well as large outliers. 

Another neural network based fusion can be accomplished as shown in Figure~\ref{fig:attention}-(b). 
FC layers are stacked on top of respective embeddings, $\mathbf{e}_v$ and $\mathbf{e}_f$, without a nonlinear activation function. 
This layer simply projects each modality embeddings into a joint audio-visual subspace.  
Then, the projected embeddings, $\widetilde{\mathbf{e}}_v$ and $\widetilde{\mathbf{e}}_f$, are combined by the summation operation, and used for the contrastive loss function as we did.
% This baseline can be more robust than System $\mathcal{A}$ in terms of data corruption or missing to some extent.
The summation based ensemble considers both modalities contribute equally, typically yielding a mean representation which can be easily biased with a large contamination~\cite{candes2011robust}.

Our method adaptively estimates the weights of each embedding to construct a joint representation. Either of weight can be turned off if the embedding would degrade the end performance.
This feature is not only robust but also able to deal with missing or a large corruption of the data.

\subsection{Voice and Face Representations}
To obtain discriminative embeddings for face and voice, $\mathbf{e}_f$ and $\mathbf{e}_v$, 
we exploit the existing deep neural network based representations.

% \subsection{Speaker embedding}\vspace{1mm}\noindent\textbf{Voice embedding}\quad% Recently, robust speaker embeddings have been actively explored using Deep Neural Networks (DNNs)~\cite{DavidSnyder2016,Heigold2016,Shon2018frame,Nagraniy2017,Zhang2017}. 
Voice embeddings generally exploit a large dataset including augmented data with added background noise. 
A voice embedding can be extracted from one of the hidden layers from a neural network trained to classify $N$ speakers in the training dataset. In a previous study, we proposed a frame-level voice embedding to extract robust speaker information by modifying the DNN structure after training is complete~\cite{Shon2018frame}. For training, the VoxCeleb1 development dataset was used. 
Details can be found in~\cite{Shon2018frame} since we used the same system.
Frame-level voice embeddings are extracted every 10ms using a 25ms frame window. 
Before fusion, a total of 10 and 100 successive voice embeddings are averaged to create a voice embedding which spans 115ms and 1015ms, respectively since a single frame-level voice embedding spanning 25ms is too short to extract voice characteristics reliably. 

% \subsection{Face embedding}\vspace{1mm}\noindent\textbf{Face embedding}\quad
Our face embeddings are extracted by using  FaceNet~\cite{Schroff2015} pre-trained on CASIA-WebFace.\footnote{\url{https://github.com/davidsandberg/facenet}\\ We used this reproduced open model, which has been improved by the maintainers with several modifications.
The modifications include the dimension change of the last layer from 128-D to 512-D. We use the last 512-D \texttt{FC7} layer activation of this FaceNet version as the face embedding.}
Since the provided face region annotations in the VoxCeleb datasets are coarse, 
% provided video clips which is loosely cropped face region, so 
we re-align and crop faces by the face and landmark detectors in Dlib.\footnote{\url{http://dlib.net}}




\section{Experiments}
In this section, we evaluate the proposed method with various baselines. 
In Sec.~\ref{sec:exp_fusion}, we compare the person verification performance with several multimodal fusion approaches as well as unimodal methods in the ordinary scenario that both modal data is given.
Then, we demonstrate the robustness of the proposed method against corrupted data in Sec.~\ref{sec:exp_corrupt}.
Moreover, we analyze the behavior of the attention layer according to interpretable attributes, including head pose and facial appearance traits, in Sec.~\ref{sec:exp_analysis}.

\subsection{Experimental Environment}\label{sec:exp_env}
For our experiments, 
we used the VoxCeleb1 \& 2 datasets~\cite{Nagraniy2017,Chung2018}, which include multimedia data with a reliable pre-processing step to obtain face regions and voice segments.
% we need multimedia data with a reliable pre-processing step to obtain face regions and voice segments.% Fortunately, the VoxCeleb1 \& 2 datasets were made publicly available, and included pre-proposed video and audio clips %complimentarily in the process of automated pipeline to crawl the dataset from YouTube% ~\cite{Nagraniy2017,Chung2018}. 
VoxCeleb1 \& 2 have more than 1,281,352 utterances from 7,365 speakers and both datasets have development and test set splits.
For verification performance measurement, we made a test trial set using the VoxCeleb2 Test set which contains 36,693 video clips from 120 speakers. 
We made 300 positive trials (i.e., the same speaker from different clips) and 300 negative trials (i.e., different speaker) trials per speaker, for a total of 71,790 trials.\footnote{The number is slightly less than 72,000 because there are a few individuals who have less than five video clips.}. We used cosine similarity to measure the distance of two embeddings.

Voice and face embeddings were extracted in 600 and 512 dimension respectively. For training the fusion network ($\mathcal{A},\mathcal{B},\mathcal{C}$), we extracted 1 frame per second and its relevant audio segment with 0.115 sec and 1.015 sec. Both embeddings were L2-normalized to have unit length before feeding into the fusion network. To test, we extract a single frame and its relevant audio segment randomly in each video clip. Thus, a total of 36,693 still images and 0.115 sec (or 1.015 sec) audio segments are used for the test trials. The performance was measured in terms of Equal Error Rate (EER) and minimum Detection Cost Function (mDCF) ($P_\textit{target}=0.01$)~\cite{sre16}. 

\subsection{Fusion Performance}\label{sec:exp_fusion}
As shown in Table~\ref{tab:fusion}, the voice embedding shows significantly worse performance than the face embedding. This is natural because we only use 0.115 sec, 1.015 sec which is a very short segment to extract reliable representations from text-independent speech.  Score-level fusion was done using logistic regression by calibrating on the VoxCeleb2 development set~\cite{Brummer2006}. The system $\mathcal{A, B}$ and $\mathcal{C}$ show neural network-based fusion approaches. While system $\mathcal{A}$ and $\mathcal{B}$ shows slightly better performance than the score-level fusion on EER, System $\mathcal{C}$ show a notable gain in both EER and mDCF.

% \begin{table}[ht]% \centering% \caption{Person verification performance on Voxceleb 2 test set}% \label{tab:fusion}% \resizebox{0.3\textwidth}{!}{%% \begin{tabular}{l|cc}% \hlineB{2}% Systems & EER & mDCF \\ \hlineB{2}% Voice embedding ($\mathbf{e}_v$) & 41.27 & 0.999 \\ % Face embedding ($\mathbf{e}_f$)& 8.03 & 0.631 \\ \hline% Score-level fusion & 7.83 & 0.623 \\ % System $\mathcal{A}$& 7.74 & 0.634 \\ % System $\mathcal{B}$& 7.81 & 0.625 \\ % System $\mathcal{C}$ (Proposed) & \textbf{7.46} &\textbf{ 0.611} \\ \hlineB{2}% \end{tabular}%% }% \end{table}\begin{table}[t]
\centering
\resizebox{0.4\textwidth}{!}{%
\begin{tabular}{l|cc|cc}
\hlineB{2}
 & \multicolumn{2}{c|}{$l$=0.115 sec} & \multicolumn{2}{c}{$l$=1.015 sec} \\ \hline
Systems & EER & mDCF & EER & mDCF\\ \hlineB{2}
Voice embedding ($\mathbf{e}_v$) & 41.27 & 0.999 & 14.50 & 0.863 \\ 
Face embedding ($\mathbf{e}_f$)& 8.03 & 0.631 & 8.03 & 0.631\\ \hline
Score-level fusion & 7.83 & 0.623 & 5.78 & 0.491 \\ 
System $\mathcal{A}$& 7.74 & 0.634 & 5.52 & 0.478 \\ 
System $\mathcal{B}$& 7.81 & 0.625 & 5.56 & 0.472\\ 
System $\mathcal{C}$ (Proposed) & \textbf{7.46} &\textbf{ 0.611} & \textbf{5.29} &\textbf{ 0.456} \\ \hlineB{2}
\end{tabular}%
}
\caption{Person verification performance on VoxCeleb2 test set. $l$ is a length of audio segment to extract voice embedding.}
\label{tab:fusion}
\end{table}\subsection{Effect on Corrupted and Missing Modality}\label{sec:exp_corrupt}
To see the performance under a corrupted or missing modality of either voice and face, we generated random noise drawn from a standard normal distribution and zero vector. Random noise mimics embeddings from an corrupted modality from an image without a face or audio without a voice due to an error in the pre-processing step. The zero vector is for the case of a  missing modality and this can be easily handled by switching the multi-modal system to unimodal system. However, we were interested in the scenario where we only used a single universal system and measured the performance when either modality did not exist. In table~\ref{tab:missing}, the proposed system $\mathcal{C}$ shows better performance for both the corrupted and missing modality condition by assessing the quality of the embedding in the attention layer.

\begin{table}[t]
\centering
\begin{subtable}[t]{0.45\textwidth}
\centering
\resizebox{1\linewidth}{!}{%
\setlength{\tabcolsep}{2.5pt}
\begin{tabular}{c|cc|cc|cc|cc}
\hlineB{2}
 & \multicolumn{4}{c|}{Voice null embeddings} & \multicolumn{4}{c}{Face null embeddings} \\ \cline{2-9} 
 & \multicolumn{2}{c|}{Random} & \multicolumn{2}{c|}{Zeros} & \multicolumn{2}{c|}{Random} & \multicolumn{2}{c}{Zeros} \\ \hline
Systems & EER & mDCF & EER & mDCF & EER & mDCF & EER & mDCF \\ \hlineB{2}
Score fusion & 8.05 & 0.633 & 8.03 & \textbf{0.631} & 49.99 & 0.999 & 41.27 & 0.999 \\ 
System $\mathcal{A}$ & 8.51 & 0.712 & 7.59 & 0.648 & 38.81 & 0.999 & 35.51 & 0.999 \\ 
System $\mathcal{B}$ & 8.76 & 0.748 & 7.51 & 0.637 & 37.74 & 0.999 & \textbf{34.12} & 0.999 \\ \hline
System $\mathcal{C}$
 & \multirow{ 2}{*}{\textbf{7.77}} & \multirow{ 2}{*}{\textbf{0.626}} & \multirow{ 2}{*}{\textbf{7.50}} & \multirow{ 2}{*}{0.633} & \multirow{ 2}{*}{\textbf{37.23}} & \multirow{ 2}{*}{0.999} & \multirow{ 2}{*}{34.22} & \multirow{ 2}{*}{0.999} \\ 
 (Proposed) & & & & & & & & \\
 \hlineB{2}
\end{tabular}%
}
\caption{$l= 0.115$ sec}
\end{subtable}
\vspace{5pt}

\begin{subtable}[t]{0.45\textwidth}
\centering
\resizebox{1\linewidth}{!}{%
\setlength{\tabcolsep}{2.5pt}
\begin{tabular}{c|cc|cc|cc|cc}
\hlineB{2}
 & \multicolumn{4}{c|}{Voice null embeddings} & \multicolumn{4}{c}{Face null embeddings} \\ \cline{2-9} 
 & \multicolumn{2}{c|}{Random} & \multicolumn{2}{c|}{Zeros} & \multicolumn{2}{c|}{Random} & \multicolumn{2}{c}{Zeros} \\ \hline
Systems & EER & mDCF & EER & mDCF & EER & mDCF & EER & mDCF \\ \hlineB{2}
Score fusion & 8.19 & 0.634 & 8.03 & \textbf{0.631} & 28.18 & 0.995 & 14.5 & \textbf{0.863} \\ 
System $\mathcal{A}$ & 8.64 &	0.732&	7.64&	0.649&	15.42&	0.960&	13.27&	0.897 \\ 
System $\mathcal{B}$ & 8.69&	0.724&	\textbf{7.61}&	0.647&	16.52&	0.970&	14.55&	0.901 \\ \hline
System $\mathcal{C}$
 & \multirow{ 2}{*}{\textbf{7.89}} & \multirow{ 2}{*}{\textbf{0.623}} & \multirow{ 2}{*}{7.65} & \multirow{ 2}{*}{0.636} & \multirow{ 2}{*}{\textbf{12.64}} & \multirow{ 2}{*}{\textbf{0.905}} & \multirow{ 2}{*}{\textbf{12.23}} & \multirow{ 2}{*}{0.871} \\ 
 (Proposed) & & & & & & & & \\
 \hlineB{2}
\end{tabular}%
}
\caption{$l= 1.015$ sec}
\end{subtable}

\caption{Performance under corrupted and missing modality on either voice and face. $l$ is a length of audio segment to extract voice embedding.}
\label{tab:missing}
\end{table}

Interestingly, the neural network based fusion systems, particularly the proposed fusion approach, obtain better performance than using a unimodal embedding, even for the case that information is only partially available.
In the neuroscience study, it has been observed that unimodal perception gets a benefit from the multisensory association of ecologically valid and sensory redundant stimulus pair ~\cite{Kreiegstein2006multisensory}.
As an extension of this observation, we can interpret as the fusion network learns the association of the multisensory data, and it becomes available to extract more robust feature even without multisensory data.

% Robustness against missing or corrupted modality% \begin{table*}[ht]% \centering% \caption{Corrupted and missing modality on either voice and face }% \label{tab:missing}% \resizebox{0.78\textwidth}{!}{%% \begin{tabular}{c|cc|cc|cc|cc}% \hlineB{2}%  & \multicolumn{4}{c|}{Voice null embeddings} & \multicolumn{4}{c}{Face null embeddings} \\ \cline{2-9} %  & \multicolumn{2}{c|}{Random} & \multicolumn{2}{c|}{Zeros} & \multicolumn{2}{c|}{Random} & \multicolumn{2}{c}{Zeros} \\ \hline% Systems & EER & mDCF & EER & mDCF & EER & mDCF & EER & mDCF \\ \hlineB{2}% Score-level fusion & 8.05 & 0.633 & 8.03 & \textbf{0.631} & 49.99 & 0.999 & 41.27 & 0.999 \\ % System $\mathcal{A}$ & 8.51 & 0.712 & 7.59 & 0.648 & 38.81 & 0.999 & 35.51 & 0.999 \\ % System $\mathcal{B}$ & 8.76 & 0.748 & 7.51 & 0.637 & 37.74 & 0.999 & \textbf{34.12} & 0.999 \\ % System $\mathcal{C}$%  & \textbf{7.77} & \textbf{0.626} & \textbf{7.50} & 0.633 & \textbf{37.23} & 0.999 & 34.22 & 0.999 \\ %  (Proposed) %  \hlineB{2}% \end{tabular}%% }% \end{table*}\subsection{Analysis of the Attention Layer}\label{sec:exp_analysis}
We analyze the behavior of the attention layer in our networks.
In order to parse what information it has learned and its behavior according to interpretable attributes, we conduct control experiments with facial appearance attributes.

By measuring probabilities of face/voice attention weights conditioned by an attribute in the test set, we investigate the existence of the statistical correlation between the attribute and the attention, and its tendency.
We obtain the attributes of the VoxCeleb2 test set by using the state-of-the-art, Rude et al.~\cite{rudd2016moon} and Feng et al.~\cite{feng2018prn} for 40 facial appearance attributes~(defined in the CelebA dataset~\cite{Liu2015}) and 3D head orientation, respectively.
We focus on the relationship between \emph{the behavior of attention weights} and attributes, considering the fact that Kim et al. \cite{kim2018learning} already showed the connections of face/voice representations with certain demographic attributes. 

As a statistical measure, given an attribute $A$, we measure the expectation of the probability $\mathbb{E} P(\alpha_f {>} {\bar \alpha}_f|A{=}\textrm{true})$, ${\bar \alpha}_f$ denotes the global mean of the face attention over all the test data, and likewise for the voice.
Since the probability estimate follows the expectation of the Bernoulli trial, we measure the statistical significance by 95$\%$ binomial proportion (Wald) confidence interval.
While the attribute estimation methods have extremely low-failure rate profile, taking into account subtle outlier effects, we conservatively regard the 95$\%$-confidence lower bound estimates as being a significant signal if greater than 60$\%$ (greater than the random chance).

From Table~\ref{tab:headorientation}, we could not find any correlation between head orientation and attention weights. We postulate that the FaceNet embedding is learned to be sufficiently head orientation invariant, so the attention layer turns out to be insensitive to the quality of the embedding according to the orientation. Table~\ref{tab:attribute} shows the 7 attributes that their lower bound is above the 60\%. It is interesting that, in the case that a person is with the temporary attributes, such as ``Wearing Hat,'' ``Sideburns,'' ``Goatee'' and ``Mustache'', the fusion system is likely to concentrate on the voice with a much higher chance than random. 
Also, the very strong attribute like ``Bald,'' ``Blond hair'' and ``Straight Hair'' shows correlation with attention weights.
% , although the attention weight tells ''Bald'' is not a discriminative attribute for the face.% \sscm{Do we need to address all attributes in the table? or increase the lower bound threshhold to 60 to 65?... }\begin{table}[t]
\centering
\begin{subtable}[ht]{1\linewidth}
\centering
\resizebox{1\textwidth}{!}{%
\begin{tabular}{ c|cc|cc|cc}
\hlineB{2}
Head orientation & \multicolumn{2}{c|}{ $|\theta|$\textless30\textdegree} & \multicolumn{2}{c|}{30\textdegree\textless$|\theta|$\textless60\textdegree} & \multicolumn{2}{c}{60\textdegree\textless$|\theta|$} \\ \hline
 & V (\%) & F (\%) & V (\%) & F (\%) & V (\%) & F (\%) \\ \hline
Yaw & 43 & 57 & 46& 54 & 44 & 56 \\ 
Pitch & 44 & 56 & 41 & 59 & 42 & 58 \\ 
Roll & 44 & 56 & 43 & 57 & 47 &53 \\ \hlineB{2}
\end{tabular}%
}
\caption{Head orientation attributes. (V: voice, F: face)}
\label{tab:headorientation}
\end{subtable}
\vspace{5pt}

\begin{subtable}[ht]{1\linewidth}
\centering
\resizebox{0.7\textwidth}{!}{%
\begin{tabular}{c|c|c|c}
\hlineB{2}
Facial Attibutes & Voice (\%) & Face (\%) & 95\% C.I. \\ \hline
% Bald & \textbf{74.89} & 25.11 & 70.86 \\ 
% % Big Lips & \textbf{63.54} & 36.45 & 62.50 \\ 
% Blond Hair & 32.17 & \textbf{67.83} & 66.33 \\
% % Chubby & \textbf{71.50} & 28.50 & 68.97 \\ 
% Goatee & \textbf{70.06} & 29.94 & 68.68 \\ 
% Mustache & \textbf{72.96} & 27.04 & 71.23 \\ 
% % Oval Face & \textbf{62.72} & 37.28 & 60.83 \\ 
% Sideburns & \textbf{65.60} & 34.40 & 63.79 \\ 
% Straight Hair & 29.65 & \textbf{70.35} & 69.26 \\ 
% Wearing Hat & \textbf{72.62} & 27.38 & 70.49 \\
Bald & \textbf{74.89} & 25.11 & $\pm$ 4.02 \\ 
% Big Lips & \textbf{63.54} & 36.45 & 62.50 \\ 
Blond Hair & 32.17 & \textbf{67.83} & $\pm$ 1.51 \\
% Chubby & \textbf{71.50} & 28.50 & 68.97 \\ 
Goatee & \textbf{70.06} & 29.94 & $\pm$ 1.38 \\ 
Mustache & \textbf{72.96} & 27.04 & $\pm$ 1.73 \\ 
% Oval Face & \textbf{62.72} & 37.28 & 60.83 \\ 
Sideburns & \textbf{65.60} & 34.40 & $\pm$ 1.81 \\ 
Straight Hair & 29.65 & \textbf{70.35} & $\pm$ 1.09 \\ 
Wearing Hat & \textbf{72.62} & 27.38 & $\pm$ 2.14 \\ \hlineB{2}
\end{tabular}%
}
\caption{Facial appearance attributes}
\label{tab:attribute}
\end{subtable}
\caption{The expectation of $ P(\alpha_v {>} {\bar \alpha}_v|A{=}\textrm{true})$ and $ P(\alpha_f {>} {\bar \alpha}_f|A{=}\textrm{true})$, where $A$ denotes attributes.
C.I. stands for the (Wald) confidence interval.
For head orientation, the front face is represented by all the angle of yaw, pitch and roll equal to 0\textdegree.
}
\end{table}% Please add the following required packages to your document preamble:% \usepackage{graphicx}% \begin{table}[ht]% \centering% \caption{The expectation of the $ P(a_v {>} {\bar a}_v|A{=}\textrm{true})$, $ P(a_f {>} {\bar a}_f|A{=}\textrm{true})$ and 95\%-confidence lower bound. $A$ is facial attribute.}% \label{tab:attribute}% \begin{subtable}[ht]{1\linewidth}% \centering% \resizebox{1\textwidth}{!}{%% \begin{tabular}{c|c|c|c}% \hlineB{2}% Facial Attibutes & Voice (\%) & Face (\%) & Lower bound \\ \hline% Bald & \textbf{74.89} & 25.11 & 70.86 \\ % % Big Lips & \textbf{63.54} & 36.45 & 62.50 \\ % Blond Hair & 32.17 & \textbf{67.83} & 66.33 \\% % Chubby & \textbf{71.50} & 28.50 & 68.97 \\ % Goatee & \textbf{70.06} & 29.94 & 68.68 \\ % Mustache & \textbf{72.96} & 27.04 & 71.23 \\ % % Oval Face & \textbf{62.72} & 37.28 & 60.83 \\ % Sideburns & \textbf{65.60} & 34.40 & 63.79 \\ % Straight Hair & 29.65 & \textbf{70.35} & 69.26 \\ % Wearing Hat & \textbf{72.62} & 27.38 & 70.49 \\ \hlineB{2}% \end{tabular}%% }% \caption{Facial appearance attributes}% \end{subtable}% \end{table}% \subsection{discussion}% \sscm{I think section 4.3 and 4.4 might cover for detailed discussion of the system.}

\section{Conclusion}


Motivated from the recent studies about the multi-modal association, we proposed a feature-level attentive fusion network for audio-visual online person verification task. 
The temporally synced face image and voice segment assumption encourages the network to learn about the quality of the embedding to verify a person's identity. 
The learned embeddings of both modalities share a compatible space (co-embedding space) by virtue of the simple linear combination rule to obtain the fused representation. 
Besides the better performance than the traditional score-level fusion, it has a large advantage to handle the severe condition such as the presence of the corrupted and missing modality. 
The attention mechanism is also analyzed to understand the correspondence between attention weights and interpretable attributes of visual perception.
In addition to visual appearance traits, it would be interesting to further investigate the attention behavior in terms of speech characteristics, such as pitch, language, dialect, etc, as a future direction. 
% For future work, we will further investigate the analysis in terms of speech characteristics, such as pitch, language, dialect, etc. 



\clearpage
\newpage

\bibliographystyle{IEEEbib_r1}
\small
\bibliography{refs}







\end{document}
