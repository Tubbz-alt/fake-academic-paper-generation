\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ificcvfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}
%%%%%%%%% TITLE
\title{Learning Deep Features for Discriminative Localization}

\author{
Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba \\
Computer Science and Artificial Intelligence Laboratory, MIT\\
\texttt{\{bzhou,khosla,agata,oliva,torralba\}@csail.mit.edu}
}

\maketitle
%\thispagestyle{empty}


%%%%%%%%% ABSTRACT
\begin{abstract}

In this work, we revisit the global average pooling layer proposed in \cite{lin2013network}, and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1\% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2\% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them.
%Object localization, a building block task of visual understanding, has recently received lots of attention, given the impressive performances of deep learning architectures at finding where learned objects are in an image. To train these networks to perform object localization, ground truth -- annotated images with bounding boxes around the objects of interest -- is required. This limits the scope of objects, and combination of objects, that can be detected. Here, we propose a method, termed Class Activation Mapping (CAM), that remedies this limit. In a Convolutional Neural Network (CNN), by mapping the categorical knowledge learned by the final layer back to the convolutional layers, a class activation map is generated that highlights the informative objects and parts detected by the CNN. As such, the CAM method enable	s the classification-trained CNN to perform object localization directly, in a single forward path, without training on bounding box annotations. We show that our weakly supervised CNN methods can achieve reasonably good localization performance, even coming close to some fully supervised CNN methods on the test set of the benchmarks. Furthermore, we show that we can use the CAM to find generic visual features useful for both object classification and localization tasks, across various recognition datasets.

%to visualize the class scores predicted by the convolutional neural network (ConvNet) on an image.
%With the help of CAM technique, the ConvNet could classify the image into some class and localize the discriminative class-specific image regions in a single forward pass. 
%We first evaluate the effectiveness of the method on several ConvNets for object localization on ILSVRC localization benchmark. 

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

%Object recognition is one of the most important challenges in computer vision. There are two aspects of the object recognition: image-level visual classification and region-level object localization and detection. Images are annotated with class labels as the ground-truth for object classification, while image regions are annotated with bounding boxes as the ground-truth for object localization and detection. It is more expensive to collect the bounding box annotationthan collecting image-level class labels for a large-scale of images. Recently there has been great effort to build large-scale image datasets by crowd sourcing such as ImageNet \cite{deng2009imagenet} and Places \cite{zhou2014learning}, where millions of images are annotated with image-level class labels. 

%With the rise of large-scale image datasets like ImageNet \cite{deng2009imagenet} and Places \cite{zhou2014learning}, convolutional neural networks or ConvNet have achieved the state-of-the performance on a number of classification, localization and detection tasks surpassing methods based on hand-crafted features \cite{krizhevsky2012imagenet, sermanet2013overfeat,szegedy2014going,simonyan2014very,girshick2014rich}. For these ConvNets with state-of-the-art performance on ILSVRC classification and localization benchmarks, the common training pipeline is to train the ConvNets first on 1.3 million ImageNet data with 1000 object classes for classification, then the classification-trained network are fine-tuned with bounding box annotation for object localization and detection. The limitation here is that the precise bounding box annotations for a large number of object classes is costly to collect. Here we explore the possibility of leveraging the classification-trained networks for localization, without the annotation of bounding box. Some localization results from our classification-trained network are shown in Figure \ref{figure_cover}. 

%Deep convolutional neural networks or ConvNets trained from the large-scale image datasets like ImageNet \cite{deng2009imagenet} and Places \cite{zhou2014learning} have achieved the state-of-the performance on a number of classification, localization and detection tasks surpassing methods based on hand-crafted features \cite{krizhevsky2012imagenet, sermanet2013overfeat,szegedy2014going,simonyan2014very,girshick2014rich}. Meanwhile, deep feature, as the activation response from the higher layer of the ConvNet, is used as generic visual feature with high performance across various recognition datasets \cite{razavian2014cnn,donahue2014decaf}. 

Recent work by Zhou \textit{et al}~\cite{zhou2014object} has shown that the convolutional units of various layers of convolutional neural networks (CNNs) actually behave as object detectors despite no supervision on the location of the object was provided. Despite having this remarkable ability to localize objects in the convolutional layers, this ability is lost when fully-connected layers are used for classification. Recently some popular fully-convolutional neural networks such as the Network in Network (NIN) \cite{lin2013network} and GoogLeNet \cite{szegedy2014going} have been proposed to avoid the use of fully-connected layers to minimize the number of parameters while maintaining high performance. 

\begin{figure}
\begin{center}
\includegraphics[width=1\linewidth]{figure/cover.pdf}
\end{center}
\vspace*{-4mm}
 %\caption{\textbf{Classification-trained network localizes the informative and discriminative regions of the class in the image}. a) Sample from ImageNet dataset. The bounding box of ground-truth is in green and the bounding box generated from the class activation map on the right is in red. b) Deep feature from our network is used to localize actions as a generic localizable feature.}\label{figure_cover}
\caption{A simple modification of the global average pooling layer combined with our class activation mapping (CAM) technique allows the classification-trained CNN to both classify the image and localize class-specific image regions in a single forward-pass e.g., the toothbrush for \textit{brushing teeth} and the chainsaw for \textit{cutting trees}.}\label{figure_cover}
%Discriminative image regions of two action classes are highlighted by our method when performing action recognition.
\end{figure}

In order to achieve this, \cite{lin2013network} uses \textit{global average pooling} which acts as a structural regularizer, preventing overfitting during training. In our experiments, we found that the advantages of this global average pooling layer extend beyond simply acting as a regularizer - In fact, with a little tweaking, the network can retain its remarkable localization ability until the final layer. This tweaking allows identifying easily the discriminative image regions in a single forward-pass for a wide variety of tasks, even those that the network was not originally trained for. As shown in Figure \ref{figure_cover}(a), a CNN trained on object categorization is successfully able to localize the discriminative regions for action classification as the objects that the humans are interacting with rather than the humans themselves. %More surprisingly, the CNNs are able to localize the discriminative components in a cluttered synthetic image that can be hard for even humans to identify\footnote{Try it out for yourself here: http://gandalf.psych.umn.edu/users/kersten/kersten-lab/camouflage/camouflageChallenge.html} (Figure \ref{figure_cover}(b)).


Despite the apparent simplicity of our approach, for the weakly supervised object localization on ILSVRC benchmark~\cite{ILSVRCijcv15}, our best network achieves 37.1\% top-5 test error, which is rather close to the 34.2\% top-5 test error achieved by fully supervised AlexNet~\cite{krizhevsky2012imagenet}. Furthermore, we demonstrate that the localizability of the deep features in our approach can be easily transferred to other recognition datasets for generic classification, localization, and concept discovery.\footnote{Our models are available at: \href{http://cnnlocalization.csail.mit.edu}{http://cnnlocalization.csail.mit.edu}}.


%Understanding why the ConvNet works so well, what is being learned inside the ConvNet, and the other potential applicability of the ConvNet is crucial. The knowledge of the class learned by convNet is visualized through the back-propagation \cite{simonyan2013deep}, while the visual patterns learned by each convolutional unit at each layer is visualized and further annotated with different levels of semantics \cite{zeiler2014visualizing,zhou2014object}. Basically the ConvNet is learning to detect the discriminative visual patterns in the image that is most relevant to the final classification. Zhou \textit{et al.} \cite{zhou2014object} shows that ConvNet learns object detectors within itself as a result of learning to recognize scenes. That's because the discriminative objects have strong contribution to some classification such as bed to bedroom and toilet to bathroom. However, it is still not clear why ConvNet reaches some classification decision given an image, \textit{i.e.}, which image regions make the ConvNet come to some classification decision. 

%A more intepretable network architecture could allow us to better understand the decision making of ConvNet. Conventional ConvNets (for example the standard AlexNet \cite{krizhevsky2012imagenet} or VGGnet \cite{simonyan2014very}) perform convolutions (including max-pool and reLU procedures) in the lower layers of the network. Then the feature map from the last convolutional layer are vectorized and fed into the fully connected layers, lastly followed by the final categorical softMax layer. The feature map from the convolutional layers still contain the spatial information, but it is further shuttled by the fully connected layers. On the other hand, as the fully connected layers contains more than 80\% of the model parameters, while their training is prone to overfitting \cite{hinton2012improving}, recently there are some networks that remove the fully connected layers but still achieve comparable classification performance, such as Network in Network \cite{lin2013network} and GoogLeNet \cite{szegedy2014going}. The effect of removing the fully connected layers is largely compensated by the more complex convolutional layer structures such as MLP in \cite{lin2013network} and Inception layers in \cite{szegedy2014going}. Without the fully connected layers, the ConvNet becomes more intepretable. Through the global average pooling layer, the final categorical layer is directly connected to the convolutional layers, which makes it much easier to analyze the classification decision of the ConvNet for any given image. 

%In this paper, we propose a technique called Class Activation Mapping (\textbf{CAM}) for any ConvNet with Global Average Pooling structure (shorted as \textbf{ConvNet+GAP} in the following sections), to visualize the class scores predicted by the ConvNet on the image. The resulting class activation map highlights the discriminative image regions that make ConvNet classify the image to some class. With the CAM technique, the classification-trained ConvNet+GAP could be used for object localization, even without training on any bounding box annotation. We quantitatively evaluate the localizability of ConvNet+GAP enabled by the CAM technique for the weakly supervised object localization on ILSVRC localization benchmark. Our best weakly supervised ConvNet method achieves 37.1\% top-5 test error, which is even coming close to the 34.2\% top-5 test error of the fully supervised AlexNet. Furthermore, we demonstrate that using the same CAM technique the deep feature from the ConvNet+GAP could be easily generalized to other recognition datasets for generic classification and localization. Figure \ref{figure_cover} shows some examples of applying the CAM technique to some ConvNet+GAP for object localization and discriminative concept discovery. 

%which enables the classification-trained convolutional network for discriminative localization, without using any bounding box annotation or object proposals. The key idea of CAM is to distill the discriminative knowledge learned by categorical layer to the previous convoluational layer. By bridging the last convolutional layer and categorical layer with a simple CAM module, while the ConvNet is being trained to classify an image into some class, it is explicitly learning to localize the discriminative parts of the class. After the training, the ConvNet could classify an image into some class and localize the class-specific discriminative regions jointly in a single forward-pass. We quantitatively evaluate the ConvNet with Class Activation Mapping (CAMnet) for weakly supervised object localization on ILSVRC benchmark, our best CAMnet achieves 37.1\% top-5 test error, which is even close to the 34.2\% top-5 test error done by fully supervised AlexNet.  Furthermore, we demonstrate that the localizability of the deep feature from the CAMnet could be easily generalized to other recognition datasets for generic classification, localization, and concept discovery.


%Recently there are some weakly supervised object localization methods based on ConvNet that only use image-level class label as training samples for object localization. But these methods are evaluated on Pascal VOC, which has only thousands of images for 20 object classes \cite{oquab2014weakly,bergamo2014self,song2014learning}. Weakly supervised object localization on large-scale image dataset like ImageNet is much more challenging and less explored.

%In this paper, we propose a technique called Class Activation Mapping (CAM), which enables the classification-trained convolutional network for discriminative localization, without using any bounding box annotation or object proposals. The key idea of CAM is to distill the discriminative knowledge learned by categorical layer to the previous convoluational layer. By bridging the last convolutional layer and categorical layer with a simple CAM module, while the ConvNet is being trained to classify an image into some class, it is explicitly learning to localize the discriminative parts of the class. After the training, the ConvNet could classify an image into some class and localize the class-specific discriminative regions jointly in a single forward-pass. We quantitatively evaluate the ConvNet with Class Activation Mapping (CAMnet) for weakly supervised object localization on ILSVRC benchmark, our best CAMnet achieves 37.1\% top-5 test error, which is even close to the 34.2\% top-5 test error done by fully supervised AlexNet.  Furthermore, we demonstrate that the localizability of the deep feature from the CAMnet could be easily generalized to other recognition datasets for generic classification, localization, and concept discovery.

\subsection{Related Work}
Convolutional Neural Networks (CNNs) have led to impressive performance on a variety of visual recognition tasks~\cite{krizhevsky2012imagenet,zhou2014learning,girshick2014rich}. Recent work has shown that despite being trained on image-level labels, CNNs have the remarkable ability to localize objects~\cite{bergamo2014self,oquab2014weakly,cinbis2015weakly,oquab2014learning}. In this work, we show that, using the right architecture, we can generalize this ability beyond just localizing objects, to start identifying exactly which regions of an image are being used for discrimination. Here, we discuss the two lines of work most related to this paper: weakly-supervised object localization and visualizing the internal representation of CNNs.

\textbf{Weakly-supervised object localization:} There have been a number of recent works exploring weakly-supervised object localization using CNNs~\cite{bergamo2014self,oquab2014weakly,cinbis2015weakly,oquab2014learning}. Bergamo \textit{et al}~\cite{bergamo2014self} propose a technique for self-taught object localization involving masking out image regions to identify the regions causing the maximal activations in order to localize objects. Cinbis \textit{et al}~\cite{cinbis2015weakly} combine multiple-instance learning with CNN features to localize objects.  
Oquab \textit{et al}~\cite{oquab2014learning} propose a method for transferring mid-level image representations and show that some object localization can be achieved by evaluating the output of CNNs on multiple overlapping patches. However, the authors do not actually evaluate the localization ability. On the other hand, while these approaches yield promising results, they are not trained end-to-end and require multiple forward passes of a network to localize objects, making them difficult to scale to real-world datasets. Our approach is trained end-to-end and can localize objects in a single forward pass.

The most similar approach to ours is the work based on global max pooling by Oquab \textit{et al}~\cite{oquab2014weakly}. Instead of global \textit{average} pooling, they apply global \textit{max} pooling to localize a point on objects. However, their localization is limited to a point lying in the boundary of the object rather than determining the full extent of the object. We believe that while the \textit{max} and \textit{average} functions are rather similar, the use of average pooling encourages the network to identify the complete extent of the object. The basic intuition behind this is that the loss for average pooling benefits when the network identifies \textit{all} discriminative regions of an object as compared to max pooling. This is explained in greater detail and verified experimentally in Sec.~\ref{sec:locresults}. Furthermore, unlike~\cite{oquab2014weakly}, we demonstrate that this localization ability is generic and can be observed even for problems that the network was not trained on.

We use \textit{class activation map} to refer to the weighted activation maps generated for each image, as described in Section~\ref{sec:cam}. We would like to emphasize that while global average pooling is not a novel technique that we propose here, the observation that it can be applied for accurate discriminative localization is, to the best of our knowledge, unique to our work. We believe that the simplicity of this technique makes it portable and can be applied to a variety of computer vision tasks for fast and accurate localization.

\textbf{Visualizing CNNs:} There has been a number of recent works~\cite{zeiler2014visualizing,mahendran2004understanding,dosovitskiy2015inverting,zhou2014object} that visualize the internal representation learned by CNNs in an attempt to better understand their properties. Zeiler \textit{et al}~\cite{zeiler2014visualizing} use deconvolutional networks to visualize what patterns activate each unit. Zhou \textit{et al.}~\cite{zhou2014object} show that CNNs learn object detectors while being trained to recognize scenes, and demonstrate that the same network can perform both scene recognition and object localization in a single forward-pass. Both of these works only analyze the convolutional layers, ignoring the fully-connected thereby painting an incomplete picture of the full story. By removing the fully-connected layers and retaining most of the performance, we are able to understand our network from the beginning to the end. 

Mahendran \textit{et al}~\cite{mahendran2004understanding} and Dosovitskiy \textit{et al}~\cite{dosovitskiy2015inverting} analyze the visual encoding of CNNs by inverting deep features at different layers. While these approaches can invert the fully-connected layers, they only show what information is being preserved in the deep features without highlighting the relative importance of this information. Unlike~\cite{mahendran2004understanding} and~\cite{dosovitskiy2015inverting}, our approach  can highlight exactly which regions of an image are important for discrimination. Overall, our approach provides another glimpse into the soul of CNNs.

%CNN trained on millions of images has shown impressive performance for large-scale visual classification tasks, such as object classification \cite{krizhevsky2012imagenet} and scene recognition \cite{zhou2014learning}, while deep feature from the CNN is used as generic visual feature with high accuracy for a lot of recognition tasks \cite{razavian2014cnn,donahue2014decaf}, compared with these previous hand-designed visual feature. 

%Meanwhile, CNN is applied to object detection and localization tasks with outstanding performance. RCNN framework \cite{girshick2014rich} combines the object proposal with CNN feature to detect objects. In the framework of OverFeat \cite{sermanet2013overfeat}, starting from the classification-trained network, the final classifier layer is replaced by a regression and the whole network is fine-tuned to predict object bounding boxes using the ground-truth bounding box annotation. Recently there has been some work on weakly supervised object detection with CNN for Pascal VOC dataset, by leveraging the object proposal \cite{bergamo2014self, Hoffman14Lsda} or multi-scale sliding-window training \cite{oquab2014weakly} or multiple instance learning \cite{cinbis2015weakly}. Those CNN-based weakly supervised object localization and detection methods either require extra object proposal \cite{bergamo2014self} or iterative learning process \cite{oquab2014weakly,cinbis2015weakly}, which is hard to be generalized to other recognition datasets or scalable to thousands of classes like these thousands of classes in ImageNet. Oquab \textit{et al.} \cite{oquab2014learning} extracts hundreds of overlapping patches from a single image then generates the score map by combining the predicted scores of each patch. Our method can generate the class activation maps in a single pass, which is significantly faster. In \cite{oquab2014weakly}, the standard CNN trained on ImageNet is extended to fully CNN then the stochastic gradient descent with global max-pooling is used to fine-tune the fully CNN to Pascal VOC and COCO for object detection. In \cite{long2014fully} the classification-trained CNN is extended to fully convolutional neural network for semantic segmentation, by considering the fully connected layers as $1\times1$ convolutions. We follow the similar path to improve the network architecture for applications beyond classification. 


%Recently, there is a line of work that tries to better understand the CNN. The work by \cite{zeiler2014visualizing} introduces a deconvolution procedure to visualize what activates each unit. Some other work suggest that the CNN learns a distributed code for objects \cite{agrawal2014analyzing,szegedy2013intriguing}. Yosinski \textit{et al.} \cite{yosinski2014transferable} use transfer learning to measure how generic/specific are the learned deep feature. Nguyen \textit{et al.} \cite{nguyen2015deep} shows that the CNN can be easily fooled by synthesized examples which are totally unrecognizable to humans. Mahendran \textit{et al} \cite{mahendran2004understanding} and Dosovitskiy \textit{et al.} \cite{dosovitskiy2015inverting} analyze the visual encoding of CNNs by inverting deep features at different layers. Zhou \textit{et al.} \cite{zhou2014object} show that CNNs learn object detectors within itself as a result of learning to recognize scenes, and demonstrate that the same network can perform both scene recognition and object localization in a single forward-pass. Our work aims at understanding why CNNs reach some classification decision given an image, i.e., which image regions lead the CNN to this classification decision. The proposed Class Activation Mapping technique explicitly visualizes the class score given by the CNN on the image, highlighting the discriminative regions of the class. It also opens door for a lot of interesting applications: we could enable the classification-trained CNN for object localization or visual pattern discovery, without training on bounding box annotation. 

%The outline of the paper is as follows: Section 2 introduces the Class Activation Mapping and how it enables the classification-trained network for discriminative localization. In Section 3, we first quantitatively evaluate the localization ability of the several current networks equipped with CAM (CAMnet) on ILSVRC localization benchmark , then we demonstrate the deep features from the CAMnet not only have the state-of-the-art performance on various recognition datasets but also could localize the informative regions of the class in the new dataset. S
\section{Class Activation Mapping}
\label{sec:cam}

\begin{figure*}
\includegraphics[width=1\linewidth]{figure/cam.pdf}
 \caption{Class Activation Mapping: the predicted class score is mapped back to the previous convolutional layer to generate the class activation maps (CAMs). The CAM highlights the class-specific discriminative regions.}\label{figure_module}
\end{figure*}

In this section, we describe the procedure for generating \textit{class activation maps} (CAM) using global average pooling (GAP) in CNNs. A class activation map for a particular category indicates the discriminative image regions used by the CNN to identify that category (e.g., Fig.~\ref{figure_examplemapping}). The procedure for generating these maps is illustrated in Fig.~\ref{figure_module}.

We use a network architecture similar to Network in Network~\cite{lin2013network} and GoogLeNet~\cite{szegedy2014going} - the network largely consists of convolutional layers, and just before the final output layer (softmax in the case of categorization), we perform global average pooling on the convolutional feature maps and use those as features for a fully-connected layer that produces the desired output (categorical or otherwise). Given this simple connectivity structure, we can identify the importance of the image regions by projecting back the weights of the output layer on to the convolutional feature maps, a technique we call class activation mapping.

%The global average pooling proposed in Network in Network \cite{lin2013network} and GoogLeNet \cite{szegedy2014going} makes the CNN have less parameters. As there is a direct connection between the convolutional layers and final softmax layer, we want to better analyze and understand the decision making of the CNN and make CNN more interpretable. Based on the CNN with Global Average Pooling, we propose a simple but effective technique termed Class Average Mapping to visualize the class score predicted by the CNN-GAP. 

%The technique of Class Activation Mapping is based on the direct connection between the softmax layer and final convolutional layer through the global average pooling. 
As illustrated in Fig.~\ref{figure_module}, global average pooling outputs the spatial average of the feature map of each unit at the last convolutional layer. A weighted sum of these values is used to generate the final output. Similarly, we  compute a weighted sum of the feature maps of the last convolutional layer to obtain our class activation maps. We describe this more formally below for the case of softmax. The same technique can be applied to regression and other losses.

%on the CAM module, which could be mounted to any existing networks. CAM module is composed of three parts: the first part is the convolutional composition layer, where each composition unit generates a feature map from the previous convolutional layer; the second part is the global average pooling layer, which takes the spatial average of each feature map of the composition unit; the third part is the categorical softMax layer, which will weighted sum average global pooled values and output the class score. The CAM module will be trained along with the whole network.

For a given image, let $f_{k}(x,y)$ represent the activation of unit $k$ in the last convolutional layer at spatial location $(x, y)$. Then, for unit $k$, the result of performing global average pooling, $F^{k}$ is $\sum_{x,y}f_{k}(x,y)$. Thus, for a given class $c$, the input to the softmax, $S_c$, is $\sum_{k}w^{c}_{k}F_{k}$ where $w^{c}_{k}$ is the weight corresponding to class $c$ for unit $k$. Essentially, $w^{c}_{k}$ indicates the \textit{importance} of $F_{k}$ for class $c$. Finally the output of the softmax for class $c$, $P_c$ is given by $\frac{\exp(S_c)}{\sum_{c}\exp(S_c)}$. Here we ignore the bias term: we explicitly set the input bias of the softmax to $0$ as it has little to no impact on the classification performance.

%Assume that given an image the activation of the last convolutional layer is $f_{k}(x,y)$, where $x$ and $y$ is the spatial grid of the feature map and $k$ is the convolution unit index. Then each activation at the global pooling layer is $F^{k} = \sum_{x,y}f_{k}(x,y)$. Then the class score $c$ before softmax is
%\begin{align}
%S_c = \sum_{k}w^{c}_{k}F_{k}
%\end{align}
%where $P_c = \frac{\exp(S_c)}{\sum_{c}\exp(S_c)}$ is predicted probability score of the class $c$ and $w^{c}_{k}$ is the learned weight of regression indicating how important the activation response $F_{k}$ contribute to classifying this image to class $c$. Here we ignore the bias term: In the CNN training we explicitly set the bias term of soft max as zero as there is almost no effect on the classification. 

By plugging $F_{k} = \sum_{x,y}f_{k}(x,y)$ into the class score, $S_c$, we obtain
\begin{align}\label{eq:linearscore}
\nonumber S_c = &\sum_{k}w^{c}_{k}\sum_{x,y}f_{k}(x,y)\\
=&\sum_{x,y}\sum_{k}w^{c}_{k}f_{k}(x,y).
\end{align}
We define $M_{c}$ as the class activation map for class $c$, where each spatial element is given by
\begin{align}
M_{c}(x,y) = \sum_{k}w^{c}_{k}f_{k}(x,y).
\end{align} 
Thus, $S_c =\sum_{x,y}M_{c}(x,y)$, and hence $M_{c}(x,y)$ directly indicates the importance of the activation at spatial grid $(x,y)$ leading to the classification of an image to class $c$.

Intuitively, based on prior works~\cite{zhou2014object,zeiler2014visualizing}, we expect each unit to be activated by some visual pattern within its receptive field. Thus $f_k$ is the map of the presence of this visual pattern. The class activation map is simply a weighted linear sum of the presence of these visual patterns at different spatial locations. By simply upsampling the class activation map to the size of the input image, we can identify the image regions most relevant to the particular category.

In Fig.~\ref{figure_examplemapping}, we show some examples of the CAMs output using the above approach. We can see that the discriminative regions of the images for various classes are highlighted. In Fig.~\ref{figure_multipleprediction} we highlight the differences in the CAMs for a single image when using different classes $c$ to generate the maps. We observe that the discriminative regions for different categories are different even for a given image. This suggests that our approach works as expected. We demonstrate this quantitatively in the sections ahead.

%From the previous visualization on the activation patterns of the convolutional units \cite{zhou2014object,zeiler2014visualizing}, each unit would be activated discriminatively by some visual pattern. Thus the activation $f_{k}(x,y)$ from the convolutional layer indicates some visual pattern emerging at the image within the receptive field of the unit $k$ at spatial grid $(x,y)$. The linear class score is computed from the linear weighted summation based on learned $w^{c}_{k}$ then average summation over all the spatial grids. Thus the class activation map highlights the spatial regions which contribute to the linear score of the class $c$ predicted by the CNN. By simply upsampling the class activation map to input image size, we could localize which image regions are most relevant to the classification predicted by the CNN.

%Figure \ref{figure_examplemapping} shows some examples of the class activation map generated from one of our trained CNN-GAP (GoogLeNet-GAP) for four classes in ImageNet. We can see that the class activation map identifies the informative object parts of a class, such as the head of the animal in the classes \textit{briard} and \textit{hen},  the plates in the class \textit{barbell}, and bell in the class \textit{bell cote}.

%Besides, by varying the class $c$ we could generate different class activation map $M_{c}$ for the same image. Figure \ref{figure_multipleprediction} shows two image examples of the class activation maps generated from the top-5 predicted classes by one of our CNN-GAP (GoogLeNet-GAP). We can see that different class activation maps have different highlighted regions. For example, for the image with ground-truth class label as dome, dome activation map highlights the upper round part of the compound, while the palace activation map highlights the lower flat part of the compound.

\begin{figure}
\begin{center}
\includegraphics[width=1\linewidth]{figure/heatmap_class.pdf}
\end{center}
\vspace*{-4mm}
 \caption{The CAMs of four classes from ILSVRC~\cite{ILSVRCijcv15}. The maps highlight the discriminative image regions used for image classification e.g., the head of the animal for \textit{briard} and \textit{hen}, the plates in \textit{barbell}, and the bell in \textit{bell cote}.}\label{figure_examplemapping}
\end{figure}


\begin{figure}
\begin{center}
\includegraphics[width=1\linewidth]{figure/multipleCAM.pdf}
\end{center}
\vspace*{-4mm}
 \caption{Examples of the CAMs generated from the top 5 predicted categories for the given image with ground-truth as dome. The predicted class and its score are shown above each class activation map. We observe that the highlighted regions vary across predicted classes e.g., \textit{dome} activates the upper round part while \textit{palace} activates the lower flat part of the compound.}\label{figure_multipleprediction}
\end{figure}

%For object classification, given an image, conventional ConvNets (for example the standard AlexNet \cite{krizhevsky2012imagenet} or VGGnet \cite{simonyan2014very}) perform convolutions (including max-pool and reLU procedures) in the lower layers of the network. Then the feature map from the last convolutional layer are vectorized and fed into the fully connected layers, lastly followed by the final categorical softMax layer. To further train the classification network for object localization, its final softMax logistic regression layer is replaced with bounding box prediction layer and fine-tuned under Euclidean loss with the annotation of ground-truth bounding boxes on the images \cite{simonyan2014very,sermanet2013overfeat}. Thus training ConvNet for object classification and for object localization are taken as two tasks. 



%We propose a technique called Class Activation Mapping (CAM) that could explicitly enable the classification network for \textit{discriminative localization}, without training on bounding box. We denote the localization derived from classification-trained models as discriminative localization, since the localized regions are usually the informative and discriminative object parts detected by convNet for classification, not necessarily the whole body of the object class. 

%The key idea of CAM is to bridge the convolutional layers and final categorical layer explicitly, so the discriminative knowledge of the categorical layer could be mapped back to the convolutional feature map where the spatial information still remains. Thus we could visualize which spatial location on the convolutional feature map that lead to the classification decision of the ConvNet. The illustration of CAM is in \ref{figure_module}. After removing any fully connected layers, we mount the CAM module behind the convolutional layers of any existing network. Through a single forward pass the classification network could not only classify the image into some class, but also output the class activation map that indicates which image regions are most relevant to the classification. 

%\subsection{Revisit Global Average Pooling}

%ConvNets, like AlexNet \cite{krizhevsky2012imagenet}, OverFeat network \cite{sermanet2013overfeat}, and VGG network \cite{simonyan2014very}, all have fully connected layers, on which a lot of experiments show that fully connected layers indeed improve the classification performance. The fully connected layers link the convolution layers with the final categorical layer. The idea of the fully connected layer in ConvNet has been derived from the traditional neural network \cite{bengio2009learning}. The three fully connected layers in the AlexNet contains more than 90\% of the model parameters, which are prone to overfitting. Then dropout is proposed by Hinton et al.\cite{hinton2012improving} as a regularizer to improve the generalization ability of the network learning. On the other hand, some networks which remove the fully connected layers also achieve comparable classification performance, such as Network in Network \cite{lin2013network} and GoogLeNet \cite{szegedy2014going}. The effect of removing the fully connected layers is largely compensated by more complex convolution-like layer structures such as MLP in \cite{lin2013network} and Inception layers in \cite{szegedy2014going} along with a global average pooling layer. 

%By removing the fully connected layers and adding the global average pooling layer, the number of network model parameters is decreased significantly, while the network becomes more interpretable. As shown in \cite{lin2013network}, the global average pooling layer acts as a prefixed transformation matrix connecting the output of last convolutional layer with the categorical layer softMax as a regularizer. Global average pooling is to take the spatial average of each feature map of the convolution units, the resulting vector is then fed into the softMax layer and weighted summed into class score. In \cite{lin2013network} each element of the resulting average vector is directly the linear score $S_c$ in the logistic regression of softMax layer, while in \cite{szegedy2014going} the resulting average vector is fully connected to softMax layer. So that the average vector could be considered as global pooling layer, which is then fully connected to softMax layer. 

%Based on the global average pooling, we formulate the technique of the Class Activation Mapping, to couple object classification and discriminative localization in a single forward pass.

%\subsection{Class Activation Mapping}

%\begin{figure}
%\begin{center}
%\includegraphics[width=1\linewidth]{figure/module.pdf}
%\end{center}
% \caption{\textbf{The module of Class Activation Mapping}. CAM module could be integrated behind the convolutioanl layers of any existing networks. CAM module outputs both the predicted class score and class activation map which indicate the discriminative regions relevant to the predicted class. }\label{figure_module}
%\end{figure}
%
%The technique of Class Activation Mapping is based on the CAM module, which could be mounted to any existing networks. CAM module is composed of three parts: the first part is the convolutional composition layer, where each composition unit generates a feature map from the previous convolutional layer; the second part is the global average pooling layer, which takes the spatial average of each feature map of the composition unit; the third part is the categorical softMax layer, which will weighted sum average global pooled values and output the class score. The CAM module will be trained along with the whole network.
%
%Formally assume that given an image, the activation of the convolutional composition layer is $f^{k}_{h,w}$, where $h$ and $w$ is the spatial grid of the feature map and $k$ is the composition unit index. Then each activation at the global pooling layer is $F^{k} = \sum_{h,w}f^{k}_{h,w}$. Then the linear score of the class $c$ at softMax layer is
%\begin{align}
%S_c = \sum_{k}w_{c}^{k}F^{k}
%\end{align}
%where $P_c = \frac{\exp(S_c)}{\sum_{c}\exp(S_c)}$ is predicted probability score of the class $c$ and $w_{c}^{k}$ is the learned weight of regression indicating how important the activation $F^{k}$ contribute to classify this image to class $c$. Here we ignore the bias term. By plugging $F^{k} = \sum_{h,w}f^{k}_{h,w}$ into the linear class score, we have
%\begin{align}
%\nonumber S_c = &\sum_{k}w_{c}^{k}\sum_{h,w}f^{k}_{h,w}\\
%=&\sum_{h,w}\sum_{k}w_{c}^{k}f^{k}_{h,w}.
%\end{align}
%We define $M_{c}$ as the class activation map for class $c$, where each spatial element is
%\begin{align}
%M_{c}(h,w) = \sum_{k}w_{c}^{k}f^{k}_{h,w}
%\end{align} 
%since we have $S_c =\sum_{h,w}M_{c}(h,w)$, $M_{c}(h,w)$ directly indicates how important the activation at spatial grid $(h,w)$ leads to classifying the image into class $c$.
%
%From the previous visualization on the activation patterns of the convolutional units \cite{zhou2014object,zeiler2014visualizing}, each unit would be activated discriminatively by some visual pattern. Thus the activation $f^{k}_{h,w}$ from the convolutional composition layer also indicates some visual pattern emerging at the image within the receptive field of the unit $k$ at spatial grid $(h,w)$. Through the linear weighted summation based on learned $w_{c}^{k}$ then average summation over all the spatial grids, we get the linear score of the class $c$ at softMax layer. Thus the class activation map interpret the linear score of the class $c$ predicted by the ConvNet back to the image. By simply upsampling the class activation map to input image size, we could know which image regions most relevant to the classification predicted by the ConvNet.
%
%Figure \ref{figure_examplemapping} shows some examples of the class activation map generated from a ConvNet with CAM module for two classes in ImageNet. We can see that the classification-trained network learns to localize the most informative object parts of the class.
%
%Besides, by varying the class $c$ we could generate different class activation map $M_{c}$ for the same image. Figure \ref{figure_multipleprediction} shows two example of multiple class activation maps.
%
%\begin{figure}
%\begin{center}
%\includegraphics[width=1\linewidth]{figure/heatmap_class.pdf}
%\end{center}
% \caption{\textbf{Examples of class activation maps from two animal classes in ImageNet}. ConvNet with CAM module (CAM-GoogLeNet) learns to localize the discriminative part of the class from the classification training. The localized image regions usually indicate the most informative object parts, such as the head of the hen and Yorkshire terrier. The predicted class score of the class activation map is also given.}\label{figure_examplemapping}
%\end{figure}
%
%
%\begin{figure}
%\begin{center}
%\includegraphics[width=1\linewidth]{figure/multipleCAM.pdf}
%\end{center}
% \caption{\textbf{Examples of class activation maps of top 5 predicted classes}. Since there are multiple objects in the image, each class activation map highlights different discriminative regions for that class. The predicted class name and score is given in each map.}\label{figure_multipleprediction}
%\end{figure}

%TODO there is still some problem for the negative class activation map... so the following paragraph might be removed.
%The class activation map could be further interpreted in several ways: 1) By weighted sum the feature maps which have positive value $w_{c}^{k}$ and negative value $w_{c}^{k}$ respectively, we could know that which spatial region and visual pattern have positive contribution and have negative contribution to the classification of some class, \textit{e.g.} the emergence of bed increases the classification of bedroom class but decreases the classification of toilet class; 2) by comparing the different class activation map, we would identity why CNN makes this classification decision, rather than another decision, which would help better understand how the CNN works. Here we denote the positive activation map $M_{c}^{+}$, the negative activation map $M_{c}^{-}$, and the relative activation map $M_{c_{1},c_{2}}$ as follows,
%\begin{align}
%M_{c}^{+} = \sum_{k'}w_{c}^{k'}f^{k}_{h,w}(\textbf{I}), w_{c}^{k'}>0\\
%M_{c}^{-} = \sum_{k'}w_{c}^{k'}f^{k}_{h,w}(\textbf{I}), w_{c}^{k'}<0\\
%M_{c_{1},c_{2}} = \sum_{k}(w_{c_{1}}^{k}-w_{c_{2}}^{k})f^{k}_{h,w}(\textbf{I}).
%\end{align} 
%Here we can see that if $P_{c_{1}}/P_{c_{2}} = \exp(\sum_{h,w}M_{c_{1},c_{2}}(h,w))$, image \textbf{I} will have higher probability to be classified to class $c_{1}$ if $\sum_{h,w}M_{c_{1},c_{2}}(h,w)>0$. The class activation maps help us better interpret the classification decision of CNN with the information of local image regions.

%More interesting is that the deep feature with localization capacity from the CNN-GAP could be easily transferred to other recognition datasets as generic localizable visual feature. By using the re-trained classifier's weight $w_{c}^{k}$ (usually it's the weight of a linear SVM) on the new dataset to reconstruct the class activation map, discriminative image regions of the class in the new dataset could be identified. 

%In the following experiment section, we first quantitatively evaluate the effectiveness of the CAM technique for CNN-GAP for weakly supervised object localization on ILSVRC benchmark. Then we apply the deep feature from the CNN-GAP for generic classification, localization, and concept discovery, showing that the CAM technique could enable the deep feature to localize discriminative regions of class accurately as a generic localizable visual feature.
%We evaluate the transfer ability of localization for the deep feature in the following experiment section.

\textbf{Global average pooling (GAP) vs global max pooling (GMP):} Given the prior work~\cite{oquab2014weakly} on using GMP for weakly supervised object localization, we believe it is important to highlight the intuitive difference between GAP and GMP. We believe that GAP loss encourages the network to identify the extent of the object as compared to GMP which encourages it to identify just one discriminative part. This is because, when doing the average of a map, the value can be maximized by finding \textit{all} discriminative parts of an object as all low activations reduce the output of the particular map. On the other hand, for GMP, low scores for all image regions except the most discriminative one do not impact the score as you just perform a max. We verify this experimentally on ILSVRC dataset in Sec.~\ref{sec:weaklocalization}: while GMP achieves similar classification performance as GAP, GAP outperforms GMP for localization.

\section{Weakly-supervised Object Localization}
\label{sec:weaklocalization}
In this section, we evaluate the localization ability of CAM when trained on the ILSVRC 2014 benchmark dataset~\cite{ILSVRCijcv15}. We first describe the experimental setup and the various CNNs used in Sec.~\ref{sec:locsetup}. Then, in Sec.~\ref{sec:locresults} we verify that our technique does not adversely impact the classification performance when learning to localize and provide detailed results on weakly-supervised object localization.

%We evaluate the classification and localization effectiveness of the CAM technique for CNN-GAP. In the first set of experiments we compare the classification performance of different network architecture with the original configuration and with the global average pooling. After that we perform various experiments that use the proposed CAM technique for object and concept localization with weak supervision.

\subsection{Setup}
\label{sec:locsetup}
For our experiments we evaluate the effect of using CAM on the following popular CNNs: AlexNet~\cite{krizhevsky2012imagenet}, VGGnet~\cite{simonyan2014very}, and GoogLeNet~\cite{szegedy2014going}. In general, for each of these networks we remove the fully-connected layers before the final output and replace them with GAP followed by a fully-connected softmax layer. 

We found that the localization ability of the networks improved when the last convolutional layer before GAP had a higher spatial resolution, which we term the \textit{mapping resolution}. In order to do this, we removed several convolutional layers from some of the networks. Specifically, we made the following modifications: For AlexNet, we removed the layers after \texttt{conv5}
(i.e., \texttt{pool5} to \texttt{prob}) resulting in a mapping resolution of $13 \times 13$. For VGGnet, we removed the layers after \texttt{conv5-3} (i.e., \texttt{pool5} to \texttt{prob}), resulting in a mapping resolution of $14 \times 14$. For GoogLeNet, we removed the layers after \texttt{inception4e} (i.e., \texttt{pool4} to \texttt{prob}), resulting in a mapping resolution of $14 \times 14$. To each of the above networks, we added a convolutional layer of size $3\times3$, stride $1$, pad $1$ with 1024 units, followed by a GAP layer and a softmax layer. Each of these networks were then fine-tuned\footnote{Training from scratch also resulted in similar performances.} on the 1.3M training images of ILSVRC~\cite{ILSVRCijcv15} for 1000-way object classification resulting in our final networks AlexNet-GAP, VGGnet-GAP and GoogLeNet-GAP respectively.

For classification, we compare our approach against the original AlexNet~\cite{krizhevsky2012imagenet}, VGGnet~\cite{simonyan2014very}, and GoogLeNet~\cite{szegedy2014going}, and also provide results for Network in Network (NIN)~\cite{lin2013network}. For localization, we compare against the original GoogLeNet\footnote{This has a lower mapping resolution than GoogLeNet-GAP.}, NIN and using backpropagation~\cite{simonyan2013deep} instead of CAMs. Further, to compare average pooling against max pooling, we also provide results for GoogLeNet trained using global max pooling (GoogLeNet-GMP).

We use the same error metrics (top-1, top-5) as ILSVRC for both classification and localization to evaluate our networks. For classification, we evaluate on the ILSVRC validation set, and for localization we evaluate on both the validation and test sets. 

%As our primary goal is localization, it can be helpful to have a high spatial resolution of the feature map of the last convolutional layer when adding the global average pooling, here we call \textit{mapping resolution}. For this reason, the specific modifications that we made of each network are the following: for AlexNet we remove the layers after conv5 (original pool5, 3 fully connected layers, and categorical layer are removed), the mapping resolution is $13\times13$; For VGGnet, we remove the layers after conv5-3 (original 3 fully connected layers and the categorical layer are removed), the mapping resolution is $14\times14$; For GoogLeNet, we remove layers after inception4e (original pool4 and inception5a-b, global average pooling and categorical layer are removed). After that, one convolutional layer with 1024 convolution units is added at the end of each modified network followed by the global average pooling layer and softMax layer. All the CNN-GAP are fine-tuned on ImageNet 1.3 million classification training data. We denote the modified networks with the global average pooling structure as AlexNet-GAP, VGGnet-GAP, and GoogLeNet-GAP. Note that CNN-GAPs also could be trained from scratch with similar performance.


%We modify each of these networks to introduce global average pooling by removing all fully-connected layers before the final output and replacing them with global average pooling followed by a softmax. Our goal was to modify these networks as little as possible while allowing for the CAM technique.

%We modify these pre-trained networks, and add to them the global average pooling by modifying the pre-trained networks with the minimal necessary removal. 


\subsection{Results}
\label{sec:locresults}
We first report results on object classification to demonstrate that our approach does not significantly hurt classification performance. Then we demonstrate that our approach is effective at weakly-supervised object localization.

\textbf{Classification:} Tbl.~\ref{network_classificationValidation} summarizes the classification performance of both the original and our GAP networks. We find that in most cases there is a small performance drop of $1 - 2\%$ when removing the additional layers from the various networks. We observe that AlexNet is the most affected by the removal of the fully-connected layers. To compensate, we add two convolutional layers just before GAP resulting in the AlexNet*-GAP network. We find that AlexNet*-GAP performs comparably to AlexNet. Thus, overall we find that the classification performance is largely preserved for our GAP networks. Further, we observe that GoogLeNet-GAP and GoogLeNet-GMP have similar performance on classification, as expected. Note that it is important for the networks to perform well on classification in order to achieve a high performance on localization as it involves identifying both the object category and the bounding box location accurately.

\textbf{Localization:} In order to perform localization, we need to generate a bounding box and its associated object category. To generate a bounding box from the CAMs, we use a simple thresholding technique to segment the heatmap. We first segment the regions of which the value is above 20\% of the max value of the CAM. Then we take the bounding box that covers the largest connected component in the segmentation map. We do this for each of the top-5 predicted classes for the top-5 localization evaluation metric. Fig.~\ref{fig:localizationexample}(a) shows some example bounding boxes generated using this technique. The localization performance on the ILSVRC validation set is shown in Tbl.~\ref{networkvalidationset}, and example outputs in Fig.~\ref{fig:activationmap}.

We observe that our GAP networks outperform all the baseline approaches with GoogLeNet-GAP achieving the lowest localization error of $43\%$ on top-5. This is remarkable given that this network was not trained on a single annotated bounding box. We observe that our CAM approach significantly outperforms the backpropagation approach of~\cite{simonyan2013deep} (see Fig.~\ref{fig:localizationexample}(b) for a comparison of the outputs). Further, we observe that GoogLeNet-GAP significantly outperforms GoogLeNet on localization, despite this being reversed for classification. We believe that the low mapping resolution of GoogLeNet ($7\times7$) prevents it from obtaining accurate localizations. Last, we observe that GoogLeNet-GAP outperforms GoogLeNet-GMP by a reasonable margin illustrating the importance of average pooling over max pooling for identifying the extent of objects.

%\subsection{Classification Performance on ILSVRC Classification Validation Set}
%\label{sec:ilsvrc}

%For our experiments we selected the network architectures AlexNet \cite{krizhevsky2012imagenet}, VGGnet \cite{simonyan2014very}, and GoogLeNet \cite{szegedy2014going}. We modify these pre-trained networks\footnote{The pre-trained network models are downloaded from Caffe Model-Zoo https://github.com/BVLC/caffe/wiki/Model-Zoo}, and add to them the global average pooling by modifying the pre-trained networks with the minimal necessary removal. 

%By removing some network layers the classification accuracy will drop accordingly. Another important factor for the localization ability of ConvNet+GAP is the spatial sizes of feature map of the last convolutional layer, here we call it \textit{mapping resolution}. In the following experiment, we show that keeping a high mapping resolution is crucial to discriminative localization. Thus specifically, for AlexNet we remove the layers after conv5 (original pool5, 3 fully connected layers, and softmax layer are removed), the mapping resolution is $13\times13$; For VGGnet, we remove the layers after conv5 (original 3 fully connected layers and the softmax layer are removed), the mapping resolution is $14\times14$; For GoogLeNet, we remove layers after inception4e (original pool4 and inception5a-b, global average pooling and softmax layer are removed). One convolutional layer with 1024 convoluation units is mounted after each modified network followed by the global average pooling layer and softMax layer. The mapping resolution is also $14\times14$. All the CNN-GAPs are fine-tuned on ImageNet 1.3 million classification training data. We denote the modified networks with the global average pooling structure as AlexNet-GAP, VGGnet-GAP, and GoogLeNet-GAP. Note that CNN-GAPs also could be trained from scratch with similar performance.

%As the main interest of the proposed technique is the localization ability, we need to keep enough spatial resolution of the feature map of the last convolutional layer when adding the global average pooling, here we call \textit{mapping resolution}. For this reason, the specific modifications that we made of each network are the following: for AlexNet we remove the layers after conv5 (original pool5, 3 fully connected layers, and categorical layer are removed), the mapping resolution is $13\times13$; For VGGnet, we remove the layers after conv5-3 (original 3 fully connected layers and the categorical layer are removed), the mapping resolution is $14\times14$; For GoogLeNet, we remove layers after inception4e (original pool4 and inception5a-b, global average pooling and categorical layer are removed). After that, one convolutional layer with 1024 convolution units is added at the end of each modified network followed by the global average pooling layer and softMax layer. All the CNN-GAP are fine-tuned on ImageNet 1.3 million classification training data. We denote the modified networks with the global average pooling structure as AlexNet-GAP, VGGnet-GAP, and GoogLeNet-GAP. Note that CNN-GAPs also could be trained from scratch with similar performance.


%After the fine-tuning, we test the classification performance on the validation set of ILSVRC for the different CNN-GAPs and baseline networks. The results are listed in Table \ref{network_classificationValidation}. We can see that for AlexNet, removing the fully connected layers brings a large performance drop, since the original network only has 5 convolutional layers. Thus we also train a CNN with the AlexNet architecture but adding two extra convolutional layers after the conv5 to compensate the performance drop. This CNN is denoted by AlexNet$^{*}$-GAP. For VGGnet-GAP and GoogLeNet-GAP, since they have more than 12 convolutional layers, removing fully connected layers or some convolutional layers only brings 1\%$\sim$2\% performance drop. We will show that these performance drops are a good trade-off to enable the classification-trained networks for discriminative localization. 


%As we discussed before, the original GoogLeNet and Network in Network (NIN) already have global average pooling structure in the network architecture, so we also could apply our CAM technique to their networks for the discriminative localization. Thus we also evaluate them in the localization experiments. The mapping resolution for GoogLeNet and NIN is $7\times7$, for NIN is $6\times6$.

%Figure \ref{fig:activationmap} shows some examples of class activation maps generated by the the CNN-GAPs and baseline networks on two images from ILSVRC validation set. We can see that CNN-GAPs accurately predict the object class and localize the discriminative object parts on the image. Through the technique of CAM, GoogLeNet and NIN also can localize discriminative object parts, but the localized region is not accurate due to their low mapping resolution. We also show in Figure \ref{fig:activationmap} the class-specific saliency map generated by the back-propagation method \cite{simonyan2013deep} on AlexNet and on GoogLeNet. In these cases we observe that the salient region is more cluttered and less concrete.


\begin{table}\caption{Classification error on the ILSVRC validation set.}
\label{network_classificationValidation}
\centering
\footnotesize
\begin{tabular}{ l | c | c }
  \hline  
  \hline                       
  Networks & top-1 val. error & top-5 val. error \\
    \hline   
VGGnet-GAP& 33.4 & 12.2 \\
GoogLeNet-GAP& 35.0 & 13.2 \\
AlexNet$^{*}$-GAP & 44.9 & 20.9 \\
AlexNet-GAP & 51.1 & 26.3 \\
\hline
  GoogLeNet & 31.9 & 11.3 \\ 
  VGGnet & 31.2 &  11.4 \\    
  AlexNet &  42.6 &  19.5  \\
    NIN & 41.9 & 19.6 \\
    \hline 
    GoogLeNet-GMP & 35.6 & 13.9 \\    
    \hline
\end{tabular}
\end{table}

\begin{figure*}
\begin{center}
\includegraphics[width=1\textwidth]{figure/heatmapAll.pdf}
\end{center}
\vspace{-4mm}
\caption{Class activation maps from CNN-GAPs and the class-specific saliency map from the backpropagation methods.}
\label{fig:activationmap}
\end{figure*}

%\textbf{Global average pooling v.s. global max pooling.} We further evaluate the effectiveness of two different global pooling on the results. Global average pooling has been shown in \cite{lin2013network} as a effective training regularizer. Our experiments also show that the networks with global average pooling could be trained from the scratch using the usual training process, where they converge to the similar performance with the fine-tuned networks. But the networks with global max pooling cannot be trained from scratch, as they always diverge and stay a low accuracy as 1\%. On the other hand, we find that the global max pooling could be fine-tuned on the pre-trained network, then we evaluate the GoogLeNet with global max pooling (GoogLeNet-GMP) on classification and localization respectively. As the GoogLeNet-GMP shown in Table \ref{network_classificationValidation} and Table \ref{networkvalidationset}, GoogLeNet-GMP has similar classification performance to GoogLeNet-GAP, but it has worse localization performance. It could be because that networks with global max pooling don't have the linear relation between the class activation map the the classification score in Eq.\ref{eq:linearscore}. Our result is also consistent to the result in \cite{oquab2014weakly} that the weakly supervised trained network with global max pooling performs well for object center location prediction, but much worse on predicting the extent of objects for object detection. Thus we use global average pooling for all the following experiments.

%\subsection{Weakly Supervised Object Localization on ILSVRC Benchmark}

%As we discussed before, the original GoogLeNet and Network in Network (NIN) already have global average pooling structure in the network architecture, so we also could apply our CAM technique to their networks for the discriminative localization. Thus we also evaluate them in the localization experiments. The mapping resolution for GoogLeNet and NIN is $7\times7$, for NIN is $6\times6$.

%The class activation map identifies the location of the discriminative object parts given an object class. Thus, the class activation map can be used for object localization, despite the network is trained for classification only. We evaluate the localization ability of CNN-GAP using the CAM technique on ILSVRC localization benchmark~\cite{ILSVRCijcv15}, which requires the bounding box localization for 1000 classes with 50 images per class in the validation set and 100 images per class in the test set. We will show that even training with no bounding box annotation, we can achieve reasonably accurate localization performance with the CAM technique to CNN-GAP.

%As we discussed before, the original GoogLeNet and Network in Network (NIN) already have global average pooling structure in the network architecture. Then we can also apply our CAM technique to these networks for the discriminative localization. The mapping resolution is $7\times7$ for GoogLeNet and $6\times6$ for NIN.

%To generate bounding boxes from the class activation map generated by CAM on any CNN-GAPs, we use simple thresholding technique to segment the heat map and take the bounding box that covers the largest connected component in the segmentation map. For each of the top-5 predicted classes, we generate the class activation map, then take the largest bounding box segmented from each class activation map. Some examples of the class activation map along with the ground-truth bounding box and predicted bounding boxes are shown in Figure \ref{fig:localizationexample}(a).  

%We also compute the class-specific saliency map generated by the back-propagation method \cite{simonyan2013deep} on AlexNet. Some examples of these maps can be seen in Figure \ref{fig:localizationexample}(b). We observe that the high response on the class-specific saliency map generated by the back-propagation method is scattered in wide regions.

%The quantitative results of the localization performance are shown in table \ref{networkvalidationset}. GoogLeNet-GAP achieves the highest localization accuracy. Although GoogLeNet has higher classification accuracy compared to GoogLeNet-GAP, it performs worse in the localization task because of its lower mapping resolution. 

To further compare our approach with the existing weakly-supervised~\cite{simonyan2013deep} and fully-supervised~\cite{szegedy2014going,sermanet2013overfeat,szegedy2014going} CNN methods, we evaluate the performance of GoogLeNet-GAP on the ILSVRC test set. We follow a slightly different bounding box selection strategy here: we select two bounding boxes (one tight and one loose) from the class activation map of the top 1st and 2nd predicted classes and one loose bounding boxes from the top 3rd predicted class. We found that this heuristic was helpful to improve performances on the validation set. The performances are summarized in Tbl.~\ref{networktestset}. GoogLeNet-GAP with heuristics achieves a top-5 error rate of 37.1\% in a weakly-supervised setting, which is surprisingly close to the top-5 error rate of AlexNet (34.2\%) in a fully-supervised setting. While impressive, we still have a long way to go when comparing the fully-supervised networks with the same architecture (i.e., weakly-supervised GoogLeNet-GAP vs fully-supervised GoogLeNet) for the localization.


\begin{table}\caption{Localization error on the ILSVRC validation set. \textit{Backprop} refers to using~\cite{simonyan2013deep} for localization instead of CAM.}
\label{networkvalidationset}
\centering
\footnotesize
\begin{tabular}{ l | c | c }   
  \hline  
  \hline
  Method & top-1 val.error & top-5 val. error \\
    \hline  
  GoogLeNet-GAP&  \textbf{56.40} & \textbf{43.00} \\
  VGGnet-GAP& 57.20 & 45.14 \\
  GoogLeNet & 60.09 & 49.34\\      
  AlexNet$^{*}$-GAP & 63.75 & 49.53 \\     
  AlexNet-GAP & 67.19 & 52.16 \\
  NIN  & 65.47 & 54.19 \\
    \hline    
  Backprop on GoogLeNet & 61.31 & 50.55 \\  
  Backprop on VGGnet & 61.12 & 51.46 \\
  Backprop on AlexNet & 65.17 & 52.64 \\
  \hline  
    GoogLeNet-GMP  & 57.78 & 45.26 \\
    \hline 
\end{tabular}
\end{table}

\begin{table}\caption{Localization error on the ILSVRC test set for various weakly- and fully- supervised methods.}\label{networktestset}
\centering
\footnotesize
\begin{tabular}{ l | c | c  }
  \hline  
  \hline                       
  Method & supervision &  top-5 test error \\
    \hline  
  GoogLeNet-GAP (heuristics)  & weakly & \textbf{37.1} \\     
  GoogLeNet-GAP  & weakly & 42.9 \\ 
  Backprop \cite{simonyan2013deep} & weakly & 46.4 \\
      \hline 
  GoogLeNet \cite{szegedy2014going} & full & 26.7 \\
  OverFeat \cite{sermanet2013overfeat} & full & 29.9 \\        
  AlexNet \cite{szegedy2014going} & full & 34.2 \\
  \hline  
\end{tabular}
\end{table}

\begin{figure*}
\begin{center}
\includegraphics[width=1\textwidth]{figure/imagenet_localization.pdf}
\end{center}
\caption{a) Examples of localization from GoogleNet-GAP. b) Comparison of the localization from GooleNet-GAP (upper two) and the backpropagation using AlexNet (lower two). The ground-truth boxes are in green and the predicted bounding boxes from the class activation map are in red.}
\label{fig:localizationexample}
\end{figure*}

%\subsection{Scene recognition on SUN fully annotated data}
%[TODO: not sure if semantics could be included here]
%As shown in the previous section, the discriminative image regions learned by CAMnet are the informative parts of the object. From the empirical analysis on the ConvNet learned  for scene recognition \cite{zhou2014object}, object detectors within the internal representation emerges as a result of learning to classify scene categories. But it is still unclear for a given scene image what are the regions that actually activate the CNN to classify the whole image to that scene category. Based on the CAMnet we would further analyze the semantics of the activated scene regions and the correlation between scene annotation data.
%
%We fine-tune the CAM-GoogLeNet on Places Database and use it for scene recognition. As for the classification rate, the top-1 error on test set of Places205 is 47.87\%, the top-5 error on test set of Places205 is 17.3\%, which outperforms the top-1 error 50.0\% and top-5 error 18.9\% for AlexNet trained on Places205 from scratch \cite{zhou2014learning}. We call this CAMnet finetuned at Places205 as CAM-GoogleNet-Places.
%
%We test the CAM-GoogleNet-Places on the fully annotated images from the SUN database \cite{xiao2010sun}. The SUN database contains 8220 fully annotated images from the same 205 place categories used to train Places-CAMnet. The activation maps of examples are shown in Figure \ref{fig:scenedetection}.
%
%[TODO] Experiment 1: analyze the semantics of the activated regions: put all the annotated images into the Places-CAMnet, segment out the activation regions. Then use the similar methodology in ICLR paper to see the object class distribution for each category. \ref{fig:scenecategoryregion}
%
%[TODO] Experiment 2: estimate the correlation between the discriminative objects, similar to the Figure \ref{fig:correlation}.

%\begin{figure}
%\begin{center}
%%\includegraphics[width=1\textwidth]{figs/preferredImages2.eps}
%\vspace{+60mm}
%\end{center}
%\caption{Example of scene class activation maps from Places-CAMnet. Original Image, object activation map, graphCut foreground.}
%\label{fig:scenedetection}
%\end{figure}
%
%\begin{figure}
%\begin{center}
%%\includegraphics[width=1\textwidth]{figs/preferredImages2.eps}
%\vspace{+60mm}
%\end{center}
%\caption{Example of scene class activation maps from Places-CAMnet. Original Image, object activation map, graphCut foreground.}
%\label{fig:scenecategoryregion}
%\end{figure}
%
%\begin{figure}
%\begin{center}
%%\includegraphics[width=1\textwidth]{figs/preferredImages2.eps}
%\vspace{+60mm}
%\end{center}
%\caption{a) Object frequency in SUN. b) Counts of the objects in the activated regions. c) Frequency of the most informative objects for scene classification in the annotation data.}
%\label{fig:correlation}
%\end{figure}


\section{Deep Features for Generic Localization}

The responses from the higher-level layers of CNN (e.g., \texttt{fc6}, \texttt{fc7} from AlexNet) have been shown to be very effective generic features with state-of-the-art performance on a variety of image datasets~\cite{donahue2014decaf,razavian2014cnn,zhou2014learning}. Here, we show that the features learned by our GAP CNNs also perform well as generic features, and as bonus, identify the discriminative image regions used for categorization, despite not having being trained for those particular tasks. To obtain the weights similar to the original softmax layer, we simply train a linear SVM~\cite{fan2008liblinear} on the output of the GAP layer.

%The common scenario of using deep features is that the images are fed into the CNN and the response of higher-level layer (usually fc7 response for AlexNet or the response at the global average pooling for GoogLeNet) are extracted as deep feature vector. Then a linear SVM is trained on these extracted deep feature vectors. Here we show that the deep feature from our CNN-GAP not only performs well as generic feature for classification on various datasets, but also can localize the discriminative image regions for different classes in a dataset using the CAM technique. We take the response at the global average pooling layer of the CNN-GAP as the deep feature. For the discriminative localization on a new dataset, we use the weights of the newly trained SVM to generate the class activation map.

First, we compare the performance of our approach and some baselines on the following scene and object classification benchmarks: SUN397~\cite{xiao2010sun}, MIT Indoor67~\cite{quattoni2009recognizing}, Scene15~\cite{lazebnik2006beyond}, SUN Attribute~\cite{patterson2012sun}, Caltech101~\cite{fei2007learning}, Caltech256~\cite{griffin2007caltech}, Stanford Action40~\cite{yao2011human}, and UIUC Event8~\cite{li2007and}. The experimental setup is the same as in~\cite{zhou2014learning}. In Tbl.~\ref{dataset_comparison}, we compare the performance of features from our best network, GoogLeNet-GAP, with the \texttt{fc7} features from AlexNet, and \texttt{ave pool} from GoogLeNet. 

As expected, GoogLeNet-GAP and GoogLeNet significantly outperform AlexNet. Also, we observe that GoogLeNet-GAP and GoogLeNet perform similarly despite the former having fewer convolutional layers. Overall, we find that GoogLeNet-GAP features are competitive with the state-of-the-art as generic visual features.

More importantly, we want to explore whether the localization maps generated using our CAM technique with GoogLeNet-GAP are informative even in this scenario. Fig.~\ref{fig:genericlocalization} shows some example maps for various datasets. We observe that the most discriminative regions tend to be highlighted across all datasets. Overall, our approach is effective for generating localizable deep features for generic tasks.

In Sec.~\ref{sec:finegrained}, we explore fine-grained recognition of birds and demonstrate how we evaluate the generic localization ability and use it to further improve performance. In Sec.~\ref{sec:pattern} we demonstrate how GoogLeNet-GAP can be used to identify generic visual patterns from images.

%e.g., in Stanford Action40, the mop is localized for \textit{cleaning the floor}, while for \textit{cooking} the pan and bowl are localized. Similarly, for other datasets the discriminative regions are localized. This demonstrates the effectiveness of our approach at producing generic localizable deep features.

%The classification accuracies are listed in table \ref{dataset_comparison}. We can see that the deep features from GoogLeNet-GAP and GoogLeNet outperform the deep features from AlexNet in large margin. Even though GoogLeNet-GAP has fewer number of layers compared to the GoogLeNet, its deep features, as generic visual features, have comparable classification performance to the deep features from the GoogLeNet across various datasets.

%Besides the state-of-the-art classification performance, by using our CAM technique the deep feature from the CNN-GAP can localize the discriminative regions of the class. Figure \ref{fig:genericlocalization} shows the localization examples on various datasets. The activated regions indicate the location of the most informative part relevant to the visual concept. For example in Stanford Action40 dataset, for the action class \textit{cleaning the floor} the deep feature localizes the sweepers as the discriminative part and for the action class \textit{cooking} it localizes the pan and bowl as the informative parts of the class; In SUN397, for the scene class \textit{excavation} the deep feature localizes the digger, while for the scene class \textit{playground} it localizes the play facilities, as the informative objects of the the class.

%We further evaluate the deep features from the GoogLeNet-GAP on the following scene and object classification benchmarks: SUN397 \cite{xiao2010sun}, MIT Indoor67 \cite{quattoni2009recognizing}, Scene15 \cite{lazebnik2006beyond}, SUN Attribute \cite{patterson2012sun}, Caltech101 \cite{fei2007learning}, Caltech256 \cite{griffin2007caltech}, Stanford Action40 \cite{yao2011human}, and UIUC Event8 \cite{li2007and}. The experimental setup is the same as in \cite{zhou2014learning} and  We compare with the deep feature (fc7 response) from the standard AlexNet, and the deep feature from the original GoogLeNet. 

%The classification accuracies are listed in table \ref{dataset_comparison}. We can see that the deep features from GoogLeNet-GAP and GoogLeNet outperform the deep features from AlexNet in large margin. Even though GoogLeNet-GAP has fewer number of layers compared to the GoogLeNet, its deep features, as generic visual features, have comparable classification performance to the deep features from the GoogLeNet across various datasets.

%Besides the state-of-the-art classification performance, by using our CAM technique the deep feature from the CNN-GAP can localize the discriminative regions of the class. Figure \ref{fig:genericlocalization} shows the localization examples on various datasets. The activated regions indicate the location of the most informative part relevant to the visual concept. For example in Stanford Action40 dataset, for the action class \textit{cleaning the floor} the deep feature localizes the sweepers as the discriminative part and for the action class \textit{cooking} it localizes the pan and bowl as the informative parts of the class; In SUN397, for the scene class \textit{excavation} the deep feature localizes the digger, while for the scene class \textit{playground} it localizes the play facilities, as the informative objects of the the class.


\subsection{Fine-grained Recognition}
\label{sec:finegrained}

In this section, we apply our generic localizable deep features to identifying 200 bird species in the CUB-200-2011~\cite{WelinderEtal2010} dataset. The dataset contains 11,788 images, with 5,994 images for training and 5,794 for test. We choose this dataset as it also contains bounding box annotations allowing us to evaluate our localization ability. Tbl.~\ref{birdresult} summarizes the results.

We find that GoogLeNet-GAP performs comparably to existing approaches, achieving an accuracy of 63.0\% when using the full image without any bounding box annotations for both train and test. When using bounding box annotations, this accuracy increases to 70.5\%. Now, given the localization ability of our network, we can use a similar approach as Sec.~\ref{sec:locresults} (i.e., thresholding) to first identify bird bounding boxes in both the train and test sets. We then use GoogLeNet-GAP to extract features again from the crops inside the bounding box, for training and testing. We find that this improves the performance considerably to 67.8\%. This localization ability is particularly important for fine-grained recognition as the distinctions between the categories are subtle and having a more focused image crop allows for better discrimination.

Further, we find that GoogLeNet-GAP is able to accurately localize the bird in 41.0\% of the images under the 0.5 intersection over union (IoU) criterion, as compared to a chance performance of 5.5\%. We visualize some examples in Fig.~\ref{fig:bird}. This further validates the localization ability of our approach.

%We first show that the discriminative localization from classification could improve the classification itself. We conduct the fine-grained classification experiment on the bird dataset of CUB-200-2011 \cite{WelinderEtal2010}. This dataset has 200 bird classes, with totally 11,788 images. There are several annotations per images: 15 part locations, 312 attributes, and 1 bounding box for the bird location. To leverage every annotations, there are a lot of fine-grained classification methods proposed for this dataset \cite{gavves2014local,zhang2013deformable,zhang2014part}.  We first evaluate two baselines, the first baseline is that we directly extract the deep feature from GoogLeNet-GAP on the whole image for the train set, then the linear SVM is trained and evaluated the classification accuracy on the test images. The accuracy is 62.9\%. For the second baseline we crop the bird regions out using the bounding box annotation, then extract the deep features from GoogLeNet-GAP on the bird regions again, finally linear SVM is trained and evaluated on the test image crops. The accuracy is 70.7\%. Then they could be considered as the lower bound and upper bound performance, as we would like to see how the discriminative localization could improve the classification performance. Here we follow a cascade process of feature extraction->localization->feature extraction: We first extract GoogLeNet-GAP feature on the whole image for the training set, then train a SVM then generate one image crop for each image using the resulting class activation map, then we extract the GoogLeNet-GAP feature on each image crop again. The linear SVM is trained on the train image crops then evaluated on the test image crops. The accuracy is improved around 5\% to 67.5\%. As shown in Table \ref{birdresult}, we also include the results from other methods as comparison. We can see that with only image-level class labels, our method could still achieve reasonably high performance on the fine-grained classification. The examples of the class activation maps on CUB200 are shown in Figure \ref{fig:bird}. These highlighted regions indicate the unique fine-grained attribute of different bird categories. Then we evaluate how accurate the bounding box localization is. We count the localized bounding box as correct if the IoU ratio between the localized bounding box and the ground-truth bounding box is larger than 0.5. The bounding box localization accuracy is 40.97\%, which is much higher than the 5.52\% accuracy of the baseline in which we randomly shift the upper-left corner of the localized bounding box over the whole image.

\begin{table}\caption{Fine-grained classification performance on CUB200 dataset. GoogLeNet-GAP can successfully localize important image crops, boosting classification performance.}
\centering
\footnotesize
\begin{tabular}{ l | c | c }
\hline
  \hline                       
  Methods & Train/Test Anno. & Accuracy \\
    \hline  
    % bbox classification accuracy 0.27788
 %   Deep Feature GoogLeNet & n/a & 60.9\% \\ 
% Deep feature GoogLeNet & n/a & 60.9\% \\
    GoogLeNet-GAP on full image & n/a & 63.0\% \\
    GoogLeNet-GAP on crop & n/a & 67.8\% \\
	GoogLeNet-GAP on BBox & BBox & 70.5\% \\
    \hline   
    Alignments \cite{gavves2014local} & n/a & 53.6\% \\
    Alignments \cite{gavves2014local} & BBox & 67.0\%\\
    DPD \cite{zhang2013deformable} & BBox+Parts & 51.0\% \\
    DeCAF+DPD \cite{donahue2014decaf} & BBox+Parts & 65.0\%\\
    PANDA R-CNN \cite{zhang2014part} & BBox+Parts & 76.4\% \\
    \hline  
\end{tabular}\label{birdresult}
\end{table}

\begin{figure}
\begin{center}
\includegraphics[width=1\linewidth]{figure/bird_CUB200.pdf}
\end{center}
\caption{CAMs and the inferred bounding boxes (in red) for selected images from four bird categories in CUB200. In Sec.~\ref{sec:finegrained} we quantitatively evaluate the quality of the bounding boxes (41.0\% accuracy for 0.5 IoU). We find that extracting GoogLeNet-GAP features in these CAM bounding boxes and re-training the SVM improves bird classification accuracy by about 5\% (Tbl.~\ref{birdresult}).}
\label{fig:bird}
\end{figure}


\begin{table*}\caption{Classification accuracy on representative scene and object datasets for different deep features. }\label{dataset_comparison}
\centering
\footnotesize
\begin{tabular}{lcccccccc}
%\begin{tabular}{lllllllll}
\hline
\hline
 &SUN397&MIT Indoor67&Scene15&SUN Attribute&Caltech101&Caltech256&Action40 & Event8 \\
\hline
\texttt{fc7} from AlexNet       & 42.61 & 56.79 & 84.23 & 84.23 & 87.22 & 67.23 & 54.92 & 94.42 \\
\texttt{ave pool} from GoogLeNet     & 51.68 & 66.63 & 88.02 & 92.85 & 92.05  & 78.99    & 72.03 & 95.42\\
\texttt{gap} from GoogLeNet-GAP  & 51.31 & 66.61 & 88.30 & 92.21 & 91.98  & 78.07    & 70.62 & 95.00\\
\hline
\end{tabular}
\label{tableResultsDeepSceneFeat}
\end{table*}

\begin{figure*}
\begin{center}
\includegraphics[width=1\textwidth]{figure/genericDataset.pdf}
\end{center}
\vspace*{-4mm}
\caption{Generic discriminative localization using our GoogLeNet-GAP deep features (which have been trained to recognize objects). We show 2 images each from 3 classes for 4 datasets, and their class activation maps below them. We observe that the discriminative regions of the images are often highlighted e.g., in Stanford Action40, the mop is localized for \textit{cleaning the floor}, while for \textit{cooking} the pan and bowl are localized and similar observations can be made in other datasets. This demonstrates the generic localization ability of our deep features.}
\label{fig:genericlocalization}
\end{figure*}

%Furthermore, we apply the deep feature from CAM-GoogLeNet for the concept discovery on weakly labeled image collections \cite{zhou2014conceptlearner}. In \cite{zhou2014conceptlearner}, deep feature from AlexNet is used as image feature to learn thousands of visual concepts, which are then used to predict the possible visual concepts given a new image. We follow the same concept discovery pipeline and the images with text description from SBU dataset \cite{ordonez2011im2text}. By just replacing the deep feature with the deep feature of CAMnet, we could localize the predicted visual concepts directly on the image shown in Figure \ref{fig:conceptdiscovery}. This localization property of the deep feature from CAMnet is very useful to other deep feature-related applications. For example, the image captioning work \cite{fang2014captions,lrcn2014} associate the concepts generated in the sentence to the image region directly, without any object proposals or multiple instance learning.

\subsection{Pattern Discovery}
\label{sec:pattern}

In this section, we explore whether our technique can identify common elements or patterns in images beyond objects, such as text or high-level concepts. Given a set of images containing a common concept, we want to identify which regions our network recognizes as being important and if this corresponds to the input pattern. We follow a similar approach as before: we train a linear SVM on the GAP layer of the GoogLeNet-GAP network and apply the CAM technique to identify important regions. We conducted three pattern discovery experiments using our deep features. The results are summarized below. Note that in this case, we do not have train and test splits $-$ we just use our CNN for visual pattern discovery.

%From the previous experimental results, we can see that the CNN-GAP discovers the discriminative common patterns within a class of images during the classification training. This capability is relevant to discovering common objects or patterns of interest among a set of images \cite{bagon2010detecting,faktor2014clustering}. In our case, the deep feature from CNN-GAP provide a compact visual representation of the common object/object parts for the category at image level, while the CAM technique provides an explicit way to recover the discriminative common object/object parts at the image region. Thus we apply the deep feature from CNN-GAP along with the CAM technique for \textit{discriminative pattern discovery}: Given a few images, we want to discover the possible common visual pattern. Notice that the task is quite challenging, since we assume the image-level deep feature plus the linear classifier would encode and discover the common patterns across these images. Specifically, we train a linear SVM on the extracted deep feature from GoogLeNet-GAP by taking the given images as the positive set, all the other images which could either be images from other classes or irrelevant images randomly sampled from a large image dataset as the negative set. After we train a binary linear SVM for each class, we compute the class activation maps for the images in the positive set. Note that there is no splitting of training and testing in this case. 

%\footnote{All the experimental details are included in the supplementary materials.}. 


\textbf{Discovering informative objects in the scenes:} We take 10 scene categories from the SUN dataset~\cite{xiao2010sun} containing at least $200$ fully annotated images, resulting in a total of 4675 fully annotated images. We train a one-vs-all linear SVM for each scene category and compute the CAMs using the weights of the linear SVM. In Fig.~\ref{fig:scene} we plot the CAM for the predicted scene category and list the top 6 objects that most frequently overlap with the high CAM activation regions for two scene categories. We observe that the high activation regions frequently correspond to objects indicative of the particular scene category.

%\item  \textbf{Discover fine-grained attributes for the bird categories}: We take 4 bird subcategories from the Blackbird specie, totally 235 images, from the CUB-200 dataset \cite{WelinderEtal2010}. A one-vs-all linear SVM is trained then its weights are used to generate class activation map. The image samples and the localized regions are shown in Figure \ref{fig:bird}. These highlighted regions indicate the unique fine-grained attribute of the bird category.

\textbf{Concept localization in weakly labeled images:} Using the hard-negative mining algorithm from~\cite{zhou2014conceptlearner}, we learn concept detectors and apply our CAM technique to localize  concepts in the image. To train a concept detector for a short phrase, the positive set consists of  images that contain the short phrase in their text caption, and the negative set is composed of randomly selected images without any relevant words in their text caption. In Fig.~\ref{fig:conceptlearner}, we visualize the top ranked images and CAMs for two concept detectors. Note that CAM localizes the informative regions for the concepts, even though the phrases are much more abstract than typical object names.

\textbf{Weakly supervised text detector:} We train a weakly supervised text detector using 350 Google StreetView images containing text from the SVT dataset~\cite{wang2011end} as the positive set and randomly sampled images from outdoor scene images in the SUN dataset \cite{xiao2010sun} as the negative set. As shown in Fig.~\ref{fig:textdetection}, our approach highlights the text accurately without using bounding box annotations.

\begin{figure}
\begin{center}
\includegraphics[width=1\linewidth]{figure/scene.pdf}
\end{center}
\vspace*{-4mm}
\caption{Informative objects for two scene categories. For the dining room and bathroom categories, we show examples of original images (top), and list of the $6$ most frequent objects in that scene category with the corresponding frequency of appearance. At the bottom: the CAMs and a list of the 6 objects that most frequently overlap with the high activation regions.}
\label{fig:scene}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=1\linewidth]{figure/conceptlearner.pdf}
\end{center}
\vspace*{-4mm}
\caption{Informative regions for the concept learned from weakly labeled images. Despite being fairly abstract, the concepts are adequately localized by our GoogLeNet-GAP network.}
\label{fig:conceptlearner}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=1\linewidth]{figure/textdetection.pdf}
\end{center}
\vspace*{-4mm}
\caption{Learning a weakly supervised text detector. The text is accurately detected on the image even though our network is not trained with  text or any bounding box annotations.}
\label{fig:textdetection}
\end{figure}

 
 \textbf{Interpreting visual question answering:} We use our approach and localizable deep feature in the baseline proposed in \cite{zhou2015vqa} for visual question answering. It has overall accuracy 55.89\% on the test-standard in the Open-Ended track. As shown in Fig.~\ref{fig:vqa}, our approach highlights the image regions relevant to the predicted answers.

\begin{figure}
\begin{center}
\includegraphics[width=1\linewidth]{figure/VQA_CAM.pdf}
\end{center}
\caption{Examples of highlighted image regions for the predicted answer class in the visual question answering.}
\label{fig:vqa}
\end{figure}

\section{Visualizing Class-Specific Units}

Zhou \textit{et al}~\cite{zhou2014object} have shown that the convolutional units of various layers of CNNs act as visual concept detectors, identifying  low-level concepts like textures or materials, to high-level concepts like objects or scenes. Deeper into the network, the units become increasingly discriminative. However, given the fully-connected layers in many networks, it can be difficult to identify the importance of different units for identifying different categories. Here, using GAP and the ranked softmax weight, we can directly visualize the units that are most discriminative for a given class. Here we call them the \textit{class-specific units} of a CNN. 

Fig.~\ref{fig:unitvisualization} shows the class-specific units for AlexNet$^{*}$-GAP trained on ILSVRC dataset for object recognition (top) and Places Database for scene recognition (bottom).
%\footnote{AlexNet$^{*}$-GAP is trained from scratch on 2.5 million images of 205 scene categories from Places Database.}
We follow a similar procedure as~\cite{zhou2014object} for estimating the receptive field and segmenting the top activation images of each unit in the final convolutional layer. Then we simply use the softmax weights to rank the units for a given class. From the figure we can identify the parts of the object that are most discriminative for classification and exactly which units detect these parts. For example, the units detecting dog face and body fur are important to  \textit{lakeland terrier}; the units detecting sofa, table and fireplace are important to the  \textit{living room}. Thus we could infer that the CNN actually learns a bag of words, where each word is a discriminative class-specific unit. A combination of these class-specific units guides the CNN in classifying each image.


\begin{figure}
\begin{center}
 \includegraphics[width=1\linewidth]{figure/unit.pdf}
\end{center}
\vspace*{-4mm}
\caption{Visualization of the class-specific units for AlexNet*-GAP trained on ImageNet (top) and Places (bottom) respectively. The top 3 units for three selected classes are shown for each dataset. Each row shows the most confident images segmented by the receptive field of that unit. For example, units detecting blackboard, chairs, and tables are important to the classification of  \textit{classroom} for the network trained for scene recognition.}
\label{fig:unitvisualization}
\end{figure}

\section{Conclusion}

%Visual concept detectors in various scales emerge from training CNN for object or scene classification \cite{zhou2014object}. Thus there is attention mechanism inside the CNN that detects the most discriminative visual pattern relevant to the classification of some class in the image. Our work leverages the global pooling module to enable the CNN to highlight these discriminative visual patterns. There is other effort that proposes network module that explicitly allow the spatial manipulation of data within the network \cite{transformer2015}. 

In this work we propose a general technique called Class Activation Mapping (CAM) for CNNs with global average pooling. This enables classification-trained CNNs to learn to perform object localization, without using any bounding box annotations. Class activation maps allow us to visualize the predicted class scores on any given image, highlighting the discriminative object parts detected by the CNN. We evaluate our approach on weakly supervised object localization on the ILSVRC benchmark, demonstrating that our global average pooling CNNs can perform accurate object localization. Furthermore we demonstrate that the CAM localization technique generalizes to other visual recognition tasks i.e., our technique produces generic localizable deep features that can aid other researchers in understanding the basis of discrimination used by CNNs for their tasks.

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
