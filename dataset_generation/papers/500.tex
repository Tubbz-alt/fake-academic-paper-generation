% Template for ICIP-2017 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{epstopdf}
\usepackage{url}
\graphicspath{{./ICIP2017Picture/CNNArchitexture/}{./ICIP2017Picture/artifact/}{./ICIP2017Picture/lowcontrast/}{./ICIP2017Picture/Variaty/}}
% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
%\title{AUTHOR GUIDELINES FOR ICIP 2017 PROCEEDINGS MANUSCRIPTS}
\title{GLOBAL AND LOCAL INFORMATION BASED DEEP NETWORK FOR SKIN LESION SEGMENTATION}
%
% Single address.
% ---------------
%\name{Author(s) Name(s)\thanks{Thanks to XYZ agency for funding.}}
%\address{Author Affiliation(s)}
\name{Jin Qi$^{\star\bigtriangledown\dagger}$\thanks{Thanks to the Center for Information in Medicine in the University of Electronic Science and Technology of China for Funding.},
Miao Le$^{\star}$, Chunming Li$^{\star}$ and Ping Zhou$^{\diamond\dagger}$\thanks{$^\dagger$Corresponding Author: Jin Qi (jqi@uestc.edu.cn); Ping Zhou (ping\_zhou950809@126.com)}}
\address{$^{\star}$Electrical Engineering Department,
          $^{\bigtriangledown}$Center for Information in Medicine, $^{\diamond}$Cancer Hospital/Center \\
in University of Electronic Science and Technology of China \\
        Xiyuan Avenue 2006, West Gaoxin District, Chengdu, Sichuan Province, 611731, China }
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
\begin{document}
%\ninept
%
\maketitle
%
\begin{abstract}
%The abstract should appear at the top of the left-hand column of text, about
%0.5 inch (12 mm) below the title area and no more than 3.125 inches (80 mm) in
%length.  Leave a 0.5 inch (12 mm) space between the end of the abstract and the
%beginning of the main text.  The abstract should contain about 100 to 150
%words, and should be identical to the abstract text submitted electronically
%along with the paper cover sheet.  All manuscripts must be in English, printed
%in black ink.
With a large influx of dermoscopy images and a growing shortage of dermatologists, automatic dermoscopic image analysis plays an essential role in skin cancer diagnosis. In this paper, a new deep fully convolutional neural network (FCNN) is proposed to automatically segment melanoma out of skin images by end-to-end learning with only pixels and labels as inputs. Our proposed FCNN is capable of using both local and global information to segment melanoma by adopting skipping layers. The public benchmark database consisting of 150 validation images, 600 test images and 2000 training images in the melanoma detection challenge 2017 at International Symposium Biomedical Imaging 2017 is used to test the performance of our algorithm. All large size images (for example, $4000\times 6000$ pixels)  are reduced to much smaller images with $384\times 384$ pixels (more than 10 times smaller).  We got and submitted preliminary results to the challenge without any pre or post processing.  The performance of our proposed method could be further improved by data augmentation and by avoiding image size reduction.
%Our method achieves Jaccard Index (JI) 81.6\% and is ranked in the fourth position among 79 submissions from 38 participants according to JI. The experimental results demonstrate that our method achieves favorite results without any feature designing process.       

\end{abstract}
%
\begin{keywords}
Skin Lesion, Dermoscopy Image, Deep Learning, Fully Convolutional Neural Network, Segmentation.
%One, two, three, four, five
\end{keywords}
%
\section{Introduction}
\label{sec:intro}

%These guidelines include complete descriptions of the fonts, spacing, and
%related information for producing your proceedings manuscripts. Please follow
%them and if you have any questions, direct them to Conference Management
%Services, Inc.: Phone +1-979-846-6800 or email
%to \\\texttt{ip17@securecms.com}.
Over five million cases of skin cancer are newly diagnosed in the United States annually\cite{Siegel16-Nature}. Early detection of melanoma, one of the most lethal forms of skin cancer, is critical in finding curable melanomas as well as increasing survival rate\cite{Freedberg1999738,Charles2001}. With a large influx of dermoscopy images, arriving of inexpensive consumer dermatoscope attachments for smart phone\cite{MoleScope} and a growing shortage of dermatologists\cite{Kimball2008741}, automatic dermoscopic image analysis plays an essential role in timely skin cancer diagnosis. Especially, lesion segmentation is one of the most important steps in dermoscopy image access.   

However, automatic lesion segmentation from dermoscopy images is a challenging task. The relatively low contrast and obscure boundaries between early stage skin lesions and normal skin regions make it very difficult to distinguish lesion area from normal skin region. The situation deteriorates rapidly when skin lesions are blurred or excluded by artifacts such as natural hairs, veins, artificial ruler marks, color calibration charts, and air bubbles, etc. Some example images from  
"ISBI 2017 Skin Lesion Analysis Towards Melanoma Detection Challenge" (ISBI2017 SLATMDC)\cite{2017-ISBI-SkinLesionChallenge} are shown in Fig.\ref{fig:example-artifacts}.
%\begin{figure}[htb]
%\begin{minipage}[b]{.48\linewidth}
%  \centering
%  \centerline{\includegraphics[width=3.0cm,height=2cm]{ISIC_0000014_contourGT-crop}}
%%  \vspace{2.0cm}
%%  \centerline{(a) Low contrast}\medskip
%\end{minipage}
%%
%\begin{minipage}[b]{.48\linewidth}
%  \centering
%  \centerline{\includegraphics[width=3.0cm,height=2cm]{ISIC_0000020_contourGT-crop}}
%%  \vspace{1.5cm}
%%  \centerline{(b)Low contrast}\medskip
%\end{minipage}
%\hfill
%\begin{minipage}[b]{0.48\linewidth}
%  \centering
%  \centerline{\includegraphics[width=3.0cm,height=2cm]{ISIC_0000036_contourGT-crop}}
%%  \vspace{1.5cm}
%%  \centerline{(c) Large intraclass variance}\medskip
%\end{minipage}
%\begin{minipage}[b]{0.48\linewidth}
%  \centering
%  \centerline{\includegraphics[width=3.0cm,height=2cm]{ISIC_0000072_contourGT-crop}}
%%  \vspace{1.5cm}
%%  \centerline{(d) Large intraclass variance}\medskip
%\end{minipage}
%\begin{minipage}[b]{0.48\linewidth}
%  \centering
%  \centerline{\includegraphics[width=3.0cm,height=2cm]{ISIC_0007322_contourGT-crop}}
%%  \vspace{1.5cm}
%%  \centerline{(e) Artifact}\medskip
%\end{minipage}
%\hfill
%\begin{minipage}[b]{0.48\linewidth}
%  \centering
%  \centerline{\includegraphics[width=3.0cm,height=2cm]{ISIC_0000043_contourGT-crop}}
%%  \vspace{1.5cm}
%%  \centerline{(f) Artifact}\medskip
%\end{minipage}   
%\end{figure} 

\begin{figure}[htb]

\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=3.0cm,height=2cm]{ISIC_0000014_contourGT-crop.jpg}}
%  \vspace{2.0cm}
%  \centerline{(a) Low contrast}\medskip
\end{minipage}
%
\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=3.0cm,height=2cm]{ISIC_0000020_contourGT-crop.jpg}}
%  \vspace{1.5cm}
%  \centerline{(b)Low contrast}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=3.0cm,height=2cm]{ISIC_0000036_contourGT-crop.jpg}}
%  \vspace{1.5cm}
%  \centerline{(c) Large intraclass variance}\medskip
\end{minipage}
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=3.0cm,height=2cm]{ISIC_0000072_contourGT-crop.jpg}}
%  \vspace{1.5cm}
%  \centerline{(d) Large intraclass variance}\medskip
\end{minipage}
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=3.0cm,height=2cm]{ISIC_0007322_contourGT-crop.jpg}}
%  \vspace{1.5cm}
%  \centerline{(e) Artifact}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=3.0cm,height=2cm]{ISIC_0000043_contourGT-crop.jpg}}
%  \vspace{1.5cm}
%  \centerline{(f) Artifact}\medskip
\end{minipage} 
\caption{Difficult example images with low contrast, large intraclass variety, and artifacts. Ground truth contours are marked in blue color. First row: low contrast between skin lesion and normal skin regions; Second row: large intraclass variance; Last row: artifacts in images. All images are from ISBI2017 SLATMDC\cite{2017-ISBI-SkinLesionChallenge}.}
\label{fig:example-artifacts}
%
\end{figure}

Many works have been done to try to deal with this difficult problem. Previously, some researchers proposed to segment lesions based on hand-crafted features\cite{Tommasi2006,Celebi2007362,Ganster2001,Schaefer2014,Jafari2016ICIP}. Recently, convolutional neural network(CNN) is adopted to improve medical image segmentation accuracy\cite{ronneberger2015unet,Chen2017135DCAN}. Deep CNN based skin cancer recognition has been initially proven to achieve dermatologist-level classification accuracy\cite{Esteva2017DermatologicstLevel-Nature}.  Fully convolutional neural network(FCNN) has shown promising segmentation results in natural images(PASCAL VOC) by fusing multi-scale prediction maps from multiple layers in itself\cite{Long2015FullyCN}. FCNN is based on a simple end-to-end learning and multi-scale contextual information is automatically explored in FCNN framework.

Inspired by \cite{Long2015FullyCN}, we propose a new FCNN framework for skin lesion segmentation in this paper. We pick up the VGG 16-layer net(publicly available from the Caffe model zoo) pretrained in ImageNet, discard its last classification layer, and replace all fully connected layers by convolution layers with randomly initialized weights. The weights in the remaining layers are kept to tackle small medical image dataset problem based on transfer learning. We add skipping layers to fuse multi-scale prediction maps from multiple layers. Different from \cite{Long2015FullyCN} which fuses prediction maps by pixel-wise plus operation, we treat multi-scale prediction maps as multi-scale feature maps and fuse them by concatenation operation. With our concatenating fusing strategy to keep fused features in higher space, we can capture more distinguishing local features and global features. Then we add a randomly initialized $1\times1$ convolution layer as the last layer to finish prediction with fused features as inputs. 

In \cite{Yu2016ResidualNet}, very deep fully convolutional residual network (FCRN) was proposed to do skin lesion segmentation. The authors in \cite{Yu2016ResidualNet} focused on residual network and very deep net(more than 50 layers). However, our proposed FCNN has different structures and is relatively shallower(less than 20 layers). Our experimental results are still comparable with that from \cite{Yu2016ResidualNet}. Our major contributions in this paper are summarized as follows:

\begin{itemize}
\item {We design a novel fully convolutional neural network for skin lesion segmentation in dermatoscopic images with end-to-end learning, transfer learning, and pixel by pixel prediction. }
\item{We treat multi-scale prediction maps as feature maps and propose a new fusing layer with a concatenation operation to combine these multi-scale contextual information to obtain better distinguishing features. We also add a convolution layer as the last layer of our FCNN to take the fused features as inputs to finish prediction.} 
\item{We evaluate our proposed FCNN on the public dermatoscopy image dataset from  
"ISBI 2017 Skin Lesion Analysis Towards Melanoma Detection Challenge"\cite{2017-ISBI-SkinLesionChallenge}.
 We got and submitted preliminary results to the challenge without any pre or post processing.} 
\end{itemize}       

The remainder of this paper is organized as follows. We detail our method in Section \ref{sec:method}. Experimental results are reported in Section \ref{sec:experiment}. Section \ref{sec:discussion} is our discussion and conclusion.


\section{PROPOSED FULLY CONVOLUTIONAL NETWORK}
\label{sec:method}

%All printed material, including text, illustrations, and charts, must be kept
%within a print area of 7 inches (178 mm) wide by 9 inches (229 mm) high. Do
%not write or print anything outside the print area. The top margin must be 1
%inch (25 mm), except for the title page, and the left margin must be 0.75 inch
%(19 mm).  All {\it text} must be in a two-column format. Columns are to be 3.39
%inches (86 mm) wide, with a 0.24 inch (6 mm) space between them. Text must be
%fully justified.
In this section, we detail our proposed fully convolutional network for skin lesion segmentation. Due to relatively small number of medical images and labels available, we begin with pretrained network in image classification task to design our own dense prediction network in order to use transfer learning. Skip layers are added to incorporate multi-scale information into our network.    
The architecture of our proposed FCNN is illustrated in Fig. \ref{fig:architecture-network}.
\begin{figure*}[htb]

\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=0.9\linewidth,height=5.5cm]{SystemFrameworkFig.pdf}}
%  \vspace{2.0cm}
%  \centerline{(a) Result 1}\medskip
\end{minipage}
%
%\begin{minipage}[b]{.48\linewidth}
%  \centering
%  \centerline{\includegraphics[width=4.0cm]{image3}}
%%  \vspace{1.5cm}
%  \centerline{(b) Results 3}\medskip
%\end{minipage}
%\hfill
%\begin{minipage}[b]{0.48\linewidth}
%  \centering
%  \centerline{\includegraphics[width=4.0cm]{image4}}
%%  \vspace{1.5cm}
%  \centerline{(c) Result 4}\medskip
%\end{minipage}
%
\caption{Architecture of our proposed FCNN. Blue vertical bar with solid contour: convolution layer; red vertical bar with dash contour: pooling layer; Blue small square: $1\times 1$ convolution layer; Blue horizontal bar: deconvolution layer for upsampling; Blue rectangular cube: fusing layer with concatenation operation.  Our FCNN learns to use multi-scale information.}
\label{fig:architecture-network}
%
\end{figure*}
\subsection{Basic Network Architecture}
 We pick up the pretrained VGG 16-layer net (publicly available from the Caffe model zoo) consisting of 13 convolution layers, 3 fully connected layers, 15 RELU layers, 5 pooling layers, 2 dropout layers and 1 softmax layer.  This VGG net has been trained for natural image classification in ImageNet. For our dense segmentation purpose, we discard its last two classification layers (last fully connected layer and softmax layer) and replace the remaining 2 fully connected layers with randomly initialized convolution layers. The learned weights in the remaining layers are kept to tackle small medical image dataset problem based on transfer learning. Thus, The modified network is a fully convolutional network which is able to take arbitrary size of images as inputs.

\subsection{Adding Skip Layers}
We add skipping layers to fuse multi-scale prediction maps from multiple layers. Different from \cite{Long2015FullyCN} which fuses prediction maps by pixel-wise sum operation, we treat multi-scale prediction maps as multi-scale feature maps and fuse them by concatenation operation. Specifically, after each pooling layer we add an $1\times1$ convolution layer to compute a dense prediction map with 2 channels (number of classes for each pixel in this paper). The $1\times1$ convolution layer can be considered as a classifier to classify each pixel independently\cite{Long2015FullyCN}.  An $1\times1$ convolution layer is also attached to the last convolution layer in basic network from above subsection. Thus we already added 6 $1\times1$ convolution layers in total to the basic network.  
The outputs of the 6 added $1\times1$ convolution layers are 6 dense prediction maps (2 channels each map).

The above 6 dense prediction maps have different resolutions or sizes. We add multiple deconvolution layers\cite{Long2015FullyCN} to upsample the 6 dense maps to the original size of the input image. The weights of deconvolution layers are randomly initialized and all of them can be learned automatically\cite{Long2015FullyCN}. Then we add a concatenation layer to concatenate the 6 upsampled prediction maps (with the same size as input image) in third dimension to form a final prediction feature map with $6\times2=12$ channels. 

With our concatenating fusing strategy to keep fused features in higher space (12 dimension in this paper), we can capture more distinguishing local features and global features. Then we add a randomly initialized $1\times1$ convolution layer after the above added concatenation layer as the last layer to compute the final prediction map with the concatenated feature map as input. In training stage, we add a softmax loss layer to fine-tune our network. In test, the loss layer is removed. 

\section{EXPERIMENTAL RESULTS}
\label{sec:experiment}

%The paper title (on the first page) should begin 1.38 inches (35 mm) from the
%top edge of the page, centered, completely capitalized, and in Times 14-point,
%boldface type.  The authors' name(s) and affiliation(s) appear below the title
%in capital and lower case letters.  Papers with multiple authors and
%affiliations may require two or more lines for this information. Please note
%that papers should not be submitted blind; include the authors' names on the
%PDF.
\subsection{Dataset}
We use the public dermatoscopy image dataset from  
"ISBI 2017 Skin Lesion Analysis Towards Melanoma Detection Challenge"\cite{2017-ISBI-SkinLesionChallenge} to evaluate the performance of our proposed method in this paper.
This dataset consists of 150 validation images, 600 test images and 2000 training images.
The ground truth binary mask images from expert manual segmentation are also provided  with pixel value 255 and 0 indicating lesion pixel and skin pixel, respectively. 

%In this
%section, we show our preliminary results for this challenge. 
% our experimental results and ranking and compare them with the challenge results and ranking from all participants' algorithms(97 submissions from 38 participants).

\subsection{Implementation}
Our proposed method is implemented with Maltlab by using the open source deep learning toolbox  "MatConvNet"\cite{vedaldi15matconvnet}. Our computer has a NVIDIA GTX1080 GPU card, Windows 7 system, an Intel i7-7700K processor with 4.5 GHz and 64G memory.  
In order to tackle the small medical data problem, we utilize a pretrained VGG 16-layer net (publicly available from the Caffe model zoo) to initialize some weights of our proposed FCNN based on transfer learning idea.  
Stochastic gradient descent(SGD) is used to fine-tune our proposed FCNN with batch size 6, momentum 0.9, weight decay 0.0001, learning rate 0.001.  
%Our FCNN is very efficient with the average segmentation speed 2 images/second (image size $3000\times 4000$ pixel). 

\subsection{Evaluation Metrics}
We also use the same metrics as the challenge organizer to evaluate the performance of our proposed method. These metrics are sensitivity(SE), specificity(SP), accuracy(AC), Jaccard index(JA) and Dice coefficient(DI). 
%They are defined as follows.
%\begin{equation}
%SE=\frac{N_{tp}}{N_{tp}+N_{fn}},SP=\frac{N_{tn}}{N_{tn}+N_{fp}}
%\end{equation}
%\begin{equation}
%AC=\frac{N_{tp}+N_{tn}}{N_{tp}+N_{fn}+N_{tn}+N_{fp}}
%\end{equation}
%\begin{equation}
%JA=\frac{N_{tp}}{N_{tp}+N_{fn}+N_{fp}},DI=\frac{2 N_{tp}}{2 N_{tp}+N_{fn}+N_{fp}}
%\end{equation}
%where $N_{tp}, N_{tn}, N_{fp}$ and $N_{fn}$ denote the number of pixels which are true
%positive, true negative, false positive and false negative, respectively.
%A pixel is a true positive or a false negative if a lesion pixel is predicted with class lesion or non-lesion. A pixel is a true negative or a false positive if a non-lesion pixel is predicted with class non-lesion or lesion. 

According to the challenge rules,
these criteria are calculated for each test image and then are averaged
on the whole test set to get the final
results. Methods are ranked by their respective JA values.

\subsection{Performance of Our Method}
We demonstrate the good performance of our proposed method by giving qualitative segmentation results and quantitative results.
\subsubsection{Qualitative Results}
To show how our proposed method works on difficult images, we show our segmentation results of some challenging images in Fig. \ref{fig:qualitative} where ground truth contours and our segmented contours are marked in blue and red color, respectively. Images with low contrast, irregular shapes and artifacts are shown in the first row, second row, last row of Fig. \ref{fig:qualitative}, respectively. 

From Fig. \ref{fig:qualitative}, it can be seen that our method works very well 
in all of these challenging images.
\begin{figure}[htb]
\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=3.0cm,height=2cm]{ISIC_0000014_contourGTAlg-crop.jpg}}
%  \vspace{2.0cm}
%  \centerline{(a) Low contrast}\medskip
\end{minipage}
%
\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=3.0cm,height=2cm]{ISIC_0000020_contourGTAlg-crop.jpg}}
%  \vspace{1.5cm}
%  \centerline{(b)Low contrast}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=3.0cm,height=2cm]{ISIC_0000036_contourGTAlg-crop.jpg}}
%  \vspace{1.5cm}
%  \centerline{(c) Large intraclass variance}\medskip
\end{minipage}
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=3.0cm,height=2cm]{ISIC_0000072_contourGTAlg-crop.jpg}}
%  \vspace{1.5cm}
%  \centerline{(d) Large intraclass variance}\medskip
\end{minipage}
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=3.0cm,height=2cm]{ISIC_0007322_contourGTAlg-crop.jpg}}
%  \vspace{1.5cm}
%  \centerline{(e) Artifact}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=3.0cm,height=2cm]{ISIC_0000043_contourGTAlg-crop.jpg}}
%  \vspace{1.5cm}
%  \centerline{(f) Artifact}\medskip
\end{minipage}
%
\caption{Example segmentation results of challenging images. Ground truth contours and our detected contours are marked in blue color and red color, respectively. First row: low contrast images; Second row: large intraclass variance; Last row: artifacts.}
\label{fig:qualitative}
%
\end{figure}

\subsubsection{Quantitative Results}
We got and submitted preliminary results to the challenge without any pre or post processing.  
%The performance of our proposed method could be further improved by data augmentation and by avoiding image size reduction.
%According to the ranking rule from the challenging organization \cite{2017-ISBI-SkinLesionChallenge}, we also give the leaderboard 
%of all participants' methods and ours based on their respective JA values in Table \ref{tab:metrics}. These metrics values except ours are directly from the challenge website\footnote{\url{https://challenge.kitware.com/#phase/566744dccad3a56fac786787}}.  
%
%From Table \ref{tab:metrics}, it can be seen that  our proposed method is ranked in the fourth position with JA value $81.6\%$.  Our JA value $81.6\%$ is comparable with the top three JA values $84.3\%$, $82.9\%$ and $82.2\%$. However, our proposed method is very simple and has a simple structure possessing special properties including fully CNN, end-to-end learning, pixel by pixel segmentation, transfer learning, and implicitly using multi-scale information.  Furthermore, the performance of our method can be further improved by augmenting training data and searching for best parameters including learning rate, momentum and weight decay, etc. 

%In summary, our proposed FCNN achieves favorite results in the challenge dataset\cite{David2016-ISBIChallenge} and has the potential to obtain better performance by simple operations such as data augmentation, best parameter searching and pre-post processing.
%\begin{table}[htb]
%\caption{Leaderboard with top 15 participants' methods and our algorithm in terms of JA values (only top 15 methods are shown for saving space). All of the JA value except ours are directly from the challenge website \cite{David2016-ISBIChallenge}. Our algorithm ranks 4th out of  79 submissions and 38 participants with JA value $81.6\%$.}
%\label{tab:metrics}
%\centering\scalebox{0.9}{
%%\begin{tabular}{|p{0.7cm}|p{2.5cm}|p{3.cm}|p{0.8cm}|}
%\begin{tabular}{|c|c|c|c|}
%\hline
%Rank   & User &  Title & JA  \\
%\hline
% 1 &	Urko Sanchez &	ExB	Fri &0.843\\
%\hline
%2	 & Lequan Yu	 & CUMED	 &	0.829\\
%\hline
%3	 &Mahmudur Rahman &	test	 &	0.822\\
%\hline
%\bf{4}  &\bf{ ours} & \bf{ours}  & \bf{0.816}\\
%\hline
%5	 &Jeremy Kawahara	 &sfu-mial	 &	0.811\\
%\hline
%6	 &neda zamani  &	TMUteam-PART1 &	0.810\\
%\hline
%7	 &Maciel Zortea	 &UiT-Seg	 &	0.806\\
%\hline
%8 &	Xulei Yang	 &IHPC\_CS	 &	0.799\\
%\hline
%9 &	thanh nguyentang	 &cnn\_crf	 &	0.797\\
%\hline
%10	 &Jose Luis &	Part1 	 &	0.791\\
%\hline
%11	 &marco romelli	 &First submission test	 &	0.786\\
%\hline
%12 &	Juana M. 	 &Group on AMA &	0.782\\
%\hline
%13 &	Sailesh Conjeti	 &CAMP- TUM &	0.778\\
%\hline
%14	 &Haidi Fan	 &BUAA	 &	0.739\\
%\hline
%15 &	Ahmed Afifi	 &Mufic\_IT	 &	0.730\\
%\hline
%16	 &Bogyu Park &	INHA IILAB	 &	0.725\\
%%Dataset & The whole process (seconds/image) & AM feature computation (seconds/image)  \\
%%\hline
%%SPD2010 datase (image size 355$\times$ 390 pixels) & 20.4049 & 16.2553 \\
%\hline
%\end{tabular}}
%
%\end{table}


\section{DISCUSSION AND CONCLUSION}
\label{sec:discussion}
In this paper, we build a new FCNN for skin lesion segmentation.  We got and submitted preliminary results to the challenge without any pre or post processing.  The performance of our proposed method could be further improved by data augmentation and by avoiding image size reduction.
%We treat prediction maps as feature maps and propose a new fusing layer/strategy which uses concatenating operation rather than summation operation in  \cite{Long2015FullyCN}.   Beginning with pretrained network, we use transfer learning strategy to deal with small medical data problem.  Multi-scale contextual information is automatically incorporated into our framework. Our proposed method owns very interesting properties such as end-to-end learning, pixel by pixel segmentation, transfer learning, and implicitly using multi-scale information. 
%
%The performance of our proposed method in this paper is tested in the public challenge dataset\cite{2017-ISBI-SkinLesionChallenge}. 
%%We obtain favorite experimental results and our method is in fourth rank among the challenge participants' methods(79 submissions and 38 participants) according to JA value. 
%The performance of our method can be easily improved by simple operations such as data augmentation, best parameter searching and pre-post processing.  We will investigate these strategies in the future work to enhance our algorithm.
%
%Although our proposed method in this paper is used for skin lesion segmentation, the framework is so general that it can be used for any segmentation task without any extra efforts.

%\section{CONCLUSION}
%\label{sec:conclusion}
%
%Major headings, for example, "1. Introduction", should appear in all capital
%letters, bold face if possible, centered in the column, with one blank line
%before, and one blank line after. Use a period (".") after the heading number,
%not a colon.
%
%\subsection{Subheadings}
%\label{ssec:subhead}
%
%Subheadings should appear in lower case (initial word capitalized) in
%boldface.  They should start at the left margin on a separate line.
% 
%\subsubsection{Sub-subheadings}
%\label{sssec:subsubhead}
%
%Sub-subheadings, as in this paragraph, are discouraged. However, if you
%must use them, they should appear in lower case (initial word
%capitalized) and start at the left margin on a separate line, with paragraph
%text beginning on the following line.  They should be in italics.
%
%\section{PRINTING YOUR PAPER}
%\label{sec:print}
%
%Print your properly formatted text on high-quality, 8.5 x 11-inch white printer
%paper. A4 paper is also acceptable, but please leave the extra 0.5 inch (12 mm)
%empty at the BOTTOM of the page and follow the top and left margins as
%specified.  If the last page of your paper is only partially filled, arrange
%the columns so that they are evenly balanced if possible, rather than having
%one long column.
%
%In LaTeX, to start a new column (but not a new page) and help balance the
%last-page column lengths, you can use the command ``$\backslash$pagebreak'' as
%demonstrated on this page (see the LaTeX source below).
%
%\section{PAGE NUMBERING}
%\label{sec:page}
%
%Please do {\bf not} paginate your paper.  Page numbers, session numbers, and
%conference identification will be inserted when the paper is included in the
%proceedings.
%
%\section{ILLUSTRATIONS, GRAPHS, AND PHOTOGRAPHS}
%\label{sec:illust}
%
%Illustrations must appear within the designated margins.  They may span the two
%columns.  If possible, position illustrations at the top of columns, rather
%than in the middle or at the bottom.  Caption and number every illustration.
%All halftone illustrations must be clear black and white prints.  Colors may be
%used, but they should be selected so as to be readable when printed on a
%black-only printer.
%
%Since there are many ways, often incompatible, of including images (e.g., with
%experimental results) in a LaTeX document, below is an example of how to do
%this \cite{Lamp86}.
%
%\section{FOOTNOTES}
%\label{sec:foot}
%
%Use footnotes sparingly (or not at all!) and place them at the bottom of the
%column on the page on which they are referenced. Use Times 9-point type,
%single-spaced. To help your readers, avoid using footnotes altogether and
%include necessary peripheral observations in the text (within parentheses, if
%you prefer, as in this sentence).

% Below is an example of how to insert images. Delete the ``\vspace'' line,
% uncomment the preceding line ``\centerline...'' and replace ``imageX.ps''
% with a suitable PostScript file name.
% -------------------------------------------------------------------------
%\begin{figure}[htb]
%
%\begin{minipage}[b]{1.0\linewidth}
%  \centering
%  \centerline{\includegraphics[width=8.5cm]{image1}}
%%  \vspace{2.0cm}
%  \centerline{(a) Result 1}\medskip
%\end{minipage}
%%
%\begin{minipage}[b]{.48\linewidth}
%  \centering
%  \centerline{\includegraphics[width=4.0cm]{image3}}
%%  \vspace{1.5cm}
%  \centerline{(b) Results 3}\medskip
%\end{minipage}
%\hfill
%\begin{minipage}[b]{0.48\linewidth}
%  \centering
%  \centerline{\includegraphics[width=4.0cm]{image4}}
%%  \vspace{1.5cm}
%  \centerline{(c) Result 4}\medskip
%\end{minipage}
%%
%\caption{Example of placing a figure with experimental results.}
%\label{fig:res}
%%
%\end{figure}


% To start a new column (but not a new page) and help balance the last-page
% column length use \vfill\pagebreak.
% -------------------------------------------------------------------------
%\vfill
%\pagebreak

%\section{COPYRIGHT FORMS}
%\label{sec:copyright}
%
%You must include your fully completed, signed IEEE copyright release form when
%form when you submit your paper. We {\bf must} have this form before your paper
%can be published in the proceedings.
%
%\section{REFERENCES}
%\label{sec:ref}
%
%List and number all bibliographical references at the end of the
%paper. The references can be numbered in alphabetic order or in
%order of appearance in the document. When referring to them in
%the text, type the corresponding reference number in square
%brackets as shown at the end of this sentence \cite{C2}. An
%additional final page (the fifth page, in most cases) is
%allowed, but must contain only references to the prior
%literature.

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{strings,refs}

\end{document}
