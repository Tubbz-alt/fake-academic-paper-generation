
%% bare_jrnl.tex
%% V1.4a
%% 2014/09/17
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8a or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_conf_compsoc.tex,
%%                    bare_jrnl_compsoc.tex, bare_jrnl_transmag.tex
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices and paper sizes can       ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[onecolumn,11pt,draftcls,journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  %\graphicspath{{../../}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/tex-archive/info/epslatex/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Do not use the stfloats baselinefloat ability as IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/dblfloatfix/




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/endfloat/
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/url/
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )
\usepackage{algorithmic}
%\usepackage{caption}
\usepackage{url}
\usepackage[export]{adjustbox}
\usepackage{amsmath}
\usepackage{rotating}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{multirow}
\usepackage{array}
% Rex's marcos
\def \dibp{the document image binarization problem\,}
\newcommand{\im}[1]{{\bf{#1}}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\def\fwI{.25}
\def\fwIb{.16}
\def\fwII{.2}
\def\fwIII{.45}
\def\fwIV{.6}
\def\fwV{.22}
\def\fhI{4.2cm}
\def\fhIb{2.2cm}
\def\sp{0.1cm}
\hyphenation{op-tical net-works semi-conduc-tor}
\def\citeDIBCO{\cite{DIBCO2009,DIBCO2010,DIBCO2011,DIBCO2012,DIBCO2013,DIBCO2014}\,}
\def\etal{{\textit{et al.}}}



\begin{document}


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}



%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Learning Document Image Binarization from Data}
\author{ {Yue Wu, Stephen Rawls, Wael AbdAlmageed and Premkumar Natarajan}
\thanks{ Authors are all associated with Information Sciences Institute, 
University of South California, Marina Del Ray, California 90230. Email: \{yue\_wu, srawls, wamageed, pnataraj\}@isi.edu.}}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\author{Michael~Shell,~\IEEEmembership{Member,~IEEE,}
%        John~Doe,~\IEEEmembership{Fellow,~OSA,}
%        and~Jane~Doe,~\IEEEmembership{Life~Fellow,~IEEE}% <-this % stops a space
%\thanks{M. Shell is with the Department
%of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
%GA, 30332 USA e-mail: (see http://www.michaelshell.org/contact.html).}% <-this % stops a space
%\thanks{J. Doe and J. Doe are with Anonymous University.}% <-this % stops a space
%\thanks{Manuscript received April 19, 2005; revised September 17, 2014.}}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~13, No.~9, September~2014}%
%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2014 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\setlength{\intextsep}{3pt} % Vertical space above & below [h] floats
\setlength{\textfloatsep}{3pt}
\setlength{\abovecaptionskip}{3pt}
\setlength{\belowcaptionskip}{1pt}
\setlength{\dbltextfloatsep}{1pt}

\begin{abstract}
%\boldmath
In this paper we present a fully trainable binarization solution for degraded document images. Unlike previous attempts that often used simple features with a series of pre- and post-processing, our solution encodes all heuristics about whether or not a pixel is foreground text into a high-dimensional feature vector and learns a more complicated decision function. In particular, we prepare features of three types: 1) existing features for binarization such as \textit{intensity}  \cite{Otsu1975}, \textit{contrast} \cite{Su2009,Su2013}, and \textit{Laplacian} \cite{Howe2011,Howe2013}; 2) reformulated features from existing binarization decision functions such those in \cite{Niblack1986} and \cite{Sauvola2000}; and 3) our newly developed features, namely the Logarithm Intensity Percentile (LIP) and the Relative Darkness Index (RDI). Our initial experimental results show that using only selected samples (about 1.5\% of all available training data), we can achieve a binarization performance comparable to those fine-tuned (typically by hand), state-of-the-art methods. Additionally, the trained document binarization classifier shows good generalization capabilities on out-of-domain data.
\end{abstract}

\IEEEpeerreviewmaketitle
\section{Introduction}
% * <janice@isi.edu> 2015-01-27T19:34:32.877Z:
%
% 
%
% * <janice@isi.edu> 2015-01-27T19:34:31.773Z:
%
% 
%
% no \IEEEPARstart
As one of the most fundamental preprocessing methods in various document analysis work \cite{DIBCO2009,DIBCO2010,DIBCO2011,DIBCO2012,DIBCO2013,DIBCO2014, Su2013, Combined2014, Howe2013,6065466}, document binarization aims to convert a color or grayscale document image into a monotonic image, where all text pixels of interest are marked in black with a white background. Mathematically, given a document image $\im{D}\!\!=\!\!\{D_{i,j}\}_{i\in[1,W], j\in[1,H]}$ of size $W\!\!\times\!\!H$, image binarization assigns each pixel $D_{i,j}$ a binary class label $B_{i,j}$ according to a decision function $f_{\rm{binarize}}(\cdot)$ in a meaningful way, namely
\begin{equation}
B_{i,j}=\left\{
\begin{array}{r@{}cl}
{\rm{foreground\,class\,}} 1 ,&&{\rm{if}}f_{\rm{binarize}}(D_{i,j}) < 0\\
{ \rm{background\,class\,}} 0 ,&&{\rm{else}} 
\end{array}
\right.
\label{eq.bin}
\end{equation}
A successful document binarization process discards irrelevant and noisy information while preserving meaningful information in the binary image $\im{B}\!\!=\!\!\{B_{i,j}\}$. This process reduces the space to represent a document image, and largely simplifies the complexity of advanced document analysis tasks \cite{Howe2011}.

Although human do not often face many difficulties in identifying texts even on some low-quality document images,  \dibp is indeed subjective and ill-posed \cite{Howe2011}, and it involves many different challenges and combinations of challenges. For example, several of the well-known ones are 1) how to handle document degradations like ink blob, fade text etc.; 2) how to deal with uneven lighting; and 3) how to differentiate bleed-through text from normal text. In such difficult scenarios, human actually uses high-level knowledge that might not be easily captured by low-level features--such as a script character set and background texture analysis--to help decide which pixel is foreground text. 

Classic solutions more or less seek heuristic thresholds in simple feature spaces. 
This can be further grouped into the so-called {\textit{global thresholding}} and {\textit{local thresholding}} methods \cite{Combined2014} according to whether this threshold is location independent or not.
For example, Otsu's method \cite{Otsu1975} binarizes a pixel $D_{i,j}$ by comparing its \textit{pixel intensity} $I_{i,j}$ to an optimal global threshold $G_{\rm{th}}$ derived from  \textit{intensity histogram} \cite{Otsu1975} as shown in \eqref{eq.otsu}
\begin{equation}
f_{\rm{Otsu}}(i,j)=I_{i,j} - G_{\rm{th}}
\label{eq.otsu}
\end{equation}
In contrast Niblack's method \cite{Niblack1986} uses the decision function \eqref{eq.niblack}
\begin{equation}
f_{\rm{Niblack}}(i,j)= I_{i,j}-\mu^R_{i,j} +k_{\rm{Niblack}}\sigma^R_{i,j} 
\label{eq.niblack}
\end{equation}
where $k_{\rm{niblack}}$ is a parameter below 0, and $\mu^R_{i,j}$ and $\sigma^R_{i,j}$ denote the mean and standard deviation of pixel intensities within a region $R$ of size $w\!\!\times\!\!h$. 
%Specifically, they are defined as shown in Eqs. \eqref{eq.localmean} and \eqref{eq.localdev}, respectively. 
%\begin{equation}
%\textstyle\mu^R_{i,j} = \sum_{p=-w/2}^{w/2}\sum_{q=-h/2}^{h/2}{^{I_{i+p,j+q}}/_{wh}}
%\label{eq.localmean}
%\end{equation}
%\begin{equation}
%{\sigma_{i,j}^{R}}^2 = \textstyle\sum_{p=-w/2}^{w/2}\sum_{q=-h/2}^{h/2}{^{I_{i+p,j+q}^2}/_{ wh}} -{\mu^R_{i,j}}^2
%\label{eq.localdev}
%\end{equation}
Although heuristic solutions are very efficient--may only requiring a constant number of operations per pixel, and work fairly well on many well-conditioned document images, it is clear that simple features and decision functions are insufficient for handling difficult cases. 

To achieve robust document binarization, many efforts are being made in the areas of 1) image normalization/adaptation, 2) discriminative feature space, and 3) more complicated decision functions. For example, Lu \etal \cite{Lu2010} proposes a local thresholding approach that mainly relies on background estimation and stroke estimation. Su \etal \cite{Su2009,Su2013} finds that Otsu's thresholding helps attain more discriminative power in a local contrast feature space. Sauvola \etal \cite{Sauvola1997,Sauvola2000} adds the parameter $S_{\rm{Sauvola}}$ to allow a non-linear decision plane \eqref{eq.sauvola}.
\begin{equation}
f_{\rm{Sauvola}}(i,j)\!\!=\!\! I_{i,j}-\mu^R_{i,j}\!-\!k_{\rm{Sauvola}}\mu^R_{i,j}(^{\sigma^R_{i,j}}/_{S_{\rm{Sauvola}}} \!\!-\!\!1 )  
\label{eq.sauvola}
\end{equation}
Although many of these attempts work well when method assumptions are satisfied and method parameters are appropriate, adapting a heuristic binarization method to a new domain is often not easy. Indeed, Lazzara et al. \cite{Sauvola2014} show that the original Sauovla method might fail even for well-scanned document images because of text fonts of different sizes. 

Unsupervised learning recently dominates document binarization area. In \cite{Su2012}, a document image is first clustered into three classes, namely foreground, background and uncertain, and pixels in the uncertain class will be further classified into either the foreground or background class according to their distances from these two classes. In \cite{Howe2011,Howe2013}, an image is first transformed into a Laplacian feature space, and a global energy function is constructed to  
ensure that resulting binary labels are optimal in the sense of a predefined Markov random field. In \cite{EoE2012}, an unsupervised ensemble of expert frameworks is used to combine multiple binarization candidates. Although these methods do not require a training stage, some rely on theoretical models or heuristic rules whose assumptions may not be necessarily satisfied, some require expensive iterative tuning and optimizations, and thus no surprise to see they are not reliable for certain types of degradations \cite{Phase2014}. 

Although image binarization is clearly a classification problem, supervised learning-based binarization solutions are still rare in the community. In this letter we discuss our initial attempts to solve the \dibp using supervised learning. The remainder of our paper is organized as follows: Section II overviews our solution and discusses all used features. Section III provides implementation details related to training and testing. Section IV shows our experimental results, and Section V concludes this paper.  

\section{Feature Engineering}
Our goal is to develop a generic solution without preset parameters and pre- or post-processing. Specifically, we are interested in learning a decision function $f_{\rm{ours}}(\cdot)$ that maps a $n$d feature vector $\vec{X}_{i,j}$ extracted around a pixel $D_{i,j}$ to a binary space $\{0,1\}$ in a meaningful way, i.e.
\begin{equation}
B_{i,j}=f_{\rm{ours}}(\vec{X}_{i,j})
\label{eq.binours}
\end{equation}
Detailed feature engineering discussions are given below.

\subsubsection{Existing Features}
Since a number of simple tasks can be accomplished just by applying Otsu's method. We thus include a pixel intensity $I_{i,j}$ and its deviation from the Otsu's threshold as features below
\begin{equation}
X_{i,j}^{\rm{Local int.}} = I_{i,j}
\end{equation}
\begin{equation}
X_{i,j}^{\rm{Otsu\,diff.}} = I_{i,j} - G_{\rm{th}}
\end{equation}
In addition, we also use local statistics of Eqs. \eqref{eq.localmean} and \eqref{eq.localdev}, but with respect to different scales, i.e.,
\begin{equation}\label{eq.localmean}
X_{i,j}^{\rm{Local\,avg.}|R}\!=\!\mu_{i,j}^{R}\!=\!\sum_{p\!=\!-^{w}\!/\!_{2}}^{^{w}\!/\!_{2}}\sum_{q\!=\!-^{h}\!/\!_{2}}^{^{h}\!/\!_{2}}{^{I_{i+p,j+q}}/_{wh}}
\end{equation}
\begin{equation}\label{eq.localdev}
X_{i,j}^{\rm{Local\,std.}|R} \!\!=\!\!\sigma_{i,j}^{R}\!=\!\sum_{p\!=\!-^{w}\!/\!_{2}}^{^{w}/_{2}}\sum_{q\!=\!-^{h}\!/\!_{2}}^{^{h}\!/\!_{2}}{^{I_{i+p,j+q}^2}/_{ wh}} \!-\!{\mu^R_{i,j}}^2
\end{equation}
where we make the size $w\!\!=\!\!h\!\!=\!\!ks$  of local window $R$ be associated with scales $k\in[1,2,4,8]$, and estimate stroke width $s$ using Su's method \cite{Su2013}. Inspired by the success of the Su \cite{Su2009,Su2013} and Howe methods \cite{Howe2011,Howe2013}, we include their contrast and Laplacian features shown in \eqref{eq.su} and \eqref{eq.howe}. %Fig. \ref{fig.su} shows sample results of these features.
\begin{equation}
X_{i,j}^{{\rm{Su}}|R}\!\!=\!\!{{\displaystyle\argmax_{p,q\in R}\{I_{i+p,j+q}\}-\displaystyle\argmin_{p,q\in R}\{I_{i+p,j+q}\}}\over{{\displaystyle\argmax_{p,q\in R}\{I_{i+p,j+q}\}\!+\!\displaystyle\argmin_{p,q\in R}\{I_{i+p,j+q}\}}}\!+\!\epsilon_{\rm{su}}}
\label{eq.su}
\end{equation}
\begin{equation}
X_{i,j}^{{\rm{Howe}}|R}=\nabla^2\mu_{i,j}^{R}
\label{eq.howe}
\end{equation}

\subsubsection{Exponential Truncated Niblack Index}
To include Niblack's decision function in our considerations, we first rearrange terms in \eqref{eq.niblack} according to $f_{\rm niblack}\!\!=\!\!0$, as shown below
\begin{equation}
k_{\rm{Niblack}}(i,j|R)= (I_{i,j}-\mu_{i,j}^R)/\sigma_{i,j}^{R}
\end{equation} 
and then compute a so-called Exponential Truncated Niblack Index (ETNI) feature as follows.
\begin{equation}
X_{i,j}^{{\rm{ETNI}}|R} =\left\{
\begin{array}{rl}
\exp\{k_{\rm{Niblack}}(i,j|R)\},&{\rm if} I_{i,j}\leq\mu_{i,j}^R\\
1,&\rm otherwise
\end{array}
\right.
\label{eq.TNI}
\end{equation}
%In this way, we successfully map the original $k_{\rm{Niblack}}$ whose range is $(-\infty,\infty)$ to the range of $(0,1]$, with the hope of learning a more meaningful relative local threshold $k_{\rm{Niblack}}$. 
Fig. \ref{fig.etni} compares an image in the original form and its corresponding ETRI feature space. %Note $X_{i,j}^{{\rm{ETNI}}|R}$ is now in the range of $(0,1]$.

\begin{figure}[!h]
\centering\scriptsize
\includegraphics[width=\fwI\linewidth,frame]{2010-HW04}{(a)}
\includegraphics[width=\fwI\linewidth,frame]{etni}{(b)}
\caption{ETRI features for image DIBCO2010\_HW04. (a) Original image; (b) ETNI feature for $R$ of size $64\times 64$. }\label{fig.etni}
\end{figure}

\subsubsection{ Logistic Truncated Sauvola Index}
Similarly, we rearrange terms in Sauvola's decision function \eqref{eq.sauvola} according to $f_{\textrm{Sauvola}}=0$ for its key parameter $k_{\rm{Sauvola}}$ as follows,
\begin{equation}
k_{\rm{Sauvola}}(i,j|R) \propto \displaystyle{I_{i,j}/\mu_{i,j}^R-1\over \sigma_{i,j}^{R}-S_{\rm{Sauvola}}}
\end{equation} 
Since $k_{\rm{Sauvola}}(i,j|R)$ could be $(-\infty,\infty)$, we normalize this index by using the logistic function shown in Eq. \eqref{eq.TSI}, and call it the Logistic Truncated Sauvola Index (LTSI),
\begin{equation}
X_{i,j}^{{\rm{LTSI}}|\!R} \!\!\!\!=\!\!
\left\{\!\!\!\!
\begin{array}{c@{}l}
0&,{\rm if} \sigma_{i,j}^{R}\!\!>\!\!S_{\rm{Sauvola}}\\
( \!1\!\!+\!\!\exp\!\{\!-\!\!k_{\rm{Sauvola}}(\!i,\!j|R)\}\!)\!^{\tiny{-\!1}}&,\rm otherwise
\end{array}
\right.
\label{eq.TSI}
\end{equation}
where the range of $X_{i,j}^{{\rm{TSI}}|R}$ is $[0\!,\!1]$, and the condition $\sigma_{i,j}^{R}\!<\!S_{\rm{Sauvola}}$ ensures the sign consistency of $k_{\rm{Sauvola}}(i,j|R)$. LTSI thus reflects the Sauvola decision surface. A sample result of the LTSI feature is given in Fig. \ref{fig.ltsi}.

\begin{figure}[!h]
\centering\scriptsize
\includegraphics[width=\fwI\linewidth,frame]{2011-PR5}{(a)}
\includegraphics[width=\fwI\linewidth,frame]{ltsi}{(b)}
\caption{LTSI features for image DIBCO2011\_PR05. (a) Original image; (b) LTSI feature for $R$ of size $8\times 8$. }\label{fig.ltsi}
\end{figure}

\subsubsection{Logarithm Intensity Percentile Features}
Intuitively, the darkness of a pixel is related to whether it is a text pixel. Given a region $S$, the percentile of the pixel's intensity can be computed as
\begin{equation}{\rm perc}(i,j|S) = \sum_{p,q\in S} {\im{1}_{[0,\infty)}(I_{i,j}- I_{i+p,j+q})\over \|S\|}
\label{eq.perc}
\end{equation}
where $\im{1}_{[0,\!\infty)}(t)$ denotes the indicator function whose value is 1 when $t\!\in\![0\!,\!\infty)$ and 0 otherwise, and $\|\!\cdot\!\|$ denotes the cardinality function. It is clear that this percentile is a type of rank feature, and thus is invariant to any monotonic transform on the original intensity space. To give a higher resolution for lower percentiles, we use the logarithm version of \eqref{eq.perc} as shown in \eqref{eq.LIP}, and call it Logarithm Intensity Percentile (LIP) feature. Here $\rm{Th}_{perc}$ is a threshold ( =.01 in this paper). 
\begin{equation}
X_{i,j}^{{{\rm{LIP}}|S}} =\left\{
\begin{array}{l}
\!\!1.0, {\rm if}\,{\rm perc}(i,j|S)\leq\rm{{Th}_{perc}}\\
\!\!\displaystyle\log_{\rm{{Th}_{perc}}}({\rm perc}(i,j|S) ), {\rm otherwise}
\end{array}
\right.
\label{eq.LIP}
\end{equation}

With regard to $S$, we make parallelogram $S$ cover multiple rows, columns, diagonals, and inverse diagonals. The number of rows, columns, diagonals and inverse diagonals in $S$ is made to be $k$ times the estimated stroke width $s$. Finally, we also compute the LIP features with respect to the entire image and the maximum percentile among all previously extracted LIP features. Fig. \ref{fig.lip} shows the original document with its corresponding features in the LIP spaces. As one can see, the LIP space indeed provides more discriminative powers. 
\begin{figure}[!h]
\centering\scriptsize
\begin{tabular}{@{}c@{}m{.05cm}@{}c@{}m{.05cm}@{}c@{}m{.05cm}@{}c@{}m{.05cm}@{}c@{}m{.05cm}@{}c@{}}
\includegraphics[width=\fwIb\linewidth,frame]{2011-HW1}&&
\includegraphics[width=\fwIb\linewidth,frame]{perc_global_a}&&
\includegraphics[width=\fwIb\linewidth,frame]{perc_row_a}&&
\includegraphics[width=\fwIb\linewidth,frame]{perc_col_a}&&
\includegraphics[width=\fwIb\linewidth,frame]{perc_diag_a}&&
\includegraphics[width=\fwIb\linewidth,frame]{perc_max_a}\\{(a)}&&
{(b)}&&
{(c)}&&
{(d)}&&
{(e)}&&
{(f)}\\
\end{tabular}
\caption{LIP features for image DIBCO2011\_HW1. (a) original  image; (b) global LIP; (c)-(e) LIP along row, column, and diagonal; and (f) max LIP of all directions.}\label{fig.lip}
\end{figure}

\subsubsection{Relative Darkness Index Features}
Inspired by the great success of local ternary patterns(LTP) \cite{LTP2010} in face recognition, we borrow their essences here. LTP relies on the comparison of a center pixel's intensity with each pixel in a set of neighbors $\{N_1, \cdots, N_k\}$ that are on a radius $r$ circle, and the $l$th code in a length-$k$ code string is defined as
\begin{equation}{\rm{ltp}}( P_{i,j}, l ) = \left\{
\begin{array}{rcl}
+1, &{\rm if}& I_{i +r_l, j+c_l} \geq I_{i,j} + \rm{tol}\\
-1, &{\rm if}& I_{i +r_l, j+c_l} \leq I_{i,j}-\rm{tol}\\
0, &{\rm if }& | I_{i +r_l, j+c_l} - I_{i,j}|<\rm{tol}\\
\end{array}
\right.
\end{equation}
where $r_l$ and $c_l$ denote the relative coordinates of a neighbor $N_l$ w.r.t. a center pixel, and $\rm{tol}$ is a preset tolerance. However, the number of possible LTP codes is often huge to effectively encode. Though one may reduce this number by considering all shift-equivalent codes as one, or separating a ternary code into two binary codes, we find that the simple frequency count of each code in a code string has already revealed many intrinsic properties of pixels, and we call them the Relative Darkness Index (RDI) features. 
Precisely, given the code ${\cal{C}}$ and neighbors on a radius $r$ circle, the RDI feature can be defined as below
\begin{equation}
X_{i,j}^{{\rm{RDI}}| {\cal{C}}, r } = \sum_{l=1}^{k}{{\im{1}}_{0}({\rm{ltp}}(P_{i,j}, l)-{{\cal{C}}})\over k}.
\label{fig.rdi}
\end{equation}
As one can see from Fig.\ref{fig.rdi}(c-e), most of the nearly homogeneous background parts are of high code 0 indices; pixels close to strong edges are dominated by code+1 indices, and foreground text pixels have high response on code-1 indices. To further enhance RDI's discriminative power, we compute the ratios of one code to the sum of itself and another code as well (see Fig.\ref{fig.rdi}(f-h)).
\begin{figure}[!ht]
\centering\scriptsize
\includegraphics[width=\fwII\linewidth,frame]{original}{(a)}
\includegraphics[width=\fwII\linewidth,frame]{original_2d}{(a)}
\includegraphics[width=\fwII\linewidth,frame]{rdi_c0}{(c)}
\includegraphics[width=\fwII\linewidth,frame]{rdi_c+1}{(d)}
\includegraphics[width=\fwII\linewidth,frame]{rdi_c-1}{(e)}
\includegraphics[width=\fwII\linewidth,frame]{rdi_c0+1}{(f)}
\includegraphics[width=\fwII\linewidth,frame]{rdi_c+1-1}{(g)}
\includegraphics[width=\fwII\linewidth,frame]{rdi_c-10}{(h)}
\caption{RDI features for image DIBCO2013\_PR05 (darker pixels indicate a value close to 0). (a) original color image; (b) original image; (c)-(e) RDI feature $X^{{\rm{RDI}}| {\cal{C}}, 8 }$for ${\cal{C}}\in\{0\!,\!-1\!,\!+1\}$, respectively; (f) ${ X^{{\rm{RDI}}| {\cal{C}}\!=\!+\!1, 8 }\over X^{{\rm{RDI}}| {\cal{C}}\!\in\!\{0,+1\}, 8 }}$ ; (g)${ X^{{\rm{RDI}}| {\cal{C}}\!=\!-\!1, 8 }\over X^{{\rm{RDI}}| {\cal{C}}\!\in\!\{-1,+1\}, 8 }}$ ; and (h) ${ X^{{\rm{RDI}}| {\cal{C}}\!=\!+\!0, 8 }\over X^{{\rm{RDI}}| {\cal{C}}\!\in\!\{-1,0\}, 8 }}$ .}
\label{fig.rdi}
\end{figure}
\subsubsection{Other Features}
Besides of features discussed above, we extract features from the global image statistics, including the mean and standard deviation of the entire image intensities, the mean and standard deviation of the percentile image, the 32 bins of normalized histogram (sum to 1) for image intensities, and the 32 bins of a normalized logarithmed histogram. 

\section{Training and Testing Settings}
\setcounter{subsubsection}{0}
In experiments, we use the widely accepted Document Image Binarization Contest (DIBCO) from 2009 to 2014 \citeDIBCO as our training and testing data; it totals 76 images. We adopt the leave-one-out strategy where we first pick a DIBCO image set of a particular year as our testing set, and use the rest as our training set.  

\subsubsection{Feature Summary}
We summarize all used features with dimensions and corresponding normalization considerations in Table \ref{tab.feat}. Here, the stroke width $s$ can be estimated via various methods; we use Su's method \cite{Su2013}. `Scale' indicates the side of local square region $R$. 
\begin{table}[!h]
\centering
\scriptsize
\caption{Used Features }\label{tab.feat}
\begin{tabular}{rrrr}
\hline
\bf{Type}&\bf{Scale}&\bf{Dimension}&\bf{Normalization}\\\hline
Local int.&N/a&1&divide by 255\\
Otsu diff.&N/a&1&divide by 255\\
Local avg./std.&1$s$,2$s$,4$s$,8$s$&4/4&divide by 255\\
Su/Howe&1,1$s$,2$s$,4$s$&4/4&MinMax\\
ETRI/LTSI&1$s$,2$s$,4$s$,8$s$&4/4&N/a\\
LIP&1,1$s$,2$s$,4$s$,8$s$&1+4$\times$4+1&N/a\\
RDI&1,1$s$,2$s$,4$s$,8$s$&5$\times$6&N/a\\
Global int. avg./std.&N/a&1/1&divide by 255\\
Global perc. avg./std.&N/a&1/1&N/a\\
Global int./perc. loghist. &N/a&32/32&N/a\\\hline
Total& & 142 &\\
\hline
\end{tabular}
\end{table}


\subsubsection{Sampling Strategy}
Selecting training samples is essential in task. First, one may not be handle a big training set of this task. These 76 images totally contain more than 80 million pixels. Assuming each feature is store in {\sf{float32}} format, we need $80\!\!\times \!\!142\!\!\times\!\!4$MB ($\gg$256GB) memory for just training features, while this requirement clearly beyond the capacities of most computers nowadays. Second, one may notice the imbalanced training data. We know both the background nontext class and foreground text class in the binarization problem actually cover different subclasses \cite{Su2012,ramirez2010transition}, while we also know nearly homogeneous background and foreground dominate our training data. 

To solve both problems, we first artificially classify all pixels in an image into 16 subclasses, each is represented as a 4-bit string, where each bit indicates whether or not this pixel should be treated as a pixel in Otsu's foreground, in Niblack's foreground, within $s$ pixels away from reference image edges, and in a reference annotated foreground. We draw the same number of random samples for each subclass. Fig. \ref{fig.sampling} illustrates the samples we extracted that balanced both foreground and background subclasses. 
\begin{figure}[!h]
\centering
\scriptsize
\includegraphics[width=.33\linewidth,frame]{coded_image0}{(a)}
\includegraphics[width=.33\linewidth,frame]{sampling}{(b)}
\caption{Sampling strategy. (a) pixels with subclass labels for image DIBCO2012-HW02 (each color denotes a subclass); (b) samples extracted from DIBCO2012-HW02 image balanced subclasses (red/green dots indicate background/foreground.)}
\label{fig.sampling}
\end{figure}

\subsubsection{Training and Testing Strategies}
In all of the following experiments, we perform a two-pass training. We first extract 9,600 samples (subclass balanced) from each training image and train a simple classifier, say Gaussian Naive Bayes. We use this classifier to decode all training images, and extract additional 9,600 erroneous samples (subclass balanced) from each image, and use  all extracted samples to train an more complicated {\textsl{sklearn}} \cite{sklearn}{\textit{ ExtraTrees}} classifier \cite{geurts2006extremely}. Note in total we extract 19,200 samples per image, which only account for roughly about 1.5\% of all samples. Classifier parameters are obtained from a 10-folded cross-validation using all samples. A final classifier is trained by using all extracted samples and validated parameters. Fig. \ref{fig.importance} plots the feature importance of each feature type in terms of the overall contribution and the averaged dimensional contribution with respect to each feature type. As one can see, RDI, Global int. hist. and LIP are the three most useful feature categories in terms of overall contributions; and Su, LTSI and RDI are the three best features in terms of dimensional contributions. 

In testing, we use the final classifier to predict the class label for all pixels in a testing image. Depending on the size of an image, the decoding time may vary between 5s to 30s.

\begin{figure}[!h]
\centering
\scriptsize
\includegraphics[width=0.495\linewidth,trim= 4.5cm 0.5cm 1.5cm 2cm,clip=true]{feature_importance2}
\includegraphics[width=0.495\linewidth,trim= 4.5cm 0.5cm 1.5cm 2cm,clip=true]{feature_importance}
\caption{Feature importance. Left: overall importance of each feature type; and Right: dimensional feature importance for each feature type. }
\label{fig.importance}
\end{figure}
\section{Experimental Results}\setcounter{subsubsection}{0}
\subsubsection{Performance on DIBCO Datasets}
Table II lists performance of our proposed supervised binarization solution over the DIBCO 2012 \cite{DIBCO2012}, 2013 \cite{DIBCO2013}, and 2014 \cite{DIBCO2014} datasets using standard metrics F1-score, peak signal-to-noise ratio (PSNR), and distance reciprocal distortion (DRD) ( metric definitions can be found in  \citeDIBCO). As we can see, our performance is comparable to the top five methods. We also notice that our binarization classifier's performance is very stable among all three datasets, especially since it always keeps a DRD score below 3 pixels. Sample decoding results are compared to the top two contest methods in Fig. \ref{fig.2014}. As one can see, our supervised solution successfully learnt knowledge to handle difficult cases: 1) faded text; and 2) text on a dirty background.  
\begin{table}[!h]
\centering\scriptsize
\caption{ Performance Evaluations On DIBCO Datasets}
\begin{tabular}{l|lrrrr}
\hline
%\multicolumn{5}{c}{\bf{DIBCO 2012}}\\
&\bf{Method} & \bf{Contest Rank} & \bf{F1}\% & \bf{PSNR} & \bf{DRD} \\\hline
\multirow{6}{*}{\begin{sideways}{\bf{DIBCO2012}}\end{sideways}}&\cite{Howe2011} & 1 & 89.47 & 21.80 & 3.400\\
&Lelore \etal's \cite{DIBCO2012}   & 2 & 92.85 & 20.57 & 2.660\\
&\cite{Su2009}  & 3 & 91.54 & 20.14 & 3.048\\
&Nina's \cite{DIBCO2012}    & 4 & 90.38 & 19.30 & 3.348\\
&Yazid \etal's  \cite{DIBCO2012} & 5 & 91.85 & 19.65 & 3.056\\\cline{2-6}
&Ours   &   & 92.01 & 19.92 & 2.601\\
\hline
\multirow{6}{*}{\begin{sideways}{\bf{DIBCO-2013}}\end{sideways}}&Su \etal 's method \cite{DIBCO2013} & 1 & 92.12 & 20.68 & 3.100 \\
&\cite{Howe2013}  & 2 & 92.70 & 21.29 & 3.180 \\
&\cite{EoE2012}  & 3 & 91.81 & 20.68 & 4.020 \\
&\cite{6482566} & 4 & 91.69 & 20.54 & 3.590 \\
&\cite{ramirez2010transition}   & 5 & 90.92 & 19.32 & 3.910 \\\cline{2-6}
&Ours   &   & 91.40 & 20.13 & 2.637\\
\hline
\multirow{6}{*}{\begin{sideways}{\bf{DIBCO-2014}}\end{sideways}}&Mesquita \etal 's \cite{DIBCO2014} & 1 & 96.88 & 22.66 & 0.902 \\
&\cite{Howe2013}  & 2 & 96.63 & 22.40 & 1.001 \\
&\cite{nafchi2013historical}  & 3 & 93.35 & 19.45 & 2.194 \\
&Ziaratban \etal 's \cite{DIBCO2014}   & 4 & 89.24 & 18.94 & 4.502 \\
&Mitianoudis \etal 's \cite{DIBCO2014}      & 5 & 89.77 & 18.49 & 4.502 \\\cline{2-6}
&Ours   &   & 92.69 & 19.47 & 2.571\\
\hline
\end{tabular}
\end{table}

\begin{figure}[!h]
\centering\scriptsize
\begin{tabular}{@{}c@{}m{.05cm}@{}c@{}}{DIBCO2012\_H07}&&{DIBCO2014\_HW07}\\
\includegraphics[height=\fhIb,frame]{H07}&&\includegraphics[height=\fhIb,frame]{DIBCO2014_HW07}{(a)}\\
\includegraphics[height=\fhIb,frame]{H07_GTH}&&\includegraphics[height=\fhIb,frame]{DIBCO2014_HW07_GTH}{(b)}\\
\includegraphics[height=\fhIb,frame]{11_H07}&&\includegraphics[height=\fhIb,frame]{H07_P02}{(c)}\\
\includegraphics[height=\fhIb,frame]{4a_H07}&&\includegraphics[height=\fhIb,frame]{H07_P06}{(d)}\\
\includegraphics[height=\fhIb,frame]{DIBCO2012_HW07_dec}&&\includegraphics[height=\fhIb,frame]{DIBCO2014_HW07_dec}{(e)}
\end{tabular}
\caption{Binarization results for image. (a) original images; (b) reference binarized images (highlighted red regions indicate disagreements); (c) results of contest rank 1; (d) results of contest rank 2; and (e) our results.}
\label{fig.2014}
\end{figure}

\subsubsection{Learning Curve}
As we mentioned previously, only about 1.5\% of all available training samples are used in our experiments. We investigate the relationship between the amount of training samples and the binarization performance using the test set of DIBCO 2012 in Table \ref{tab.sample}.  As in many pattern recognition problems, the improvement of binarization performance gets smaller as the number of samples increases. 

\begin{table}[!h]
\centering\scriptsize
\caption{ Performance v.s. Training Samples }
\label{tab.sample}
\begin{tabular}{r|rrrrrrr}
\hline
\bf{\#Samples} &\bf{1,920} &\bf{5,760} &\bf{9,600} &\bf{13,440} &\bf{15,360} &\bf{17,280} &\bf{19,200} \\\hline
 \bf{F1}\% & 91.47 & 91.77 & 91.90 & 91.93 & 91.95 & 92.01 & 92.01 \\
 \bf{PSNR} & 19.64 & 19.81 & 19.86 & 19.86 & 19.88 & 19.93 & 19.92 \\
 \bf{DRD} & 2.797 & 2.689 & 2.637 & 2.634 & 2.618 & 2.599 & 2.601 \\
 \hline
\end{tabular}
\end{table}

\subsubsection{Document Binarization in the Wild }
Although images in DIBCO datasets have already covered a wide range of variations, there are clearly more variations and combinations of variations that are not included in DIBCO training data. We therefore test our learned classifier on out-of-domain document images, and we observe satisfactory results (see Fig. \ref{fig.odd}).

\begin{figure}[!h]
\centering
\begin{tabular}{c@{}c@{}c@{}}
\includegraphics[height=\fhI,frame]{Nikon}&\includegraphics[height=\fhI,frame]{tumblr}&\includegraphics[height=\fhI,frame]{chinese}\\
\includegraphics[height=\fhI,frame]{Nikon_dec}&\includegraphics[height=\fhI,frame]{tumblr_dec}&\includegraphics[height=\fhI,frame]{chinese_dec}\\
\end{tabular}
\caption{Binarization results of out-of-domain data}
\label{fig.odd}
\end{figure}
%
%\subsubsection{Failure Cases}
%During our tests on the out-of-domain data, we also observed some cases of failure. A typical case is given in Fig. \ref{fig.fail}.  As one can expect, direct binarization is inadequate for some types of out-of-domain data. However, we notice that if we predict the probability that a pixel is of the foreground text class, the resulting probability map still provides meaningful information, as shown in Fig. \ref{fig.fail}(c). As a result, one can obtain reasonably good binarization performance if an appropriate threshold is applied to the probability map, e.g. Fig. \ref{fig.fail}(d). 
%
%\begin{figure}
%\centering
%\scriptsize
%\begin{tabular}{@{}c@{}c@{}c@{}@{}c@{}}
%\includegraphics[width=\fwV\linewidth,frame]{News}{(a)}&\includegraphics[width=\fwV\linewidth,frame]{News_dec127}{(b)}&\includegraphics[width=\fwV\linewidth,frame]{News_proba}{(c)}&\includegraphics[width=\fwV\linewidth,frame]{News_dec204}{(d)}\\
%\end{tabular}
%\caption{Failure case discussion. (a) original image; (b) our raw binarized image ( $Th_{\rm pr}\!=\!.5$); (c) probability map (a darker pixel indicates a higher probability of foreground text); and (e) our re-binarized image using $Th_{\rm pr}\!=\!.2$. }
%\label{fig.fail}
%\end{figure}

\section{Conclusion}
In this paper we investigate the document binarization solution via supervised learning. Unlike previous efforts, this solution is parameter-free and fully trainable. Our experimental results showed that one can learn a reasonably well binarization decision function from a small set of carefully selected training data. Such a learned decision function not only works well for in-domain data, but can also apply to out-of-domain data.
In future work, we will explore several interesting aspects such as discriminative features (e.g., image moments and connected component attributes) and classifier adaptation on the fly.

\ifCLASSOPTIONcaptionsoff
  \newpage
\fi
\newpage
\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
\IEEEtriggercmd{\enlargethispage{-5in}}
% Generated by IEEEtran.bst, version: 1.13 (2008/09/30)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{Otsu1975}
N.~Otsu, ``A threshold selection method from gray-level histograms,''
  \emph{Automatica}, vol.~11, no. 285-296, pp. 23--27, 1975.

\bibitem{Su2009}
B.~Su, S.~Lu, and C.~L. Tan, ``Binarization of historical document images using
  the local maximum and minimum,'' in \emph{Proceedings of the 9th IAPR
  International Workshop on Document Analysis Systems}.\hskip 1em plus 0.5em
  minus 0.4em\relax ACM, 2010, pp. 159--166.

\bibitem{Su2013}
------, ``Robust document image binarization technique for degraded document
  images,'' \emph{Image Processing, IEEE Transactions on}, vol.~22, no.~4, pp.
  1408--1417, 2013.

\bibitem{Howe2011}
N.~R. Howe, ``A laplacian energy for document binarization,'' in \emph{Document
  Analysis and Recognition (ICDAR), 2011 International Conference on}.\hskip
  1em plus 0.5em minus 0.4em\relax IEEE, 2011, pp. 6--10.

\bibitem{Howe2013}
------, ``Document binarization with automatic parameter tuning,''
  \emph{International Journal on Document Analysis and Recognition (IJDAR)},
  vol.~16, no.~3, pp. 247--258, 2013.

\bibitem{Niblack1986}
W.~Niblack, \emph{An Introduction to Digital Image Processing}.\hskip 1em plus
  0.5em minus 0.4em\relax Prentice-Hall, 1986.

\bibitem{Sauvola2000}
J.~Sauvola and M.~Pietik{\"a}inen, ``Adaptive document image binarization,''
  \emph{Pattern recognition}, vol.~33, no.~2, pp. 225--236, 2000.

\bibitem{DIBCO2009}
B.~Gatos, K.~Ntirogiannis, and I.~Pratikakis, ``Icdar 2009 document image
  binarization contest (dibco 2009),'' in \emph{Document Analysis and
  Recognition (ICDAR), 2009 International Conference on}, vol.~9, 2009, pp.
  1375--1382.

\bibitem{DIBCO2010}
I.~Pratikakis, B.~Gatos, and K.~Ntirogiannis, ``H-dibco 2010-handwritten
  document image binarization competition,'' in \emph{Frontiers in Handwriting
  Recognition (ICFHR), 2010 International Conference on}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2010, pp. 727--732.

\bibitem{DIBCO2011}
------, ``Icdar 2011 document image binarization contest (dibco 2011),'' in
  \emph{Document Analysis and Recognition (ICDAR), 2011 International
  Conference on}, 2011, pp. 1506--1510.

\bibitem{DIBCO2012}
------, ``Icfhr 2012 competition on handwritten document image binarization
  (h-dibco 2012).'' \emph{ICFHR}, vol.~12, pp. 18--20, 2012.

\bibitem{DIBCO2013}
------, ``Icdar 2013 document image binarization contest (dibco 2013),'' in
  \emph{Document Analysis and Recognition (ICDAR), 2013 International
  Conference on}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2013, pp.
  1471--1476.

\bibitem{DIBCO2014}
K.~Ntirogiannis, B.~Gatos, and I.~Pratikakis, ``Icfhr2014 competition on
  handwritten document image binarization (h-dibco 2014),'' in \emph{2014 14th
  International conference on frontiers in handwriting recognition}, 2014, pp.
  809--813.

\bibitem{Combined2014}
------, ``A combined approach for the binarization of handwritten document
  images,'' \emph{Pattern Recognition Letters}, vol.~35, pp. 3--15, 2014.

\bibitem{6065466}
X.~Peng, H.~Cao, R.~Prasad, and P.~Natarajan, ``Text extraction from video
  using conditional random fields,'' in \emph{Document Analysis and Recognition
  (ICDAR), 2011 International Conference on}, Sept 2011, pp. 1029--1033.

\bibitem{Lu2010}
S.~Lu, B.~Su, and C.~L. Tan, ``Document image binarization using background
  estimation and stroke edges,'' \emph{International journal on document
  analysis and recognition}, pp. 1--12, 2010.

\bibitem{Sauvola1997}
J.~Sauvola, T.~Seppanen, S.~Haapakoski, and M.~Pietikainen, ``Adaptive document
  binarization,'' in \emph{Document Analysis and Recognition, 1997.,
  Proceedings of the Fourth International Conference on}, vol.~1.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 1997, pp. 147--152.

\bibitem{Sauvola2014}
G.~Lazzara and T.~G{\'e}raud, ``Efficient multiscale sauvola’s
  binarization,'' \emph{International Journal on Document Analysis and
  Recognition (IJDAR)}, vol.~17, no.~2, pp. 105--123, 2014.

\bibitem{Su2012}
B.~Su, S.~Lu, and C.~L. Tan, ``A learning framework for degraded document image
  binarization using markov random field,'' in \emph{Pattern Recognition
  (ICPR), 2012 21st International Conference on}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2012, pp. 3200--3203.

\bibitem{EoE2012}
R.~F. Moghaddam, F.~F. Moghaddam, and M.~Cheriet, ``Unsupervised ensemble of
  experts (eoe) framework for automatic binarization of document images,'' in
  \emph{Document Analysis and Recognition (ICDAR), 2013 12th International
  Conference on}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2013, pp.
  703--707.

\bibitem{Phase2014}
H.~Ziaei~Nafchi, R.~Farrahi~Moghaddam, and M.~Cheriet, ``Phase-based
  binarization of ancient document images: Model and applications,'' 2014.

\bibitem{LTP2010}
X.~Tan and B.~Triggs, ``Enhanced local texture feature sets for face
  recognition under difficult lighting conditions,'' \emph{Image Processing,
  IEEE Transactions on}, vol.~19, no.~6, pp. 1635--1650, 2010.

\bibitem{ramirez2010transition}
M.~A. Ram{\'\i}rez-Orteg{\'o}n, E.~Tapia, L.~L. Ram{\'\i}rez-Ram{\'\i}rez,
  R.~Rojas, and E.~Cuevas, ``Transition pixel: A concept for binarization based
  on edge detection and gray-intensity histograms,'' \emph{Pattern
  Recognition}, vol.~43, no.~4, pp. 1233--1243, 2010.

\bibitem{sklearn}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
  D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay, ``Scikit-learn:
  Machine learning in {P}ython,'' \emph{Journal of Machine Learning Research},
  vol.~12, pp. 2825--2830, 2011.

\bibitem{geurts2006extremely}
P.~Geurts, D.~Ernst, and L.~Wehenkel, ``Extremely randomized trees,''
  \emph{Machine learning}, vol.~63, no.~1, pp. 3--42, 2006.

\bibitem{6482566}
T.~Lelore and F.~Bouchara, ``Fair: A fast algorithm for document image
  restoration,'' \emph{Pattern Analysis and Machine Intelligence, IEEE
  Transactions on}, vol.~35, no.~8, pp. 2039--2048, Aug 2013.

\bibitem{nafchi2013historical}
H.~Z. Nafchi, R.~F. Moghaddam, and M.~Cheriet, ``Historical document
  binarization based on phase information of images,'' in \emph{Computer
  Vision-ACCV 2012 Workshops}.\hskip 1em plus 0.5em minus 0.4em\relax Springer,
  2013, pp. 1--12.

\end{thebibliography}
\end{document}


