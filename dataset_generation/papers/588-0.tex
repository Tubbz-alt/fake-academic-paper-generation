\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{tabularx}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi


\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Shallow and Deep Convolutional Networks for Saliency Prediction}

\author{Junting Pan\thanks{Equal contribution.} ,\ Elisa Sayrol and Xavier Giro-i-Nieto\\
Image Processing Group\\
Universitat Politecnica de Catalunya \\
Barcelona, Catalonia/Spain\\
{\tt\small xavier.giro@upc.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Kevin McGuinness\footnotemark[1] \ and Noel O'Connor\\
Insight Center for Data Analytics\\
Dublin City University\\
Dublin, Ireland\\
{\tt\small kevin.mcguinness@insight-centre.org}
}


\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
The prediction of salient areas in images has been traditionally addressed with hand-crafted features based on neuroscience principles. This paper, however, addresses the problem with a completely data-driven approach by training a convolutional neural network (convnet). The learning process is formulated as a minimization of a loss function that measures the Euclidean distance of the predicted saliency map with the provided ground truth. The recent publication of large datasets of saliency prediction has provided enough data to train end-to-end architectures that are both fast and accurate. Two designs are proposed: a shallow convnet trained from scratch, and a another deeper solution whose first three layers are adapted from another network trained for classification.
To the authors knowledge, these are the first end-to-end CNNs trained and tested for the purpose of saliency prediction.
\end{abstract}
%%%%%%%%% BODY TEXT
\section{Introduction}% What we present and why now
This work presents two approaches of end-to-end convolutional neural networks (convnets or CNNs) for saliency prediction. 
Our objective is to compute saliency maps that represent the probability of visual attention on an image, defined as the eye gaze fixation points. 
This problem has been traditionally addressed with hand-crafted features inspired by neurology studies.
In our case we have adopted a completely data-driven approach, using a large amount of annotated data for saliency prediction.
Figure \ref{fig:examples} provides an example of an image together with its ground truth saliency map and the two saliency maps predicted by the proposed convnets: a shallow one and a deep one.

%Is it necessary to include this figure? Delete in case of need of space\begin{figure}%
		\includegraphics[width=\linewidth]{./fig/front.jpg}
		\caption{Input Image (top left) and saliency maps from the ground truth (top right), our shallow convnet (bottom left) and our  deep convnet (bottom right).}
		\label{fig:examples}
\end{figure}% Convolutional network
Convnets are popular architectures in the field of deep learning and have been widely explored for visual pattern recognition, ranging from a global scale image classification ~\cite{krizhevsky2012imagenet} to a more local object detection ~\cite{girshick2014rich} or semantic segmentation ~\cite{long2015fully}.
The hierarchy of layers in convnets are inspired by biological models, and some works have pointed at a relation between the activity of certain areas in the brain with hierarchy of layers in the convnets ~\cite{agrawal2014pixels, cichy2015mapping}.
Provided with enough training data, convnets have shown impressive results, often outperforming other hand-crafted methods. 
%It has also been shown that convolutional neural networks were inspired by biological model and they work in such a way that the predictions of the convolutional network layers march the visual hierarchy of human brain.%Collecting the large amount of data necessary to train CNNs has be
The rise of convnets started from the computer vision task where more annotated data can be easily collected, that is, image classification~\cite{russakovsky2015best}.
Large datasets like ImageNet~\cite{imagenet_cvpr09} or Places~\cite{zhou2014learning} have provided enough visual examples to train the millions of parameters that most popular convnets contain.
These datasets provide thousands of images for each discrete label  typically associated to a semantic class.

The saliency prediction problem, however, poses two specific challenges different from the classic image classification.
First, collecting large amount of training data is much more costly because it requires capturing the fixation points of human observers instead of a textual label for each image.
Our work has benefited from recent publications of two 
large datasets containing images and an annotation of their salient points for humans \cite{jiang2015salicon, xu2015turkergaze}.
%annotated by the human attention %the human eye fixations on them: SALICON~\cite{jiang2015salicon} and iSun~\cite{xu2015turkergaze}.  %These datasets have adopted different approaches to measure visual attention.%While iSun was generated with an eye-tracker to annotate the gaze fixations, the SALICON dataset was built by asking humans to click on the most salient points on the image.% Xavier, even if this sentence is removed we should add the reference at the end%
Collecting this level of of data has been possible thanks to crowdsourcing approaches, the same strategy used to annotate the ImageNet and Places datasets.



The second challenge to address when using convnets for saliency prediction is that a saliency score must be estimated for each pixel in the input image, instead of a global-scale label for the whole image.
The saliency map at the output must present a spatial coherence and a smooth transition between neighbouring pixels.

The main contribution of this work is addressing the saliency prediction problem from an end-to-end perspective, by using convnets for regression rather than classification. 
We apply this strategy with two different architectures trained with two different approaches:
a shallow convnet trained from scratch, and a deep convnet that reuses parameters from the bottom three layer of a network previously trained for classification.
To the authors knowledge, these were the first convnets that formulate saliency prediction as an end-to-end regression problem.

%these two networks were the first end-to-end CNNs trained for saliency%We have addressed this problem from two different approaches% Contribution %Our main contribution has been the design of a novel end-to-end convnet for saliency prediction.%The proposed network proves its superior performance in the Large-scale Scene UNderstanding (LSUN) challenge 2015 \cite{zhanglarge}.%se datasets have provided a %While both of them provide human-generated saliency maps from a large collection of images, its intrinsic nature %Saliency prediction has attracted many research interests in recent years and has been demonstrated to be useful in many applications, e.g semantic segmentation, object recognition and visual tracking. %Many previous works have exploited the Feature Integration Theory, which combined together handcrafted image feature such as orientation, contrast, and colors. However, human fixations are mainly focused around objects; this has led to new models trying to incorporate more image features such as detectors for faces, people, and objects. %Nevertheless, current saliency models do not show a good performance.%Independent of these developments, recently advances in applying convolutional neural networks to tasks like image classification suggest that they are able to solve many computer vision problems. 

This paper is structured as follows. Section~\ref{sec:RelatedWork} presents the previous and recent works using convolutional networks for saliency prediction and detection.
Section~\ref{sec:JuntingNet} introduces the shallow convnet, while Section~\ref{sec:SalNet} presents the deep network.
Section~\ref{sec:experiments} compares both networks in terms of memory requirements. It also shows, prediction performance in the MIT Saliency Benchmark and LSUN Saliency Prediction Challenge 2015 and they are compared with other models.
%Our system is presented in Section \ref{sec:ProposedNetwork} and its results on the LSUN challenge reported in Section \ref{sec:experiments}.
Conclusions and future directions are outlined in Section~\ref{sec:conclusions}.

Our results can be reproduced with the source code and trained models available at \url{https://github.com/imatge-upc/saliency-2016-cvpr}.
\section{Related work}\label{sec:RelatedWork}

The proposed networks presents the next natural step to two main trends in deep learning: using convolutional neural networks for saliency prediction and training these networks by formulating saliency prediction as an end-to-end regression problem.
This section reviews related work in these directions.

%Motivated by these developments, we decided to design a new ConvNet (Convolutional neural Network) to model saliency prediction. An early attempt of predicting saliency with a convnet was the \textit{ensembles of Deep Networks (eDN)}~\cite{vig2014large}, which proposed an optimal blend of feature maps from three different convnet layers, that were finally combined with a simple linear classifier trained with positive (salient) or negative (non-salient) local regions.
This approach inspired \textit{DeepGaze}~\cite{kummerer2014deep} to adopt a deeper network. In particular, \textit{DeepGaze} used the existing AlexNet network~\cite{krizhevsky2012imagenet}, where the fully connected layers were removed to keep the feature maps from the convolutional layers. The response of each layer were fed into a linear model and its weights learned. 
DeepGaze would be the first case of transfer learning from a convnet for classification used for saliency, as we propose in our deeper architecture. However, we do not train a linear model to combine feature maps but directly train a stack of new convolutional layers on top of the transferred ones.
%spatially located features and the responses of each layer were fed into a saliency linear model. Transfer learning trained on the large Imagenet database for object classification was used and only the coefficients of for the linear model were learned. 
Other recent works have explored the combination of different convnets working at different resolutions to capture both global and local saliency.
%in parallel and at and combine them to include local and global features. 
Liu \etal\cite{liu2015predicting} proposed an architecture with three convnets working in parallel
%so that for a given image input, three different resolutions are computed and then equal size images regions with the same center locations are obtained and used as inputs to the three convnets. 
where the three final fully connected layers are combined in a single layer to obtain the saliency map. Unlike our work the network is trained with image regions centered on fixation and non-fixation eye locations. On the other hand, a related preprint by Srinivas and Ayush~\cite{srinivas2015deepfix} appeared during submission of our work. Their model captures information at different scales by using very deep networks. Their work is inspired by the VGG network architecture proposed by Simonyan and Zisserman~\cite{simonyan2015verydeep}. Very deep networks may obtain richer information of image semantics. Some layers use inception style convolutional blocks~\cite{Szegedy2015goingdeeper} that capture semantics at different scales. By using large receptive fields, global context is also incorporated. As in the  \textit{DeepGaze} proposal, the network needs to be trained with databases that are not specific for eye fixation but are useful to capture generic image features.

Other approaches introduce new architectures and improvements in salient object detection. Zhao \etal\cite{zhao2015saliency} use also two parallel networks to obtain local and global context modeling. The input image consists of a superpixel-centered window that is preprocessed differently to feed each of the two convnets. Fully connected layers are combined at the end to obtain the salient objects. The work by Li and Yu~\cite{liu2015visual} proposes three nested windows as inputs to three different convnet at different scales that are fused together to obtain an aggregated saliency map. Wang \etal. proposed a different pipeline~\cite{wang2015deepnetworks}: local estimation is carried out and the resulting information is used as input to obtain a global search. That is, first, to detect local saliency, a deep neural network (DNN-L) learns local patch features to determine the saliency value of each pixel. Second, the local saliency map together with global contrast and geometric information are used as global features to obtain object candidate regions. A deep neural network (DNN-G) is then trained to predict the saliency score of each object region based on global features. Finally, a very recent work introduced by Li \etal\cite{li2015deepsaliency}, combines semantic image segmentation and saliency detection, sharing the first layers that exploit extraction of effective features for object perception. Only the last layers are divided to obtain the corresponding segmentation and saliency detection images. This proposal is also inspired by the VGG very deep network introduced in~\cite{simonyan2015verydeep}. In this case, 15 layers and pretraining with the Imagenet dataset is used. 

%Our network adopts a not very deep architecture as \textit{eDN}, but it is end-to-end trained as a regression problem, avoiding the reuse of precomputed parameters from another task.%Nevertheless it still outperforms state-of-the-art models.% Fully convolutional network (CVPR Berkeley 2015)
Fully Convolutional Networks (FCNs)~\cite{long2015fully} addressed the semantic segmentation task to predict the semantic label of every individual pixel in the image. 
This approach dramatically improved previous results on the challenging PASCAL VOC segmentation benchmark~\cite{everingham2014pascal}.
% Check Edu's Fontdevila state of the art as slides
The idea of an end-to-end solution for a 2D problem as as semantic segmentation was refined by \text{DeepLab-CRF}~\cite{DBLP:journals/corr/ChenPKMY14}, where the spatial consistency of the predicted labels is improved using a Conditional Random Field (CRF), similarly to the hierarchical consistency enforced in~\cite{farabet2013learning}.


In our work we are interested in finding saliency maps rather than salient object detection by training convnets end-to-end. We also focus on novel databases that are annotated for the purpose of saliency prediction.

%In our work, we adopt the end-to-end solution for a regression problem instead of a classification one, and we also introduce a post-filtering stage, which consists of a Gaussian filtering that smoothes the resulting saliency map.% Paper on semantic segmentation by Yann Lecun, which had a posterior filering
\section{Shallow Convnet}\label{sec:JuntingNet}% Overall architecture
This section presents the first of our proposed convnets, which is based on a lightweight architecture whose parameters are trained from scratch.

%presents a new end-to-end convnet for saliency prediction.%A deep ConvNet can easily have millions of model parameters, which lead to strong ovefitting. Therefore, our model does not have a very deep architecture (5 layers), while in AlexNet there are 7 layers, and GoogLeNet has 22 layers (ILSVRC 2014). %Our proposal of a not very deep ConvNet design is motived both from our intention to mitigate the overfitting problem as well as the nature of the problems we are trying to predict: visual saliency map which are related to color, contrast and position. This, compared to, e.g. the 1.000 object class in the ILSVRC 2014.\subsection{Architecture}\label{ssec:juntingnet-arch}

The network consists of five layers with learned weights: three convolutional layers and two fully connected layers.
Each of the three convolutional layers is followed by a rectified linear unit non-lineraity (ReLU) and a max pooling layers.
Figure \ref{fig:juntingnet} shows a detailed description of each layer. The network has to a total of 64.4 million free parameters. 

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{./fig/juntingnet-arch.png}
  \caption{Architecture of the shallow convolutional network.}
  \label{fig:juntingnet}
\end{figure}% Strategies to reduce overfitting

The network was designed considering the a amount of available saliency maps for training it from scratch.
Different strategies were considered to avoid overfitting the model.
First, we used three convolutional layers rather than the five used in the classic AlexNet architecture~\cite{krizhevsky2012imagenet} (and far less than very deep networks used recently such as the thirteen used in VGG-16 \cite{simonyan2014very}).
Second, the input images are resized to $[96 \times 96]$, a much smaller dimension that the $[227 \times 227]$ used in AlexNet~\cite{krizhevsky2012imagenet}.
The three max pooling layers reduce the initial $[96 \times 96]$ feature maps down to $[10 \times 10]$ by the last of the three  poolings.

Even with the above constraints, the network still overfits significantly. We found that norm constraint regularization for the maxout layers~\cite{goodfellow2013maxout}, which computes the \textit{max} between pairs of of the previous layer’s output,
was essential to mitigate this overfitting.
We also tested using dropout~\cite{hinton2012improving} after the first fully connected layer, with a dropout ratio of 0.5 (50\% of probability to set a neuron's output value to zero), but this did not improve overfitting much, and so was not included in the final model.

Notice that the 2,304-dimensional vector at the output is mapped into a 2D array of $[48 \times 48]$, which correspond to the saliency map.
This decrease in resolution is compensated at test time by resizing the dimensions of the output to match the input image and posterior filtering using a Gaussian kernel with a standard deviation of $2.0$.

This shallow convnet was implemented using Python, NumPy, and the deep learning library Theano~\cite{bergstra2010theano, bastien2012theano}. 
Processing was performed on an NVIDIA GTX 980 GPU with 2048 CUDA cores and 4GB of RAM. 
% It took about 8 hours to train and the predictions takes about 200ms for an image. 
It took between 6 and 7 hours to train for the SALICON dataset, and 5 to 6 hours for the iSUN dataset.
Saliency prediction requires 200 ms per image.

% Elisa: why dropout is not working in this architecture?%Despite the loss of visual resolution at the output, this reduction also reduces the amount of model parameters and  prevents overfitting. %The max-pooling layer selects the maximum value of every $[2\times 2]$ region, taking strides of two pixels.% Elisa: I don't think we should say 1x1 convolutions% Kevin: I agree with Elisa, I don't understand how fully connected layers are 1x1 convolutions...% Not very deep due to little training data%For example, the best performing convnets in the ImageNet ILSRVC challenge range from the 7 layers in the AlexNet network~\cite{krizhevsky2012imagenet} used in ImageNet (2012), 19 layers from the VGG net (2014)~\cite{Simonyan14c}, and 22 layers in the GoogLeNet network (2014)~\cite{szegedy2015going}.% The benefits of a light design: less memory and less training images necessary%This design with less parameters necessary to define the network filters, also allows being trained only with the saliency maps available in the state of the art datasets.% Kevin: the above sentence feels a bit fuzzy: can you be more precise here?% which, after the pooling operations within the network, it maps into a 48x48 output layer for saliency prediction.% Resize to 96 x 96%The first block in JuntingNet is a resizing module which guarantees that the amount of parameters to learn in the network is consistent with the amount of training data.%The amount of training images is $6,000$ for iSun and $10,000$ for SALICON, figures much smaller than the $1,200,000$%The two datasets tested with this network contain images of 480x640 pixels (SALICON) or random dimensions (iSUN), and their corresponding ground truth saliency maps present the same size as the related image.%%First at all, we have to face the problem that the images provide in the both datasets has very big size for deep learning, in SALICON we have 480x640 pixel and in iSUN we have a random size. %%The ground truth for both datasets has the same size as the stimuli image. %Considering the fixed 480x640 dimensions for SALICON, there are 307,200 saliency values to learn from a training dataset of 10,000 images.%for training, we proposed then, to reduce the image size to 96x96 and the ground truth saliency map to 48x48, which could be easier to learn for our ConvNet. %All three color channels are fed directly to the ConvNet.%The detailed description of the convnet stages is the following:%\begin{enumerate}%\item The input volume has size of $[96 \times 96 \times 3]$ (RGB image), a size smaller than the $[227 \times 227 \times 3]$ proposed in AlexNet~\cite{krizhevsky2012imagenet}. Similarly to the shallow depth, this design parameter is motivated to reduce the possibilities of overfitting. %by the smaller size of the training dataset if compared with the 1.2M images from ImageNet ILSRVC 2012.%This input size is further reduced to 48x48 on the output saliency layer due to the pooling layers in the network.%\item The receptive field of the first 2D convolution is of size $[5\times5]$, and its outputs define a convolutional layer with 32 neurons.%This layer is followed by a ReLU activation layer, which applies an element wise non-linearity.%After, a max pooling layer progressively reduces the spatial size of the input image. %Despite the loss of visual resolution at the output, this reduction also reduces the amount of model parameters and  prevents overfitting. %The max-pooling layer selects the maximum value of every $[2\times 2]$ region, taking strides of two pixels. %\item The output of the previous stage has a size of $[46 \times 46 \times 32]$. \textcolor{blue}{Junting: Why 46 ? I thought it should be of 48 if it is a result of a max poolof 2 on a feature map of 96x96...}%The receptive field of this second stage is $[3\times 3]$. Again, this is followed by a RELU layer and a max-pooling layer of size $[2 \times 2]$.%\item Finally, the last convolutional layer is fed with an input of size $[22 \times 22 \times 64]$. %The receptive of this layer is also of $[3 \times 3]$ and it has 64 neurons. %A ReLU and max pooling layers are stacked too. % \item A first fully connected layer receives the output of the third convolutional layer with a dimension of $[10 \times 10 \times 64]$. It contains a total of 4,608 neurons.% \item The second fully connected layer consist of a maxout layer with 2,304 neurons.% The maxout operation~\cite{goodfellow2013maxout} computes the pairs of the previous layer’s output. % %Using a maxout layer instead of classic fully connected layer helps to reduce overfitting. % \item Finally, the output of the last maxout layer is the saliency prediction array. % The array is reshaped to have 2D dimensions and resized to the stimuli image size. % Finally, a 2D Gaussian filter with a standard deviation of 3.0 is applied.% \end{enumerate}\subsection{Training}

This shallow network was trained from scratch twice, each time from a different dataset. 
A first model was built using the $10,000$ saliency maps from the SALICON dataset \cite{jiang2015salicon}, and a second model using the $6,000$ saliency maps from the iSUN dataset.
Both datasets are described in detail in Section~\ref{ssec:datasets}.
Given the smaller amount of images available in the iSUN dataset \cite{xu2015turkergaze}, a slight modification was introduced in this second model:
%It must be noted that the architecture presented in Section \ref{ssec:juntingnet-arch} was slightly modified in the iSUN case because only 6,000 images were available for training instead of the 10,000 available for SALICON.
the depth of the third convolutional network was of 64 instead of 128, as depicted in Figure~\ref{fig:juntingnet}.
%with $10,000$ saliency maps from the SALICON dataset \cite{jiang2015salicon}, which is described in detail later in Section~\ref{ssec:datasets}.

The weights in all layers are initialized from a normal Gaussian distribution with zero mean and a standard deviation of 0.01, with biases initialized to 0.1.
The network was trained with stochastic gradient descent (SGD) and the Nesterov momentum method, which we found helps convergence. 
The learning rate changed over time, starting with a higher learning rate 0.03 and decreased during training to 0.0001. 
We trained the network for 1,000 epochs.
For validation purposes, we split the training data into 80\% for training and the rest for periodic validation. 
A data augmentation technique was used by mirroring all images. All considered saliency maps were normalized to $[0,1]$.

% Descriptionof the 
The filters learned in the first convolutional layer are shown in Figure \ref{fig:juntingnet-filters}.
They present a similar pattern to other similar filters learned for classification convnets \cite{zeiler2014visualizing, zeiler2014visualizing}, where edge detectors can be identified.
It is noticeable how these type of filters arise also when training our network on saliency maps.

\begin{figure}
  \centering
  \includegraphics[width=0.3\columnwidth]{./fig/juntingnet-filters.png}
  \caption{Filters learned for the first convolutional layer of the shallow convnet (best viewed from a distance).}
  \label{fig:juntingnet-filters}
\end{figure}%In particular, this networks was trained twice: with $6,000$ saliency maps from the iSun dataset and .%This adopted shallow depth is used to prevent overfitting, which can be a risk for models with a many parameters.%Further details this dataset are presented in  %The parameters of our network are fit by minimizing an Euclidean loss function defined directly on the ground truth saliency maps.% Tricks to reduce overfitting%The limited amount of training data for our architecture made overfitting a significant challenge, so we used different techniques to minimize its effects.% Initialization of layer weights% Dataset partitions% Figures \ref{fig:train-isun} and \ref{fig:train-salicon} present the learning curves for the iSUN and SALICON models, respectively.% \begin{figure}%% 		\includegraphics[width=\linewidth]{./fig/isun.png}% 		\caption{Learning curves for iSUN models.}% 		\label{fig:train-isun}% \end{figure}% \begin{figure}%% 		\includegraphics[width=\linewidth]{./fig/salicon.png}% 		\caption{Learning curves for SALICON models.}% 		\label{fig:train-salicon}% \end{figure}
\section{Deep Convnet}\label{sec:SalNet}

The second approach explored in this paper is the adaptation of an existing very deep convnet trained for image classification for the task of saliency prediction.
Previous work~\cite{zeiler2014visualizing} has noted how, in image classification tasks, the model parameters from the lowest levels in the convnets converge in a few epochs.
This observation, together with visualization of the filters learned at these layers~\cite{simonyan2013deep}, suggest that these layers perform low-level visual task in vision, such as the detection of colors or textures.
Our hypothesis is that these lower layers trained for classification can also be transferred for the task of saliency prediction.
We propose a second convnet which adapts these pre-trained filters and combines them with new layers specifically trained for saliency.

\subsection{Architecture}\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{./fig/salnet-arch.pdf}
	\caption{Architecture of the deep convolutional network}
	\label{fig:salnet-arch}
\end{figure}

Figure~\ref{fig:salnet-arch} illustrates the layer architecture of the network, composed of 10 weight layers and a total of 25.8 million parameters.
%, which gives a model size of 97.5MB when stored as 32-bit floating point numbers. 
The architecture of the first 3 weight layers is compatible with that of the VGG network from~\cite{chatfield2014devil}. Each convolutional layer is followed by a rectified linear unit non-linearity (ReLU). Pooling layers follow the first two convolutional layers, effectively reducing the width and height of the feature maps in the intermediate layers by a factor of four. A deconvolution layer follows the final convolution to produce a saliency map that matches the input width and height.



To choose the final network architectures, we experimented with many different different variants, testing each on a held-out validation set of 1,000 images. In general we found that: 1) adding more layers improves accuracy; 2) adding more feature maps per layer usually improves accuracy too; and 3) using dropout regularization did not significantly improve accuracy but did increase training time. The final network design was primarily constrained in resolution, number of layers, and layer depth by the amount of available GPU memory.

We used transfer learning to initialize the weights for the first three convolutional layers with the pre-trained weights from the \emph{VGG\_CNN\_M} network from~\cite{chatfield2014devil}. This acts as a regularizer and improves the final network result. The remaining weights were initialized randomly using the strategy from~\cite{he2015rectifiers}. 

\subsection{Training}

We trained our network on 9,000 of the 10,000 training images in the SALICON dataset, setting aside 1,000 images for validation (ground truth for the SALICON validation set had not yet been released when this network was first trained). We used several standard pre-processing techniques on both the input images and the target saliency maps. We subtracted the mean pixel value of the training set from the image pixels to zero center them and rescaled the resulting values linearly to be in the interval $[-1,1]$. We similarly preprocessed the saliency maps by subtracting the mean and scaling to $[-1,1]$. Both the input images and the saliency maps were downsampled by half to $320 \times 240$ prior to training.

The network was trained using stochastic gradient descent with Euclidean loss using a batch size of 2 images for 24,000 iterations. During training, the network was validated against the validation set after every 100 iterations to monitor convergence and overfitting. We used the standard $L^2$ weight regularizer (weight decay), and halved the learning rate every 100 iterations. The network took approximately 15 hours to train on a NVIDIA GTX Titan GPU running the Caffe framework \cite{jia2014caffe}. We normalized the base learning rate by the number of predictions per image, to give a learning rate of $0.01 / (320 \times 240) \approx 1.3 \times 10^{-7}$. Using a larger learning rate causes the learning to diverge.

The network was trained on inputs of size $320 \times 240$, but in principle, it can handle images of any size, since it only consists of convolutional and pooling layers. In practice, the input size is constrained by the amount of GPU memory (or RAM) needed to store the outputs of the intermediary layers. Nevertheless, the network has the advantage that it can be sized to match the aspect ratio of any image, and indeed use this approach for the images in the MIT300 benchmark in the next section.


\section{Experiments}\label{sec:experiments}%\subsection{Implementation}\subsection{Memory requirements}

The architectures of the two networks present different requirements in terms of memory resources.
These resources are dedicated to two different tasks: the parameters that define the network, and the blob data that characterizes network response at the different processing stages.

The parameters that define the network are fit during training, and, together with the architecture layout, correspond to the actual characterization of the network.
These parameters characterize the output of each neuron in the net, which can be defined as $f(w^T x+b)$, where $w$ describes the filter parameters in the convolutional layers, $b$ corresponds to the biases and $f$ is the non-linearity.
Each neuron, therefore, has parameters $w$ and $b$, which are fit during backpropagation.

The data associated to the input image is the second source of memory requirements.
The input image is hierarchically process in the convnet, creating multiple intermediate feature maps (or data blobs) after each processing stage.

Table \ref{tab:memory} presents the complementary memory requirements for each of the two convnets.
These values have been obtained from the architectures of the shallow and very deep networks described in Figures \ref{fig:juntingnet} and \ref{fig:salnet-arch}, respectively.
The estimation assumes 32-bit floating points to store parameters and layer output (4 bytes per value).
The memory estimate for blob data assumes test time (forward pass only):
at train time this value is doubled to account for the error signal during backpropagation.
%Notice that the necessary amount for training the network is twice the necessary for test, because the backpropagated error must also be stored.The number of parameters for both networks are much lower than the very deep networks used in classification. For example, the 19 layers version of VGG net requires 144 million parameters \cite{Simonyan14c}.

Our shallow network requires far less memory for the layer outputs, but has significantly more parameters (due to the fully connected layers). This explains why our deep network does not overfit, whereas stronger regularization is necessary to fit the shallow one. Since the shallow network needs less memory for the layer outputs, it is possible to make batch size on this network very large at test time, allowing it to process many more images at once.

% Xavier, Kevin, in order to avoid names of networks I propose the following text, if you decide to change the names: "Our Shallow Network requires far less memory for the data (blobs), but has significantly more parameters (due to the fully connected layers). This explains why our Deep Network doesn't really overfit, whereas more regularization is needed to fit the Shallow one. Since this one needs less memory for the blobs (layer outputs), the batch size could potentially be made very large at test time on the Shallow Network, allowing to process many more images at once.(Elisa)\begin{table}
\begin{center}
\begin{tabularx}{\columnwidth}{Xrr}
\toprule
				& Shallow   		& Deep \\
\midrule
Data 			& 2.29 MB 			& 123.65 MB \\
Parameters  	& 244.64 MB			&  98.44 MB \\
\cmidrule(r){2-3}
Total (train) 	& 249.22 MB			& 345.74 MB	\\
Total (test)	& 246.93 MB			& 222.09 MB \\
\bottomrule
\end{tabularx}
\end{center}
\caption{Approximate memory requirements for each convnet.}
\label{tab:memory}
\end{table}\subsection{Datasets}\label{ssec:datasets}

The two convnets were assessed using images and metrics considered in the public MIT Saliency Benchmark \cite{judd2009learning} LSUN challenge 2015 \cite{zhanglarge}.

These four datasets capture a broad range of image types and experimental set ups.
The MIT300 and CAT2000 datasets are smaller in size, but provide fixations points captured in a controlled environment of expert users. 
On the other hand, the iSUN and SALICON datasets have a large amount of saliency maps corresponding to images from the existing SUN and MS CoCo dataset, but these maps were collected via crowdsourcing on Amazon Mechanical Turk, exposing them to crowdsourcing loss~\cite{carlier2015assessment}.

\begin{description}
%\item[CAT2000 \cite{borji2015cat2000}] The CAT2000 dataset has been specifically build taking into consideration 20 categories associated to the expected behaviour of the observers. These categories address both bottom-up attention clues (eg. \textit{patterns}) as well as top-down ones (eg. \textit{social}). Other considered categories include \textit{satellite}, \textit{sketch}, \textit{inverted} images or \textit{affective}.
\item[SALICON \cite{jiang2015salicon} ] This is the largest dataset available for saliency prediction and was used to train our models.
It was built from images of the \textit{Microsoft CoCo: Common Objects in Context} \cite{lin2014microsoft} dataset, which inspired the SALICON naming: \textit{SALIency in CONtext}. The pixel-wise semantic annotations provided by CoCo allow combining and comparing saliency data with semantic ones.
However, the saliency maps in SALICON were not collected with eyetrackers as in most popular datasets for saliency prediction, but with mouse clicks captured in a crowdsourcing campaign.
\item[iSUN \cite{xu2015turkergaze}] The iSUN dataset has been built with an online game using  webcams to track player eye gaze. The dataset uses natural scene images from the SUN database \cite{xiao2010sun}, a large dataset  organized in 397 scene categories.
\item[MIT1003 and MIT300 \cite{judd2009learning}] This dataset is the most well-known among saliency prediction researchers. It is accompanied by an online benchmark maintained by its authors.
The MIT1003 dataset consists of both images and fixation points that can be used for training. The fixation points for the MIT300 dataset are not public: the dataset can only be used for benchmarking.
The stimuli images in these datasets consist of indoor and outdoor natural scenes from the Flickr Creative Commons and LabelMe \cite{russell2008labelme} datasets. The MIT datasets are the smallest of the considered datasets, so results on these sets have the most potential for overfitting. %The semantic annotation of the later images is also available from the LabelMe dataset.
\end{description}\begin{table*}
\begin{center}
\begin{tabular}{lllllll}
\toprule
Dataset							&	Description			 & Capture device 	& Observers & Train 	& Validation & Test \\
\midrule
%MIT1003 \cite{judd2012benchmark}& Flickr and LabelMe \cite{russell2008labelme}  & Eyetracker	& 39 & 903 & 100	& - \\
%CAT2000 \cite{borji2015cat2000} & 20 categories 		& Eyetracker	& 24 & 2,000		& -	& 2,000 \\
SALICON	\cite{jiang2015salicon} & Microsoft CoCo \cite{lin2014microsoft} & Mouse clicks	& Crowd & 10,000 	& 5,000	& 5,000 \\
iSUN 	\cite{xu2015turkergaze} & SUN \cite{xiao2010sun} & Eyetracker	& Crowd & 6,000 	& 926	& 2,000 \\
MIT300 \cite{mit-saliency-benchmark}	& Flickr and LabelMe \cite{russell2008labelme} 	& Eyetracker	& 39 & - & -	& 300 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Description of the three datasets used in our experiments.}
\label{tab:datasets}
\end{table*}\subsection{Results}

Saliency prediction evaluation has received the attention of several researchers, resulting in various proposed approaches.
Our experiments consider several of these, in a similar way to the MIT saliency benchmark~\cite{Judd_2012}. %, mit-saliency-benchmark}.
Some of these metrics compare the predicted saliency maps with the maps generated from the fixation points of the ground truth, while some other metrics directly compare with the fixation points.
In the result tables that follow, we have sorted the different techniques based on the AUC Judd metric.

% How we trained our convnets
Where not otherwise stated, our convnets were trained with images from the SALICON~\cite{jiang2015salicon} dataset and tested on images from iSUN~\cite{xu2015turkergaze} and MIT300 datasets to avoid overfitting.
The one exception to this is our submission for the LSUN 2015 challenge, where our shallow network was trained with training and validation data from iSUN, and assessed on the test partition.

The presented shallow and deep convnets were compared quantitatively on the validation partition of the iSUN dataset.
%The performance in terms of saliency prediction of our two convnets was compared on the validation partition of the iSUN dataset.
The results (Table~\ref{tab:iSUN-val}) show a similar performance of both networks in the the 926 images of this dataset.
%that the deep network provides better results in all metrics, at the cost of speed.

Figure~\ref{fig:qualitative} presents a qualitative comparison of the two networks, showing the predicted saliency maps alongside the ground truth fixation maps.
These examples show a different behaviour between the two networks, with the shallow one presenting a bias towards the central part of the image.
The deep network, on the other hand, offers a higher spatial resolution thanks to its architecture with larger feature maps.
%\ref{fig:juntingnet}\begin{figure*}
		\begin{center}
		\includegraphics[width=0.8\linewidth]{./fig/qualitative-6.jpg}
        \end{center}
		\caption{Saliency maps generated by our shallow and deep network on the SALICON and iSUN validation data.}
		\label{fig:qualitative}

\end{figure*}%Figure~\ref{fig:salnet-examples} shows some examples of images, predicted saliency, and ground truth obtained with both convnets..\begin{table}
\begin{center}
\begin{tabularx}{\columnwidth}{Xrrr}
\toprule
	AUC				 			& Shuffled 	& Borji &  Judd \\
\midrule
Deep Convnet 					& 0.63 			& 0.78			& 0.80 \\
Shallow Convnet	 				& 0.64 			& 0.77			& 0.79 \\
\bottomrule
\end{tabularx}
\end{center}
\caption{Comparison of AUC measures for our deep and shallow convnets on iSUN validation.}
\label{tab:iSUN-val}
\end{table}% \begin{table*}% \begin{center}% \begin{tabular}{|l|c|c|c|c|c|}% \hline% 								&	Similarity 	& CC 	& AUC shuffled 	& AUC Borji & AUC Judd \\% \hline\hline% Deep Convnet				& $0.53$ 	& $0.60$	& $0.72$ 		& $0.85$	& $0.83$ \\% Shallow Convnet 			& TBD 		& TBD		& TBD 			& TBD		& TBD \\% \hline% \end{tabular}% \end{center}% \caption{Results for the SALICON validation set}% \label{tab:SALICON-val}% \end{table*}% LSUN CHALLENGE 2015%We also assessed the performance of each network in the framework of two open benchmarks for saliency prediction: the LSUN saliency prediction challenge 2015 and the MIT300 dataset on the MIT Saliency Benchmark.

Our shallow convnet was the winner of the 2015 LSUN saliency prediction challenge~\cite{zhanglarge}. 
This challenge required participants to evaluate their algorithms on the test partitions of the iSUN and the SALICON datasets.
Our network was trained only with images from the training and validation partitions of each dataset separately, so images from different datasets were never mixed for these experiments.
Table~\ref{tab:iSUN} and Table~\ref{tab:SALICON} include the results provided by the organizers of the challenge for the iSUN and SALICON datasets.
The scores obtained for every measure considered demonstrate the superior performance of our shallow network compared with the other participants.
%Some qualitative results are also provided in Figure \ref{fig:juntingnet}.\begin{table*}
\begin{center}
\begin{tabular}{lrrrrr}
\toprule
					&	Similarity 	& CC 		& AUC shuffled 	& AUC Borji & AUC Judd \\
\midrule
%\textbf{Very deep ConvNet}	& TBD 	& TBD	& TBD 		& TBD	& TBD \\
\textbf{Shallow Convnet (iSUN)}	& \textbf{0.6833} 		& \textbf{0.8230}	& \textbf{0.6650} 		& \textbf{0.8463}	& \textbf{0.8693} \\
Xidian  				& $0.5713$ 		& $0.6167$	& $0.6484$ 		& $0.7949$	& $0.8207$ \\
WHU IIP 			& $0.5593$ 		& $0.6263$	& $0.6307$ 		& $0.7960$	& $0.8197$ \\
LCYLab 				& $0.5474$ 		& $0.5699$	& $0.6259$ 		& $0.7921$	& $0.8133$ \\
Rare 2012 Improved \cite{riche2013rare2012}	& $0.5199$ 		& $0.5199$	& $0.6283$ 		& $0.7582$	& $0.7846$ \\
\midrule
Baseline: BMS \cite{zhang2013saliency}		& $0.5026$ 		& $0.3465$	& $0.5885$ 		& $0.6560$	& $0.6914$ \\
Baseline: GBVS \cite{harel2006graph}		& $0.4798$ 		& $0.5087$	& $0.6208$ 		& $0.7913$	& $0.8115$ \\
Baseline: Itti \cite{itti1998model}		& $0.4251$ 		& $0.3728$	& $0.6024$ 		& $0.7262$	& $0.7489$ \\
\bottomrule
\end{tabular}
\end{center}
\caption{Results for the iSUN test set, according to the LSUN Challenge 2015.}
\label{tab:iSUN}
\end{table*}\begin{table*}
\begin{center}
\begin{tabular}{lrrrrr}
\toprule
					&	Similarity 	& CC 		& AUC shuffled 	& AUC Borji & AUC Judd \\
\midrule
%\textbf{Very deep ConvNet}	& TBD 	& TBD	& TBD 		& TBD	& TBD \\
\textbf{Shallow Convnet}	& \textbf{0.5198} 		& \textbf{0.5957}	& \textbf{0.6698} 		& \textbf{0.8291}	& \textbf{0.8364} \\
WHU IIP 			& $0.4908$ 		& $0.4569$	& $0.6064$ 		& $0.7759$	& $0.7923$ \\
Rare 2012 Improved \cite{riche2013rare2012}	& $0.5017$ 		& $0.5108$	& $0.6644$ 		& $0.8047$	& $0.8148$ \\
Xidian			 	& $0.4617$ 		& $0.4811$	& $0.6809$ 		& $0.7990$	& $0.8051$ \\
\midrule
Baseline: BMS \cite{zhang2013saliency}		& $0.4542$ 		& $0.4268$	& $0.6935$ 		& $0.7699$	& $0.7899$ \\
Baseline: GBVS \cite{harel2006graph}		& $0.4460$ 		& $0.4212$	& $0.6303$ 		& $0.7816$	& $0.7899$ \\
Baseline: Itti \cite{itti1998model}		& $0.3777$ 		& $0.2046$	& $0.6101$ 		& $0.6603$	& $0.6669$ \\
\bottomrule
\end{tabular}
\end{center}
\caption{Results for the SALICON test set, according to the LSUN Challenge 2015. }
\label{tab:SALICON}
\end{table*}

Both of the proposed convnets were also evaluated on the MIT300 dataset~\cite{judd2009learning} of the MIT Saliency Benchmark~\cite{Judd_2012}.
Table~\ref{tab:mit300} compares our results with some other top performers in this benchmark. % other state-of-the-art.
Our deep convnet achieves similar results to the ones obtained by Deep Gaze 1~\cite{kummerer2014deep} at the upper part of the table. The shallow convnet performs worse but still in the upper part of a table which, in its full version, compares 47 different models.

%Unfortunately, the unavailability of the code/models from the top-scoring participants in the MIT benchmark \cite{huan2015salicon, srinivas2015deepfix, kummerer2014deep} has prevented us from benchmarking their results against ours on other datasets. % Xavi: I commented out this point because I think it sounds a bit negative toward non-peer reviewed publications. CVPR reviewers may have different ideas on the value of arXiv as a dissemination mechanism. Also, one of the reviewers could easily be associated with deepfix!%; the description of Deepfix \cite{srinivas2015deepfix} is so far only available on arXiv. \begin{table*}
\begin{center}
\begin{tabular}{lrrrrr}
\toprule
									&	Similarity 	& CC 		& AUC shuffled 	& AUC Borji & AUC Judd \\
\midrule
Baseline: Infinite Humans 			& $1.00$ 		& $1.00$	& $0.80$ 		& $0.87$	& $0.91$ \\
SALICON  \cite{huan2015salicon} (*)	& $0.60$ 		& $0.74$	& $0.74$ 		& $0.85$	& $0.87$ \\
DeepFix \cite{srinivas2015deepfix} (**)	& $0.67$ 	& $0.78$	& $0.71$ 		& $0.80$	& $0.87$ \\
Deep Gaze 1 \cite{kummerer2014deep}	& $0.39$	& $0.48$ 	& $0.66$		& $0.83$	& $0.84$  \\
\textbf{Deep Convnet}	& \textbf{0.52} 	& \textbf{0.58}	& \textbf{0.69} & \textbf{0.82}	& \textbf{0.83} \\
BMS \cite{zhang2013saliency} 	& $0.51$ 		& $0.55$	& $0.65$ 		& $0.82$	& $0.83$ \\

eDN \cite{vig2014large} 		& $0.41$ 		& $0.45$	& $0.62$ 		& $0.81$	& $0.82$ \\
GBVS \cite{harel2006graph}		& $0.48$ 		& $0.48$	& $0.63$ 		& $0.80$	& $0.81$ \\
%								&	Similarity 	& CC 		& AUC shuffled 	& AUC Borji & AUC Judd \\
Judd \cite{judd2009learning} 	& $0.42$ 		& $0.47$	& $0.60$ 		& $0.80$	& $0.81$ \\
\textbf{Shallow Convnet}		& \textbf{0.46} & \textbf{0.53} & \textbf{0.64} & \textbf{0.78}	& \textbf{0.80} \\
Mr-CNN \cite{liu2015predicting} & $0.48$ 		& $0.48$	& $0.69$ 		& $0.75$	& $0.79$ \\
Rare 2012 Improved \cite{riche2013rare2012}	& $0.46$ 		& $0.42$	& $0.67$ 		& $0.75$	& $0.77$ \\
Baseline: One human		& $0.38-0.46$ 		& $0.52-0.65$	& $0.63-0.67$ 	& $0.66-0.71$	& $0.80-0.83$ \\
\bottomrule
\end{tabular}
\end{center}
\caption{Results of the MIT300 dataset. (*-to be published, (**-non-peer reviewed}
\label{tab:mit300}
\end{table*}% Overfitting problems
A detailed analysis of the MIT300 results suggests a potential dataset bias~\cite{torralba2011unbiased} in these benchmarks.
Notice how while GBVS \cite{harel2006graph} clearly outperforms our shallow convnet for MIT300 (Table \ref{tab:mit300}), its results are much lower than our shallow convnet or Rare 2012 Improved \cite{riche2013rare2012} for the iSUN (Table \ref{tab:iSUN}) and SALICON datasets (Table \ref{tab:SALICON}).
Unlike many of the other top performing results on the MIT benchmark (DeepFix~\cite{srinivas2015deepfix}, Deep Gaze 1~\cite{kummerer2014deep}, eDN~\cite{vig2014large}, and Judd~\cite{judd2009learning}), our networks were not trained or fine-tuned on the MIT1003 dataset, but trained purely on SALICON data. Our strong results across multiple datasets and benchmarks demonstrate the generality of our models.
%On the contrary, other higher scoring works such as  used data from MIT1003 to train their models.% \begin{table*}% \begin{center}% \begin{tabular}{|l|c|c|c|c|c|}% \hline% 							&	Similarity 		& CC 			& AUC shuffled 			& AUC Borji 	& AUC Judd \\% \hline\hline% Our Deep Convnet				& TBD 			& TBD			& TBD 					& TBD			& TBD \\% BMS \cite{zhang2013saliency} 	& \textbf{0.61} & \textbf{0.67}	& \textbf{0.59} 		& $0.84$		& \textbf{0.85} \\% eDN \cite{vig2014large} 		& $0.52$ 		& $0.54$		& $0.55$ 				& $0.84$		& \textbf{0.85} \\% Judd \cite{judd2009learning} 	& $0.46$ 		& $0.54$		& $0.56$ 				& $0.84$		& $0.84$ \\% \textbf{Our Shallow Convnet}		& TBD 			& TBD 			& TBD 					& TBD			& TBD \\% Baseline: Center Gaussian		& $0.42$ 		& $0.46$		& $0.50$ 				& $0.81$		& $0.83$ \\% Baseline: One human				& $0.45-0.79$ 	& $-0.09-0.96$	& $0.40-0.87$ 			& $0.47-0.92$	& $0.41-0.95$ \\% Baseline: Random Chance			& $0.32$ 		& $0.00$		& $0.50$ 				& $0.50$		& $0.50$ \\% \hline% \end{tabular}% \end{center}% \caption{Results of the CAT2000 test set (* non-peer-reviewed publications).}% \label{tab:mit2000}% \end{table*}% \begin{figure}% 	\centering% 	\includegraphics[width=\columnwidth]{./fig/salnet-examples.pdf}% 	\caption{Example saliency maps produced by our deep network for saliency detection on the SALICON dataset (images are taken from our custom validation set). The top row are the original images. The center row shows the predicted saliency maps produced by our network. The bottom row shows the SALICON ground truth.}% 	\label{fig:salnet-examples}% \end{figure}% \begin{figure*}% 		\includegraphics[width=\textwidth]{./fig/result.PNG}% 		\caption{Saliency maps generated by our shallow and deep network on the iSUN validation test.}% 		\label{fig:qualitative}% \end{figure*}
\section{Conclusions}\label{sec:conclusions}

We propose a novel end-to-end approach for training convnets in the task of saliency prediction.
%The proposed networks have been trained and tested over datasets annotated for the purpose of saliency prediction.
The excellent results of both architectures in state-of-the-art benchmarks demonstrate the superior performance of our convnets with respect to hand-crafted solutions and highlight the importance of an end-to-end formulation of saliency prediction.%, as shown in the results obtained in the LSUN 2015 challenge. %showing the trade off between a shallow network that can be trained from scratch with the available data, with a deeper convnet which benefits from transfer learning from another computer vision task.

The comparison between our shallow and deep networks trained on SALICON data has provided similar results for the iSUN dataset, but a better result for the deep network on MIT300.
On the other hand, the shallow network requires less memory at train time and generates saliency maps much faster because it has fewer layers. %computational resources both at train and at test time 
Both networks rank highly in the MIT300 benchmark despite not being trained on this dataset.
This clearly demonstrates the generalization performance of the networks and robustness to dataset biases.

% Consider to add that in the conclusions (Elisa)%\textit{The proposed networks have been trained and tested over datasets annotated for the purpose of saliency prediction.}%trained only with the datasets of visual saliency provided by the LSUN challenge. %Both approaches prove the superior performance of convnet with respect to hand-crafted features, as shown in the results obtained in the LSUN 2015 challenge. %\textit{Our results demonstrate that a not very deep convnets are capable of achieving good results on a highly challenging task}.  %\textcolor{blue}{Junting, Kevin and Elisa: Please add your conclusions as bullet points and we will wrap up a final text once we have them all.}%\textit{Our experiments can be considered as preliminary, as only one configuration and set up was considered}.%suggest that our results can be improved with larger datasets and longer training time.%\textit{We expect that a more elaborate study of the architecture, use of the dataset and training parameters could still improve the reported performance}.%that increasing the amount of fixation data and longer training time we can achieved a better result.  %We expect that this initial work on end-to-end training for saliency prediction will arise the interest on hope that with our success of using deep learning techniques in the saliency prediction task can wake up more interest between researchers to apply this technique on other more complex computer vision problems.% In the end, the network’s size is limited mainly by the size of the training datasets, the GPU memory and training time. % Future work:% - further experiments with other architecture not losing resolution% - use semantic classifiers.
%\input{7_additional}
\section*{Acknowledgements}\label{sec:acknowledgements}

This publication has emanated from research conducted with the financial support of Science Foundation Ireland (SFI) under grant number SFI/12/RC/2289.
This work has been developed in the framework of the project BigGraph TEC2013-43935-R, funded by the Spanish Ministerio de Econom\'ia y Competitividad and the European Regional Development Fund (ERDF). 
The Image Processing Group at the UPC is a SGR14 Consolidated Research Group recognized and sponsored by the Catalan Government (Generalitat de Catalunya) through its  AGAUR office.
We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GeForce GTX Titan Z used in this work.

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}