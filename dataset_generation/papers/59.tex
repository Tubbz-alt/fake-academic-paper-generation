\documentclass{article}
\usepackage{spconf,amsmath,graphicx}

\usepackage[english]{babel}
\usepackage[T1]{fontenc}  % choose the output font encoding 
\usepackage[latin1]{inputenc} % allows the user to input accented characters
\usepackage{url}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{xcolor}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{booktabs}
% \usepackage{multirow}	% spf add 09/10/2015
\usepackage{epsfig}
\usepackage{pstricks}
\usepackage{pst-node}
\usepackage{pst-grad}
\usepackage{ifthen}
\usepackage{algorithm}
\usepackage{algorithmic}	% spf add 18/09/2015
% \usepackage{algpseudocode}
\usepackage{setspace}           % For commands \singlespacing, \onehalfspacing, \doublespacing

\usepackage{epstopdf}	% spf add 22/05/2015
\usepackage{enumerate} % spf add 28/07/2015
% \usepackage[skip=0pt]{caption} % spf add 28/07/2015, reduce the space between figure and caption
%\usepackage[skip=0pt]{caption} 
\usepackage{hyperref}
\usepackage{multicol} % split the algorithm into two columns
\usepackage{enumitem} % customize a list environment, such as itemize, enumerate
\usepackage{cite} % add hyphen between multiple reference
\usepackage{appendix}


\allowdisplaybreaks
\newtheorem{Algorithm}{Algorithm}
\newtheorem{Theorem}{Theorem} % add by spf *************
\newtheorem{Lemma}{Lemma} % add by spf *************
\newtheorem{Proposition}{Proposition} % add by spf *************
\newtheorem{Proof}{Proof} % add by spf *************
\newtheorem{Definition}{Definition} % add by spf *************
% \extrafloats{100} % spf add 24/11/2015

%\newcommand{\mypar}[1]{{\bf #1.}}

\newcommand{\mypar}[1]{\bigskip\noindent {\bf #1.}}

\newcommand\mynote[1]{\mbox{}\marginpar{\footnotesize\raggedright\hspace{0pt}\color{blue}\emph{#1}}}

\newcommand{\answer}[1]{\medskip\noindent \textcolor[rgb]{0.00,0.00,1.00}{#1}\bigskip}

\newcommand{\qed}{\hfill $\Box$}

\definecolor{red}{RGB}{153,0,0}

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}} % spf add 08/10/2015
\newcommand{\mc}[3]{\multicolumn{#1}{#2}{#3}}  % spf add 28/10/2015

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


% Set separation between 2 floats, between text and last or first float, text and intext float.
\setlength{\textfloatsep}{6pt plus 2.0pt minus 2.0pt}
\setlength{\floatsep}{6pt plus 2.0pt minus 2.0pt}
\setlength{\textfloatsep}{6pt plus 2.0pt minus 2.0pt}
%\textfloatsep ? distance between floats on the top or the bottom and the text;
%\floatsep ? distance between two floats;
%\intextsep ? distance between floats inserted inside the page text (using h) and the text proper.
%The default values in the article document class with the 10pt option are:
%\textfloatsep: 20.0pt plus 2.0pt minus 4.0pt;
%\floatsep: 12.0pt plus 2.0pt minus 2.0pt;
%\intextsep: 12.0pt plus 2.0pt minus 2.0pt.

%% --------------------------------------------------------------------------
%\usepackage{subfigure}
%\usepackage{subfig}  % 23/05/2016
%\usepackage{alphalph} % % 23/05/2016
%\usepackage{epstopdf}
%\usepackage{epsfig}
%
%\usepackage{color}
%%\usepackage[cmex10]{amsmath}
%\usepackage{amsmath,bm}
%
%\usepackage{booktabs}
%\usepackage{colortbl}
%\usepackage{pstricks}
%\usepackage{pst-node}
%\usepackage{pst-grad}
%\usepackage{pst-plot}
%% \usepackage{pst-sigsys} %**************
%\usepackage{pstricks-add}
%\usepackage{ifthen}
%\usepackage{algorithm}
%\usepackage{algorithmic}	% spf add 18/09/2015
%%\usepackage{algorithmicx}   % spf add 04/09/2017
%%\usepackage{algpseudocode}
%\usepackage{float}
%\usepackage{fp}
%\usepackage{cite}
%\usepackage{enumerate}
%%\usepackage{appendix}
%%\usepackage{bibspacing} % spf add 26/05/2016
%\usepackage{multicol} % split the algorithm into two columns
%
%%\usepackage[%
%%style=numeric-comp,sorting=none,
%%sortcites=true,doi=false,url=false,
%%giveninits=true,hyperref]{biblatex}
%%\usepackage[style=numeric,firstinits=true]{biblatex}
%
%\newfloat{algorithm}{t}{lop}
%
%\newcommand{\mypar}[1]{{\bf #1.}}
%
%\newcommand\mynote[1]{\mbox{}\marginpar{\footnotesize\raggedright\hspace{0pt}\color{blue}\emph{#1}}}
%
%\theoremstyle{plain}
%\newtheorem{Definition}{Definition}
%\newtheorem{Theorem}{Theorem}
%\newtheorem{Corollary}[Theorem]{Corollary}
%\newtheorem{Proposition}{Proposition}
%\newtheorem{Lemma}{Lemma}
%\newtheorem{Assumption}{Assumption}
%
%\definecolor{red}{RGB}{153,0,0}
%\definecolor{green}{RGB}{0,153,0}			
%\definecolor{blue}{RGB}{0,0,153}
%\definecolor{darkred}{RGB}{90,0,0}
%\definecolor{darkgreen}{RGB}{0,90,0}
%\definecolor{darkblue}{RGB}{0,0,90}			
%
%\newcommand\modification[1]{\textcolor{blue}{#1}}
%\newcommand{\mc}[3]{\multicolumn{#1}{#2}{#3}}



%% 固定列宽的表格列里居中文字 fixed width and centering. 
%\usepackage{array}
%\begin{tabular}{|p{2cm}<{\centering}|}
%	居中
%\end{tabular}


% Example definitions.
% --------------------
%\def\x{{\mathbf x}}
%\def\L{{\cal L}}

\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bPsi}{\boldsymbol{\Psi}}
\newcommand{\bPhi}{\boldsymbol{\Phi}}
\newcommand{\bpsi}{\boldsymbol{\psi}}
\newcommand{\bphi}{\boldsymbol{\phi}}



% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Multimodal Image Denoising based on Coupled Dictionary Learning}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%


%% ---------------
\name{
	Pingfan Song$^{\star}$ \quad
	Miguel R.\ D.\ Rodrigues$^{\star}$  
	\thanks{
		This work was supported by the Royal Society International Exchange Scheme IE160348, by UCL Overseas Research Scholarship (UCL-ORS) and by China Scholarship Council (CSC).
	}
}
\address{				
	$^{\star}$ Department of Electronic and Electrical Engineering, University College London, UK%
	%	$^{\sharp}$ Department of Electrical Engineering, Technion -- Israel Institute of Technology, Israel \\
	%	$^{\dagger}$ School of Engineering and Physical Sciences, Heriot-Watt University, UK
}


\begin{document}
\ninept
%
\maketitle
%

\begin{abstract}
	In this paper, we propose a new multimodal image denoising approach to attenuate white Gaussian additive noise in a given image modality under the aid of a guidance image modality. 
	%
%	The proposed approach explicitly incorporates 3 priors: 1) sparsity prior: an image patch can be sparsely represented by a linear combination of a few bases/atoms from a dictionary. 2) self-similarity prior: similar image patches may be found in a natural image. 3) cross-similarity prior: different image modalities, e.g., RGB / infrared images, often share diverse attributes, such as certain edges, textures and other structure primitives. 
	%
	The proposed coupled image denoising approach consists of two stages: coupled sparse coding and reconstruction. The first stage performs joint sparse transform for multimodal images with respect to a group of learned coupled dictionaries, followed by a shrinkage operation on the sparse representations. Then, in the second stage, the shrunken representations, together with coupled dictionaries, contribute to the reconstruction of the denoised image via an inverse transform.
	%
%	The required dictionaries are learned by our coupled dictionary learning algorithm which is adapted from the block coordinate descent algorithm.	
	%
	The proposed denoising scheme demonstrates the capability to capture both the common and distinct features of different data modalities. This capability makes our approach more robust to inconsistencies between the guidance and the target images,  thereby overcoming drawbacks such as the texture copying artifacts. Experiments on real multimodal images demonstrate that the proposed approach is able to better employ guidance information to bring notable benefits in the image denoising task with respect to the state-of-the-art.

	
	
%	Our approach, which captures the similarities and disparities between different image modalities in a learned sparse feature domain in \emph{lieu} of the original image domain, consists of two phases. The coupled dictionary learning phase is used to learn a set of dictionaries that couple different image modalities in the sparse feature domain given a set of training data. In turn, the coupled denoising phase leverages such coupled dictionaries to construct a clean version of the noisy target image given another related image modality. One of the merits of our sparsity-driven approach relates to the fact that it overcomes drawbacks such as the texture copying artifacts commonly resulting from inconsistency between the guidance and target images. Experiments on both synthetic data and real multimodal images demonstrate that incorporating appropriate guidance information via joint sparse representation induced by coupled dictionary learning brings notable benefits in the denoising task with respect to the state-of-the-art.

\end{abstract}
	

% Note that keywords are not normally used for peerreview papers.
\begin{keywords}
	Multimodal image denosing, coupled dictionary learning, joint sparse representation, guidance information
\end{keywords}



\section{Introduction}
\label{sec:intro}

\vspace{-0.2cm}

Image Denoising is a type of techniques that attenuate the noise in the corrupted images and at the same time preserve the details faithfully. Serving as a fundamental image processing operation, image denoising plays a critical role in various application scenarios such as object detection, image recognition and remote sensing\cite{shao2014heuristic}.
%
Typical image denoising approaches that focus on single modality images have been thoroughly investigated, which can be categorized into two classes according to the image representation: spatial domain based local and nonlocal filters\cite{zhu2010automatic,bouboulis2010adaptive, buades2005non,nguyen2017bounded,talebi2014global,romano2015boosting}; and (predefined or learned) transform domain based approaches~\cite{dabov2007image, zhang2010two, chatterjee2012patch, elad2006image, mairal2009non}.

However, in many practical application scenarios, it is commonly noticed that a certain scene is often imaged using various sensors that yield different image modalities. For example, in remote sensing domain, it is typical to have various image modalities of earth observations, such as a panchromatic band version, a multispectral bands version, and an infrared (IR) band version~\cite{gomez2015multimodal,loncan2015hyperspectral}. These different bands often exhibit similar textures, edges, corners, boundaries, or other salient features. 
%In medical imaging domain, multi-contrast scans for the same underlying anatomy~\cite{cherry2006multimodality,townsend2008multimodality,catana2013pet}, such as simultaneous PET/MRI scans\footnote{positron emission tomography (PET) and magnetic resonance imaging (MRI)}, MRI T1/T2 scans, also indicate strong correlation. 
In colorization\cite{levin2004colorization} task, the output image has both chrominance channels and luminance channel which share consistent edges. These scenarios call for approaches that can capitalize on the availability of multiple image modalities of the same scene to address the image denoising task.
%that is, purify the noisy image modality with the aid of the clean images of a different modality. 
%
A variety of joint image filtering approaches have been proposed to capitalize on the availability of additional \emph{guidance} images, also referred to as \emph{side information}\cite{renna2016classification,mota2017compressed}, to aid the processing of target modalities\cite{kopf2007joint,he2013guided,ham2017robust,li2016deep,shen2015multispectral,zhang2014rolling}. 
The basic idea behind these methods is that the structural details of the guidance image can be transferred to the target image. However, these methods tend to introduce notable texture-copying artifacts, i.e. erroneous structure details that are not originally present in the target image because such methods typically fail to distinguish similarities and disparities between the different image modalities. 


%For example, in the medical imaging domain, simultaneous PET-MRI scanning provides both positron emission tomography (PET) and magnetic resonance imaging (MRI) data for the same underlying anatomy~\cite{catana2013pet}. 
%For instance, A bundle of commercial satellite data packages (e.g., Landsat 7, SPOT, GeoEye) is commonly composed of several images with different modalities and different resolution, e.g., one 15m (high-resolution) resolution panchromatic band image, six 30m resolution (medium-resolution) multispectral bands images, and one 60m resolution (low-resolution) thermal infrared band image. 
%(PET) and magnetic resonance imaging (MRI) data for the same underlying anatomy~\cite{cherry2006multimodality,townsend2008multimodality,catana2013pet}
%\cite{gomez2015multimodal,fang2014spectral,romero2015unsupervised,chen2014deep,shekhar2014joint,loncan2015hyperspectral}
%
%\textcolor{red}{Another compelling application of multimodal data processing is the RGB-depth image denoising. As modern depth camera has very limited lateral resolution while intensity images tend to have high-resolution, it is appealing to super-resolve depth images with high-resolution intensity images as the prior images to provide auxiliary information\cite{he2013guided, ham2017robust}. }

%Numerous contributions have been made to address this problem from diverse points of view, such as "statistical estimators, spatial adaptive filters, transform-domain methods, morphological analysis". In recent years, driven by sparse representation, sparse denoising has drawn a lot of attention. 
%
%At first, sparse representation based on various wavelet transform and soft shrinkage lead to inspiring denoising performance. In parallel, matching pursuit\cite{mallat1993matching} and the basis pursuit denoising\cite{chen2001atomic} propose to address the image denoising problem via sparse decomposition over redundant dictionaries. Follow this line, learned redundant dictionaries are applied to solve this problem as they are more adaptive to the data, thus promoting the sparsity. 

\begin{figure}[t]
	\centering
	\includegraphics[width= 9cm]{CoupledDenoising.pdf}  
	\caption{Proposed coupled image denoising scheme.}
	%		encompassing a training stage and a testing stage.}
	\label{Fig:Diagram}
\end{figure}



This paper proposes a new effective multimodal image denoising approach based on coupled dictionary learning. With joint sparse representation induced by coupled dictionaries as the bridge, our approach incorporates a given clean guidance image as the side information to aid the denoising of the target image of interested modality. In particular, the proposed design has the ability to take into account both similarities and disparities between target and guidance images in order to deliver superior denoising performance. Further, we also incorporate self-similarity prior in our algorithm to enhance the denoising performance.




%In comparison with state-of-the-art approaches~\cite{kopf2007joint,he2013guided,ham2017robust,li2016deep,shen2015multispectral}, our approach can better model the common and distinct features of the different data modalities. This capability makes our approach more robust to inconsistencies between the guidance and the target images, as both the target noisy image and the guidance image are taken into account during the estimation of the target clean image, instead of unilaterally transferring the structure details from the clean guidance image. Therefore, one of the merits of our sparsity-driven approach is that it overcomes drawbacks such as the texture copying artifacts commonly resulting from inconsistency between the guidance and target images. In addition, the proposed approach leverages explicit regularization to formulate a task-specific model, with more flexibility than implicit regularization\cite{kopf2007joint,he2013guided}. Furthermore, our approach is a learning-based denoising approach. Therefore, the priors used in our approach are learned from a training dataset rather than being hand-crafted and thus adapt to the target modality and guidance modality. However, the proposed approach requires much less training data, computing resources and training time than the deep-learning-based approach~\cite{li2016deep}. 
%Overall, by running the coupled dictionary learning algorithm followed by the coupled image denoising algorithm, the proposed approach achieves better performance in the multimodal image denoising task than state-of-the-art methods.



%\mypar{Contributions}
%Our contributions are as follows:
%\begin{itemize}
%	\item
%	\emph{Data Model}: We establish a patch-based data model that jointly represent a pair of multimodal image patches as the sparse linear combination of atoms from a group of coupled dictionaries. This model captures the similarities and disparities between different image modalities in their sparse feature domains in lieu of the original image domain. 
%%	In addition, the contribution of the guidance image can be flexibly balanced by a weight parameter. 
%%	In this way, we can better distinguish their similarity and take full advantage of the guidance information for denoising.
%	
%	\item
%	\emph{Coupled Dictionary Learning}: This algorithm learns the coupled dictionaries along with the joint sparse representations of the different image modalities from a set of clean or noisy image patches. 
%%	The proposed algorithm adapt the block coordinate descent algorithm\cite{mairal2010online} to implement the learning task.
%	
%	\item
%	\emph{Coupled Image Denoising}: This algorithm is to obtain the joint sparse representations of the target noisy image and the guidance image, perform shrinkage on the coefficients, and estimates a clean version of the target image. Our advanced version can also exploit the self-similarity via group-sparsity regularization.
%	
%%	\item
%%	A series of experiments are conducted on real multimodal images and demonstrate the notable advantage of our design in the joint image denoising task. In particular, our approach is able to significantly attenuate the noise, reveal the finest details and, at the same time, preserve the salient sharp features. The performance is also confirmed by significant PSNR and RMSE gains over other state-of-the-art approaches such as~\cite{kopf2007joint,he2013guided,ham2017robust,li2016deep,shen2015multispectral}.
%
%%	Joint Bilateral Filtering (JBF)\cite{kopf2007joint}, Guided image Filtering (GF)\cite{he2013guided}, Static/Dynamic Filtering (SDF)\cite{ham2017robust}, and Joint Filtering via optimizing a Scale Map (JFSM)\cite{shen2015multispectral}) and Deep Joint image Filtering (DJF)\cite{li2016deep}.
%\end{itemize}

%\mypar{Organization}
%%\subsection{Organization of the paper}
%The remainder of this paper is organized as follows. We review related work in Section \ref{sec:RelatedWork} and propose our multimodal image denoising framework, including the data model, the multimodal image denoising algorithm and the coupled dictionary learning algorithm in Section \ref{sec:SIMIS}. Section \ref{sec:Experiments} is devoted to various practical experiments. We summarize the paper in Section V.

%Section \ref{sec:Experiments} is devoted to various simulation and practical experiments, which demonstrate the favourable efficacy of side information incorporated by coupled dictionary learning into the image denoising task. We conclude in the last section and consider some perspectives.


%\subsection{Notations}
%In this paper, we adopt the following notations: denote by $\mathbf{X}^l$ and $\mathbf{X}^h$, respectively, the noisy and clean image (patches). Denote by $\mathbf{Y}$ the corresponding side information.
%Denote by $[\boldsymbol{\Psi}_{c}^l,\boldsymbol{\Psi}^{ns}]$ a pair of noisy dictionaries for $\mathbf{X}^l$. Denote by $[\boldsymbol{\Psi}_{c},\boldsymbol{\Psi}]$ a pair of clean dictionaries for $\mathbf{X}^h$. Dnote by $[\boldsymbol{\Phi}_{c},\boldsymbol{\Phi}]$ a pair of clean dictionaries for $\mathbf{Y}$. We denote the $\ell_0$ norm by $\| \cdot \|_0$,  the $\ell_1$ norm by $\| \cdot \|_1$ and the Frobenius norm by $\| \cdot \|_F$. Matrices, column vectors and scalars are denotes by boldface uppercase, boldface lowercase, and plain lowercase letters.

%We use $\boldsymbol{\Psi}$ and $\boldsymbol{\Phi}$ denote the dictionaries for modality $\mathbf{X}$ and $\mathbf{Y}$, respectively. Specifically, we use $\boldsymbol{\Psi}_{c}$ and $\boldsymbol{\Psi}$ to denote two dictionaries for modality $\mathbf{X}$ to describe two types of characteristics. Similarly, $\boldsymbol{\Phi}_{c}$ and $\boldsymbol{\Phi}$ denote two dictionaries for $\mathbf{Y}$. 
%
%The conjugate/ Hermitian transpose, transpose, rank and determinant operators are denoted as $(\cdot)^\text{H}$, $(\cdot)^\text{T}$, rank($\bullet$) and $| \cdot |$, respectively. $\mathbf{I}_N \in \mathbb{R}^{N \times N}$ denotes the identity matrix. 


\section{Coupled Image Denoising}
\label{sec:SIMIS}

\vspace{-0.2cm}

In this section, We introduce the proposed multimodal data model, the coupled image denoising and the coupled dictionary learning strategy.
%

%\subsection{Uni-modal Data Model}
%For a given set of data $\mathbf{X} \in \mathbb{R}^{N \times T}$  ( composed of $T$ signals $\mathbf{x}_i \in \mathbb{R}^N$ , $i = 1, \cdots T$) of one single modality, we assume that each signal $\mathbf{x}_i$ admits a sparse representation $\bz_i \in \mathbb{R}^K$ with respect to the dictionary $\Psi$, where each column is referred to as an atom. Accordingly, the set of data is decomposed as
%%Sparse codes matrix $\bZ = [\bz_1, \ldots, \bz_T] \in \mathbb{R}^{K \times T}$.
%\begin{equation} \label{Eq:UniDataModelX}
%\begin{split}
%%	\by_i &= \bPsi \bz_i , \quad i = 1, \cdots T.
%%	\\
%\bX &= \boldsymbol{\Psi} \bZ .
%\end{split}
%\end{equation} 
%where the coefficient matrix $\bZ = [\bz_1, \ldots, \bz_T] \in \mathbb{R}^{K \times T}$ consists of all the sparse codes.

\vspace{-0.2cm}

\subsection{Multi-modal Data Model}

%By capitalizing on this model, we propose a novel coupled image denoising scheme. Assuming that the coupled dictionaries associated with the model in \eqref{Eq:SparseModelX}, \eqref{Eq:SparseModelY} and \eqref{Eq:SparseModelX_low} have been learned, and we are now 

Given a vectorized noisy image $\mathbf{X}^{ns} \in \mathbb{R}^{N}$ of one modality and a corresponding registered clean vectorized guidance image $\mathbf{Y} \in \mathbb{R}^{N}$ of different modality as side information. We first extract (overlapping) image patch pairs from this pair of multimodal images. In particular, let $\mathbf{x}^{ns}_{i} = \mathbf{R}_i \mathbf{X}^{ns} \in \mathbb{R}^{n}$ denote the $i$-th noisy image patch from $\mathbf{X}^{ns}$ and let $\mathbf{y}_{i} = \mathbf{R}_i \mathbf{Y} \in \mathbb{R}^{n}$ denote the corresponding $i$-th clean guidance image patch extracted from $\mathbf{Y}$, where the matrix $\mathbf{R}_i$ is an $n \times N$ binary matrix that extracts the $i$-th patch from the image. Then, we propose a data model to capture the relationship -- including similarities and disparities -- between the two different modalities as follows:
%Accordingly, the matrix $\mathbf{R}_i^T$ is its transposed version that places the $i$-th patch at its proper position in the image.
\begin{align}
\mathbf{x}^{ns}_i 
&= 
\boldsymbol{\Psi}_{c} \, \mathbf{z}_i + \boldsymbol{\Psi} \, \mathbf{u}_i + \epsilon \,,
\label{Eq:SparseModelX_Noise}
\\
\mathbf{y}_i
&= 
\boldsymbol{\Phi}_{c} \, \mathbf{z}_i + \boldsymbol{\Phi} \, \mathbf{v}_i \,,
%\gamma \cdot \mathbf{y}_i
%&= \gamma \cdot \mathbf{R}_i \mathbf{Y}
%= \boldsymbol{\Phi}_{c} \, \mathbf{z}_i + \boldsymbol{\Phi} \, \mathbf{v}_i \,,
\label{Eq:SparseModelY_Noise}
\end{align}
where sparse representation $\mathbf{z}_i \in \mathbb{R}^{k}$ is common to both modalities, $\mathbf{u}_i \in \mathbb{R}^{k}$ is specific to modality $\mathbf{x}$, while $\mathbf{v}_i \in \mathbb{R}^{k}$ is specific to modality $\mathbf{y}$. In turn, $\boldsymbol{\Psi}_{c} $ and $\boldsymbol{\Phi}_{c} \in \mathbb{R}^{n \times k}$ are a pair of dictionaries associated with the common sparse representation $\mathbf{z}_i$, whereas $\boldsymbol{\Psi}$ and $\boldsymbol{\Phi} \in \mathbb{R}^{n \times k}$ are dictionaries associated with the specific sparse representations $\mathbf{u}_i$ and $\mathbf{v}_i$, respectively. 
%%
Note that the common sparse representation $\mathbf{z}_i$ connects the patches of the two different modalities. The disparities between modalities $\mathbf{x}$ and $\mathbf{y}$ are distinguished by the sparse representations $\mathbf{u}_i$ and $\mathbf{v}_i$, respectively. 
%
Parameter $\epsilon \in \mathbb{R}^n$ denotes additive zero-mean and homogeneous white Gaussian noise with the standard deviation $\sigma$.

%$\boldsymbol{\Psi}_{c} = [\psi_{c1}, \cdots, \psi_{ck}] \in \mathbb{R}^{n \times k}$ and $\boldsymbol{\Phi}_{c} = [\phi_{c1}, \cdots, \phi_{ck}] \in \mathbb{R}^{n \times k}$ $\boldsymbol{\Psi} = [\psi_{1}, \cdots, \psi_{k}] \in \mathbb{R}^{n \times k}$ and $\boldsymbol{\Phi} = [\phi_{1}, \cdots, \phi_{k}] \in \mathbb{R}^{n \times k}$

\vspace{-0.2cm}

\subsection{Coupled Image Denoising}
Assume that the coupled dictionaries $\boldsymbol{\Psi}_{c} $, $\boldsymbol{\Phi}_{c}, \boldsymbol{\Psi}, \boldsymbol{\Phi}$ have been learned, then, based on the model \eqref{Eq:SparseModelX_Noise} and \eqref{Eq:SparseModelY_Noise}, our coupled image denoising problem is addressed in two steps: coupled sparse coding and reconstruction of the denoised image.
%
%%% penalty version for images
%\begin{multline} \label{Eq:Denoise_Whole}
%%\{\mathbf{\hat{x}}_{}, \mathbf{z},\mathbf{u},\mathbf{v} \} = 
%\underset{\mathbf{X}_{}, \mathbf{z}_i,\mathbf{u}_i,\mathbf{v}_i}{\text{minimize}} \;
%\mu \left\| \mathbf{X}_{} - \mathbf{X}^{ns}_{} \right\|_2^2
%\\
%+
%\sum_i
%\left\|
%\begin{bmatrix} 
%\mathbf{x}_{i} \\ \mathbf{y}_{i}
%%\mathbf{x}_{i} \\ \gamma \, \mathbf{y}_{i}
%\end{bmatrix}
%-
%\begin{bmatrix}
%\boldsymbol{\Psi}_{c} & \boldsymbol{\Psi} & \mathbf{0} \\
%\boldsymbol{\Phi}_{c} & \mathbf{0} & \boldsymbol{\Phi} \\
%\end{bmatrix}
%%
%\begin{bmatrix}
%\mathbf{z}_i \\
%\mathbf{u}_i \\
%\mathbf{v}_i \\
%\end{bmatrix}
%\right\|_2^2
%+
%\lambda
%\left\| 
%\begin{bmatrix}
%\mathbf{z}_i \\
%\mathbf{u}_i \\
%\mathbf{v}_i \\
%\end{bmatrix} 
%\right\|_0
%\end{multline}
%\noindent
%where the first term with $\ell_2$ norm enforces the proximity between the noisy image $\mathbf{X}^{ns}$ and its denoised (and unknown) version $\mathbf{X}$.\footnote{
%	This term can also be written as a constraint $\left\| \mathbf{X}_{} - \mathbf{X}^{ns}_{} \right\|_2^2 \leq C \cdot \sigma^2$ and leads to an equivalent version.
%}
%The third term with $\ell_0$-penalty serves as the sparsity-inducing regularizer. It can also be relaxed to $\ell_1$-penalty.\footnote{
%	We denote by $\| x\|_0$ the number of nonzero elements of the vector $x$. Note, this $\ell_0$ sparsity measure is a pseudo norm as it does not preserve the homogeneity property. We denote by $\| x\|_1:= \sum_{i=1}^{m} |x[i]| $ the sum of absolute value of each element.
%	Generally, the $\ell_p$ norm of a vector $x \in \mathbb{R}^m$ is defined, for $p>1$, by $\| x \|_p := \left( \sum_{i=1}^{m} |x[i]|^p \right)^{1/p}$. Following the tradition, the $\ell_p$ pseudo norm for $p<1$ is defined by $\| x \|_p := \sum_{i=1}^{m} |x[i]|^p$.
%}
%The second quadratic "data-fitting" term ensures that each pair of multimodal image patches are well approximated by their sparse representations with respect to the coupled dictionaries.


%%% Coupled denoising algorithm with standard sparsity
%\begin{algorithm}[t] %[ht]%[h]
%	\caption{Coupled Image Denoising}
%	\label{Alg:CoupledDenoise}
%	\begin{algorithmic}[1]
%		%		\renewcommand{\algorithmicrequire}{\textbf{Task:}}
%		%		\REQUIRE
%		%		\STATE
%		%		Construct the best coupled dictionaries to sparsely represent the dataset $\mathbf{X}$ and $\mathbf{Y}$ with the sparse representations $\mathbf{Z}$, $\mathbf{U}$ and $\mathbf{V}$, by solving the optimization~\eqref{Eq:CoupledDL}.
%		%		
%		\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%		\renewcommand{\algorithmicensure}{\textbf{Output:}}
%		\REQUIRE %\Require
%		%		\STATE 
%		Corrupt image $\mathbf{X}^{ns}$ and side information $\mathbf{Y}$.
%		Learned coupled dictionaries $[\boldsymbol{\Psi}_{c},\boldsymbol{\Psi}]$ and $[\boldsymbol{\Phi}_{c},\boldsymbol{\Phi}]$.
%		
%		\ENSURE %\Ensure
%		%		\STATE
%		Estimated clean image patch $\mathbf{\hat{X}}$.
%		% % 		
%		\renewcommand{\algorithmicrequire}{\textbf{Operations:}}
%		\REQUIRE
%		\STATE 
%		{\bf Step 1}. 
%		Construct image patch pairs $\{(\mathbf{x}_i, \mathbf{y}_i) \}$ and solve the problem~\eqref{Eq:BPDN} using Orthogonal Matching Pursuit algorithm\cite{tropp2007signal} to obtain the sparse codes
%%		Use off-the-shelf sparse coding algorithms to solve the problem~\eqref{Eq:Lasso} or problem~\eqref{Eq:BPDN} to obtain the sparse codes
%		$\mathbf{z}_i$, $\mathbf{u}_i$ and $\mathbf{v}_i$.
%		\STATE 
%		{\bf Step 2}. 
%		Reconstruct $\mathbf{\hat{X}}$ as in~\eqref{Eq:Denoise_Update2}.
%		%		\STATE
%		%		Return the estimation $\mathbf{x}_{ts}$.
%	\end{algorithmic}
%\end{algorithm}	
%
%
%%% Coupled denoising algorithm with grouped-sparsity
%\begin{algorithm}[t] %[ht]%[h]
%	\caption{Coupled Image Denoising with Group Sparsity}
%	\label{Alg:CoupledDenoise_GroupSparsity}
%	\begin{algorithmic}[1]
%		%		\renewcommand{\algorithmicrequire}{\textbf{Task:}}
%		%		\REQUIRE
%		%		\STATE
%		%		Construct the best coupled dictionaries to sparsely represent the dataset $\mathbf{X}$ and $\mathbf{Y}$ with the sparse representations $\mathbf{Z}$, $\mathbf{U}$ and $\mathbf{V}$, by solving the optimization~\eqref{Eq:CoupledDL}.
%		%		
%		\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%		\renewcommand{\algorithmicensure}{\textbf{Output:}}
%		\REQUIRE %\Require
%		%		\STATE 
%		Corrupt image $\mathbf{X}^{ns}$ and side information $\mathbf{Y}$.
%		Learned coupled dictionaries $[\boldsymbol{\Psi}_{c},\boldsymbol{\Psi}]$ and $[\boldsymbol{\Phi}_{c},\boldsymbol{\Phi}]$.
%		
%		\ENSURE %\Ensure
%		%		\STATE
%		Estimated clean image patch $\mathbf{\hat{X}}$.
%		% % 		
%		\renewcommand{\algorithmicrequire}{\textbf{Operations:}}
%		\REQUIRE
%		\STATE
%		{\bf Step 1}. 
%		Construct image patch pairs $\{(\mathbf{x}_i, \mathbf{y}_i) \}$.
%		\STATE 
%		{\bf Step 2}. 
%		Using hierarchical clustering or other clustering techniques to construct groups of similar patch pairs.
%		\STATE 
%		{\bf Step 3}. 
%		Solve the grouped-sparsity regularized sparse coding problem~\eqref{Eq:BPDN_GroupSparsity} using Simultaneous Orthogonal Matching Pursuit algorithm\cite{tropp2006algorithms} to obtain the sparse codes
%		$\mathbf{z}_i$, $\mathbf{u}_i$ and $\mathbf{v}_i$.
%		\STATE 
%		{\bf Step 4}. 
%		Reconstruct $\mathbf{\hat{X}}$ as in~\eqref{Eq:Denoise_Update2}.
%		%		\STATE
%		%		Return the estimation $\mathbf{x}_{ts}$.
%	\end{algorithmic}
%\end{algorithm}	


%
%% adapted block coordinate descent for coupled dictionary learning
\begin{algorithm}[t] %[ht]%[h]
	\caption{Coupled Dictionary Learning}
	\label{Alg:CoupledBCD}
%	\begin{multicols}{2}  
		\begin{algorithmic}[1]	
			\renewcommand{\algorithmicrequire}{\textbf{Input:}}
			\renewcommand{\algorithmicensure}{\textbf{Output:}}
			\REQUIRE 
			Dataset 
			$\{(\mathbf{x}_i, \mathbf{y}_i) \}_{i=1}^T$, initial dictionaries $\boldsymbol{\Psi}_{c}, \boldsymbol{\Phi}_{c},\boldsymbol{\Psi}, \boldsymbol{\Phi} \in \mathbb{R}^{n \times k}$ (randomly selected patches), regularization parameter $\lambda$.
			
			%		\ENSURE
			%		Learned dictionaries.
			
			%		\renewcommand{\algorithmicrequire}{\textbf{Initialization:}}
			%		\REQUIRE
			%		$\mathbf{D} \leftarrow \mathbf{D}_0$, $\mathbf{A} \leftarrow \mathbf{A}_0$.
			% % 
			\renewcommand{\algorithmicrequire}{\textbf{Optimization:}}
			\REQUIRE		
			%		\FOR{$t= 1, \cdots, T$} 
			\STATE
			\textbf{Common dictionary training.} Iterating between a) and b).
%			 by iterating between step a) and b)
			\STATE
			a) Update the sparse codes $\mathbf{Z}$ as :
			\begin{equation*}
			\mathbf{z}^j \leftarrow S_\lambda\left( \mathbf{z}^j + \frac{1}{\|\mathbf{d}_j \|_2^2} \mathbf{d}_j^T (\widetilde{\mathbf{X}} - \mathbf{D} \mathbf{Z}) \right) ; \forall j=1,\cdots, k
			\end{equation*}
			where 
			$\mathbf{z}^j$ denotes the $j$-th row of $\mathbf{Z}$, $\mathbf{d}_j$ denotes the $j$-th atom of dictionary $\mathbf{D}	= 
			\begin{bmatrix}
			\boldsymbol{\Psi}_{c}  \\
			\boldsymbol{\Phi}_{c}  \\
			\end{bmatrix} $,
			$\widetilde{\mathbf{X}} = 
			\begin{bmatrix} 
			\mathbf{X} - \boldsymbol{\Psi} \mathbf{U} \\ 
			\mathbf{Y} -  \boldsymbol{\Phi} \mathbf{V}
			%			\gamma \, \mathbf{Y} -  \boldsymbol{\Phi} \mathbf{V}
			\end{bmatrix} $.
			$S_\lambda(\cdot)$ denotes the element-wise soft-thresholding operator
			$S_\lambda(\alpha) = \text{sign}(\alpha) \max(|\alpha|-\lambda, 0)$.
			
			\STATE
			b) Update the dictionary $\boldsymbol{\Psi}_{c}$ and $\boldsymbol{\Phi}_{c}$ as :
			\begin{align*}
			\mathbf{d}_j &\leftarrow \frac{1}{\mathbf{z}^j {\mathbf{z}^j}^T} (\widetilde{\mathbf{X}} - \mathbf{D}\mathbf{Z}) {\mathbf{z}^j}^T +
			\begin{bmatrix}
			\boldsymbol{\psi}_{cj}  \\
			\boldsymbol{\phi}_{cj}  \\
			\end{bmatrix}
			\\
			%		\end{align*}
			%		\begin{align*}
			\begin{bmatrix}
			\boldsymbol{\psi}_{cj}  \\
			\boldsymbol{\phi}_{cj}  \\
			\end{bmatrix}
			&\leftarrow \frac{\mathbf{d}_j}{\max(\|\mathbf{d}_j \|_2, 1)} 
			; \forall j=1,\cdots, k.
			\end{align*}
			%		\ENDFOR
			
			%		\FOR{$t= 1, \cdots, T$} 
			\STATE
			\textbf{Unique dictionary training.} Iterating between a) and b).
%			\textbf{Train the unique dictionary $\boldsymbol{\Psi}$ by iterating between step a) and b) until convergence}
			\STATE
			a) Update the sparse codes $\mathbf{U}$ as :
			\begin{equation*}
			\mathbf{u}^j \leftarrow S_\lambda\left( \mathbf{u}^j + \frac{1}{\|\boldsymbol{\psi}_j \|_2^2} \boldsymbol{\psi}_j^T (\mathbf{X} - \boldsymbol{\Psi}_c \mathbf{Z} - \boldsymbol{\Psi} \mathbf{U}) \right) ; \forall j
			\end{equation*}
			where $\mathbf{u}^j$ denotes the $j$-th row of $\mathbf{U}$, $\boldsymbol{\psi}_j$ denotes the $j$-th atom of dictionary $\boldsymbol{\Psi}$,
			
			\STATE
			b) Update the dictionary $\boldsymbol{\Psi}$ as :
			\begin{align*}
			\boldsymbol{\psi}_j &\leftarrow \frac{1}{\mathbf{u}^j {\mathbf{u}^j}^T} (\mathbf{X} - \boldsymbol{\Psi}_c \mathbf{Z} - \boldsymbol{\Psi} \mathbf{U}) {\mathbf{u}^j}^T + \boldsymbol{\psi}_j 
			\\
			\boldsymbol{\psi}_j
			&\leftarrow \frac{\boldsymbol{\psi}_j}{\max(\|\boldsymbol{\psi}_j \|_2, 1)} 
			; \forall j=1,\cdots, k.
			\end{align*}
			%		\ENDFOR
			
			\STATE
			\textrm{Unique dictionary training for $\boldsymbol{\Phi}$ is similar to $\boldsymbol{\Psi}$ but with $\mathbf{u}^j$, $\boldsymbol{\psi}_j$, $\mathbf{X}$, $\boldsymbol{\Psi}_c$ replaced by $\mathbf{v}^j$, $\boldsymbol{\phi}_j$}, $\mathbf{Y}$, $\boldsymbol{\Phi}_c$.
			%		\FOR{$t= 1, \cdots, T$} 
			%			\STATE
			%			\textbf{Train the unique dictionary $\boldsymbol{\Phi}$ by iterating between step a) and b) until convergence}
			%			\STATE
			%			a) Update the sparse codes $\mathbf{V}$ as :
			%			\begin{equation*}
			%			\mathbf{v}^j \leftarrow S_\lambda\left( \mathbf{v}^j + \frac{1}{\|\boldsymbol{\phi}_j \|_2^2} \boldsymbol{\phi}_j^T (\mathbf{Y} - \boldsymbol{\Phi}_c \mathbf{Z} - \boldsymbol{\Phi} \mathbf{V}) \right) ; \forall j
			%			\end{equation*}
			%			where $\mathbf{v}^j$ denotes the $j$-th row of $\mathbf{V}$, $\boldsymbol{\phi}_j$ denotes the $j$-th atom of dictionary $\boldsymbol{\Phi}$,
			%			
			%			\STATE
			%			b) Update the dictionary $\boldsymbol{\Phi}$ as :
			%			\begin{align*}
			%			\boldsymbol{\phi}_j &\leftarrow \frac{1}{\mathbf{v}^j {\mathbf{v}^j}^T} (\mathbf{Y} - \boldsymbol{\Phi}_c \mathbf{Z} - \boldsymbol{\Phi} \mathbf{V}) {\mathbf{v}^j}^T + \boldsymbol{\phi}_j 
			%			\\
			%			\boldsymbol{\phi}_j
			%			&\leftarrow \frac{\boldsymbol{\phi}_j}{\max(\|\boldsymbol{\phi}_j \|_2, 1)} 
			%			; \forall j=1,\cdots, k.
			%			\end{align*}
			%			%		\ENDFOR
			\STATE
			\textbf{Return} dictionaries $\boldsymbol{\Psi}_{c}, \boldsymbol{\Phi}_{c},\boldsymbol{\Psi}, \boldsymbol{\Phi}$.
		\end{algorithmic}
%	\end{multicols}
\end{algorithm}	
%%\clearpage


\mypar{Step 1: Coupled sparse coding}
In this stage, we address a series of parallel sparse coding problems formulated as
%%
%\begin{equation} \label{Eq:Lasso}
%\begin{array}{cl}
%\underset{\mathbf{z}_i,\mathbf{u}_i,\mathbf{v}_i}{\text{min}} 
%&  \! \! \! \! \!
%\left\|
%\begin{bmatrix} 
%\mathbf{x}^{ns}_{i} \\ \mathbf{y}_{i}
%%\mathbf{x}^{ns}_{i} \\ \gamma \, \mathbf{y}_{i}
%\end{bmatrix}
%-
%\begin{bmatrix}
%\boldsymbol{\Psi}_{c} & \boldsymbol{\Psi} & \mathbf{0} \\
%\boldsymbol{\Phi}_{c} & \mathbf{0} & \boldsymbol{\Phi} \\
%\end{bmatrix}
%%
%\begin{bmatrix}
%\mathbf{z}_i \\
%\mathbf{u}_i \\
%\mathbf{v}_i \\
%\end{bmatrix}
%\right\|_2^2
%+
%\lambda
%\left\| 
%\begin{bmatrix}
%\mathbf{z}_i \\
%\mathbf{u}_i \\
%\mathbf{v}_i \\
%\end{bmatrix} 
%\right\|_0
%\end{array}
%\end{equation}
%
%\noindent
%According to the theory of the Lagrangian multipliers, Problem \eqref{Eq:Lasso} can also be written in an equivalent constrained form
%% constraint version 
\begin{equation} \label{Eq:BPDN}
\small %\footnotesize %\scriptsize
\begin{array}{cl}
\underset{\mathbf{z}_i, \mathbf{u}_i,\mathbf{v}_i}{\min} 
\,
%& \! \! \! \! \!
\left\| 
\begin{bmatrix}
\mathbf{z}_i \\
\mathbf{u}_i \\
\mathbf{v}_i \\
\end{bmatrix} 
\right\|_0
\,
%\\
\textrm{s.t. }
\;
%&  \! \! \! \! \!
\left\|
\begin{bmatrix} 
\mathbf{x}^{ns}_{i} \\ \mathbf{y}_{i}
%\mathbf{x}^{ns}_{i} \\ \gamma \, \mathbf{y}_{i}
\end{bmatrix}
-
\begin{bmatrix}
\boldsymbol{\Psi}_{c} & \boldsymbol{\Psi} & \mathbf{0} \\
\boldsymbol{\Phi}_{c} & \mathbf{0} & \boldsymbol{\Phi} \\
\end{bmatrix}
%
\begin{bmatrix}
\mathbf{z}_i \\
\mathbf{u}_i \\
\mathbf{v}_i \\
\end{bmatrix}
\right\|_2^2
\leq C \, \sigma^2
\end{array}
\end{equation}
where the standard deviation $\sigma$ represents the noise level, and $C$ is a constant. The objective with $\ell_0$-penalty serves as the sparsity-inducing regularizer.\footnote{
	We denote by $\| x\|_0$ the number of nonzero elements of the vector $x$. Note, this $\ell_0$ sparsity measure is a pseudo norm as it does not preserve the homogeneity property. 
%	It can also be relaxed to $\ell_1$-penalty. We denote by $\| x\|_1:= \sum_{i=1}^{m} |x[i]| $ the sum of absolute value of each element.
	%	Generally, the $\ell_p$ norm of a vector $x \in \mathbb{R}^m$ is defined, for $p>1$, by $\| x \|_p := \left( \sum_{i=1}^{m} |x[i]|^p \right)^{1/p}$. Following the tradition, the $\ell_p$ pseudo norm for $p<1$ is defined by $\| x \|_p := \sum_{i=1}^{m} |x[i]|^p$.
}
The quadratic "data-fitting" constraint ensures that each pair of multimodal image patches are well approximated by their sparse representations with respect to the coupled dictionaries. 
%
In our case, we stick to $\ell_0$ penalty for the sparse coding as it usually leads to better denoising performance than $\ell_1$ penalty, which is also observed in \cite{mairal2009non,mairal2014sparse}.
%\footnote{Sometimes, the guidance information will be multiplied with a weight parameter $\gamma$, leading to $\gamma \, \mathbf{y}_{i}$ as a substitute for the original version $\mathbf{y}_{i}$ in both coupled image denoising phase and coupled dictionary learning phase simultaneously. The weight $\gamma$ is able to balance the contribution of target modality and guidance modality information to the coupled sparse coding and coupled dictionary learning process. 
	%Specifically, for the coupled sparse coding task, $\gamma$ has an impact on the selection of common atom pairs from $\boldsymbol{\Psi}_{c}$ and $\boldsymbol{\Phi}_{c}$, when the coherence, i.e. the inner product, between data pair and atom pair is employed for the atom selection criterion. For example, if setting $\gamma \ll 1$, the target image modality $\mathbf{x}^{ns}$ will determine the sparse coding results, which reduces to conventional standard sparse coding. In contrast, assume that we set $\gamma \gg 1$, then the amplified guidance information $\gamma \mathbf{y}$ overwhelms $\mathbf{x}^{ns}$. Thus, the atom pair selection mainly depends on the inner product between $\gamma \mathbf{y}$ and atoms from $\boldsymbol{\Phi}_{c}$. Therefore, the guidance information plays a decisive role in the sparse coding. 
	%If the guidance information exhibit significant similarity as the signal of interest, a larger weight is preferable. Otherwise, a smaller weight should be selected for less similar modalities. 
%}

%Note that, compared with conventional standard sparse coding problems involving only the target image modality, our formulations~\eqref{Eq:BPDN} also integrate the side information $\mathbf{y}_{i}$ into the sparse coding task. Since the increase in the amount of available information is akin to the increase of the number of measurements in a Compressive Sensing scenario~\cite{donoho2006compressed,candes2006robust}, one can expect to obtain a more accurate estimate of the sparse codes. This accounts for the key advantage of our coupled image denoising. 


%In our case, we stick to $\ell_0$ penalty for the sparse coding as it usually leads to better denoising performance than $\ell_1$ penalty, which is also observed in \cite{mairal2009non,mairal2014sparse}.}

\vspace{-0.2cm}

\mypar{Step 2: Reconstruction}
Given the sparse codes $\mathbf{z}_i,\mathbf{u}_i$, the denoised image of interested modality can be estimated by solving \eqref{Eq:Denoise_Update}, which leads to a closed form solution \eqref{Eq:Denoise_Update2}.
%% penalty version 
\begin{equation} \label{Eq:Denoise_Update}
\footnotesize
%\{\mathbf{\hat{x}}_{}, \mathbf{z},\mathbf{u},\mathbf{v} \} = 
\underset{\mathbf{X}_{} }{\min} \;
\mu \left\| \mathbf{X}_{} - \mathbf{X}^{ns}_{} \right\|_2^2
+
\sum_i
\left\| \mathbf{R}_i \mathbf{X} - ( \boldsymbol{\Psi}_{c} \mathbf{z}_i + \boldsymbol{\Psi} \mathbf{u}_i) \right\|_2^2
\end{equation}
%
\begin{equation} \label{Eq:Denoise_Update2}
%\small
\footnotesize
\mathbf{\widehat{X}} = \Big( \mu \mathbf{I} + \sum_{i} \mathbf{R}_i^T \mathbf{R}_i \Big)^{-1} 
\Big( \mu \mathbf{X}^{ns} + \sum_i \mathbf{R}_i^T (\boldsymbol{\Psi}_{c} \mathbf{z}_i + \boldsymbol{\Psi} \mathbf{u}_i) \Big) 
\end{equation} 
For $\mu = 0$, this expression just represents the average of the denoised image patches on the overlapping areas, leading to a purified image. For non-zero $\mu$, the estimation operation also refers to the original noisy image during the average.
%The coupled image denoising algorithm is described in Algorithm~\ref{Alg:CoupledDenoise}. 




\subsection{Coupled Image Denoising with Group Sparsity}
%Aforementioned coupled image denoising approach only exploit the standard sparsity of image patches. 
It is widely observed that natural images exhibit significant self-similarity property. The self-similarity prior is exploited to average out the noise among multiple similar patches\cite{dabov2007image,mairal2009non}. Similar phenomena are also commonly observed in multimodal images, which motivates us to integrate self-similarity prior into our scheme via group sparsity regularization, which imposes that similar patch pairs share the same sparsity patterns in their representations.

%Due to the potential instability of sparse coding (note, the $\ell_0$ pseudo norm is piecewise constant and its relaxed counterpart $\ell_1$ norm is only piecewise differentiable), estimated sparse representations for similar patches sometimes admit different sparse patterns\cite{mairal2009non}. To overcome this drawback, we exploit group sparsity\cite{tropp2006algorithms,yuan2006model} to address a simultaneous sparse coding problem, which imposes that similar patch pairs share the same sparsity patterns in their representations.

%
%% show the schematic diagram of Sparsity vs. Grouped-Sparsity
%\begin{figure}[t]
%	\centering	
%	\includegraphics[width= 6cm]{./FigureFlowChart/GroupSparsity.pdf}  
%	\caption{Sparsity vs. Grouped-Sparsity: Grey squares represents nonzeros values in vectors (left) or matrix (right)}
%	\label{Fig:Grouped-Sparsity}
%\end{figure}

%Let $\mathbf{A} = [\boldsymbol{\alpha}_1, \ldots, \boldsymbol{\alpha}_l] \in \mathbb{R}^{k \times l}$ denote a matrix consisting of a set of vectors. We want to enforce the same sparsity pattern for each vector in this group, shown as Figure~\ref{Fig:Grouped-Sparsity}. To this end, a grouped-sparsity regularizer\cite{tropp2006algorithms,yuan2006model} is defined on the matrix $\mathbf{A}$ as:
%\begin{equation}
%\|\mathbf{A} \|_{p,q} := \sum_{i=1}^{n} \| \boldsymbol{\alpha}^i \|_{q}^{p},
%\end{equation}
%where, $\boldsymbol{\alpha}^i$ denotes the $i$-th row of the matrix $\mathbf{A}$. In practice, one usually chooses $(p,q) = (0,\infty)$, leading to $\ell_{0,\infty}$ matrix pseudo norm, as a generalization of $\ell_0$ vector pseudo norm. This amounts to restricting the number of nonzero rows of $\mathbf{A}$. Alternatively, one can also choose $(p,q) = (1,2)$, leading to $\ell_{1,2}$ matrix norm, as the generalization of $\ell_1$ vector norm. $\ell_{1,2}$ norm can be regarded as the relaxation of $\ell_{0,\infty}$ pseudo norm, similar as the relation between $\ell_1$ norm and $\ell_0$ pseudo norm.

The grouped-sparsity of a matrix $\mathbf{A} \in \mathbb{R}^{3k \times l}$ is defined as
$$
\|\mathbf{A} \|_{p,q} := 
%[\boldsymbol{\alpha}_1, \ldots, \boldsymbol{\alpha}_l] = 
\left\|
\begin{bmatrix}
\mathbf{z}_1 & \ldots & \mathbf{z}_l \\
\mathbf{u}_1 & \ldots & \mathbf{u}_l \\
\mathbf{v}_1 & \ldots & \mathbf{v}_l \\
\end{bmatrix}
\right\|_{p,q}
=
\sum_{i=1}^{k} 
\| \mathbf{z}^i \|_q^p + \| \mathbf{u}^i \|_q^p + \| \mathbf{v}^i \|_q^p
%\left\|
%\begin{bmatrix}
%\mathbf{z}_i  \\
%\mathbf{u}_i  \\
%\mathbf{v}_i  \\
%\end{bmatrix}
%\right\|_{q}^{p}
$$
where, $\mathbf{z}^i$, $\mathbf{u}^i$, $\mathbf{v}^i$ denote the $i$-th row of matrix $[\mathbf{z}_1, \ldots, \mathbf{z}_l]$, $[\mathbf{u}_1, \ldots, \mathbf{u}_l]$, and $[\mathbf{v}_1, \ldots, \mathbf{v}_l]$, respectively. In our case, we choose $(p,q) = (0,\infty)$, leading to $\ell_{0,\infty}$ matrix pseudo norm, as a generalization of $\ell_0$ vector pseudo norm. This amounts to restricting the number of nonzero rows of $\mathbf{A}$. 
%Alternatively, one can also choose $(p,q) = (1,2)$, leading to $\ell_{1,2}$ matrix norm, as the generalization of $\ell_1$ vector norm. $\ell_{1,2}$ norm can be regarded as the relaxation of $\ell_{0,\infty}$ pseudo norm, similar as the relation between $\ell_1$ norm and $\ell_0$ pseudo norm.

To obtain groups of similar patches, we apply hierarchical clustering to separate all the patch pairs into $M$ different clusters according to their similarity. 
%Patch pairs in the $i$-th cluster constitute the set $S_i$, defined as
%\begin{equation}
%S_i: = \left\{ j=1, \ldots, n. \quad \text{s.t.} \left\| 
%\begin{bmatrix}
%\mathbf{x}_i \\
%\mathbf{y}_i \\
%\end{bmatrix} 
%-
%\begin{bmatrix}
%\mathbf{x}_j \\
%\mathbf{y}_j \\
%\end{bmatrix} 
%\right\|_2^2 \leq \xi \right\}
%\end{equation} 
%where $(\mathbf{x}_i, \mathbf{y}_i)$ denotes the centroid of this cluster, $\xi$ is a threshold. 
Given the learned coupled dictionaries, sparse coding for the $i$-th cluster $S_i$ of patch pairs with a grouped-sparsity regularizer amounts to solving
\begin{equation} \label{Eq:BPDN_GroupSparsity}
\begin{array}{cl}
\underset{\mathbf{A}_i}{\min}
& \! \! \! \!
\| \mathbf{A}_i \|_{p,q} \;
\\
\text{s.t.} 
& \! \! \! \!
\sum\limits_{j \in S_i} 
\left\|
\begin{bmatrix} 
\mathbf{x}^{ns}_{j} \\ \mathbf{y}_{j}
%\mathbf{x}^{ns}_{j} \\ \gamma \, \mathbf{y}_{j}
\end{bmatrix}
-
\begin{bmatrix}
\boldsymbol{\Psi}_{c} & \boldsymbol{\Psi} & \mathbf{0} \\
\boldsymbol{\Phi}_{c} & \mathbf{0} & \boldsymbol{\Phi} \\
\end{bmatrix}
%
\begin{bmatrix}
\mathbf{z}_j \\
\mathbf{u}_j \\
\mathbf{v}_j \\
\end{bmatrix}
\right\|_2^2
\leq
|S_i| C \sigma^2  
\end{array}
\end{equation}
where $|S_i|$ denotes the cardinality of the cluster $S_i$.

\vspace{-0.2cm}

\subsection{Coupled Dictionary Learning}

%\vspace{-0.2cm}

The coupled image denoising require a group of coupled dictionaries to perform the sparse transform/decomposition task. 
These required coupled dictionaries are learned using our couple dictionary learning algorithm.

Given a corpus of registered image patch pairs $\{(\mathbf{x}_i, \mathbf{y}_i) \}_{i=1}^T$, our coupled dictionary learning problem is posed as follows:
%let $\{(\mathbf{z}_i$, $\mathbf{u}_i, \mathbf{v}_i)\}_{i=1}^T$ denote their sparse representations, 
%\footnote{ In the dictionary learning task, we prefer  $\ell_1$ penalty as it brings some benefits owing to the convexity and better stability\cite{mairal2009non,mairal2014sparse}.}
%
\begin{equation} \label{Eq:CoupledDL_Clean}
\small
\begin{array}{cl}
\underset{ 
	\begin{subarray}{c}
	\left\{ \boldsymbol{\Psi}_{c}, \boldsymbol{\Psi}, \boldsymbol{\Phi}_{c}, \boldsymbol{\Phi} \right\}  \\
	\{ \mathbf{z}_i, \mathbf{u}_i, \mathbf{v}_i \}
	\end{subarray}}
{ \text{minimize}}
&  \! \! \! \!
\sum\limits_{i=1}^T
\frac{1}{2}
\left\|
\begin{bmatrix}
\mathbf{x}_i \\ \mathbf{y}_i 
%\mathbf{x}_i \\ \gamma \, \mathbf{y}_i 
\end{bmatrix}
-
\begin{bmatrix}
\boldsymbol{\Psi}_{c} & \boldsymbol{\Psi} & \mathbf{0} \\
\boldsymbol{\Phi}_{c} & \mathbf{0} & \boldsymbol{\Phi} \\
\end{bmatrix}
%
\begin{bmatrix}
\mathbf{z}_i \\
\mathbf{u}_i \\
\mathbf{v}_i \\
\end{bmatrix}
\right\|_2^2
+
\lambda
\left\| 
\begin{bmatrix}
\mathbf{z}_i \\
\mathbf{u}_i \\
\mathbf{v}_i \\
\end{bmatrix} 
\right\|_1
\\
\text{subject to}
%& % \! \! \! \!
%\|\mathbf{z}_i \|_0 
%+ \|\mathbf{u}_i \|_0
%+ \|\mathbf{v}_i \|_0 \leq s, \; \forall i.
%\\
& \! \! \! \!
\left\| \begin{bmatrix} \psi_{cj} \\ \phi_{cj} \end{bmatrix} \right\|_2^2 \leq 1, 
\| \psi_j \|_2^2 \leq 1, \| \phi_j \|_2^2 \leq 1, \forall j.
%	 		\|\mathbf{z}_i \|_0 \leq s_z, 
%	 		\|\mathbf{u}_i \|_0 \leq s_u, 
%	 		\|\mathbf{v}_i \|_0 \leq s_v, \\
%	 		& \; \forall i = 1, 2, \ldots T .
\end{array}
\end{equation}
\noindent
where, the first term with $\ell_2$ norm promotes the fidelity of sparse representations to the signals and the regularization term with $\ell_1$ norm promotes sparsity for the representations. The $\ell_2$ norm constraints for each atom pair $[\psi_{cj} ; \phi_{cj}]$ and individual atom $\psi_j$ and $\phi_j$ are used to avoid trivial solution.

%Otherwise, the atoms may become infinitely large and accordingly the sparse codes may become infinitely small, resulting in the sparsity constraint term ineffective, since the value of the objective is invariant to simultaneously scaling the whole dictionary by a scalar and the whole sparse representation by its inverse. In addition, the $\ell_2$ norm constraints for each atom pair $[\psi_{cj} ; \phi_{cj}]$ also introduce a competition between $\psi_{cj}$ and $\phi_{cj}$, which allows unit power to be allocated to each atom pair from $\boldsymbol{\Psi}_{c}$ and $\boldsymbol{\Phi}_{c}$ in proportion to their contribution/criticality. Specifically, an atom pair $[\psi_{cj}; \psi_{cj}]$ usually represent a pair of cooccurrence patterns in the two modalities $\mathbf{X}$ and $\mathbf{Y}$. If the pattern $\psi_{cj}$ is prominent and conspicuous in modality $\mathbf{x}$ while the pattern $\phi_{cj}$ is inconspicuous in modality $\mathbf{y}$, $\psi_{cj}$ will be allocated more power than $\phi_{cj}$, and vice versa.

%We mentioned in the coupled image denoising phase that the guidance information sometimes will be multiplied with a weight parameter $\gamma$, leading to $\gamma \, \mathbf{y}_{i}$ as a substitute for the original version. Accordingly, in the coupled dictionary learning phase, the same weight parameter $\gamma$ should also be used to determine how much contribution the side information will make in the learning processing. Specifically, for an extreme case that $\gamma = 0$, we only refer to the target modality during the learning and the side information makes no contribution. Thus, the total unit power will be allocated to each atom $\psi_{cj}$ of $\boldsymbol{\Psi}_{c}$. In contrast, when $\gamma > 1$, the side information plays a more important role and therefore more power will be allocated to $\phi_{cj}$ than to $\psi_{cj}$.

%Note that we can use standard algorithms to approximate the solution to \eqref{Eq:CoupledDL_Clean} -- such as orthogonal matching pursuit (OMP) algorithm~\cite{tropp2007signal} and iterative hard-thresholding algorithm~\cite{blumensath2009iterative}.




The coupled dictionary learning problems in \eqref{Eq:CoupledDL_Clean} is a non-convex optimization problem. 
%Furthermore, it differs from other standard dictionary learning problems in terms of extra structure constraints, i.e., the two zero blocks located in the specific positions in the global dictionary. In order to handle the non-convex optimization problems, 
We solve it via an alternating optimization scheme that performs a) sparse coding and b) dictionary update alternatively. During the sparse coding stage, we fix the all the dictionaries and obtain the sparse representations, while during the dictionary updating stage, we fix the sparse codes and update the all the dictionaries. The dictionary updating formulations are adapted from Block Coordinate Descent algorithm~\cite{mairal2010online}, where we train the common dictionaries simultaneously while train the unique dictionaries individually, as shown in Algorithm \ref{Alg:CoupledBCD}.

%a) During the sparse coding stage, we fix the all the dictionaries and obtain the sparse representations by solving:
%\begin{equation} \label{Eq:CoupledDL_Clean_SparseCoding}
%\small
%\begin{array}{cl}
%\underset{ 
%	\begin{subarray}{c}
%	\{ \mathbf{z}_i, \mathbf{u}_i, \mathbf{v}_i \}
%	\end{subarray}}
%{ \text{minimize}}
%& \! \! \! \!
%\sum\limits_{i=1}^T
%\frac{1}{2}
%\left\|
%\begin{bmatrix} 
%\mathbf{x}_i \\ \mathbf{y}_i
%%\mathbf{x}_i \\ \gamma \, \mathbf{y}_i 
%\end{bmatrix}
%-
%\begin{bmatrix}
%\boldsymbol{\Psi}_{c} & \boldsymbol{\Psi} & \mathbf{0} \\
%\boldsymbol{\Phi}_{c} & \mathbf{0} & \boldsymbol{\Phi} \\
%\end{bmatrix}
%%
%\begin{bmatrix}
%\mathbf{z}_i \\
%\mathbf{u}_i \\
%\mathbf{v}_i \\
%\end{bmatrix}
%\right\|_2^2
%+
%\lambda
%\left\| 
%\begin{bmatrix}
%\mathbf{z}_i \\
%\mathbf{u}_i \\
%\mathbf{v}_i \\
%\end{bmatrix} 
%\right\|_1
%%\\
%%\text{subject to}
%%& % \! \! \! \!
%%\|\mathbf{z}_i \|_0 
%%+ \|\mathbf{u}_i \|_0
%%+ \|\mathbf{v}_i \|_0 \leq s, \; \forall i.
%\end{array}
%\end{equation}
%
%b) During the dictionary updating stage, we fix the sparse codes and update the all the dictionaries via solving:
%%
%\begin{equation} \label{Eq:CoupledDL_Clean_DictUpdate}
%\begin{array}{cl}
%\underset{ 
%	\begin{subarray}{c}
%	\left\{ \boldsymbol{\Psi}_{c}, \boldsymbol{\Psi}, \boldsymbol{\Phi}_{c}, \boldsymbol{\Phi} \right\}
%	\end{subarray}}
%{ \text{minimize}}
%& % ! \! \! \!
%\sum\limits_{i=1}^T
%\frac{1}{2}
%\left\|
%\begin{bmatrix} 
%\mathbf{x}_i \\ \mathbf{y}_i
%%\mathbf{x}_i \\ \gamma \, \mathbf{y}_i 
%\end{bmatrix}
%-
%\begin{bmatrix}
%\boldsymbol{\Psi}_{c} & \boldsymbol{\Psi} & \mathbf{0} \\
%\boldsymbol{\Phi}_{c} & \mathbf{0} & \boldsymbol{\Phi} \\
%\end{bmatrix}
%%
%\begin{bmatrix}
%\mathbf{z}_i \\
%\mathbf{u}_i \\
%\mathbf{v}_i \\
%\end{bmatrix}
%\right\|_2^2
%\\
%\text{subject to}
%&
%\left\| \begin{bmatrix} \psi_{cj} \\ \phi_{cj} \end{bmatrix} \right\|_2^2 \leq 1, 
%\| \psi_j \|_2^2 \leq 1, \| \phi_j \|_2^2 \leq 1, \forall j.
%\end{array}
%\end{equation}


%Various off-the-shelf algorithms can be employed to approximate the solution to \eqref{Eq:CoupledDL_Clean_SparseCoding} -- such as iterative soft-thresholding algorithm~\cite{daubechies2004iterative}, least angle regression algorithm~\cite{efron2004least} and to solve \eqref{Eq:CoupledDL_Clean_DictUpdate} -- such as K-SVD\cite{aharon2006img}, Block Coordinate Descent\cite{mairal2010online}.

%In order to better deal with the special requirement in the coupled dictionary learning formulation, we propose a specialized algorithm adapted from Block Coordinate Descent dictionary learning algorithm, as shown in Algorithm~\ref{Alg:CoupledBCD}. The key idea is to train the common dictionaries simultaneously while train the unique dictionaries individually.

%\noindent
%This problem -- which we call global sparse coding because it updates all the sparse representations -- is solved using the orthogonal matching pursuit (OMP) algorithm \cite{tropp2007signal} or iterative hard-thresholding algorithm~\cite{blumensath2009iterative}.
%
%\noindent
%Algorithm~\ref{Alg:CoupledBCD} shows how we adapt Block Coordinate Descent dictionary learning algorithm \cite{mairal2010online} to solve the coupled dictionary learning problem. %Algorithm~\ref{Alg:CK-SVD} shows how we adapt K-SVD\cite{aharon2006img} to solve problem \eqref{Eq:CoupledDL_Clean} and \eqref{Eq:CDL_BPDN}. 


%%% decompose CDL Problem into the following sub-problems
%\noindent
%To this end, we adapt Block Coordinate Descent Dictionary Learning \cite{mairal2010online} algorithm for our coupled dictionary learning case. The key idea is to train the common dictionaries simultaneously while train the unique dictionaries individually. 
%%To this end, we adapt K-SVD\cite{aharon2006img} algorithm for our coupled dictionary learning case. The key idea is to update common dictionaries simultaneously while updating unique dictionaries individually. 
%
%Specifically, we further decompose Problem~\eqref{Eq:CoupledDL_Clean} into the following sub-problems~\eqref{Eq:PartialDictionaryUpdate_Com} - \eqref{Eq:PartialDictionaryUpdate_Phi}, so that we can sequentially train the common dictionaries and the unique dictionaries. That is, we fix the unique dictionaries $\boldsymbol{\Psi}$, $\boldsymbol{\Phi}$ and only update the common dictionaries $\boldsymbol{\Psi}_{c}$ and $\boldsymbol{\Phi}_{c}$ by solving
%%
%\begin{equation} \label{Eq:PartialDictionaryUpdate_Com}
%\begin{array}{cl}
%\underset{
%	\begin{subarray}{c}
%	\boldsymbol{\Psi}_{c}, \boldsymbol{\Phi}_{c}, \mathbf{Z}	\\
%	%\boldsymbol{\Psi}, \boldsymbol{\Phi} 
%	\end{subarray}}{ \text{min}}
%& \! \! \! \!
%%\,
%\frac{1}{2}
%\left\|
%\begin{bmatrix} 
%\mathbf{X} - \boldsymbol{\Psi} \mathbf{U} \\
%\gamma \mathbf{Y} - \boldsymbol{\Phi} \mathbf{V}
%\end{bmatrix}
%-
%\begin{bmatrix}
%\boldsymbol{\Psi}_{c}  \\
%\boldsymbol{\Phi}_{c}  \\
%\end{bmatrix}
%%
%\mathbf{Z}
%\right\|_F^2 
%+ \sum\limits_{i=1}^T \lambda \|z_i\|_1
%\\
%%\; \;
%\text{s.t.}
%& \! \! \! \!
%%\,
%\left\| \begin{bmatrix} \psi_{cj} \\ \phi_{cj} \end{bmatrix} \right\|_2^2 \leq 1, \forall j.
%%\| \psi_j \|_2^2 \leq 1, \| \phi_j \|_2^2 \leq 1, \forall j.
%\end{array}
%\end{equation}
%The algorithm alternates between global sparse coding~\eqref{Eq:CoupledDL_Clean_SparseCoding} and local common dictionary update~\eqref{Eq:PartialDictionaryUpdate_Com} for a few iterations until the procedure converges. Next, we fix the already learned common dictionaries and train the unique dictionaries in a similar alternating manner, including global sparse coding~\eqref{Eq:CoupledDL_Clean_SparseCoding} and following two local unique dictionary update operations:
%%\footnote{
%%	Owing to the SVD operation in the dictionary update, atoms from the common dictionary pair [$\boldsymbol{\Psi}_{c}$; $\boldsymbol{\Phi}_{c}$] and the unique dictionaries $\boldsymbol{\Psi}$ and $\boldsymbol{\Phi}$ have unit $\ell_2$ norm automatically.
%%}
%%
%\begin{equation} \label{Eq:PartialDictionaryUpdate_Psi}
%\begin{array}{cl}
%\underset{\boldsymbol{\Psi}, \mathbf{U} }{ \text{min}}
%&
%%\,
%\frac{1}{2}
%\left\|
%\left( 
%\mathbf{X} - \boldsymbol{\Psi}_{c} \mathbf{Z} 
%\right)
%- \boldsymbol{\Psi} \mathbf{U}
%\right\|_F^2 
%+ \sum\limits_{i=1}^T \lambda \|u_i\|_1
%\\
%%\;\;
%\text{s.t.}
%&
%\,
%\| \psi_j \|_2^2 \leq 1, \forall j.
%\end{array}
%\end{equation}
%%
%\begin{equation} \label{Eq:PartialDictionaryUpdate_Phi}
%\begin{array}{cl}
%\underset{\boldsymbol{\Phi}, \mathbf{V}}{ \text{min}}
%&
%%\,
%\frac{1}{2}
%\left\|
%\left(
%\mathbf{Y}
%-		
%\boldsymbol{\Phi}_{c} \mathbf{Z}
%\right)
%-
%\boldsymbol{\Phi} \mathbf{V}
%\right\|_F^2 
%+ \sum\limits_{i=1}^T \lambda \|v_i\|_1
%\\
%%\;\;
%\text{s.t.}
%&
%\,
%\| \phi_j \|_2^2 \leq 1, \forall j.
%\end{array}
%\end{equation}	
%
%%\noindent
%%\textcolor{red}{
%%Our empirical experiments show that such global sparse coding~\eqref{Eq:CDL_SparseCoding} + local dictionary update~\eqref{Eq:PartialDictionaryUpdate_Com} - \eqref{Eq:PartialDictionaryUpdate_Phi} manner generally gives better training performance than global sparse coding~\eqref{Eq:CDL_SparseCoding} + global dictionary update~\eqref{Eq:CDL_DictUpdate}. 
%%}
%

%
%Akin to other conventional dictionary learning formulations, our coupled dictionary learning problem also exhibits sign ambiguity and permutation ambiguity, thus admitting multiple global optima\cite{mairal2014sparse}\footnote{
%	For a solution ($\mathbf{D},\mathbf{A}$), the pair ($\mathbf{D} \mathbf{H},\mathbf{H}^{-1}\mathbf{A}$) is also a solution, where matrix $\mathbf{H}$ is the product of a diagonal matrix with +1 and -1's on its diagonal with a permutation matrix (in particular, $\mathbf{H}$ is thus orthogonal). Therefore, for a dictionary with $K$ atoms, the dictionary learning problem admits $K!2^K$ global optima.}, 
%even though the extra structure constraints, i.e., the two zero blocks located in the specific positions in the global dictionary reduce the number of global optima from $K!2^K$ to $3!\left(\frac{K}{3} \right)! 2^K$. Our algorithm can not guarantee the convergence to a global optimum, either, due to the non-convexity nature of Problem \eqref{Eq:CoupledDL_Clean}. However, the problem is convex with respect to the dictionaries when the sparse codes are fixed or vice versa. This property ensures that the proposed algorithm usually converges to a local optimum and performs reasonably well in practice, which is also confirmed in our experiments.



%%PSNR (dB) and RMSE, comparison with joint denoising approaches include\cite{ham2017robust,li2016deep,shen2015multispectral,zhang2014rolling,he2013guided,kopf2007joint}.
\begin{table*}[t]
	\scriptsize
	\centering
	\caption{Multimodal image denoising performance in terms of average PSNR and RMSE at different noise levels.}
	\begin{tabular}{l| ll| ll| ll| ll| ll| ll |ll }
		%		\toprule[1pt]
		\hline \hline
		Noise & \multicolumn{2}{c|}{JBF\cite{kopf2007joint}} & \multicolumn{2}{c|}{GF\cite{he2013guided}} & \multicolumn{2}{c|}{SDF\cite{ham2017robust}} & \multicolumn{2}{c|}{DJF\cite{li2016deep}} & \multicolumn{2}{c|}{JFSM\cite{shen2015multispectral}} & \multicolumn{2}{c|}{Proposed} & \multicolumn{2}{c}{Proposed+} \\
		$\sigma$/PSNR & RMSE & PSNR & RMSE & PSNR & RMSE & PSNR & RMSE & PSNR & RMSE & PSNR & RMSE & PSNR & RMSE & PSNR \\
		\hline
		4/36.08 & 0.0153 & 36.75 & 0.0111 & 39.51 & 0.0163 & 35.97 & 0.0119 & 38.84 & 0.0170 & 35.45 & \textbf{0.0085} & \textbf{41.57} & 0.0090 & 41.07 \\
		8/30.07 & 0.0172 & 35.66 & 0.0152 & 36.78 & 0.0172 & 35.46 & 0.0137 & 37.46 & 0.0171 & 35.39 & \textbf{0.0116} & \textbf{38.88} & 0.0117 & 38.80 \\
		12/26.54 & 0.0189 & 34.82 & 0.0167 & 35.87 & 0.0186 & 34.76 & 0.0162 & 36.11 & 0.0184 & 34.76 & 0.0140 & 37.18 & \textbf{0.0139} & \textbf{37.28} \\
		16/24.05 & 0.0203 & 34.16 & 0.0185 & 35.01 & 0.0204 & 33.90 & 0.0180 & 35.19 & 0.0198 & 34.12 & 0.0162 & 35.93 & \textbf{0.0159} & \textbf{36.12} \\
		20/22.11 & 0.0215 & 33.63 & 0.0200 & 34.34 & 0.0227 & 32.95 & 0.0198 & 34.45 & 0.0216 & 33.38 & 0.0182 & 34.92 & \textbf{0.0178} & \textbf{35.13} \\
		24/20.52 & 0.0227 & 33.16 & 0.0213 & 33.79 & 0.0255 & 31.93 & 0.0209 & 33.91 & 0.0232 & 32.75 & 0.0201 & 34.03 & \textbf{0.0195} & \textbf{34.30} \\
		%		\bottomrule[1pt]
		\hline \hline
	\end{tabular}
	\label{Tab:PSNR_RMSE}
\end{table*}

%
% show trained dictionaries
\begin{figure}[t]
	\centering
	%	\subfigure[Learned dictionaries]
	%	{
	%		\centering
	\includegraphics[width = 7cm]{CDL_NIR_RGB_4Dicts.pdf} %, height = 4cm
	%	}
	%	% ---------------------------------------------------	
	
	\vspace{-0.2cm}
	
	\caption{Learned coupled dictionaries for infrared/RGB images. 256 atoms are shown here. The top two indicate the common and unique dictionaries learned for infrared images. The bottom two display dictionaries learned from corresponding guidance modality.}
	\label{Fig:LearnedD}
\end{figure}
%\addtocounter{figure}{-1} % cheat latex figure counter so that the number of next figure is the same as before.
%\begin{figure*}[t]
%%	\addtocounter{subfigure}{2} %cheat latex figure counter so that the number of following subfigure continues.
%	\begin{minipage}[b]{0.06\linewidth}
%		\centering%		{\footnotesize Input} \hfill % \vfill
%	\end{minipage} 	
%	\caption{}
%\end{figure*}




% show PSNR and RMSE
\begin{figure}[t]
	\centering
	\begin{minipage}[b]{0.48\linewidth}
		\centering
		\includegraphics[width = 4.2cm, height = 2.8cm]{PSNR-eps-converted-to.pdf} 
	\end{minipage}
	\begin{minipage}[b]{0.48\linewidth}
		\centering
		\includegraphics[width = 4.2cm, height = 2.8cm]{RMSE-eps-converted-to.pdf}
	\end{minipage}

	\vspace{-0.2cm}
	
	\caption{Multimodal image denoising in terms of PSNR and RMSE with respect to noise level. We compare our basic approach (Ours) and the advanced version (Ours+) with state-of-the-art joint image filtering approaches, such as JBF\cite{kopf2007joint}, GF\cite{he2013guided}, SDF\cite{ham2017robust}, JFSM\cite{shen2015multispectral} and DJF\cite{li2016deep}.}
	\label{Fig:PSNR_RMSE}
\end{figure}


%% denoised images for only 3 noise levels
\begin{figure*}[th]
	\begin{multicols}{2}  
		\centering
		%---------------------------------------------------
		% true images
		\begin{minipage}[b]{0.1\linewidth}
			\centering{\footnotesize Truth}  
		\end{minipage} 
		\begin{minipage}[b]{0.28\linewidth}
			\centering
			\includegraphics[width = 2.5cm, height = 2.5cm]{ImgNo1_Noise8_X_true-eps-converted-to.pdf}
		\end{minipage} 
		\begin{minipage}[b]{0.28\linewidth}
			\centering
			\includegraphics[width = 2.5cm, height = 2.5cm]{ImgNo1_Noise16_X_true-eps-converted-to.pdf}
		\end{minipage} 
		\begin{minipage}[b]{0.28\linewidth}
			\centering
			\includegraphics[width = 2.5cm, height = 2.5cm]{ImgNo1_Noise24_X_true-eps-converted-to.pdf}
		\end{minipage} 
		\\
		% ------------------------------------------- 
		% Noisy images
		\begin{minipage}[b]{0.1\linewidth}
			\centering{\footnotesize Input}  
		\end{minipage} 
		\begin{minipage}[b]{0.28\linewidth}
			\centering
			\includegraphics[width = 2.5cm, height = 2.5cm]{ImgNo1_Noise8_X_low-eps-converted-to.pdf}
		\end{minipage} 
		\begin{minipage}[b]{0.28\linewidth}
			\centering
			\includegraphics[width = 2.5cm, height = 2.5cm]{ImgNo1_Noise16_X_low-eps-converted-to.pdf}
		\end{minipage} 
		\begin{minipage}[b]{0.28\linewidth}
			\centering
			\includegraphics[width = 2.5cm, height = 2.5cm]{ImgNo1_Noise24_X_low-eps-converted-to.pdf}
		\end{minipage} 
		\\
		% ------------------------------------------- 
		% GF
		\begin{minipage}[b]{0.1\linewidth}
			\centering{\footnotesize GF~\cite{he2013guided}}  
		\end{minipage}  
		\begin{minipage}[b]{0.28\linewidth}
			\centering
			\includegraphics[width = 2.5cm, height = 2.5cm]{ImgNo1_Noise8_X_GF-eps-converted-to.pdf}
		\end{minipage} 
		\begin{minipage}[b]{0.28\linewidth}
			\centering
			\includegraphics[width = 2.5cm, height = 2.5cm]{ImgNo1_Noise16_X_GF-eps-converted-to.pdf}
		\end{minipage} 
		\begin{minipage}[b]{0.28\linewidth}
			\centering
			\includegraphics[width = 2.5cm, height = 2.5cm]{ImgNo1_Noise24_X_GF-eps-converted-to.pdf}
		\end{minipage} 
		\\
		% ------------------------------------------- 
		% GF
		\begin{minipage}[b]{0.1\linewidth}
			\centering{\footnotesize ErrMap GF~\cite{he2013guided}}  
		\end{minipage} 
		\begin{minipage}[b]{0.28\linewidth}
			\centering
			\includegraphics[width = 2.5cm, height = 2.5cm]{ImgNo1_Noise8_X_resid_GF-eps-converted-to.pdf}
		\end{minipage} 
		\begin{minipage}[b]{0.28\linewidth}
			\centering
			\includegraphics[width = 2.5cm, height = 2.5cm]{ImgNo1_Noise16_X_resid_GF-eps-converted-to.pdf}
		\end{minipage} 
		\begin{minipage}[b]{0.28\linewidth}
			\centering
			\includegraphics[width = 2.5cm, height = 2.5cm]{ImgNo1_Noise24_X_resid_GF-eps-converted-to.pdf}
		\end{minipage} 
		\\
		% ------------------------------------------- 
		\begin{minipage}[b]{0.1\linewidth}
			\centering	% {\phantom{{\footnotesize XXX}}} 
		\end{minipage}
		\begin{minipage}[b]{0.28\linewidth}
			\centering{\footnotesize $\sigma=8$}
		\end{minipage} 
		\begin{minipage}[b]{0.28\linewidth}
			\centering{\footnotesize $\sigma=16$} 
		\end{minipage} 
		\begin{minipage}[b]{0.28\linewidth}
			\centering{\footnotesize $\sigma=24$} 
		\end{minipage}
		\\
		% -------------------------------------------
		% DJF
		\begin{minipage}[b]{0.1\linewidth}
			\centering{\footnotesize DJF~\cite{li2016deep}} %  
		\end{minipage} 
		\begin{minipage}[b]{0.28\linewidth}
			\centering
			\includegraphics[width = 2.5cm, height = 2.5cm]{ImgNo1_Noise8_X_DJF-eps-converted-to.pdf}
		\end{minipage} 
		\begin{minipage}[b]{0.28\linewidth}
			\centering
			\includegraphics[width = 2.5cm, height = 2.5cm]{ImgNo1_Noise16_X_DJF-eps-converted-to.pdf}
		\end{minipage} 
		\begin{minipage}[b]{0.28\linewidth}
			\centering
			\includegraphics[width = 2.5cm, height = 2.5cm]{ImgNo1_Noise24_X_DJF-eps-converted-to.pdf}
		\end{minipage} 
		\\
		% -------------------------------------------
		% DJF
		\begin{minipage}[b]{0.1\linewidth}
			\centering{\footnotesize ErrMap DJF~\cite{li2016deep}} %  
		\end{minipage}  
		\begin{minipage}[b]{0.28\linewidth}
			\centering
			\includegraphics[width = 2.5cm, height = 2.5cm]{ImgNo1_Noise8_X_resid_DJF-eps-converted-to.pdf}
		\end{minipage} 
		\begin{minipage}[b]{0.28\linewidth}
			\centering
			\includegraphics[width = 2.5cm, height = 2.5cm]{ImgNo1_Noise16_X_resid_DJF-eps-converted-to.pdf}
		\end{minipage} 
		\begin{minipage}[b]{0.28\linewidth}
			\centering
			\includegraphics[width = 2.5cm, height = 2.5cm]{ImgNo1_Noise24_X_resid_DJF-eps-converted-to.pdf}
		\end{minipage} 
		\\
		% -------------------------------------------
		%	% CDN with standard sparsity
		%	\begin{minipage}[b]{0.1\linewidth}
		%		\centering
		%		{\footnotesize Ours} %  
		%	\end{minipage} 
		%	\begin{minipage}[b]{0.28\linewidth}
		%		\centering
		%		\includegraphics[width = 2.5cm, height = 2.5cm]{./Figures/CDN/ImgNo1_Noise4_X_CDN-eps-converted-to.pdf}
		%	\end{minipage} 
		%	\begin{minipage}[b]{0.28\linewidth}
		%		\centering
		%		\includegraphics[width = 2.5cm, height = 2.5cm]{./Figures/CDN/ImgNo1_Noise12_X_CDN-eps-converted-to.pdf}
		%	\end{minipage} 
		%	\begin{minipage}[b]{0.28\linewidth}
		%		\centering
		%		\includegraphics[width = 2.5cm, height = 2.5cm]{./Figures/CDN/ImgNo1_Noise20_X_CDN-eps-converted-to.pdf}
		%	\end{minipage} 
		%	\\
		% -------------------------------------------
		%	% CDN with standard sparsity
		%	\begin{minipage}[b]{0.1\linewidth}
		%		\centering
		%		{\footnotesize ErrMap Ours} %  
		%	\end{minipage} 
		%	\begin{minipage}[b]{0.28\linewidth}
		%		\centering
		%		\includegraphics[width = 2.5cm, height = 2.5cm]{./Figures/CDN/ImgNo1_Noise8_X_resid_CDN-eps-converted-to.pdf}
		%	\end{minipage} 
		%	\begin{minipage}[b]{0.28\linewidth}
		%		\centering
		%		\includegraphics[width = 2.5cm, height = 2.5cm]{./Figures/CDN/ImgNo1_Noise16_X_resid_CDN-eps-converted-to.pdf}
		%	\end{minipage} 
		%	\begin{minipage}[b]{0.28\linewidth}
		%		\centering
		%		\includegraphics[width = 2.5cm, height = 2.5cm]{./Figures/CDN/ImgNo1_Noise24_X_resid_CDN-eps-converted-to.pdf}
		%	\end{minipage} 
		%	\\
		% -------------------------------------------
		% CDN with group sparsity
		\begin{minipage}[b]{0.1\linewidth}
			\centering{\footnotesize Ours+}  
		\end{minipage} 
		\begin{minipage}[b]{0.28\linewidth}
			\centering
			\includegraphics[width = 2.5cm, height = 2.5cm]{ImgNo1_Noise8_X_CDN-eps-converted-to.pdf}
		\end{minipage} 
		\begin{minipage}[b]{0.28\linewidth}
			\centering
			\includegraphics[width = 2.5cm, height = 2.5cm]{ImgNo1_Noise16_X_CDN-eps-converted-to.pdf}
		\end{minipage} 
		\begin{minipage}[b]{0.28\linewidth}
			\centering
			\includegraphics[width = 2.5cm, height = 2.5cm]{ImgNo1_Noise24_X_CDN-eps-converted-to.pdf}
		\end{minipage} 
		\\
		% -------------------------------------------
		% CDN with group sparsity
		\begin{minipage}[b]{0.1\linewidth}
			\centering{\footnotesize ErrMap Ours+}  
		\end{minipage} 
		\begin{minipage}[b]{0.28\linewidth}
			\centering
			\includegraphics[width = 2.5cm, height = 2.5cm]{ImgNo1_Noise8_X_resid_CDN-eps-converted-to.pdf}
		\end{minipage} 
		\begin{minipage}[b]{0.28\linewidth}
			\centering
			\includegraphics[width = 2.5cm, height = 2.5cm]{ImgNo1_Noise16_X_resid_CDN-eps-converted-to.pdf}
		\end{minipage} 
		\begin{minipage}[b]{0.28\linewidth}
			\centering
			\includegraphics[width = 2.5cm, height = 2.5cm]{ImgNo1_Noise24_X_resid_CDN-eps-converted-to.pdf}
		\end{minipage} 
		\\
		% ------------------------------------------- 
		\begin{minipage}[b]{0.1\linewidth}
			\centering	% {\phantom{{\footnotesize XXX}}} 
		\end{minipage}
		\begin{minipage}[b]{0.28\linewidth}
			\centering{\footnotesize $\sigma=8$}
		\end{minipage} 
		\begin{minipage}[b]{0.28\linewidth}
			\centering{\footnotesize $\sigma=16$} 
		\end{minipage} 
		\begin{minipage}[b]{0.28\linewidth}
			\centering{\footnotesize $\sigma=24$} 
		\end{minipage}
		% -------------------------------------------	
	\end{multicols}	
	
	\vspace{-0.8cm}
	
	\begin{multicols}{1}  
	\begin{minipage}[b]{0.98\linewidth}{\footnotesize Colorbar}
		\includegraphics[width = 16.8cm, height=0.5cm]{Colorbar64-eps-converted-to.pdf}
	\end{minipage}
	\end{multicols}	

	\vspace{-0.6cm}

	\caption{Visual comparison for multimodal image denoising.}
	%	 Each row represents one approach and its denoising results corresponding to different noise levels.}
	\label{Fig:DenoisedIms}
\end{figure*}

%\vspace{-0.4cm}

\section{Experiments}
\label{sec:Experiments}

\vspace{-0.2cm}

We now present a series of experiments to validate the effectiveness of the proposed multimodal image denoising approach. The dataset is from the EPFL infrared/RGB image database~\cite{brown2011multi}\footnote{\url{http://ivrl.epfl.ch/supplementary_material/cvpr11/}}. Each pair of infrared image and RGB image in the dataset have been registered with each other. The target modality is near infrared images and the guidance modality is RGB images. As the response of near infrared band exhibit poor correlation with the response of the visible band, it is usually difficult to infer the brightness of an infrared image given a corresponding RGB version. Thus, it is challenging to take good advantage of the RGB modality to purify the noisy infrared version.
%
%All the images in our dataset are houses and buildings that contain many fine textures and sharp edges. This makes the denoising task more challenging than purifying images with smoother textures, as it is very likely to smooth the sharp details during the denoising process.
%
%
The image pairs are randomly separated into two disjointed groups: training group (21 images) and testing group (8 images). We add zero-mean white Gaussian noise with different standard deviations $\sigma = [4,8,12,16,20,24]$ into the testing infrared images to generate the noisy version.
%The practical multispectral/RGB datasets are obtained from the Columbia multispectral database\footnote{\url{http://www.cs.columbia.edu/CAVE/databases/multispectral/}}.

%various state-of-the-art approaches, such as~\cite{kopf2007joint,he2013guided,ham2017robust,li2016deep,shen2015multispectral} 
We compare our approach with state-of-the-art joint image filtering approaches, including Joint Bilateral Filtering (JBF)\cite{kopf2007joint}, Guided image Filtering (GF)\cite{he2013guided}, Static/Dynamic Filtering (SDF)\cite{ham2017robust}, Deep Joint image Filtering (DJF)\cite{li2016deep} and Joint Filtering via optimizing a Scale Map (JFSM)\cite{shen2015multispectral}. The same guidance images as in our approach are leveraged for these comparison approaches.
%and their parameters are tuned to be optimum for the modalities and noise levels.\footnote{The parameters of these approaches have been tuned to produce the best results for the infrared/RGB modalities and different noise levels $\sigma = [4,8,12,16,20,24]$. \\	
%	Parameters for JBF: window size = [7,9,11,13,15,17], $\sigma_d$=[4,5,6,7,8,9], $\sigma_r$ = 0.01.\\
%	Parameters for GF: nhoodSize = [3,5,5,7,9,11], smoothValue  = 1e-5. \\
%	Parameters for SDF: $\lambda$ = 10, $\mu$ = 300, $\nu$ = 200, step=1.\\
%	Parameters for DJF: the net structure keeps intact as in \cite{li2016deep}, i.e., for CNN$_T$ and CNN$_G$: (9x9x3x96) -> (1x1x96x48) -> (5x5x48), for CNN$_F$: (9x9x2x64) -> (1x1x64x32) -> (5x5x32). TrainSize = 50,000 or 160,000; Epochs = 200, batchSize = 256, learningRate = 0.001~0.01, weightDecay = 0.05.\\
%	Parameters for JFSM: $\lambda$ = [14,13,11,9,7,6], $\beta$ = 0.5, maxIter = 5.
%}
We adopt the Peak Signal to Noise Ratio (PSNR) and the RMSE as the image quality evaluation metrics which are commonly used in the image denoising literature. 
%They are defined as 
%\begin{align} 
%\textrm{RMSE} &= \sqrt{ 
%	\frac{
%		\| \mathbf{X} - \widehat{\mathbf{X}} \|_F^2
%	}
%	{ N } } \,, 
%\label{Eq:Def_RMSE}
%\\
%\textrm{PSNR} &= 20 \log_{10} \frac{\textrm{PeakVal}}{\textrm{RMSE}} \,,
%\label{Eq:Def_PSNR}
%%
%%ratio = \frac{
%%	\left| \left\{d = \hat{d} : d \in \mathbf{D}, \hat{d} \in \widehat{\mathbf{D}} \right\} \right|
%%}
%%{\left| \left\{d : d \in \mathbf{D} \right\} \right| } \,,
%\end{align}
%where $\mathbf{X}$ and $\widehat{\mathbf{X}}$ denote the ground truth image and its denoised version, respectively. $N$ denotes the total number of pixels in the image and $\| \cdot \|_F$ denotes the Frobenius norm. PeakVal stands for the pixel peak value, e.g., 255 for 8 bits depth. Note, although PSNR and RMSE of one image can be inferred from each other according to the formulation~\eqref{Eq:Def_PSNR}, the average PSNR and average RMSE do not satisfy the correlation anymore. 


%the Structure SIMilarity (SSIM) index\cite{wang2004image}
%\footnote{The Structure SIMilarity (SSIM) index\cite{wang2004image} is defined as
%%The Structure SIMilarity (SSIM) index, defined as the following, is designed based on human's visual perception/characteristic. Reconstructed images with higher index value are more similar to the original images in human's vision sense.
%%{\footnotesize % \small
%%% scalor form
%\begin{equation*}
%SSIM(x,y) = 
%\frac{(2 \mu_x \mu_y + C_1)(2 \sigma_{xy} + C_2)}
%{(\mu_x^2 + \mu_y^2 + C_1) + (\sigma_x^2 + \sigma_y^2 + C_2)}
%\end{equation*}
%%%% vector form
%%\begin{equation*}
%%	SSIM(x,y) = 
%%	\frac{(2 \mu_x^T \mu_y + C_1)(2 \sigma_{xy} + C_2)}
%%	{(\mu_x^T \mu_x + \mu_y^T \mu_y + C_1) + (\sigma_x^T \sigma_x + \sigma_y^T \sigma_y + C_2)}
%%\end{equation*}
%%}
%where $\mu_x$ and $\mu_y$ denote the average intensities of image $x$ and $y$, respectively. $\sigma_x^2$ and $\sigma_y^2$ denote the variance of $x$ and $y$, respectively. $\sigma_{xy}$ denote the covariance of $x$ and $y$. $C_1 = (k_1 L)^2$ and $C_2 = (k_2 L)^2$ are two constants to stabilize the division with weak denominator, where constant $k_{1}=0.01$ and $k_{2}=0.03$ by default and $L$ denotes the dynamic range of the pixel intensity values.
%} 
%
%%Definition of Structural similarity in wikipedia
%The difference with respect to other techniques mentioned previously such as MSE or PSNR is that these approaches estimate absolute errors; on the other hand, SSIM is a perception-based model that considers image degradation as perceived change in structural information, while also incorporating important perceptual phenomena, including both luminance masking and contrast masking terms. Structural information is the idea that the pixels have strong inter-dependencies especially when they are spatially close. These dependencies carry important information about the structure of the objects in the visual scene. Luminance masking is a phenomenon whereby image distortions (in this context) tend to be less visible in bright regions, while contrast masking is a phenomenon whereby distortions become less visible where there is significant activity or "texture" in the image.
%


\vspace{-0.2cm}

\subsection{Coupled dictionary learning}
We perform coupled dictionary learning on a clean dataset, i.e., a corpus of clean multimodal image patch pairs extracted from the clean training images. We adopt a common operation to construct a patch-based training dataset. First, the clean infrared images and the corresponding RGB images with only intensity information are divided into a set of $\sqrt{n} \times \sqrt{n}$ patch pairs. Then, we remove the mean from each patch, as the DC component is always preserved well during the denoising process. Finally, we vectorize these patches to form the training datasets $\{(\mathbf{x}_i, \mathbf{y}_i) \}_{i=1}^T$ of dimension $n \times T$. Once the training dataset is prepared, we apply our coupled dictionary learning algorithm to learn the dictionary pairs $[\boldsymbol{\Psi}_{c},\boldsymbol{\Psi}]$ and $[\boldsymbol{\Phi}_{c},\boldsymbol{\Phi}]$ from the training datasets. The parameter setting is as follows: patch size $ n = 8 $, dictionary size $k= 1024$, training size $T \approx 50,000$, $\lambda = 0.05$. 
%total sparsity constraint $s = 8$.

%\footnote{The RGB images have been converted to grayscale format in advance, as we only exploit the intensity information.}
%Smooth patches with variance less than 0.02 have been eliminated as they are less informative.

Figure~\ref{Fig:LearnedD} shows the learned coupled dictionaries from the corpus of clean infrared images and corresponding RGB version. We can find that any pair of atoms from common dictionaries $\boldsymbol{\Psi}_{c}$ and $\boldsymbol{\Psi}_{c}$ capture associated edges, blobs, textures with the same direction and location, as well as exhibit considerable resemblance and strong correlation to each other. This outcome indicates that the common dictionaries have indeed captured the similarities between infrared and RGB modalities. In contrast, the learned unique dictionaries $\boldsymbol{\Psi}$ and $\boldsymbol{\Phi}$ represent the disparities of these modalities and therefore rarely exhibit resemblance.

\vspace{-0.2cm}

\subsection{Coupled image denoising}

During the coupled image denoising phase, we evaluate basic approach with standard sparsity regularization and advanced version using group sparsity regularization to incorporate self-similarity prior.
%
%Given a pair of testing noisy infrared and clean RGB images, we first divide the testing image pair into overlapping patches of size $\sqrt{n} \times \sqrt{n}$ pixels with overlapping stride equal to 1 pixel.\footnote{The overlap stride denotes the distance between corresponding pixel locations in adjacent image patches.} The DC component is also removed from each patch and stored. We vectorize these patches to construct the testing datasets $\{\mathbf{x}^{ns}_{i} \}$ and $\{\mathbf{y}_{i}\}$. Then, we perform coupled sparse coding and shrinkage operation on the testing patch pairs via solving \eqref{Eq:BPDN} with respect to the learned coupled dictionaries. For the advanced version with group sparsity regularization, the difference is that we perform hierarchical clustering on the testing datasets to congregate similar patch pairs into clusters and then solve~\eqref{Eq:BPDN_GroupSparsity} for each cluster to induce group sparsity in the coefficients. The shrunk sparse codes $\mathbf{z}_{i}$, $\mathbf{u}_{i}$ and $\mathbf{v}_{i}$ are then multiplied with the dictionary pair $[\boldsymbol{\Psi}_{c},\boldsymbol{\Psi}]$ to estimate the clean patches $\mathbf{x}_{i}$. Finally, the DC component of each patch is added back to the corresponding estimated clean patches which are then tiled together to reconstruct the final target image according to \eqref{Eq:Denoise_Update2}.
%%shown in Algorithm~\ref{Alg:CoupledDenoise}.
%
%
Figure~\ref{Fig:PSNR_RMSE} and \ref{Fig:DenoisedIms} demonstrate the denoising performance, visual quality of the purified infrared images, as well as the corresponding error maps. As shown in these figures, our approach substantially attenuates the noise and, at the same time, reliably reserves image sharp details and suppresses artifacts. Therefore, the purified infrared images by our approach are cleaner and more visually plausible than the reconstruction by the state-of-the-art joint image filtering approaches. The visual quality is also demonstrated by the error maps where the denoised infrared images using our approach exhibit the least residual for different noise levels in comparison with the competing methods. In particular, it indicates that detailed structure information such as sharp edges, textures and stripes, can be effectively captured by learned coupled dictionaries. The average PSNR and average RMSE results for the multimodal image denoising task, shown in Table~\ref{Tab:PSNR_RMSE} and Figure~\ref{Fig:PSNR_RMSE}, also confirm that our approach exhibits notable advantages over the competing methods.\footnote{
	More results can be found in the appendix of supplementary materials.}
% for example, with gains of at least 1dB for weak noise cases, e.g. $\sigma=4$. 
%More detailed results are shown in Figure~\ref{Fig:DenoisedIms_CDN}.
%\footnote{Limited by the space, more detailed results can be found in the appendix of our supplementary materials on our website(\url{http://www.ee.ucl.ac.uk/~uceeong/}).}

%state-of-the-art joint image filtering approaches JBF\cite{kopf2007joint}, GF\cite{he2013guided}, SDF\cite{ham2017robust}, DJF\cite{li2016deep} and JFSM\cite{shen2015multispectral},


\vspace{-0.2cm}

\section{Conclusion}
\label{sec:Conclusion}

\vspace{-0.2cm}

This paper proposes a new effective multimodal image denoising approach based on coupled dictionary learning. The proposed approach explicitly incorporates sparsity prior, self-similarity prior and cross-similarity prior in the data model to captures the similarities and disparities between different image modalities in a learned sparse feature domain in \emph{lieu} of the original image domain. This scheme is able to exploit a guidance image to aid the denoising of the target image of interested modality, achieving notable benefits in the task with respect to the state-of-the-art.


%\vfill
%\pagebreak
%
%\section{COPYRIGHT FORMS}
%\label{sec:copyright}
%
%You must submit your fully completed, signed IEEE electronic copyright release
%form when you submit your paper. We {\bf must} have this form before your paper
%can be published in the proceedings.

%\vfill
%\pagebreak

%\section{REFERENCES}
%\label{sec:refs}

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------

\clearpage
\pagebreak

%{
%\small
%\bibliographystyle{IEEEtran}
%\bibliography{IEEEabrv,mybib_DL}
%}

%\clearpage

%\begin{thebibliography}{1}
%	{
%		\bibitem{Gershman2010} A.~B.~Gershman, N.~D.~Sidiropoulos, S.~Shahbazpanahi, M.~Bengtsson, and B.~Ottersten, ``Convex optimization-based beamforming: From receive to transmit and network designs'', {IEEE Signal Process. Mag.}, vol.~27, no.~3, pp.~62-75, May 2010.
%		
%		\bibitem{Dahlman2011} E.~Dahlman, S.~Parkvall, and J.~Skold, \emph{4G LTE/LTE-Advanced for Mobile Broadband.} Elsevier, 2011.
%	} 
%\end{thebibliography}




% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
%	\small
	\providecommand{\url}[1]{#1}
	\csname url@samestyle\endcsname
	\providecommand{\newblock}{\relax}
	\providecommand{\bibinfo}[2]{#2}
	\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
	\providecommand{\BIBentryALTinterwordstretchfactor}{4}
	\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
		\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
		\fontdimen4\font\relax}
	\providecommand{\BIBforeignlanguage}[2]{{%
			\expandafter\ifx\csname l@#1\endcsname\relax
			\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
			\typeout{** loaded for the language `#1'. Using the pattern for}%
			\typeout{** the default language instead.}%
			\else
			\language=\csname l@#1\endcsname
			\fi
			#2}}
	\providecommand{\BIBdecl}{\relax}
	\BIBdecl
	
	\bibitem{shao2014heuristic}
	L.~Shao, R.~Yan \emph{et~al.}, ``From heuristic optimization to dictionary
	learning: A review and comprehensive comparison of image denoising
	algorithms,'' \emph{IEEE Trans. Cybern.}, vol.~44, no.~7, pp. 1001--1013,
	2014.
	
	\bibitem{zhu2010automatic}
	X.~Zhu and P.~Milanfar, ``Automatic parameter selection for denoising
	algorithms using a no-reference measure of image content,'' \emph{IEEE Trans.
		Imag. Proc.}, vol.~19, no.~12, pp. 3116--3132, 2010.
	
	\bibitem{bouboulis2010adaptive}
	P.~Bouboulis, K.~Slavakis \emph{et~al.}, ``Adaptive kernel-based image
	denoising employing semi-parametric regularization,'' \emph{IEEE Trans. Imag.
		Proc.}, vol.~19, no.~6, pp. 1465--1479, 2010.
	
	\bibitem{buades2005non}
	A.~Buades, B.~Coll \emph{et~al.}, ``A non-local algorithm for image
	denoising,'' in \emph{Proc. IEEE Conf. Comput. Vision Pattern Recog},
	vol.~2.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2005, pp. 60--65.
	
	\bibitem{nguyen2017bounded}
	M.~P. Nguyen and S.~Y. Chun, ``Bounded self-weights estimation method for
	non-local means image denoising using minimax estimators,'' \emph{IEEE Trans.
		Imag. Proc.}, vol.~26, no.~4, pp. 1637--1649, 2017.
	
	\bibitem{talebi2014global}
	H.~Talebi and P.~Milanfar, ``Global image denoising,'' \emph{IEEE Trans. Imag.
		Proc.}, vol.~23, no.~2, pp. 755--768, 2014.
	
	\bibitem{romano2015boosting}
	Y.~Romano and M.~Elad, ``Boosting of image denoising algorithms,'' \emph{SIAM
		Journal on Imaging Sciences}, vol.~8, no.~2, pp. 1187--1219, 2015.
	
	\bibitem{dabov2007image}
	K.~Dabov, A.~Foi \emph{et~al.}, ``Image denoising by sparse 3-d
	transform-domain collaborative filtering,'' \emph{IEEE Trans. Image
		Process.}, vol.~16, no.~8, pp. 2080--2095, 2007.
	
	\bibitem{zhang2010two}
	L.~Zhang, W.~Dong \emph{et~al.}, ``Two-stage image denoising by principal
	component analysis with local pixel grouping,'' \emph{Pattern Recognition},
	vol.~43, no.~4, pp. 1531--1549, 2010.
	
	\bibitem{chatterjee2012patch}
	P.~Chatterjee and P.~Milanfar, ``Patch-based near-optimal image denoising,''
	\emph{IEEE Trans. Imag. Proc.}, vol.~21, no.~4, pp. 1635--1649, 2012.
	
	\bibitem{elad2006image}
	M.~Elad and M.~Aharon, ``Image denoising via sparse and redundant
	representations over learned dictionaries,'' \emph{IEEE Trans. Image
		Process.}, vol.~15, no.~12, pp. 3736--3745, 2006.
	
	\bibitem{mairal2009non}
	J.~Mairal, F.~Bach \emph{et~al.}, ``Non-local sparse models for image
	restoration,'' in \emph{Proc. IEEE Int. Conf. Comput. Vision}.\hskip 1em plus
	0.5em minus 0.4em\relax IEEE, 2009, pp. 2272--2279.
	
	\bibitem{gomez2015multimodal}
	L.~Gomez-Chova, D.~Tuia \emph{et~al.}, ``Multimodal classification of remote
	sensing images: a review and future directions,'' \emph{Proceedings of the
		IEEE}, vol. 103, no.~9, pp. 1560--1584, 2015.
	
	\bibitem{loncan2015hyperspectral}
	L.~Loncan, L.~B. de~Almeida \emph{et~al.}, ``Hyperspectral pansharpening: A
	review,'' \emph{IEEE Geosci. Remote Sens. Mag.}, vol.~3, no.~3, pp. 27--46,
	2015.
	
	\bibitem{levin2004colorization}
	A.~Levin, D.~Lischinski \emph{et~al.}, ``Colorization using optimization,'' in
	\emph{ACM Trans. Graph.}, vol.~23, no.~3.\hskip 1em plus 0.5em minus
	0.4em\relax ACM, 2004, pp. 689--694.
	
	\bibitem{renna2016classification}
	F.~Renna, L.~Wang \emph{et~al.}, ``Classification and reconstruction of
	high-dimensional signals from low-dimensional features in the presence of
	side information,'' \emph{IEEE Trans. Inform. Theory}, vol.~62, no.~11, pp.
	6459--6492, 2016.
	
	\bibitem{mota2017compressed}
	J.~F. Mota, N.~Deligiannis \emph{et~al.}, ``Compressed sensing with prior
	information: Strategies, geometry, and bounds,'' \emph{IEEE Trans. Inform.
		Theory}, vol.~63, no.~7, 2017.
	
	\bibitem{kopf2007joint}
	J.~Kopf, M.~F. Cohen \emph{et~al.}, ``Joint bilateral upsampling,'' in
	\emph{ACM Trans. Graph.}, vol.~26, no.~3.\hskip 1em plus 0.5em minus
	0.4em\relax ACM, 2007, p.~96.
	
	\bibitem{he2013guided}
	K.~He, J.~Sun \emph{et~al.}, ``Guided image filtering,'' \emph{IEEE Trans.
		Pattern Anal. Mach. Intell.}, vol.~35, no.~6, pp. 1397--1409, 2013.
	
	\bibitem{ham2017robust}
	B.~Ham, M.~Cho \emph{et~al.}, ``Robust guided image filtering using nonconvex
	potentials,'' \emph{IEEE Trans. Pattern Anal. Mach. Intell.}, 2017.
	
	\bibitem{li2016deep}
	Y.~Li, J.-B. Huang \emph{et~al.}, ``Deep joint image filtering,'' in
	\emph{Proc. Eur. Conf. Comput. Vision}.\hskip 1em plus 0.5em minus
	0.4em\relax Springer, 2016, pp. 154--169.
	
	\bibitem{shen2015multispectral}
	X.~Shen, Q.~Yan \emph{et~al.}, ``Multispectral joint image restoration via
	optimizing a scale map,'' \emph{IEEE Trans. Pattern Anal. Mach. Intell.},
	vol.~37, no.~12, pp. 2518--2530, 2015.
	
	\bibitem{zhang2014rolling}
	Q.~Zhang, X.~Shen \emph{et~al.}, ``Rolling guidance filter,'' in \emph{Proc.
		Eur. Conf. Comput. Vision}.\hskip 1em plus 0.5em minus 0.4em\relax Springer,
	2014, pp. 815--830.
	
	\bibitem{mairal2014sparse}
	J.~Mairal, F.~Bach \emph{et~al.}, ``Sparse modeling for image and vision
	processing,'' \emph{Foundations and Trends{\textregistered} in Computer
		Graphics and Vision}, vol.~8, no. 2-3, pp. 85--283, 2014.
	
	\bibitem{mairal2010online}
	J.~Mairal, F.~Bach \emph{et~al.}, ``Online learning for matrix factorization
	and sparse coding,'' \emph{The Journal of Machine Learning Research},
	vol.~11, pp. 19--60, 2010.
	
	\bibitem{brown2011multi}
	M.~Brown and S.~S{\"u}sstrunk, ``Multi-spectral sift for scene category
	recognition,'' in \emph{Proc. IEEE Conf. Comput. Vision Pattern Recog}.\hskip
	1em plus 0.5em minus 0.4em\relax IEEE, 2011, pp. 177--184.
	
\end{thebibliography}










%\pagebreak



%
%\section{Related Work}
%\label{sec:RelatedWork}
%
%There are various image denoising approaches in the literature. Single image denoising approaches do not leverage other guidance images, whereas guided/joint image denoising approaches explicitly leverage the availability of other image modalities.
%
%
%\subsection{Single image denoising}
%In general, single image denoising approaches can be categorized into 3 classes\cite{shao2014heuristic}: spatial domain, predefined transform domain, and learned transform domain according to the image representation. Spatial domain methods include local\cite{zhu2010automatic,bouboulis2010adaptive} and nonlocal filters\cite{buades2005non,nguyen2017bounded}, which exploit the similarities between either pixels or patches in an image.
%%
%Both predefined and learned transform domain consider employing favourable priors, such as sparsity and similarity, that may be exhibited in a transform domain. In general, the difference between prefixed transform domain approaches and learned transform ones lies in the fact that the former usually exploits prefixed bases such as Wavelet and DCT in \cite{dabov2007image} while the latter employs learned bases such as principal component analysis (PCA) in \cite{zhang2010two}, and over-complete dictionaries in \cite{elad2006image}. 
%
%Based on the assumption that a latent image patch admits a local sparse representation in some transform domains while noise does not, many works have contributed to the sparse denoising direction. The sparse denoising process can be regarded as a hard or soft shrinkage on the transform coefficients followed by the inverse transform for reconstruction. A representative approach is proposed by Elad et al. in \cite{elad2006image}, where they employ K-SVD \cite{aharon2006img} to learn the transform bases, called dictionary, from a set of training image patches which are constructed from either clean training images or the target noisy image. Once the dictionary is learned, a transform, i.e., sparse decomposition, is applied on the noisy image patches via OMP\cite{tropp2007signal} which is then followed by a hard shrinkage operation on the transform coefficients, i.e. sparse codes. Finally, the estimated clean image patch is recovered by an inverse transform, i.e. multiplying the shrunk coefficients with the learned over-complete dictionary. 
%
%Later, Dabov et al. \cite{dabov2007image} propose a novel and powerful denoising strategy, called BM3D, which enhances the sparsity by incorporating patch self-similarity prior. Their hybrid strategy groups similar 2-D patches/blocks into 3-D arrays and then perform collaborative filtering which is composed of three successive steps: 3-D transformation, shrinkage, and inverse 3-D transformation. 
%%
%By unifying Elad et al.'s dictionary learning idea with Dabov et al.'s self-similarity idea, Mairal et al. \cite{mairal2009non} propose a learned simultaneous sparse coding (LSSC) framework to explicitly incorporate self-similarity via group sparsity regularization. 
%
%Inspired by the exploitation of Wiener filter in BM3D\cite{dabov2007image}, Chatterjee et al. \cite{chatterjee2012patch} develop a patch-based locally optimal Wiener filter where the parameters are learned from both geometrically and photometrically similar patches. Their design leads to near-optimal denoising performance in RMSE sense.
%%
%In contrast with above local patch-based image denoising approaches, \cite{talebi2014global} develop a global filtering paradigm that each pixel is estimated from all pixels in the image. Their statistical analysis based on a spectral decomposition of its corresponding operator demonstrates that this global filter can be implemented efficiently by sampling a fairly small percentage of the pixels in the image.
%%
%In order to closes the gap between the local patch-modeling and the global restoration task, Romano and Elad \cite{romano2015boosting} propose a recursive denoising strategy, called "SOS", which consists of (i) Strengthen the signal by adding the previous denoised image to the degraded input image, (ii) Operate the denoising method on the strengthened image, and (iii) Subtract the previous denoised image from the restored signal-strengthened outcome.
%
%%\cite{burger2012image} propose to apply a plain neural network to image denoising.
%%Other literatures propose to exploit low-rank prior, denoising auto-encoder.
%
%
%\subsection{Joint image denoising}
%
%Compared with single image denoising, joint image denoising attempts to leverage an additional guidance image to aid the denoising process for the target image, by transferring structural information of the guidance image to the target image. 
%
%%It is a particular application of joint image filtering or guided image filtering\cite{kopf2007joint,he2013guided}. 
%%The bilateral filter\cite{tomasi1998bilateral} is a widely used translation-variant edge-preserving filter that outputs a pixel as a weighted average of neighboring pixels. The weights are computed by a spatial filter kernel and a range filter kernel evaluated on the data values themselves. It smoothes the image while preserving edges. 
%
%As a generalization of bilateral filter\cite{tomasi1998bilateral}, the joint bilateral filtering\cite{kopf2007joint} proposes a translation-variant edge-preserving filter which is capable of smoothing the target image while preserving edges with reference to a guidance image. In particular, this filter outputs a pixel as a weighted average of neighboring pixels, where the weights are computed by a spatial filter kernel and a range filter kernel evaluated respectively on the guidance image and target image, thereby expecting to preserve salient information from being over-smoothed. 
%
%However, it is noticed that joint bilateral image filtering may introduce gradient reversal artifacts as it does not preserve gradient information\cite{he2013guided}. To overcome this shortcoming/limitation, He et al. proposes the guided image filtering\cite{he2013guided} based on the assumption that local areas of the guidance image and the target image in the same window exhibit a linear mapping relation. This assumption implies that the target image has an edge only when if the guidance image has an edge, which leads to a formulation mainly depending on the ratio of covariance of both images over the variance of target image. Their design has only O(1) complexity and is faster than joint bilateral filtering, as all the pixels in the same window share the same parameters. 
%
%To address notable appearance change problem caused by direct guidance information transfer, \cite{shen2015multispectral} proposes a framework that optimizes a novel scale map to explicitly model derivative-level confidence so that the representation is capable of handling structure divergence, as well as capturing commonly usable edges and smooth transitions for visually plausible image reconstruction. 
%
%As the construction of these filters considers unilaterally the static guidance image, they may suffer from the inconsistency of the local structures in the guidance and target images and incorrect transfer of structure details to the target images. To overcome this limitation, \cite{ham2017robust} proposes the robust guided image filtering, referred to as static/dynamic (SD) filtering, which jointly leverages static guidance image and dynamic target image to iteratively refine the target image. 
%
%Aforementioned joint filtering design techniques mainly employ hand-crafted features that may not reflect natural image priors well. Recent work\cite{li2016deep} proposes a Convolutional Neural Networks (CNN) based joint image filtering approach. This approach considers the structures of both input and guidance images, but requires numerous annotated images and intensive computing resources to train the deep model for each task.
%
%Our joint image denoising based on coupled dictionary learning falls into the learning-based category. The priors used in our approach are learned from a training dataset rather than being hand-crafted and thus adapt to the target modality and guidance modality. 

%\include{Appendices_V2}

\end{document}
