\documentclass[final,t]{beamer}
%%%%% NEW MATH DEFINITIONS %%%%%\usepackage{amsmath,amsfonts,bm}% Mark sections of captions for referring to divisions of figures\newcommand{\figleft}{{\em (Left)}}\newcommand{\figcenter}{{\em (Center)}}\newcommand{\figright}{{\em (Right)}}\newcommand{\figtop}{{\em (Top)}}\newcommand{\figbottom}{{\em (Bottom)}}\newcommand{\captiona}{{\em (a)}}\newcommand{\captionb}{{\em (b)}}\newcommand{\captionc}{{\em (c)}}\newcommand{\captiond}{{\em (d)}}% Highlight a newly defined term\newcommand{\newterm}[1]{{\bf #1}}% Figure reference, lower-case.\def\figref#1{figure~\ref{#1}}% Figure reference, capital. For start of sentence\def\Figref#1{Figure~\ref{#1}}\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}% Section reference, lower-case.\def\secref#1{section~\ref{#1}}% Section reference, capital.\def\Secref#1{Section~\ref{#1}}% Reference to two sections.\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}% Reference to three sections.\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}% Reference to an equation, lower-case.\def\eqref#1{equation~\ref{#1}}% Reference to an equation, upper case\def\Eqref#1{Equation~\ref{#1}}% A raw reference to an equation---avoid using if possible\def\plaineqref#1{\ref{#1}}% Reference to a chapter, lower-case.\def\chapref#1{chapter~\ref{#1}}% Reference to an equation, upper case.\def\Chapref#1{Chapter~\ref{#1}}% Reference to a range of chapters\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}% Reference to an algorithm, lower-case.\def\algref#1{algorithm~\ref{#1}}% Reference to an algorithm, upper case.\def\Algref#1{Algorithm~\ref{#1}}\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}% Reference to a part, lower case\def\partref#1{part~\ref{#1}}% Reference to a part, upper case\def\Partref#1{Part~\ref{#1}}\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}\def\ceil#1{\lceil #1 \rceil}\def\floor#1{\lfloor #1 \rfloor}\def\1{\bm{1}}\newcommand{\train}{\mathcal{D}}\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}\def\eps{{\epsilon}}% Random variables\def\reta{{\textnormal{$\eta$}}}\def\ra{{\textnormal{a}}}\def\rb{{\textnormal{b}}}\def\rc{{\textnormal{c}}}\def\rd{{\textnormal{d}}}\def\re{{\textnormal{e}}}\def\rf{{\textnormal{f}}}\def\rg{{\textnormal{g}}}\def\rh{{\textnormal{h}}}\def\ri{{\textnormal{i}}}\def\rj{{\textnormal{j}}}\def\rk{{\textnormal{k}}}\def\rl{{\textnormal{l}}}% rm is already a command, just don't name any random variables m\def\rn{{\textnormal{n}}}\def\ro{{\textnormal{o}}}\def\rp{{\textnormal{p}}}\def\rq{{\textnormal{q}}}\def\rr{{\textnormal{r}}}\def\rs{{\textnormal{s}}}\def\rt{{\textnormal{t}}}\def\ru{{\textnormal{u}}}\def\rv{{\textnormal{v}}}\def\rw{{\textnormal{w}}}\def\rx{{\textnormal{x}}}\def\ry{{\textnormal{y}}}\def\rz{{\textnormal{z}}}% Random vectors\def\rvepsilon{{\mathbf{\epsilon}}}\def\rvtheta{{\mathbf{\theta}}}\def\rva{{\mathbf{a}}}\def\rvb{{\mathbf{b}}}\def\rvc{{\mathbf{c}}}\def\rvd{{\mathbf{d}}}\def\rve{{\mathbf{e}}}\def\rvf{{\mathbf{f}}}\def\rvg{{\mathbf{g}}}\def\rvh{{\mathbf{h}}}\def\rvu{{\mathbf{i}}}\def\rvj{{\mathbf{j}}}\def\rvk{{\mathbf{k}}}\def\rvl{{\mathbf{l}}}\def\rvm{{\mathbf{m}}}\def\rvn{{\mathbf{n}}}\def\rvo{{\mathbf{o}}}\def\rvp{{\mathbf{p}}}\def\rvq{{\mathbf{q}}}\def\rvr{{\mathbf{r}}}\def\rvs{{\mathbf{s}}}\def\rvt{{\mathbf{t}}}\def\rvu{{\mathbf{u}}}\def\rvv{{\mathbf{v}}}\def\rvw{{\mathbf{w}}}\def\rvx{{\mathbf{x}}}\def\rvy{{\mathbf{y}}}\def\rvz{{\mathbf{z}}}% Elements of random vectors\def\erva{{\textnormal{a}}}\def\ervb{{\textnormal{b}}}\def\ervc{{\textnormal{c}}}\def\ervd{{\textnormal{d}}}\def\erve{{\textnormal{e}}}\def\ervf{{\textnormal{f}}}\def\ervg{{\textnormal{g}}}\def\ervh{{\textnormal{h}}}\def\ervi{{\textnormal{i}}}\def\ervj{{\textnormal{j}}}\def\ervk{{\textnormal{k}}}\def\ervl{{\textnormal{l}}}\def\ervm{{\textnormal{m}}}\def\ervn{{\textnormal{n}}}\def\ervo{{\textnormal{o}}}\def\ervp{{\textnormal{p}}}\def\ervq{{\textnormal{q}}}\def\ervr{{\textnormal{r}}}\def\ervs{{\textnormal{s}}}\def\ervt{{\textnormal{t}}}\def\ervu{{\textnormal{u}}}\def\ervv{{\textnormal{v}}}\def\ervw{{\textnormal{w}}}\def\ervx{{\textnormal{x}}}\def\ervy{{\textnormal{y}}}\def\ervz{{\textnormal{z}}}% Random matrices\def\rmA{{\mathbf{A}}}\def\rmB{{\mathbf{B}}}\def\rmC{{\mathbf{C}}}\def\rmD{{\mathbf{D}}}\def\rmE{{\mathbf{E}}}\def\rmF{{\mathbf{F}}}\def\rmG{{\mathbf{G}}}\def\rmH{{\mathbf{H}}}\def\rmI{{\mathbf{I}}}\def\rmJ{{\mathbf{J}}}\def\rmK{{\mathbf{K}}}\def\rmL{{\mathbf{L}}}\def\rmM{{\mathbf{M}}}\def\rmN{{\mathbf{N}}}\def\rmO{{\mathbf{O}}}\def\rmP{{\mathbf{P}}}\def\rmQ{{\mathbf{Q}}}\def\rmR{{\mathbf{R}}}\def\rmS{{\mathbf{S}}}\def\rmT{{\mathbf{T}}}\def\rmU{{\mathbf{U}}}\def\rmV{{\mathbf{V}}}\def\rmW{{\mathbf{W}}}\def\rmX{{\mathbf{X}}}\def\rmY{{\mathbf{Y}}}\def\rmZ{{\mathbf{Z}}}% Elements of random matrices\def\ermA{{\textnormal{A}}}\def\ermB{{\textnormal{B}}}\def\ermC{{\textnormal{C}}}\def\ermD{{\textnormal{D}}}\def\ermE{{\textnormal{E}}}\def\ermF{{\textnormal{F}}}\def\ermG{{\textnormal{G}}}\def\ermH{{\textnormal{H}}}\def\ermI{{\textnormal{I}}}\def\ermJ{{\textnormal{J}}}\def\ermK{{\textnormal{K}}}\def\ermL{{\textnormal{L}}}\def\ermM{{\textnormal{M}}}\def\ermN{{\textnormal{N}}}\def\ermO{{\textnormal{O}}}\def\ermP{{\textnormal{P}}}\def\ermQ{{\textnormal{Q}}}\def\ermR{{\textnormal{R}}}\def\ermS{{\textnormal{S}}}\def\ermT{{\textnormal{T}}}\def\ermU{{\textnormal{U}}}\def\ermV{{\textnormal{V}}}\def\ermW{{\textnormal{W}}}\def\ermX{{\textnormal{X}}}\def\ermY{{\textnormal{Y}}}\def\ermZ{{\textnormal{Z}}}% Vectors\def\vzero{{\bm{0}}}\def\vone{{\bm{1}}}\def\vmu{{\bm{\mu}}}\def\vtheta{{\bm{\theta}}}\def\va{{\bm{a}}}\def\vb{{\bm{b}}}\def\vc{{\bm{c}}}\def\vd{{\bm{d}}}\def\ve{{\bm{e}}}\def\vf{{\bm{f}}}\def\vg{{\bm{g}}}\def\vh{{\bm{h}}}\def\vi{{\bm{i}}}\def\vj{{\bm{j}}}\def\vk{{\bm{k}}}\def\vl{{\bm{l}}}\def\vm{{\bm{m}}}\def\vn{{\bm{n}}}\def\vo{{\bm{o}}}\def\vp{{\bm{p}}}\def\vq{{\bm{q}}}\def\vr{{\bm{r}}}\def\vs{{\bm{s}}}\def\vt{{\bm{t}}}\def\vu{{\bm{u}}}\def\vv{{\bm{v}}}\def\vw{{\bm{w}}}\def\vx{{\bm{x}}}\def\vy{{\bm{y}}}\def\vz{{\bm{z}}}\def\vphi{{\bm{\phi}}}\def\vvphi{{\bm{\varphi}}}% Elements of vectors\def\evalpha{{\alpha}}\def\evbeta{{\beta}}\def\evepsilon{{\epsilon}}\def\evlambda{{\lambda}}\def\evomega{{\omega}}\def\evmu{{\mu}}\def\evpsi{{\psi}}\def\evsigma{{\sigma}}\def\evtheta{{\theta}}\def\eva{{a}}\def\evb{{b}}\def\evc{{c}}\def\evd{{d}}\def\eve{{e}}\def\evf{{f}}\def\evg{{g}}\def\evh{{h}}\def\evi{{i}}\def\evj{{j}}\def\evk{{k}}\def\evl{{l}}\def\evm{{m}}\def\evn{{n}}\def\evo{{o}}\def\evp{{p}}\def\evq{{q}}\def\evr{{r}}\def\evs{{s}}\def\evt{{t}}\def\evu{{u}}\def\evv{{v}}\def\evw{{w}}\def\evx{{x}}\def\evy{{y}}\def\evz{{z}}% Matrix\def\mA{{\bm{A}}}\def\mB{{\bm{B}}}\def\mC{{\bm{C}}}\def\mD{{\bm{D}}}\def\mE{{\bm{E}}}\def\mF{{\bm{F}}}\def\mG{{\bm{G}}}\def\mH{{\bm{H}}}\def\mI{{\bm{I}}}\def\mJ{{\bm{J}}}\def\mK{{\bm{K}}}\def\mL{{\bm{L}}}\def\mM{{\bm{M}}}\def\mN{{\bm{N}}}\def\mO{{\bm{O}}}\def\mP{{\bm{P}}}\def\mQ{{\bm{Q}}}\def\mR{{\bm{R}}}\def\mS{{\bm{S}}}\def\mT{{\bm{T}}}\def\mU{{\bm{U}}}\def\mV{{\bm{V}}}\def\mW{{\bm{W}}}\def\mX{{\bm{X}}}\def\mY{{\bm{Y}}}\def\mZ{{\bm{Z}}}\def\mBeta{{\bm{\beta}}}\def\mPhi{{\bm{\Phi}}}\def\mLambda{{\bm{\Lambda}}}\def\mSigma{{\bm{\Sigma}}}% Tensor\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}\def\tA{{\tens{A}}}\def\tB{{\tens{B}}}\def\tC{{\tens{C}}}\def\tD{{\tens{D}}}\def\tE{{\tens{E}}}\def\tF{{\tens{F}}}\def\tG{{\tens{G}}}\def\tH{{\tens{H}}}\def\tI{{\tens{I}}}\def\tJ{{\tens{J}}}\def\tK{{\tens{K}}}\def\tL{{\tens{L}}}\def\tM{{\tens{M}}}\def\tN{{\tens{N}}}\def\tO{{\tens{O}}}\def\tP{{\tens{P}}}\def\tQ{{\tens{Q}}}\def\tR{{\tens{R}}}\def\tS{{\tens{S}}}\def\tT{{\tens{T}}}\def\tU{{\tens{U}}}\def\tV{{\tens{V}}}\def\tW{{\tens{W}}}\def\tX{{\tens{X}}}\def\tY{{\tens{Y}}}\def\tZ{{\tens{Z}}}% Graph\def\gA{{\mathcal{A}}}\def\gB{{\mathcal{B}}}\def\gC{{\mathcal{C}}}\def\gD{{\mathcal{D}}}\def\gE{{\mathcal{E}}}\def\gF{{\mathcal{F}}}\def\gG{{\mathcal{G}}}\def\gH{{\mathcal{H}}}\def\gI{{\mathcal{I}}}\def\gJ{{\mathcal{J}}}\def\gK{{\mathcal{K}}}\def\gL{{\mathcal{L}}}\def\gM{{\mathcal{M}}}\def\gN{{\mathcal{N}}}\def\gO{{\mathcal{O}}}\def\gP{{\mathcal{P}}}\def\gQ{{\mathcal{Q}}}\def\gR{{\mathcal{R}}}\def\gS{{\mathcal{S}}}\def\gT{{\mathcal{T}}}\def\gU{{\mathcal{U}}}\def\gV{{\mathcal{V}}}\def\gW{{\mathcal{W}}}\def\gX{{\mathcal{X}}}\def\gY{{\mathcal{Y}}}\def\gZ{{\mathcal{Z}}}% Sets\def\sA{{\mathbb{A}}}\def\sB{{\mathbb{B}}}\def\sC{{\mathbb{C}}}\def\sD{{\mathbb{D}}}% Don't use a set called E, because this would be the same as our symbol% for expectation.\def\sF{{\mathbb{F}}}\def\sG{{\mathbb{G}}}\def\sH{{\mathbb{H}}}\def\sI{{\mathbb{I}}}\def\sJ{{\mathbb{J}}}\def\sK{{\mathbb{K}}}\def\sL{{\mathbb{L}}}\def\sM{{\mathbb{M}}}\def\sN{{\mathbb{N}}}\def\sO{{\mathbb{O}}}\def\sP{{\mathbb{P}}}\def\sQ{{\mathbb{Q}}}\def\sR{{\mathbb{R}}}\def\sS{{\mathbb{S}}}\def\sT{{\mathbb{T}}}\def\sU{{\mathbb{U}}}\def\sV{{\mathbb{V}}}\def\sW{{\mathbb{W}}}\def\sX{{\mathbb{X}}}\def\sY{{\mathbb{Y}}}\def\sZ{{\mathbb{Z}}}% Entries of a matrix\def\emLambda{{\Lambda}}\def\emA{{A}}\def\emB{{B}}\def\emC{{C}}\def\emD{{D}}\def\emE{{E}}\def\emF{{F}}\def\emG{{G}}\def\emH{{H}}\def\emI{{I}}\def\emJ{{J}}\def\emK{{K}}\def\emL{{L}}\def\emM{{M}}\def\emN{{N}}\def\emO{{O}}\def\emP{{P}}\def\emQ{{Q}}\def\emR{{R}}\def\emS{{S}}\def\emT{{T}}\def\emU{{U}}\def\emV{{V}}\def\emW{{W}}\def\emX{{X}}\def\emY{{Y}}\def\emZ{{Z}}\def\emSigma{{\Sigma}}% entries of a tensor% Same font as tensor, without \bm wrapper\newcommand{\etens}[1]{\mathsfit{#1}}\def\etLambda{{\etens{\Lambda}}}\def\etA{{\etens{A}}}\def\etB{{\etens{B}}}\def\etC{{\etens{C}}}\def\etD{{\etens{D}}}\def\etE{{\etens{E}}}\def\etF{{\etens{F}}}\def\etG{{\etens{G}}}\def\etH{{\etens{H}}}\def\etI{{\etens{I}}}\def\etJ{{\etens{J}}}\def\etK{{\etens{K}}}\def\etL{{\etens{L}}}\def\etM{{\etens{M}}}\def\etN{{\etens{N}}}\def\etO{{\etens{O}}}\def\etP{{\etens{P}}}\def\etQ{{\etens{Q}}}\def\etR{{\etens{R}}}\def\etS{{\etens{S}}}\def\etT{{\etens{T}}}\def\etU{{\etens{U}}}\def\etV{{\etens{V}}}\def\etW{{\etens{W}}}\def\etX{{\etens{X}}}\def\etY{{\etens{Y}}}\def\etZ{{\etens{Z}}}% The true underlying data generating distribution\newcommand{\pdata}{p_{\rm{data}}}% The empirical distribution defined by the training set\newcommand{\ptrain}{\hat{p}_{\rm{data}}}\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}% The model distribution\newcommand{\pmodel}{p_{\rm{model}}}\newcommand{\Pmodel}{P_{\rm{model}}}\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}% Stochastic autoencoder distributions\newcommand{\pencode}{p_{\rm{encoder}}}\newcommand{\pdecode}{p_{\rm{decoder}}}\newcommand{\precons}{p_{\rm{reconstruct}}}\newcommand{\laplace}{\mathrm{Laplace}}% Laplace distribution\newcommand{\E}{\mathbb{E}}\newcommand{\Ls}{\mathcal{L}}\newcommand{\R}{\mathbb{R}}\newcommand{\emp}{\tilde{p}}\newcommand{\lr}{\alpha}\newcommand{\reg}{\lambda}\newcommand{\rect}{\mathrm{rectifier}}\newcommand{\softmax}{\mathrm{softmax}}\newcommand{\sigmoid}{\sigma}\newcommand{\softplus}{\zeta}\newcommand{\KL}{D_{\mathrm{KL}}}\newcommand{\Var}{\mathrm{Var}}\newcommand{\standarderror}{\mathrm{SE}}\newcommand{\Cov}{\mathrm{Cov}}% Wolfram Mathworld says $L^2$ is for function spaces and $\ell^2$ is for vectors% But then they seem to use $L^2$ for vectors throughout the site, and so does% wikipedia.\newcommand{\normlzero}{L^0}\newcommand{\normlone}{L^1}\newcommand{\normltwo}{L^2}\newcommand{\normlp}{L^p}\newcommand{\normmax}{L^\infty}\newcommand{\parents}{Pa}% See usage in notation.tex. Chosen to match Daphne's book.\DeclareMathOperator*{\argmax}{arg\,max}\DeclareMathOperator*{\argmin}{arg\,min}\DeclareMathOperator{\sign}{sign}\DeclareMathOperator{\Tr}{Tr}\let\ab\allowbreak


\usepackage{graphicx}

\usepackage{wrapfig}
\usepackage{resizegather}
\usepackage{multicol}
\usepackage{natbib}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
% references
%\usepackage[bibstyle=authoryear, citestyle=authoryear-comp,natbib=true, hyperref=auto]{biblatex}
  %\nocite{*}


\usepackage{caption}
\usepackage{subcaption}


%\newcommand{\E}{\mathbb{E}}
%\newcommand{\V}{\mathbb{V}}
%\newcommand{\EE}[1]{\mathbb{E} \left[ #1 \right]}
%\newcommand{\VV}[1]{\mathbb{V} \left[ #1 \right]}
%\newcommand{\Cov}[1]{\mathrm{Cov} \left( #1 \right)}

\hyphenpenalty=1000

% poster template
% \usepackage[orientation=landscape,size=a0,scale=1.4,debug]{beamerposter}
\usepackage[orientation=portrait,size=a0,scale=1.4,debug]{beamerposter}
%\usepackage[orientation=landscape,size=custom,width=141.4,height=100,scale=1.7,debug]{beamerposter}
\usepackage[ruled, vlined]{algorithm2e}
\usetheme{zurichposter}




% document properties
\title{Graph HyperNetworks for Neural Architecture Search}
\author{Chris J. Zhang$^{1, 2}$, Mengye Ren$^{1, 3}$, Raquel Urtasun$^{1,3}$
}
\institute{
$^1$ Uber Advanced Technologies Group 
$^2$ Unversity of Waterloo, 
$^3$ University of Toronto}


%------------------------------------------------------------------------------
\begin{document}

\begin{frame}{}
\begin{columns}[t]

%-----------------------------------------------------------------------------
%                                                                     COLUMN 1
% ----------------------------------------------------------------------------
\begin{column}{.47\linewidth}
    %--------- Intro Block------------ 
    \vskip -0.5cm
    \begin{exampleblock}{Summary}
    \textbf{Motivation}:
        \begin{itemize}
            \item Neural architecture search is an expensive nested optimization 
         \begin{equation*}
			a^* = \argmin_a \gL_{val}(w^*(a),a), \; \; 
			w^*(a) = \argmin_w \gL_{train}(w, a)
		\end{equation*}
		\item Instead of using SGD to learn weights, use trained hypernetwork to generate weights
		\item Graph HyperNetworks (GHN) explicitly model the topology of architectures by learning on a computation graph representation
	\end{itemize}
	
	\textbf{Strengths}:
	\begin{itemize}
		\item Fast: Using 0.42 GPU days and random search we find an architecture with competitive results on CIFAR and is transferrable to ImageNet
		\item Flexible: Applicable to a wider range of network topologies. Applied to
anytime-prediction (previously unexplored by NAS) and outperformed existing manually designed state-of-the-art models.
	\end{itemize}
    \end{exampleblock}



%    %--------- Related Block------------ 
%    %\vspace{0.75in}
%	\begin{exampleblock}{Related Work}
%	   % \vspace{-0.8in}
%	\end{exampleblock}
    
   %--------- Background Block------------ 
   % \vspace{0.8in}
    \begin{exampleblock}{Background}
	    \textbf{Graph Neural Networks}: 
	    A collection of nodes and edges $(\gV, \gE)$,
	    where each node is a RNN $U$ that individually sends and
		receives messages along the edges, 
		forming embeddings $\vh_v^{(t)}$ 
		over the horizon of $t$ message passing steps. 
				\begin{equation*}
			\vh_v^{(t+1)} = 
			\begin{cases}
			U \left(\vh_v^{(t)}, \vm_v^{(t)} \right) \ \ & \text{if node $v$ is active},\\
			\vh_v^{(t)} \ \ & \text{otherwise},
			\end{cases}
		\end{equation*}
		\begin{equation*}
			\vm_v^{(t)}=\sum_{u\in N_{in}(v)} M \left(\vh_u^{(t)} \right)
		\end{equation*}
	\begin{equation*}
	\left\{\vh_v^{(t)} | v \in \gV\right\} = 
	G_\gA^{(t)} \left(\left\{\vh_v^{(0)} | v \in \gV \right\}; \vphi \right).
	\end{equation*}

		\textbf{Hypernetworks}:
	  	A neural network that generates the parameters of
		another network. For a typical deep feedforward network with $D$ layers, 
		the parameters of the
		$j$-th layer $W_j$ can be generated by a learned function $H$
		\begin{equation*}
			W_j = H(z_j), \ \ \forall j = 1, \dots, D,
		\end{equation*}
		where $z_j$ is the layer embedding, and $H$ is shared for all layers.  	
    \end{exampleblock}    

%	  %--------- Graphical representation------------ 
%	\begin{exampleblock}{Graphical Representation}
%    \begin{itemize}
%        \item Represent a given architecture as a directed acyclic graph 
%    			$\gA = (\gV, \gE)$, 
%    \item Nodes $v \in \gV$  have an associated computational operator
%    			 $f_v(\ \cdot \ ;w_v)$
%	\item Edges $e_{u \mapsto v} = (u, v) \in \gE$ represent the flow of
%			activation tensors from node $u$ to node $v$. 
%	\item Compute node's output 
%			$x_v = \sum_{e_{u \mapsto v} \in \gE} f_v(x_u; w_v), \ \ \forall v \in \gV.$  	
%    \end{itemize}
%
%	\end{exampleblock}

	%----- Benchmark Experiments -----
    \begin{exampleblock}{NAS Benchmarks}
    \textbf{CIFAR-10}: Comparison with NAS methods which employ random search (top half) and advanced search methods (e.g. RL) (bottom half)
  	\begin{table}[t]
%\vspace{-0.7cm}
\label{table:Results1}
\footnotesize
\begin{center}
\begin{tabular}{ c c c c } 
Method & Search Cost (GPU days) & Param $\times 10^6$ & Accuracy   \\ 
\hline
SMASHv1 \citep{brock2017smash} &? & 4.6 & 94.5 \\
SMASHv2 \citep{brock2017smash} & 3 & 16.0 & 96.0\\
One-Shot Top (F=32) \citep{bender2018understanding} & 4  & 2.7 $\pm$ 0.3 & 95.5 $\pm$ 0.1\\
One-Shot Top (F=64) \citep{bender2018understanding} & 4 & 10.4 $\pm$ 1.0 & 95.9 $\pm$ 0.2\\
\hline
\hline
Random (F=32) & - & 4.6 $\pm$ 0.6 & 94.6 $\pm$ 0.3\\ 
GHN Top (F=32) &  0.42  & 5.1 $\pm$ 0.6 & 95.7 $\pm$ 0.1\\ 
\hline\hline\hline
NASNet-A  \citep{zoph2017learning} & 1800 & 3.3 & 97.35 \\
%AmoebaNet-A  + CutOut \cite{real2018regularized} & 1 GPU, 0.45 days & 4.6 & 97.11 \\
%AmoebaNet-B + CutOut \cite{real2018regularized} & 1 GPU, 0.45 days & 4.6 & 97.11 \\
ENAS Cell search  \citep{pham2018efficient} & 0.45  & 4.6 & 97.11 \\
DARTS (first order)  \citep{liu2018darts} &  1.5  & 2.9  & 97.06  \\
DARTS (second order) \citep{liu2018darts} & 4  & 3.4 & 97.17 $\pm$ 0.06\\
\hline
\hline
GHN Top-Best, 1K (F=32) & 0.84  & 5.7 & 97.16 $\pm$ 0.07 \\
\end{tabular}
\end{center}
%\footnotesize{$^*$ \cite{bender2018understanding} 16 GPU hours training + 80 GPU hours evaluation. }
\end{table}

	\textbf{ImageNet Mobile}: Comparison with NAS methods which employ advanced search methods (e.g. RL)
	% !TEX root = top.tex\begin{table}[t]
\label{table:Results3}
\footnotesize
\begin{center}
\begin{tabular}{ c c c c c c} 
Method & Search Cost & Param & FLOPs  & \multicolumn{2}{c}{Accuracy}   \\ 
& (GPU days) & $\times 10^6$  & $\times 10^6$ & Top 1 & Top 5 \\
\hline
%One-Shot Top (F=24) \citep{bender2018understanding} & 3.3+? $^*$ & 6.8 $\pm$ 0.9 & 70.7 $\pm$ 0.6\\
%One-Shot Top (F=32) \citep{bender2018understanding} & 3.3+? $^*$ & 11.9 $\pm$ 1.5 & 72.6 $\pm$ 0.4\\
NASNet-A  \citep{zoph2017learning} & 1800 & 5.3 & 564& 74.0 & 91.6 \\
%NASNet-B  \citep{zoph2017learning} & 1800 & 5.3 & 72.8 \\
NASNet-C  \citep{zoph2017learning} & 1800 & 4.9 & 558 & 72.5 & 91.0 \\
AmoebaNet-A \citep{real2018regularized} & 3150 & 5.1 & 555& 74.5  & 92.0 \\
%AmoebaNet-B \citep{real2018regularized} & 3150 & 5.3 & 74.0  \\
AmoebaNet-C \citep{real2018regularized} & 3150 & 6.4 & 570 & 75.7 & 92.4 \\
PNAS \citep{liu2017progressive} & 225 & 	5.1 &  588 & 74.2 & 91.9 \\
DARTS (second order) \citep{liu2018darts} & 4  & 4.9 & 595 &73.1 & 91.0\\
\hline
\hline
GHN Top-Best, 1K & 0.84  & 6.1  &569& 73.0 & 91.3  \\
%GHN Top-Best, 20K & 4.17  & 5.0  & 73.2  \\
\end{tabular}
\end{center}
\end{table}
	  \end{exampleblock}
	  
	  
    	%----- Correlation Experiments -----
	\begin{exampleblock}{Correlation Experiments}
	\vspace{-1cm}
	%Benchmarking correlation between predicted and true performance
		\begin{tabular}{p{0.6\textwidth}p{0.38\textwidth}}
			\begin{table}[t]
			\center\footnotesize
			Benchmarking correlation between predicted and true performance
					\footnotesize
					\begin{center}
					\begin{tabular}{ c c c c c} 
					Method & \multicolumn{2}{c}{Computation cost}   & \multicolumn{2}{c}{Correlation}    \\ 
					%\hline
					 & Initial  & Per arch.   & Rand-100 & Top-50   \\ 
					 & (GPU hours) & (GPU seconds) &&\\
					\hline
					SGD 10 Steps & - & 0.9 & 0.26 & -0.05\\
					SGD 100 Steps & - & 9 & 0.59 & 0.06\\
					SGD 200 Steps & - & 18 & 0.62 & 0.20 \\
					SGD 1000 Steps & - & 90 & 0.77 & 0.26 \\
					One-Shot & 9.8 & 0.06 & 0.58 & 0.31\\
					\hline
					\hline
					GHN & 6.1 & 0.08 & 0.68 & 0.48
				\end{tabular}
				\end{center}
			\end{table}	

			
			 &
			\center\footnotesize
			Effects of sharing parameters (SP) and passing embeddings (PE)
			 \vspace{1cm}
		   	\begin{table}[t]
				\footnotesize
				\begin{center}
				\begin{tabular}{ c c c c} 
				SP & PE & \multicolumn{2}{c}{Correlation}    \\ 
				 &&  Rand-100 & Top-50   \\ 
				\hline
				\xmark & \xmark & 0.24 & 0.15\\
				\xmark & \cmark  &  0.44 & 0.37\\
				\cmark & \cmark  & 0.68 & 0.48 
				\end{tabular}
				\end{center}
			\end{table}

			\end{tabular}
   \end{exampleblock}
    



\end{column}
\vspace{0pt}


%-----------------------------------------------------------------------------
%                                                                     COLUMN 2
% ----------------------------------------------------------------------------

\begin{column}{.47\linewidth}
    \vskip -0.5cm
    
      %--------- Graph HyperNetworks------------ 
    \begin{exampleblock}{Graph HyperNetworks}
    %\vspace{-0.4in}
	\begin{figure}
		\includegraphics[width=0.95\linewidth]{../figures/main3.pdf}
	\end{figure}    
	%\vspace{-0.5in}
	\begin{itemize}
		\item Given an input architecture with graphical representation $\gA$, 
				construct a homomorphic GNN $G_\gA$ with the same topology
		\item After graph message-passing, a shared hypernetwork (2 layer MLP) is applied to all the nodes to generate the parameters of the network
		\begin{align*}
			\tilde{\vw}=\left\{\tilde{\vw}_v | \ v \in \gV  \right\}
			   &= \left\{H\left(\vh_v^{(T)}; \vvphi\right) \big| \ v \in \gV  \right\} \\
			  &=  \left\{H\left(\vh; \vvphi\right) \big| \ \vh \in G_\gA^{(T)}\left(\left\{\vh_v^{(0)} \big| v \in \gV \right\}; \vphi\right)\right\} \\
			  &= GHN\left(\gA; \vphi, \vvphi\right).
		\end{align*}
		\item Compute gradients GHN parameters
				$\vphi, \vvphi$ using the chain rule:
				\begin{equation*}
				\nabla_{\vphi, \vvphi}{\gL_{train}(\tilde{\vw})} = \nabla_{\tilde{\vw}}{
				\gL_{train}(\tilde{\vw})} \cdot \nabla_{\vphi, \vvphi}{\tilde{\vw}}
				\end{equation*}
				\begin{align*}
				 \nabla_\vphi{\tilde{\vw}} &= \left\{ \nabla_\vh H( \vh; \vvphi) \cdot \nabla_\vphi \vh \ \big| \ \vh \in G^{(T)} \left( \{\vh_v^{(0)}\}, \gA, \vphi \right) \right\}, \\ 
				 \nabla_\vvphi{\tilde{\vw}} &= \left\{ \nabla_\vvphi H( \vh_v^{(T)}; \vvphi) \ \big| \ v \in \gV \right\} 
				\end{align*}
	\end{itemize}
	\end{exampleblock}
	
  %--------- Architectural Motifs and Stacked GNNs------------ 
  	\begin{exampleblock}{Architectural Motifs and Stacked GNNs}
  		\begin{itemize}
			\item Searching for repeated modules often improves results
			\item Stack GHN along depth dimension, sharing parameters across modules
			\item Pass along graph level embeddings to subsequent GHN modules
		\end{itemize}
	\vspace{0.8cm}
  	\begin{tabular}{p{0.58\textwidth}p{0.35\textwidth}}
  	\hspace{1cm}
  	\vspace{0.8cm}
	\parbox{1cm}{\small\begin{align*}
	%\vh_{\gA_0} &= 0,\\
		\vh_{\gA_i} &= \frac{1}{|\gV_i|}\sum_{v\in \gV_i} \left\{\vh_v^{(T)} | v \in \gV_i \right\} \\
		                 &= \frac{1}{|\gV_i|}\sum G_{\gA_i}^{(T)}\left(\left\{\vh_v^{(0)} | v \in \gV_i \right\}, \vh_{\gA_{i-1}}; \vphi\right) %\ \ \forall i > 0 
		\end{align*}} &  	
  		\vspace{-1.6in}
	  	\begin{figure}
			\includegraphics[width=\linewidth]{../figures/graph_cells.pdf}
		\end{figure}   
	\end{tabular}
	\vspace{-1in}
	\end{exampleblock}
	
  %--------- Forward backward messages----------- 
  	\begin{exampleblock}{Forward-backward GNN message passing}
	\begin{figure}[t]
	\centering
	\begin{minipage}{.48\textwidth}
	  \centering
	  \includegraphics[page=1, width=0.9\linewidth]{../figures/graph_propagation.pdf}
	  \captionsetup{labelformat=empty}
	  \captionof{figure}{Synchronous message passing}
	\label{table:Results4}
	  \label{fig:test1}
	\end{minipage}%
	\begin{minipage}{.48\textwidth}
	  \centering
	  \includegraphics[page=2, width=0.9\linewidth]{../figures/graph_propagation.pdf}
	  \captionsetup{labelformat=empty}
	  \captionof{figure}{Forward-backward message passing}
	  \label{fig:test2}
	\end{minipage}
	\end{figure}  	
  	
 	\begin{itemize}
 		\item Standard synchronous message passing updates all nodes simultanously
 		\item Passing across graphs with large diameters often runs into vanishing gradient problems
 		\item The forward-backward propagation scheme alleviates this by mimicking the order of node execution in the backpropagation algorithm
% 		\begin{equation*}
% 		 		\small
%			\vh_v^{(t+1)} = 
%			\begin{cases}
%			U \left(\vh_v^{(t)}, \vm_v^{(t)} \right) \ \ & \text{if } s(t) = v \text{ and } 1 \le t \le |\gV|\\
%			  \ \ & \text{or if } s(2|\gV| - t) = v \text{ and } |\gV| + 1 \le t < 2|\gV|,\\
%			\vh_v^{(t)} \ \ & \text{otherwise}.
%			\end{cases}
%		\end{equation*}
		\item Propagating information across a graph with diameter $|\gV|$ reduced from  $O(|\gV|^2)$ to $O(|\gV|)$ messages
 	\end{itemize}
    \end{exampleblock}
    
    
    
	%----- Anytime Experiments -----
	  \begin{exampleblock}{Anytime Prediction}
	  \textbf{Formulation:} Minimize the expected loss 
$L(f) = \E\left[ L\left( f(\rvx), B \right)\right]_{P(\rvx, B)}$
for a model $f(\cdot)$, a test example $\rvx$ and a non-deterministic computational budget $B$
drawn from the joint distribution $P(\rvx, B)$. 
	\\
	\vspace{1cm}
	\textbf{Approach:} Each node is given the following additional properties: 
	\begin{itemize}
		\item The spatial size it operates at
		\item If an early-exit classifier is attached to it.
	\end{itemize}	
		  \vspace{0.4cm}
	\begin{figure}[t]
	\centering
	\begin{minipage}{.48\textwidth}
	  \centering
	  \includegraphics[width=0.8\linewidth]{../figures/anytime_compare.pdf}
	\label{table:Results4}
	  \label{fig:test1}
	\end{minipage}%
	\begin{minipage}{.48\textwidth}
	  \centering
	  \includegraphics[width=0.8\linewidth]{../figures/anytime_randoms.pdf}
	  \label{fig:test2}
	\end{minipage}
	\end{figure}
	 \vspace{0.32cm}

    \end{exampleblock}

    
    

   
   
    .\\\vspace{10cm}
    \bibliographystyle{iclr2019_conference}
  \footnotesize
  \bibliography{iclr2019_conference}
\end{column}
\vspace{0pt}


%-----------------------------------------------------------------------------
%                                                                     COLUMN 3
% ----------------------------------------------------------------------------


\end{columns}
\end{frame}

\end{document}
