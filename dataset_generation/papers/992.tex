
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}

\makeatletter
\newcommand{\@chapapp}{\relax}%
\makeatother

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}


% % % % % % % % % % % % % Additional PACKAGES
\usepackage{color}
\usepackage[table]{xcolor} 
\usepackage{tabularx} % better table handling
\usepackage{gensymb} % \degree
\usepackage{amssymb} % \mathbb{}
\usepackage{bm} % \boldsymbol
%\usepackage{booktabs} % for \toprule, \bottomrule, etc.
\usepackage{arydshln}
\usepackage{amsmath}
\usepackage{breqn}
\usepackage{soul}
%\usepackage{subfigure}
\usepackage{epstopdf}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subfig}
\usepackage{hhline}
\usepackage[normalem]{ulem}
\usepackage[super]{nth}
\usepackage[]{appendix}
\usepackage{multibib}
\newcites{appendix}{Appendix References}

\makeatletter
\newcommand\footnoteref[1]{\protected@xdef\@thefnmark{\ref{#1}}\@footnotemark}
\makeatother


% % % % % % % % % % % % % NEW COMMANDS
\DeclareCaptionLabelFormat{andfigure}{#1~#2~\& \tablename~\thetable}

\newcommand\norm[1]{\left\lVert#1\right\rVert}

\newcommand{\martin}[1]{\textcolor{red}{[#1]}} %for displaying red texts
\newcommand{\darko}[1]{\textcolor{green}{[#1]}} 
\newcommand{\christian}[1]{\textcolor{blue}{[#1]}} 
\newcommand{\thomas}[1]{\textcolor{purple}{[#1]}} 
\newcommand{\msout}[1]{\martin{\st{#1}}}
\newcommand{\dsout}[1]{\darko{\st{#1}}}
\newcommand{\csout}[1]{\christian{\st{#1}}}
\newcommand{\tsout}[1]{\thomas{\st{#1}}}
% tabularx styles
\newcolumntype{L}[1]{>{\hsize=#1\hsize\raggedright\arraybackslash}X}%
\newcolumntype{R}[1]{>{\hsize=#1\hsize\raggedleft\arraybackslash}X}%
\newcolumntype{C}[1]{>{\hsize=#1\hsize\centering\arraybackslash}X}%

\newcolumntype{R}[2]{%
    >{\adjustbox{angle=#1,lap=\width-(#2)}\bgroup}%
    l%
    <{\egroup}%
}
\newcommand*\rot{\multicolumn{1}{R{45}{1em}}}% no optional argument here, please!

\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
%\title{Instance Segmentation and Tracking with\\ Cosine Embeddings and Recurrent CNNs}
\title{Instance Segmentation and Tracking with Cosine Embeddings and Recurrent Hourglass Networks}

% a short form should be given in case it is too long for the running head
\titlerunning{Cosine Embeddings with Recurrent Hourglass Networks}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
%\author{*, *, *, *}

\author{Christian Payer\inst{1,\thanks{This work was supported by the Austrian Science Fund (FWF): P28078-N33.}} \and
Darko \v{S}tern\inst{2} \and
Thomas Neff\inst{1} \and\\
Horst Bischof\inst{1} \and
Martin Urschler\inst{2,3}}

% index{Payer, Christian}
% index{Stern, Darko}
% index{Neff, Thomas}
% index{Bischof, Horst}
% index{Urschler, Martin}

\authorrunning{Payer et al.}
%\authorrunning{Anonymous et al.}

% (feature abused for this document to repeat the title also on left hand pages)

\institute{
$^{1}$Institute of Computer Graphics and Vision, Graz University of Technology, Austria\\
$^{2}$Ludwig Boltzmann Institute for Clinical Forensic Imaging, Graz, Austria\\
$^{3}$BioTechMed-Graz, Graz, Austria
}


%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

%\toctitle{Lecture Notes in Computer Science}
%\tocauthor{Authors' Instructions}
\maketitle


\begin{abstract}
Different to semantic segmentation, instance segmentation assigns unique labels to each individual instance of the same class.
In this work, we propose a novel recurrent fully convolutional network architecture for tracking such instance segmentations over time. 
The network architecture incorporates convolutional gated recurrent units (\mbox{ConvGRU}) into a stacked hourglass network to utilize temporal video information.
Furthermore, we train the network with a novel embedding loss based on cosine similarities, such that the network predicts unique embeddings for every instance throughout videos.
Afterwards, these embeddings are clustered among subsequent video frames to create the final tracked instance segmentations.
We evaluate the recurrent hourglass network by segmenting left ventricles in MR videos of the heart, where it outperforms a network that does not incorporate video information.
Furthermore, we show applicability of the cosine embedding loss for segmenting leaf instances on still images of plants.
Finally, we evaluate the framework for instance segmentation and tracking on six datasets of the ISBI celltracking challenge, where it shows state-of-the-art performance.
\keywords{cell, tracking, segmentation, instances, recurrent, video, embeddings}
\end{abstract}

\section{Introduction}
\label{sec:intro}

\nocite{*}

%\christian{Main TODO:
%add missing info in method
%add missing info in setup
%discuss results more thoroughly}

%Instance segmentation is important for many computer vision-based tasks, e.g., autonomous driving
Instance segmentation plays an important role in biomedical imaging tasks like cell migration, but also in computer vision based tasks like scene understanding.
%Instance segmentation plays an important role in biomedical imaging tasks, e.g., cell migration, and computer vision based tasks, e.g., scenes understanding.
%It is considerably more difficult than semantic segmentation (i.e., assigning class labels to pixels), as it needs to additionally distinguish between nearby or occluded instances. 
%It is considerably more difficult than semantic segmentation, as it does not only assigning class labels to pixels, but also distinguishes between instances within class labels.
It is considerably more difficult than semantic segmentation (e.g.,~\cite{Payer2018}), since instance segmentation does not only assign class labels to pixels, but also distinguishes between instances within each class, e.g., each individual person on an image from a surveillance camera is assigned a unique ID.
%each individual person within the class of persons is assigned a unique ID.

%Recently, neural networks trained for semantic segmentation have been used successfully in cell instance segmentation, 
%by identifying connected components on the predicted class labels~\cite{Ronneberger2015}.

Mainly due to the high performance of the U-Net~\cite{Ronneberger2015}, semantic segmentation has been successfully used as a first step in medical instance segmentation tasks, e.g., cell tracking.
However, for instances to be separated as connected components during postprocessing, borders of instances have to be treated with special care.
In the computer vision community, many methods for instance segmentation have in common that they solely segment one instance at a time.
In~\cite{He2017}, all instances are first detected and independently segmented, while in~\cite{Ren2017}, recurrent networks are used to memorize which instances were already segmented.
% segmenting one instance at a time, e.g., with recurrent networks that memorize which instances are already segmented~\cite{Ren2017}, or with region proposals like Mask-RCNN~\cite{He2017}.
Segmenting solely one instance at a time can be problematic when hundreds of instances are visible in the image, as often is the case with e.g., cell instance segmentation.
%However, detecting one instance at a time may be problematic when hundreds of instances are visible on the image.
%Recent methods that predict embeddings~\cite{Newell2017,Kong2017} are suitable for segmenting each instance simultaneously, by predicting embeddings for all pixels at once.
Recent methods are segmenting each instance simultaneously, by predicting embeddings for all pixels at once~\cite{Newell2017,Kong2017}.
%Recently, methods are segmenting each instance simultaneously, by predicting embeddings that are similar within instances and different between instances for all pixels at once~\cite{Newell2017,Kong2017}.
%Embeddings of the same instance are required to be similar, while embeddings of different instances are dissimilar.
These embeddings have similar values within an instance, but differ among instances.
%In the task of cell segmentation and tracking, temporal information has to be used for preserving instance IDs throughout videos.
%In the task of cell segmentation and tracking, temporal information has to be used for preserving instance IDs throughout videos.
In the task of cell segmentation and tracking, temporal information is an important cue to establish coherence between frames, thus preserving instances throughout videos.
Despite improvements of instance segmentation using embeddings, to the best of our knowledge, combining them with temporal information for tracking instance segmentations has not been presented.
%that predict embeddings~\cite{Newell2017,Kong2017} are suitable for segmenting each instance simultaneously, by predicting embeddings for all pixels at once.
%Such pixel-wise embeddings are required to be similar for pixels of the same instance, and dissimilar for different instances.

%Recently, the \mbox{U-Net}~\cite{Ronneberger2015} has been used successfully in cell instance segmentation.
%%Although originally developed for semantic segmentation, the U-Net is able to detect instances by identifying connected components on the predicted class labels.
%Although trained for semantic segmentation, the U-Net is able to detect instances by identifying connected components on the predicted class labels.
%
%Although originally developed for semantic segmentation, the \mbox{U-Net}~\cite{Ronneberger2015} has been used successfully in cell instance segmentation.
%By classifying pixels as foreground and background, the U-Net is able to detect instances by performing a connected component analysis.
%As it does not distinguish between individual instances, but only class labels, it has to perform a connected component analysis
%
%Although originally developed for semantic segmentation, the \mbox{U-Net}~\cite{Ronneberger2015} has been used successfully in cell instance segmentation.
%By classifying pixels as foreground and background, the U-Net is able to detect instances by performing a connected component analysis.
%As it does not distinguish between individual instances, but only class labels, it has to perform a connected component analysis
%%However, it uses a pixel-wise classification loss that distinguishes only between cell and non-cell labels, but not individual cell instances, and has to carefully handle contacting cells.

%Recent methods for instance segmentation often detect one instance at a time, e.g., with recurrent networks that memorize which instances are already segmented~\cite{Ren2017}, or with region proposals like Mask-RCNN~\cite{He2017}.
%However, detecting one instance at a time may be problematic when hundreds of instances are visible on the image.
%In contrast, methods that predict embeddings~\cite{Newell2017,Kong2017} are suitable for segmenting each instance simultaneously, by predicting embeddings for all pixels at once.
%Such pixel-wise embeddings are required to be similar for pixels of the same instance, and dissimilar for different instances.
%%As these methods detect each instance on an image at once, they are more suitable for segmenting cell images.

%Despite these recent improvements of instance segmentation, utilizing temporal information for improving instance segmentations - or even tracking instance segmentations - has not been fully explored. %did not gain as much interest.
%Despite these recent improvements of instance segmentation, utilizing temporal information in the task of tracking instance segmentations has not been fully explored. %did not gain as much interest.
%However, the computer vision community has shown that incorporating temporal information into neural networks has great potential for applications like moving objects segmentation~\cite{Tokmakov2017}.
 %did not gain as much interest.
%However, the computer vision community has shown that incorporating temporal information into neural networks has great potential for applications like moving objects segmentation~\cite{Tokmakov2017}.


%A prominent task from the biomedical imaging community, which features both segmenting and tracking of instances, is the analysis of cell microscopy images.
%Not only the segmentation of cells for delineating or counting, but also tracking instances for examining temporal behavior like cell growing or mitosis are important for analyzing healthy or dysfunctional cells.
%Recently, the \mbox{U-Net}~\cite{Ronneberger2015} achieved great success in cell segmentation.
%However, it uses a pixel-wise classification loss that distinguishes only between cell and non-cell labels, but not individual cell instances, and has to carefully handle contacting cells.
%%This is problematic for contacting cells, as no background is visible in between them.
%%To deal with this problem, the original implementation of the \mbox{U-Net}~\cite{Ronneberger2015} creates artificial background borders with high loss weights between touching cells.
%Furthermore, the \mbox{U-Net} does not incorporate temporal information, which could further improve segmentation as well as tracking performance.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/overview_v2.pdf}
\caption{Overview of our proposed framework showing input image, propagation of cosine embeddings from frame $t$ to frame $t+1$ (three randomly chosen embedding dimensions as RGB channels), and resulting clustered instances.}
\label{fig:overview}
\end{figure}

In this paper, we propose to use recurrent fully convolutional networks for embedding-based instance segmentation and tracking.
%Our proposed network architecture, which is based on the stacked hourglass network~\cite{Newell2016}, integrates \mbox{ConvGRU}~\cite{Ballas2015} recurrent units for memorizing temporal information. % inside a fully convolutional network.
To memorize temporal information, we integrate convolutional gated recurrent units (\mbox{ConvGRU}~\cite{Ballas2015}) into a stacked hourglass network~\cite{Newell2016}. % inside a fully convolutional network.
%Furthermore, we propose a novel embedding loss based on cosine similarities.
%Differently to~\cite{Newell2017} and~\cite{Kong2017}, we do not randomly sample pixel pairs from the image to compare embeddings, but we densely sample whole instance masks, where we exploit the fact that only neighboring instances need to have different embeddings (see four color map theorem~\cite{Appel1976}).
%Furthermore, we use an embedding loss based on cosine similarities, where we exploit the fact that only neighboring instances need to have different embeddings (see four color map theorem~\cite{Appel1976}).
Furthermore, we use a novel embedding loss based on cosine similarities, where we exploit the four color map theorem~\cite{Appel1976}, by requiring only neighboring instances to have different embeddings.
%by assigning different embeddings only to neighboring instances.
%the neighboring instances need to have different embeddings (see four color map theorem~\cite{Appel1976}).
%A subsequent clustering of the embeddings results in the final segmented and tracked instances.
%By clustering the outputs of the embeddings, our framework performs instance segmentation.
%In combination, our proposed framework propagates embeddings for each instance from frame to frame for.

%%%%%%%%%%%%%%%%%%%%%%%%% old intro %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Analyzing cell images from microscopy is important to gain biological insights of cell behavior.
%Not only the segmentation of cell images for delineating or counting, but also cell tracking for examining temporal behavior like cell growing or mitosis are important for analyzing healthy or diseased cells.
%%To objectively compare the state-of-the-art of cell tracking algorithms, challenges that analyze segmentation and tracking performance like~\cite{Ulman2017} are needed.
%%Although there exist well performing cell tracking methods that do not use machine learning, 
%Both the easy adaptability of machine learning algorithms to different imaging modalities as well as the recent success of deep learning~\cite{LeCun2015} suggest to use machine learning methods for cell tracking.
%%This is further confirmed by recent successes of deep learning not only in cell tracking algorithms, e.g., the \mbox{U-Net}~\cite{Ronneberger2015}, but also in other biomedical imaging tasks, i.e., landmark localization~\cite{Payer2016}.
%
%Recently the \mbox{U-Net}~\cite{Ronneberger2015} achieved great success in cell segmentation and tracking.
%%A drawback in these recent deep learning algorithms is the unsuitable loss function.
%However, it uses a pixel-wise classification loss that distinguishes only between cell and non-cell labels, but not individual cell instances.
%%This is problematic, when cells are touching each other, as the border between touching cells cannot be distinguished.
%%This is problematic for contacting cells, when no background label is visible in between.
%%The original implementation of the \mbox{U-Net}~\cite{Ronneberger2015} deals with this problem by creating artificial background borders between touching cells, and assigns these border high weight in the loss function.
%This is problematic for contacting cells, as no background is visible in between them.
%To deal with this problem, the original implementation of the \mbox{U-Net}~\cite{Ronneberger2015} creates artificial background borders with high loss weights between touching cells.
%%, and assigns these border high weight in the loss function.
%%However, this may lead to wrongly detected borders inside cells and still not manages to solve this task satisfactorily.
%However, directly segmenting the individual cell instances instead of only cell or non-cell labels would remove this drawback. 
%% If we design the methods to detect not only foreground and background labels, but individual labels for each instance, we would not have these drawbacks.
%
%%This so called instance segmentation received much attention in the computer vision community.
%%Recently, methods that predict bounding boxes of each instance, e.g., Mask-RCNN~\cite{He2017}, received much interest.
%In the computer vision community, instance segmentation received much attention, e.g., methods based on region proposals like Mask-RCNN~\cite{He2017}, and methods memorizing which instances they already detected with recurrent connections like~\cite{Ren2017}.
%%Furthermore, methods using recurrent connections (e.g., long short term memory LSTM~\cite{Hochreiter1997}, gated recurrent unit GRU~\cite{Cho2014}) detect one instance at a time and memorize, which instance they already detected~\cite{Romera-Paredes2016,Ren2017}.
%However, these methods detect only one instance at a time, which is problematic when there are hundreds of instances on one image as it is the case in cell microscopy images.
%%Other recent methods use a different approach and detect pixel-wise embedding vectors of each instance visible on the image at once~\cite{Newell2017,Kong2017}.
%%The  these embedding vectors are that pixels belonging to the same instance have simliar embedding vectors, while embedding vectors of pixels belonging to different instances have to have embedding vectors that are different.
%Other recent methods detect pixel-wise embedding vectors that are required to be similar for pixels of the same instance, and dissimilar for pixels of different instances.
%As these methods detect each instance on the image at once, they are more suitable for segmenting cell images.
%%The properties of the embedding vectors are that pixels belonging to the same instance have to have similar embedding vectors, while embedding vectors of pixels belonging to different instances have to have embedding vectors that are different.
%
%\christian{I don't like this transition at all...}
%A drawback of applying recent methods for instance segmentation for cell tracking is that they are not incorporating video information in the network architecture.
%%Another drawback of current deep learning methods for cell tracking is that they are not using video information directly in the network, although cell movements and changes in shape over time provide invaluable information for segmentation and tracking.
%However, the computer vision community has shown that incorporating video information with convolutional recurrent layers (e.g., convolutional gated recurrent unit \mbox{ConvGRU}~\cite{Ballas2015}) greatly improves results for tasks like moving objects segmentation~\cite{Tokmakov2017}.
%
%In this paper, we propose to use instance segmentation with embedding vectors combined with recurrent fully convolutional networks that incorporate video information.
%We propose a novel embedding loss adapted from~\cite{Kong2017} that is based on cosine similarities and exploits the fact that only neighboring instances need to have different embedding vectors (four color theorem~\cite{Appel1976}).
%%By clustering the outputs of the embedding vectors, our framework performs instance segmentation.
%Furthermore, our novel network architecture, which is based on the stacked hourglass network~\cite{Newell2016}, integrates \mbox{ConvGRU} recurrent units for memorizing video information into a fully convolutional network.
%This way, the network is able to propagate embedding vectors for each instance throughout videos, while
%subsequent clustering of the embedding vectors leads to the final cell instances.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%drawback unet: touching cells -> need for border weight
%better: instance segmentation
%recent success mask-rcnn~\cite{He2017}, recurrent detection~\cite{Romera-Paredes2016,Ren2017} -> drawback: one instance at a time - won't work for cell images
%other recent developments: embedding losses~\cite{Newell2017,Kong2017} -> more suitable
%
%another drawback of current methods: no video information
%inspired by convgru/convlstm that use video information \cite{Tao2017,Tokmakov2017} -> use it for cell tracking to propagate embeddings
%
%Our contributions: we use novel loss based on cosine embeddings and combine it with fcn with convgru to propagate embeddings throughout videos.
%clustering with hdbscan


%Deep Learning~\cite{LeCun2015}
%\mbox{U-Net} \cite{Ronneberger2015}
%FCN \cite{Long2015}
%Hourglass Networks \cite{Newell2016}
%Embedding Loss \cite{Harley2017}
%HDBSCAN journal \cite{Campello2015}
%Celltracking \cite{Maska2014,Ulman2017}
%Heart Segmentation dataset \cite{Suinesiaputra2014}
%Plant dataset \cite{Minervini2016,Scharr2016}
%HDBSCAN conference \cite{Campello2013}
%HDBSCAN library \cite{McInnes2017}

%Instance Segmentation
%Methods based on R-CNN \cite{Girshick2014} and Mask R-CNN \cite{He2017}
%(first) convolutional LSTM, one instance at a time \cite{Romera-Paredes2016,Ren2017}
%Associative Embedding \cite{Newell2017}
%Cosine embedding loss combined with mean shift based on rnn, \cite{Kong2017}

%Convolutional LSTM/GRU
%first, for weather forecasting \cite{Xingjian2015}
%first GRU at the same time, for action recognition \cite{Ballas2015}
%LSTM \cite{Hochreiter1997}, GRU \cite{Cho2014}
%Convolutional LSTM or GRU for video information \cite{Tao2017,Tokmakov2017}






\section{Instance Segmentation and Tracking}
\label{sec:instance_tracking}


%In Fig.~\ref{fig:overview}, our proposed framework is shown for cell instance segmentation and tracking.
Figure~\ref{fig:overview} shows our proposed framework on a cell instance segmentation and tracking example.
To distinguish cell instances, they are represented as embeddings at different time points.
By representing temporal sequences of embeddings in a recurrent hourglass network, a predictor can be learnt from the data, which allows tracking of embeddings also in the case of mitosis events.
%cell instances and detection of mitosis events.
%By temporally tracking embeddings using a recurrent hourglass network, mitosis events can be learnt from the data for prediction.
To finally generate instance segmentations, clustering of the predicted embeddings is performed.

%Our proposed framework for segmentation tracking consists of three major parts:
%First, it integrates recurrent layers into a stacked hourglass network to propagate temporal information to allow tracking.
%Second, it uses an embedding loss based on cosine similarities to distinguish between instances.
%Third, it performs clustering of the predicted embeddings to generate the final instance segmentations.

%\begin{figure}
%\centering
%\subfloat[][Input]{\includegraphics[width=0.24\textwidth]{figures/first_input.png}\label{sibfig:input}}\hfill
%\subfloat[][Embeddings]{\includegraphics[width=0.24\textwidth]{figures/first_embeddings.png}\label{subfig:embeddings}}\hfill
%\subfloat[][Embeddings]{\includegraphics[width=0.24\textwidth]{figures/second_embeddings.png}\label{subfig:embeddings}}\hfill
%\subfloat[][Segmentation]{\includegraphics[width=0.24\textwidth]{figures/second_seg.png}\label{subfig:segmentation}}
%\caption{Example image of the DIC-C2DH-HeLa dataset, predicted embeddings (three randomly chosen dimensions as RGB channels) and clustered instances.}
%\label{fig:embedding_example}
%\end{figure}

\subsection{Recurrent Stacked Hourglass Network}
\label{subsec:network}

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/architecture.pdf}
\caption{Overview of the recurrent stacked hourglass network with two hourglasses and three levels.
Yellow bars: input; blue boxes: convolutions; red boxes: \mbox{ConvGRU}; dashed black box: concatenation; green boxes: embeddings.}
\label{fig:architecture}
\end{figure}

%We modify the stacked hourglass architecture~\cite{Newell2016} and integrate convolutional gated recurrent units (\mbox{ConvGRU})~\cite{Ballas2015} to incorporate video information and to allow tracking of embeddings for subsequent frames, see Fig.~\ref{fig:architecture}.
We modify the stacked hourglass architecture~\cite{Newell2016} by integrating \mbox{ConvGRU}~\cite{Ballas2015} to propagate temporal information, as shown in Fig.~\ref{fig:architecture}.
%To incorporate video information into the networks and to allow tracking of instances for subsequent frames, we modify the stacked hourglass architecture~\cite{Newell2016} and integrate convolutional gated recurrent units (\mbox{ConvGRU})~\cite{Ballas2015}, see Fig.~\ref{fig:architecture}.
%This way, the network can memorize previous frames and is able to propagate information throughout videos.
%We use a modification of the stacked hourglass network that incorporates recurrent blocks in every level, see Fig.~\ref{fig:architecture}.
%Differently from the original stacked hourglass network, we use single convolution layers with $3\times3$ filters and 64 outputs for all blocks in the contracting and expanding paths.
Differently from the original stacked hourglass network, we use single convolution layers with $3\times3$ filters and 64 outputs for all blocks in the contracting and expanding paths, while we use \mbox{ConvGRU} with $3\times3$ filters and 64 outputs in between paths.
%Additionally, we use \mbox{ConvGRU} with $3\times3$ filters and 64 outputs in the parallel blocks of the network.
%Furthermore, we do not use residual units for the other blocks as proposed by~\cite{Newell2016}, but single convolution layers with $3\times3$ filters and 64 outputs.
%All other blocks consist of single convolution layers with $3\times3$ filters and 64 outputs.
%The not recurrent version of the hourglass network uses standard convolution layers instead of \mbox{ConvGRU}.
%The network uses image sizes of $256\times256$ pixels as inputs and outputs and consists of seven levels.
%Each level halves the size of intermediate images, while we use max pooling with stride two for downsampling and nearest neighbor interpolation for upsampling.
%For combining intermediate outputs of different levels, the outputs of parallel blocks and the upsampled outputs of the next lower level are added element-wise.
As proposed by~\cite{Newell2016}, we also stack two hourglasses in a row to improve network predictions.
Therefore, we concatenate the output of the first hourglass with the input image to use it as input for the second hourglass.
We apply the loss function on the outputs of both hourglasses, while we only use the outputs of the second hourglass for the clustering of embeddings.
% for \christian{\sout{clustering embeddings - this does not have to be the next step! see MRI} further processing}.



\subsection{Cosine Embedding Loss}
\label{subsec:loss}


\newcommand{\setinst}{\mathbb{I}}
\newcommand{\setseg}{\mathbb{S}}
\newcommand{\setsegi}{\setseg^{(i)}}
\newcommand{\setsegj}{\setseg^{(j)}}
\newcommand{\setsegbac}{\setseg^{\textit{bac}}}
\newcommand{\neighseg}{\mathbb{N}}
\newcommand{\neighsegi}{\neighseg^{(i)}}
\newcommand{\neighsegbac}{\neighseg^{\textit{bac}}}
\newcommand{\emb}{\vec{e}}
\newcommand{\meanemb}{\vec{\bar{e}}}
\newcommand{\meanembi}{\meanemb^{(i)}}
\newcommand{\loss}{L}


%Adapted from~\cite{Newell2017} and~\cite{Kong2017}, we predict $d$-dimensional embedding vectors $\vec{e}\in\mathbb{R}^d$ for each instance $i\in\setinst$ to segment.
We let the network predict a $d$-dimensional embedding vector $\emb_p\in\mathbb{R}^d$ for each pixel~$p$ of the image. % for each instance $i\in\setinst$ to segment.
%To separate instances $i\in\setinst$, embeddings need to satisfy the following two properties:
To separate instances $i\in\setinst$, firstly, embeddings of pixels $p\in\setsegi$ belonging to the same instance $i$ need to be similar, and secondly, embeddings of~$\setsegi$ need to be dissimilar to embeddings of pixels $p\in\setsegj$ of other instances $j\neq i$. %, need to be dissimilar to embeddings of~$\setsegi$.
Here, we treat background as an independent instance.
%\christian{\sout{Following from the four color map theorem~\cite{Appel1976}, \thomas{that -$>$ which states (?) } that only neighboring instances need to have different embeddings, we relax the need of dissimilarity between different instances only to the neighboring ones, since embeddings that are far apart are easily distinguishable.
%Thus, the problem of assigning different embeddings to a possibly large number of different instances is simplified.}}
Following from the four color map theorem~\cite{Appel1976}, only neighboring instances need to have different embeddings.
Thus, we relax the need of dissimilarity between different instances only to the neighboring ones, i.e., $\neighsegi = \bigcup_j\setsegj$ for all instances $j\neq i$ within pixel-wise distance $r_{\mathbb{N}}$ to instance $i$.
%which simplifies the problem of assigning different embeddings to a possibly large number of different instances.
This relaxation simplifies the problem by assigning only a limited number of different embeddings to a possibly large number of different instances.
%For each instance $i$, we define the set of pixels of the neighboring instances as $\neighsegi = \bigcup_j\setsegj$, for $j\neq i$.
%we restrict the latter property to only neighboring instances, thus relaxing the problem of assigning different embeddings.
%First, embeddings inside the instance $\setsegi$
%These embeddings need to satisfy two properties to allow pixel-wise assignment of instance IDs:
%First, embeddings of each pixel inside the segmentation~$\setsegi$ of instance $i$ need to be as similar as possible.
%Second, embeddings of pixels of other neighboring instance segmentation masks~$\neighsegi$ need to be as dissimilar as possible to the embeddings of pixels in~$\setsegi$.
%The latter property needs to hold only for neighboring instances, as instances with same embeddings that are far apart are easily distinguishable by their distance.
%Furthermore, restricting the latter property to only neighboring instances reduces the maximum number of different embeddings that the network is required to predict (which is exactly four for planar graphs~\cite{Appel1976}).

%To be able to detect the background, we additionally add an additional background instance \textit{bac} to $\setinst$, and set $\setsegbac$ to background pixels and $\neighsegbac$ to pixels of all instances.

%The latter property needs to hold only for neighboring instances, as non-neighboring instances may only a limited number of embedding vectors is needed to assign neighbors with different embeddings (exactly for for planar graphs~\cite{Appel1976}).
%Additionally, instances with same embeddings that are far apart are easily distinguishable by their distance.
%These two properties need to hold only for neighboring instances, as the four color map theorem states that four colors are sufficient to color a planar graph such that no neighboring regions have the same color~\cite{Appel1976} .
%as the four color map theorem states 
%that only four colors are needed to color a planar graph such that no adjacent regions have the same color~\cite{Appel1976}.
%that no more than four colors are required to color regions of a planar graph such that no adjacent regions have the same color~\cite{Appel1976}.

We compare two embeddings with the cosine similarity
\begin{equation}
\text{cos}(\vec{e}_1,\vec{e}_2)=\frac{\vec{e}_1\cdot\vec{e}_2}{\norm{\vec{e}_1}\norm{\vec{e}_2}},
\end{equation}
%\christian{\sout{which ranges from~$-1$ to~1.
%The value of~$1$ means the vectors have the same, $-1$ the opposite, and $0$ orthogonal direction.}}
which ranges from~$-1$ to~1, while~$-1$ indicates the vectors have the opposite, $0$ orthogonal, and $1$ the same direction.
%which ranges from~$-1$, indicating opposite, to $1$, indicating same direction, while $0$ indicates orthogonality of vectors.
We define the cosine embedding loss as
\begin{equation}
\loss=\frac{1}{|\setinst|}\sum_{i\in\setinst}\left(1-\frac{1}{|\setsegi|}\sum_{p\in\setsegi}{\text{cos}(\meanembi,\emb_p)}\right) + \left(\frac{1}{|\neighsegi|}\sum_{p\in\neighsegi}{\text{cos}(\meanembi,\emb_p)^2}\right),
\end{equation}
where the mean embedding of instance $i$ is defined as
$\meanembi=\frac{1}{|\setsegi|}\sum_{p\in \setsegi}{\emb_p}$.
%\begin{equation}
%\meanembi=\frac{1}{|\setsegi|}\sum_{p\in \setsegi}{\emb_p}.
%\end{equation}
%The network's output are $d$-dimensional embeddings $\emb\in\mathbb{R}^d$ for each pixel.
%For each instance $i$, the set $\setsegi$ defines the pixels that are part of the segmentation mask, while the set $\setsegi$ defines the pixels that are part of neighboring segmentation masks.
%Similar to~\cite{Kong2017}, we use a embedding loss that is based on the cosine similarity, which is defined as
%and set up a loss function that models both required embedding properties as
%\begin{equation}
%\lossi=\left(1-\frac{1}{|\setsegi|}\sum_{p\in\setsegi}{\text{cos}(\meanembi,\emb_p)}\right) + \left(\frac{1}{|\neighsegi|}\sum_{p\in\neighsegi}{\text{cos}(\meanembi,\emb_p)^2}\right).
%\end{equation}
By minimizing~$\loss$, the first term urges embeddings~$\emb_p$ of pixels $p\in\setsegi$ to have the same direction as the mean~$\meanembi$, which is the case when $\text{cos}(\meanembi,\emb_p)\approx1$, while the second term pushes embeddings~$\emb_p$ of pixels $p\in\neighsegi$ to be orthogonal to the mean~$\meanembi$, i.e., $\text{cos}(\meanembi,\emb_p)\approx0$.
%\thomas{what is $\neighsegi$? it should be defined somewhere I guess?}
%The final loss function is defined as the mean of the loss functions of all instances, i.e.,
%\begin{equation}
%\loss=\frac{1}{|\setinst|}\sum_{i\in\setinst}\lossi.
%\end{equation}

%\christian{note background embedding}

\subsection{Clustering of Embeddings}
\label{subsec:clustering}

%As the predicted embeddings
%To get the final segmentations of the individual instances, the predicted embeddings need to be grouped together.
To get the final segmentations from the predicted embeddings, individual groups of embeddings that describe different instances need to be identified.
%To get a segmentation, individual instances have to be assigned to each predicted embedding in the image.
%to instances in the image, the predicted embeddings have to be grouped.
%We group them into individual instances, by clustering the embeddings.
As the number of instances is not known, we perform this grouping with the clustering algorithm HDBSCAN~\cite{Campello2015} that estimates the number of clusters automatically.
%We adjust HDBSCAN by using Euclidean distances and adapting the two parameters minimal points $m_{\text{pts}}$ and minimal cluster size $m_{\text{clSize}}$ for each dataset.
For each dataset, two HDBSCAN parameters have to be adjusted: minimal points $m_{\text{pts}}$ and minimal cluster size $m_{\text{clSize}}$.
%As the number of instances is not known beforehand, we perform this grouping with the clustering algorithm HDBSCAN~\cite{Campello2015} that estimates the number of clusters by itself.
%\christian{We use HDBSCAN with Euclidean distances, while we adapt two parameters for each dataset: minimal points $m_{\text{pts}}$, and minimal cluster size $m_{\text{clSize}}$. - put this somewhere else}
%\thomas{why? how much of an impact does this have on the clusterting compared to just using the embeddings?} extend each embedding with two entries corresponding to the pixel $x$ and $y$ coordinates to incorporate spatial information and multiply each coordinate with a factor $c$, which we observed to increase performance in both \thomas{why? how much of an impact does this have on the clusterting compared to just using the embeddings?}
%To ease clustering and get additional spatial information into the embeddings, we extend each embedding with two entries corresponding to the $x$ and $y$ coordinates of the pixel and multiply each coordinate with a factor $c$. \thomas{why? how much of an impact does this have on the clusterting compared to just using the embeddings?}
%We additionally extend each embedding with two entries corresponding to the $x$ and $y$ coordinate of the pixel and multiply each coordinate with a factor $\beta$.
%We calculate the median embedding of the current two frames and define all pixels with embeddings that have a cosine similarity larger than $\sigma$ as background pixels.
To simplify clustering and still be able to detect splitting of instances, we cluster only overlapping pairs of consecutive frames at a time. % and merge them afterwards.
%To allow videos with a large number of frames, we cluster only overlapping pairs of consecutive frames at a time (which is sufficient for the detection of splitting instances \christian{put this here or in discussion?}) and merge them afterwards.
%As the number of pixels of all frames of the whole video is too high for the clustering algorithm to work effectively, we only cluster pairs of consecutive frames at a time.
%As the background usually covers more than half of the pixels in an image,
%We reduce runtime of HDBSCAN by removing the background, which usually covers more than half of the pixels in an image.
%We define the background as pixels having embeddings with a cosine similarity larger than~$t_{\text{bac}}$ to the median of all embeddings of the image.
%We identify the embedding of background pixels by calculating the median embedding of all pixels of an image and define each embedding with a cosine similarity larger than~$t_{\text{bac}}$ as background.
%\thomas{similarity compared to what? to the background?}
%At first, we calculate the median embedding of the current two frames and define all pixels with embeddings that have a cosine similarity larger than $\sigma$ as background pixels.
%Furthermore, we extend each remaining embedding with two entries corresponding to its $x$ and $y$ coordinates, multiplied with a factor $c$.
%\christian{\sout{Furthermore, we extend each remaining embedding with two entries corresponding to its $x$ and $y$ coordinates, multiplied with a factor $c$.
%We observed that this additional spatial information improves clustering performance in both runtime and results.}}
Since our embedding loss allows same embeddings for different instances that are far apart, we use both image coordinates and value of the embeddings as data points for the clustering algorithm.
% to the embedding vector
%As we allow embeddings to be the same for distant instances, we need to add spatial information to the embeddings to be able to distinguish between them.
%Therefore, we extend each vector with two entries corresponding to its $x$ and $y$ coordinates, multiplied with a factor $c$.
%After identifying the embedding clusters with HDBSCAN resulting in segmented instances, a subsequent connected component analysis filters segmentations that are smaller than $t_{\text{size}}$ to reduce false detections.
%After identifying the embedding clusters with HDBSCAN, the segmented instances are obtained by filtering clusters that are smaller than $t_{\text{size}}$ to reduce false detections.
After identifying the embedding clusters with HDBSCAN and filtering clusters that are smaller than $t_{\text{size}}$, the final segmented instances for each frame pair are obtained.

%After identifying the clusters on each consecutive frame pair, the clusters need to be merged from frame to frame.
%%\thomas{the start of this sentence sounds clumsy, maybe connect to the previous -$>$}
%As the resulting cluster IDs from HDBSCAN may differ among frame pairs and we need to identify same instances.
%For merging the predicted clusters of overlapping frame pairs, we need to identify same instances on the overlapping frame, as the resulting cluster IDs from HDBSCAN may differ among frame pairs.
%Thus, we calculate the intersection over union (IoU) between each predicted cluster on overlapping frames and match the clusters based on the highest IoU, up until the IoU of an unmatched cluster pair is below a threshold $t_{\text{match}}$.
%Having consistent cluster IDs for each instance, the resulting frames are then upsampled back to the original input image size to generate the final tracked instance segmentations.
For merging the segmented instances in overlapping frame pairs, we identify same instances by the highest intersection over union (IoU) between each segmented instance in the overlapping frame.
% and match the clusters based on the highest IoU, up until the IoU of an unmatched cluster pair is below a threshold $t_{\text{match}}$.
The resulting segmentations are then upsampled back to the original image size, generating the final segmented and tracked instances.
%tracked instance segmentations.
%We match instances starting from the instance segmentation pair with the highest IoU, until the highest IoU is below a threshold $\theta$.
%Furthermore, 
%\christian{how are parents detected? mention cc analysis, filtering of min number of pixels}

\section{Experimental Setup and Results}
\label{sec:setup}

We train the networks with TensorFlow\footnote{\url{https://www.tensorflow.org/}} and perform on-the-fly data augmentation with SimpleITK\footnote{\url{http://www.simpleitk.org/}}.
We use hourglass networks with seven levels and an input size of $256\times256$, while we scale the input images to fit.
All recurrent networks are trained on sequences of ten frames.
We refer to the supplementary material for individual training and augmentation parameters, as well as individual values of parameter described in Section~\ref{sec:instance_tracking}.
%We evaluate the individual parts of our framework on three different applications.

\noindent\textbf{Left Ventricle Segmentation:}
To show that our proposed recurrent stacked hourglass network is able to incorporate temporal information, we perform semantic segmentation on videos of short-axis MR slices of the heart from the left ventricle segmentation challenge~\cite{Suinesiaputra2014}.
%We show that incorporating \mbox{ConvGRU} into stacked hourglass networks improves segmentation results in videos. %, without any further intervention.
%For this task, we use videos of short-axis MR slices of the heart from the left ventricle segmentation challenge~\cite{Suinesiaputra2014}.
We compare the recurrent network with a non-recurrent version, where we replace each \mbox{ConvGRU} with a convolution layer to keep the network complexity the same.
%As this dataset contains slices that capture the whole heart, which may look very different depending on from where they are crop the heart center or the outer parts, we only use a subset of the central three slices.
%As this dataset aims to capture the whole heart, many outer slices do not contain parts of the left ventricle.
%Therefore, we only evaluate on the three central slices that contain both left ventricle myocardium and blood cavity (see Fig.~\ref{fig:heart}), while we define the region inside the annotated myocardium as the blood cavity.
Since outer slices do not contain parts of the left ventricle, the networks are evaluated on the three central slices that contain both left ventricle myocardium and blood cavity (see Fig.~\ref{fig:heart}). %, while we define the region inside the annotated myocardium as the blood cavity.
%As the original dataset does not provide labels for the blood cavity but only for the myocardium, we define the region inside the myocardium as the blood cavity.
%We define the region inside the annotated myocardium as the blood cavity.
We train the networks with a softmax cross entropy loss to segment three labels, i.e., background, myocardium, and blood cavity.
%We train the recurrent stacked hourglass network with ten consecutive video frames.
We use a three-fold cross-validation setup, where we randomly split datasets of 96 patients into three equally sized folds.
%As our evaluation differs from the standard evaluation protocol of the challenge, we are not able to compare to previously reported results without bias.
Table~\ref{tbl:heart} shows the IoU for our internal cross-validation of both recurrent and non-recurrent stacked hourglass networks.
%\thomas{number of embeddings for this setup?}
%the intersection over union (IoU) of the myocardium (myo) and the blood cavity (cav) for our internal cross-validation of both recurrent and non-recurrent stacked hourglass networks (see Table~\ref{tbl:heart}).

\noindent\textbf{Leaf Instance Segmentation:}
We show that the cosine embedding loss and the subsequent clustering are suitable for instance segmentation without temporal information, by evaluating on the A1 dataset of the CVPPP challenge for segmenting individual plant leaves~\cite{Minervini2016} (see Fig.~\ref{fig:plant}).
%As this dataset does not contain temporal information,
We use the non-recurrent version of the proposed network from the previous experiment to predict embeddings with 32 dimensions.
Consequently, the clustering is also performed on single images.
As we were not able to provide results on the challenge test set in time before finalizing this paper, we report results of an internal three-fold cross-validation of the 128 training images.
In consensus with~\cite{Scharr2016}, we report the symmetric best Dice (SBD) and the absolute difference in count ($|$DiC$|$) and compare to other methods in Table~\ref{tbl:plant}.
%\thomas{possibly describe the metrics? (SBD (symmetric best dice) and $|$DiC$|$ (absolute difference in count)}
%Evaluation results 
%We evaluate the cosine embedding loss for segmenting leaf instances on images of plants~\cite{Minervini2016}.
%As this dataset does not contain video information, we replace the \mbox{ConvGRU} in our proposed network with convolution layers.

%\begin{table}[htp]
%\caption{Intersection over union mean $\pm$ standard deviation for the two segmentation classes of the heart segmentation dataset.}
%\begin{center}
%\begin{tabular}{c|c|c}
%& myo & cav\\
%\hline
%non recurrent & $78.3 \pm 9.2$ & $89.1 \pm 7.7$\\
%recurrent & $79.4 \pm 8.5$ & $89.4 \pm 7.2$\\
%\end{tabular}
%\end{center}
%\label{tbl:heart}
%\end{table}%


%\begin{table}
%\centering
%\caption{Results of the left ventricle segmentation and the CVPPP leaf instance segmentation. Note that we report our results for both datasets based on a three-fold crossvalidation setup, therefore they are not directly comparable to other published results.}
%\subfloat[][Results of the left ventricle segmentation.]{\label{tbl:heart}
%\begin{tabular}{c | c | c}
%& myo & cav\\
%\hline
%non-recurrent & $78.3 \pm 9.2$ & $89.1 \pm 7.7$\\
%recurrent & $79.4 \pm 8.5$ & $89.4 \pm 7.2$\\
%\end{tabular}}\hfill
%\subfloat[][Results of the CVPPP leaf instance segmentation. Values taken from~\cite{Scharr2016}.]{\label{tbl:plant}
%\begin{tabular}{c|c|c}
%& SBD & $|$DiC$|$\\
%\hline
%RIS+CRF~\cite{Romera-Paredes2016} & $66.6 \pm 8.7$ & $1.1 \pm 0.9$\\
%MSU & $66.7 \pm 7.6$ & $2.3 \pm 1.6$\\
%Nottingham & $68.3 \pm 6.3$ & $3.8 \pm 2.0$\\
%Wageningen & $71.1 \pm 6.2$ & $2.2 \pm 1.6$\\
%IPK & $74.4 \pm 4.3$ & $2.6 \pm 1.8$\\
%IS+RA~\cite{Ren2017} & $84.9 \pm 4.8$ & $0.8 \pm 1.0$\\
%\hline
%Ours & $84.5 \pm 5.5$ & $1.5 \pm 1.2$\\
%\end{tabular}}
%\label{tbl:plant_heart}
%\end{table}



\begin{figure}[t]
\begin{minipage}{0.5\textwidth}
\centering
%NOTE: this is a hack to force correct figure numbering as it is off by one for some reason.
\refstepcounter{table}
\label{tbl:plant_heart}
%NOTE: this is a hack so the referenced subfigures have the numbering "3". This will break if we add another figure OR table before this one.
%\addtocounter{figure}{-2}
\subfloat[][Heart MRI input and segmentation.]{\label{fig:heart}
\includegraphics[width=0.45\textwidth]{figures/datasets/heart_input}\hspace{1em}
\includegraphics[width=0.45\textwidth]{figures/datasets/heart_seg}}\\
\subfloat[][Plant leaves input and instances.]{
\label{fig:plant}
\includegraphics[width=0.45\textwidth]{figures/datasets/plant073_rgb}\hspace{1em}
\includegraphics[width=0.45\textwidth]{figures/datasets/plant073_label}}
%NOTE: this is the 'reverse' of the hack described above. This breaks if another figure OR table is added before this one.
\addtocounter{figure}{-2}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\centering
\captionsetup{position=top}
\refstepcounter{figure}
\subfloat[][Quantitative results of the heart MRI left ventricle segmentation.]{
\label{tbl:heart}
\begin{tabular}{c|c|c}
& IoU$_{\text{myo}}$ & IoU$_{\text{cav}}$\\
\hline
non-recurrent & $78.3 \pm 9.2$ & $89.1 \pm 7.7$\\
recurrent & $\textbf{79.4} \pm 8.5$ & $\textbf{89.4} \pm 7.2$\\
\end{tabular}}\\
\subfloat[][Quantitative results of the CVPPP leaf instance segmentation. Values taken from~\cite{Scharr2016}.]{
\label{tbl:plant}
\begin{tabular}{c|c|c}
& SBD & $|$DiC$|$\\
\hline
RIS+CRF & $66.6 \pm 8.7$ & $1.1 \pm 0.9$\\
MSU & $66.7 \pm 7.6$ & $2.3 \pm 1.6$\\
Nottingham & $68.3 \pm 6.3$ & $3.8 \pm 2.0$\\
Wageningen & $71.1 \pm 6.2$ & $2.2 \pm 1.6$\\
IPK & $74.4 \pm 4.3$ & $2.6 \pm 1.8$\\
IS+RA~\cite{Ren2017} & $\textbf{84.9} \pm 4.8$ & $\textbf{0.8} \pm 1.0$\\
\hline
Ours & $84.5 \pm 5.5$ & $1.5 \pm 1.2$\\
\end{tabular}}
\end{minipage}
\addtocounter{figure}{2}
\captionsetup{labelformat=andfigure}
\addtocounter{figure}{-1}
\caption{Results of the left ventricle segmentation and the CVPPP leaf instance segmentation.
Values show mean $\pm$ standard deviation.
Note that we report our results for both datasets based on a three-fold cross-validation setup.
Thus, they are not directly comparable to other published results.
SBD: symmetric best Dice; $|$DiC$|$: absolute difference in count; IoU: intersection over union; myo: myocardium; cav: blood cavity.}
\label{fig:plant_heart}
\end{figure}

\noindent\textbf{Cell Instance Tracking:}
As our main experiment, we show applicability of our full framework for instance segmentation and tracking by evaluating six different datasets of cell microscopy videos from the ISBI celltracking challenge~\cite{Ulman2017}.
%We use our proposed recurrent network for predicting embeddings of size 16.
Each celltracking dataset consists of two annotated training videos and two testing videos with image sizes ranging from $512\times512$ to $1200\times1024$ and with 48 to 138 frames.
We refer to~\cite{Maska2014} for additional imaging and video parameters.
%We scale the input images to fit into the network input size of $256\times256$ and train the networks with sequences of 10 frames.
As the instance IDs in groundtruth images are consistent throughout the whole video only for tracking, but not for segmentation, we merge both tracking and segmentation groundtruth for each frame to have consistent instance IDs.
Furthermore to learn the background embeddings, we only use the frames on which every cell is segmented.
%as only these frames can be used for learning background embeddings reliably.
%We determine hyperparameters for each dataset by training on one annotated video and validating on the other.
%%We used one annotated video for training and the other for validation and hyperparameter tuning.
%For our challenge submission, we retrain the networks on both annotated videos with the determined hyperparameters.
%We determine hyperparameters for each dataset by training on one annotated video and validating on the other.
%We used one annotated video for training and the other for validation and hyperparameter tuning.
%For our challenge submission, we train the networks on both annotated training videos, with hyperparameters determined on the training videos.
%For our challenge submission, 
With hyperparameters determined on the two annotated training videos from each dataset, we train the networks for predicting embeddings of size 16 on both videos for our challenge submission.

%Although the network does identify cell instance splits, i.e., mitosis, in the celltracking datasets (see Fig.~\ref{fig:overview}), 
%we need to identify the parent ID to compete in the tracking metric of the challenge.
To compete in the tracking metric of the challenge, the framework is required to identify the parent ID of each cell.
%As the network is able identify splitting cells (e.g., mitosis seen on Fig.~\ref{fig:overview}) and create children with new IDs, we search in the previous frames for the instances with the highest IoU to identify the parent.
%As the network is able to identify splitting cells and assign new instance IDs (i.e., mitosis as seen on Fig.~\ref{fig:overview}), for each newly created instance, we search in previous frames for the parent, which we define as the instance with the highest IoU.
As the framework is able to identify splitting cells and to assign new instance IDs (i.e., mitosis as seen on Fig.~\ref{fig:overview}), the parent ID of each newly created instance is determined as the instance with the highest IoU in previous frames.
We further postprocess the cells' family tree to be consistent with the evaluation criteria, e.g., an instance ID may not be used after splitting into children.
%We refer to the supplementary material for more details and individual augmentation, training, and clustering parameters.
The results in comparison to the top performing methods are presented in Table~\ref{tbl:celltracking}.




% Rotation: \rot[<angle>][<width>]{<stuff>}

\definecolor{ours}{RGB}{251, 118, 104}
%\definecolor{ac1}{RGB}{161,231,229}
\definecolor{fr_ro_ge}{RGB}{255,255,179}
\definecolor{kth_se_1_4}{RGB}{210,206,238}
\definecolor{cvut_cz}{RGB}{168,217,251}
\definecolor{bgu_il}{RGB}{255,220,138}
\definecolor{leid_nl}{RGB}{209,252,135}
\definecolor{cuni_cz}{RGB}{252,205,229
}
\definecolor{kit_ge}{RGB}{237,237,237}
\definecolor{hd_har_ge}{RGB}{233,173,234}
\definecolor{fr_be_ge}{RGB}{174,245,187}

\begin{table}[t]
\centering
\caption{Quantitative results of the celltracking datasets for overall performance~(OP), segmentation~(SEG), and tracking~(TRA), as described in~\cite{Ulman2017}.}
  \resizebox{\textwidth}{!}{%
\begin{tabular}{c p{0.4cm} | r | r | r | r | r | r | p{0.3cm} c}
 & & & & & & & & & \\
 & & \begin{tabular}[c]{@{}l@{}}DIC-\\ HeLa\end{tabular}& \begin{tabular}[c]{@{}l@{}}Fluo-\\ MSC\end{tabular} & \begin{tabular}[c]{@{}l@{}}Fluo-\\ GOWT1\end{tabular} & \begin{tabular}[c]{@{}l@{}}Fluo-\\ HeLa\end{tabular} & \begin{tabular}[c]{@{}l@{}}PhC-\\ U373\end{tabular} & \begin{tabular}[c]{@{}l@{}}Fluo-\\ SIM+\end{tabular} \\
\hhline{--|-|-|-|-|-|-}
\multirow{ 4}{*}{OP} & \nth{1} & \cellcolor{ours} \hphantom{xx} $0.864$ & \cellcolor{bgu_il} $0.759$ & \cellcolor{kth_se_1_4} $0.951$ & \cellcolor{kth_se_1_4} $0.942$ & \cellcolor{fr_ro_ge} $0.951$ & \cellcolor{bgu_il} $0.882$ &  & \cellcolor{ours} Ours \\
 & \nth{2} & \cellcolor{fr_ro_ge} $0.828$ & \cellcolor{kth_se_1_4} $0.676$ & \cellcolor{ours} $0.914$ & \cellcolor{fr_ro_ge} $0.940$ & \cellcolor{fr_be_ge} $0.896$ & \cellcolor{fr_ro_ge} $0.878$ &  & \cellcolor{bgu_il} BGU-IL (1--2)  \\
 & \nth{3} & \cellcolor{kth_se_1_4} $0.629$ & \cellcolor{cvut_cz} $0.658$ & \cellcolor{leid_nl} $0.902$ & \cellcolor{cvut_cz} $0.928$ & \cellcolor{cvut_cz} $0.895$ & \cellcolor{kth_se_1_4} $0.874$ &  & \cellcolor{cuni_cz} CUNI-CZ \\\hhline{~|~|-|-|-|-|-|-}
& & & \cellcolor{ours} \nth{5} $0.631$ & & \cellcolor{ours} \nth{11} $0.829$ & \cellcolor{ours} \nth{4} $0.888$ & \cellcolor{ours} \nth{9} $0.810$ &  & \cellcolor{cvut_cz} CVUT-CZ \\
\hhline{==|=|=|=|=|=|=}
\multirow{ 4}{*}{SEG} & \nth{1} & \cellcolor{ours} $0.814$ & \cellcolor{bgu_il} $0.645$ & \cellcolor{kth_se_1_4} $0.927$ & \cellcolor{fr_ro_ge} $0.903$ & \cellcolor{fr_ro_ge} $0.920$ & \cellcolor{bgu_il} $0.802$ &  & \cellcolor{fr_be_ge} FR-Be-GE \\
 & \nth{2} & \cellcolor{fr_ro_ge} $0.776$ & \cellcolor{kth_se_1_4} $0.590$ & \cellcolor{leid_nl} $0.893$ & \cellcolor{kth_se_1_4} $0.893$ & \cellcolor{cvut_cz} $0.832$ & \cellcolor{kth_se_1_4} $0.791$ &  & \cellcolor{fr_ro_ge} FR-Ro-GE \\
 & \nth{3} & \cellcolor{cvut_cz} $0.464$ & \cellcolor{fr_ro_ge} $0.582$ & \cellcolor{cuni_cz} $0.887$ & \cellcolor{cvut_cz} $0.869$ & \cellcolor{fr_be_ge} $0.826$ & \cellcolor{fr_ro_ge} $0.781$ &  & \cellcolor{hd_har_ge} HD-Har-GE \\\hhline{~|~|-|-|-|-|-|-}
& & & \cellcolor{ours} \nth{5} $0.496$ & \cellcolor{ours} \nth{4} $0.880$ & \cellcolor{ours} \nth{10} $0.749$ & \cellcolor{ours} \nth{5} $0.793$ & \cellcolor{ours} \nth{8} $0.718$ &  & \cellcolor{kit_ge} KIT-GE \\
\hhline{==|=|=|=|=|=|=}
\multirow{ 4}{*}{TRA} & \nth{1} & \cellcolor{ours} $0.915$ & \cellcolor{bgu_il} $0.873$ & \cellcolor{kth_se_1_4} $0.976$ & \cellcolor{kth_se_1_4} $0.991$ & \cellcolor{ours} $0.983$ & \cellcolor{fr_ro_ge} $0.975$ &  & \cellcolor{kth_se_1_4} KTH-SE (1--4) \\
 & \nth{2} & \cellcolor{fr_ro_ge} $0.881$ & \cellcolor{ours} $0.765$ & \cellcolor{ours} $0.947$ & \cellcolor{cvut_cz} $0.987$ & \cellcolor{fr_ro_ge} $0.981$ & \cellcolor{bgu_il} $0.961$ &  & \cellcolor{leid_nl} LEID-NL \\
 & \nth{3} & \cellcolor{kth_se_1_4} $0.797$ & \cellcolor{kth_se_1_4} $0.763$ & \cellcolor{kit_ge} $0.925$ & \cellcolor{hd_har_ge} $0.986$ & \cellcolor{kth_se_1_4} $0.977$ & \cellcolor{kth_se_1_4} $0.957$ &  &  \\\hhline{~|~|-|-|-|-|-|-}
& & & &  & \cellcolor{ours} \nth{12} $0.909$ & & \cellcolor{ours} \nth{10} $0.902$ &  & \\
\hhline{--|-|-|-|-|-|-}
\end{tabular}
}
\label{tbl:celltracking}
\end{table}



\section{Discussion and Conclusion}
\label{sec:conclusion}

Up to our knowledge, we are the first to present a method that incorporates temporal information into a network to allow tracking of embeddings for instance segmentation.
We perform three experiments to show different aspects of our novel method, i.e., temporal segmentation, instance segmentation, and combined instance segmentation and tracking. %indicating that our approach is widely applicable.
%Thus, we show that our approach is widely applicable.
Thus, we demonstrate the wide applicability of our approach.

We use the left ventricle segmentation experiment to show that our novel recurrent stacked hourglass network can be used for incorporating temporal information.
%Since we use this dataset only to show feasibility of our proposed network ,
%Therefore, the results of the experiment should not be compared to the reported results of the challenge.
%Since we simplified the evaluation protocol of the challenge, the results of the experiment should not be directly compared to the reported challenge results.
%Nevertheless, it can be seen that incorporating ConvGRU deeply inside the architecture improves the results over the baseline stacked hourglass network, while benefit of such deep incorporation compared to having recurrent layers e.g., in the end, needs to be shown.
It can be seen from the results of the experiment that incorporating ConvGRU between contracting and expanding path deeply inside the architecture improves over the baseline stacked hourglass network.
Nevertheless, since we simplified the evaluation protocol of the challenge, the results of the experiment should not be directly compared to other reported results.
Moreover, benefits of such deep incorporation compared to having recurrent layers on other positions in the network~\cite{Ren2017} remain to be shown.

This paper also contributes with a novel embedding loss based on cosine similarities.
Most of the methods that use embeddings for differentiating between instance segmentations are based on maximizing distances of embeddings in the Euclidean space, e.g.,~\cite{Newell2017}.
When using such embedding losses, we observed problems when combining them with recurrent networks, presumably due to unrestricted embedding values.
To overcome these problems, we use cosine similarities that normalize embeddings.
%Except for~\cite{Kong2017}, who also use cosine similarities in their embedding loss functino
The only other work that suggests cosine similarities for instance segmentation with embeddings is the unpublished work of~\cite{Kong2017}.
However, compared to their embedding loss that takes all instances into account, our novel loss focuses only on neighboring ones, which can be beneficial for optimization in the case of a large number of instances.
We evaluate our novel loss on the CVPPP challenge dedicated to instance segmentation from still images.
While waiting for the results of the competition, our method evaluated with three-fold cross-validation shows to be in line with the currently leading method, and has a significant margin to the second best.
Moreover, compared to the leading method~\cite{Ren2017}, the architecture of our method is considerably simpler. 

%we are not able to compare to previously reported results without bias.
%However, as we only want to show a proof-of-concept of our proposed network,


%The results of the experiment indicate that our incorporation of ConvGRU into the stacked hourglass network improves segmentation tasks, however to fully compare to other recurrent networks further experiments have to be performed to validate

%Results of our experiments show that our proposed framework is suitable for instance segmentation and tracking.
%We have shown in the evaluation of videos of MR slices of the heart that our proposed recurrent stacked hourglass network that incorporates \mbox{ConvGRU} is able to use temporal information.
%Furthermore, evaluation on images of plant leaves shows that our proposed cosine embedding loss combined with HDBSCAN clustering achieves competitive instance segmentation results.
%Finally, our complete framework for segmentation and tracking achieves top performance in the celltracking datasets, by combining the recurrent stacked hourglass network with the cosine embedding loss.

%Dealing with the large variability in visual appearance, size and number of cells,
In our main experiment for segmentation and tracking of instances, we evaluate our method on the ISBI celltracking challenge, showing large variability in visual appearance, size and number of cells.
Our method achieves two first and two second places among the six submitted datasets in the tracking metric.
For the dataset DIC-HeLa, having a dense layout of cells as seen in Fig.~\ref{fig:overview}, we outperform all other methods in both tracking and segmentation metrics. %, due to the dense layout of the cells, where the embedding loss works particularly well.
On the dataset Fluo-GOWT1 we rank overall second.
On the datasets Fluo-HeLa and Flou-SIM+, which consist of images with small cells, our method does not perform well due to  the need to downsample images for the network to process them.
When the downsampling results in drastic reduction of cell sizes, our method fails to create instance segmentations, %from which tracking cannot recover,
%thus leading to poor tracking performance.
thus explaining the not satisfying performance also in tracking.
To increase the resolution and consequently improve segmentation and tracking, we could split the input image into multiple smaller parts, similarly as done in~\cite{Ronneberger2015}.

In conclusion, our work has shown that embeddings for instance segmentation can be successfully combined with recurrent networks incorporating temporal information to perform instance tracking.
In future work, we will investigate the possibility of incorporating the required clustering step inside of a single end-to-end trained network, which could simplify the framework and further improve the segmentation and tracking results.

%As the different celltracking datasets show a large variability in visual appearance, size and number of cells, there does not exist the one single top performing method for all datasets, but different methods rank in the top three (see Table~\ref{tbl:celltracking}).
%%As compared to other methods (e.g.,~\cite{Ronneberger2015}), we do not perform tiled evaluation to increase
%The results show that our method performs particularly well in terms of the tracking metric, where it achieves two first and two second places within the six submitted datasets.
%This further confirms that our proposed recurrent stacked hourglass network successfully propagates temporal information.
%For the dataset DIC-HeLa we outperform all other methods in both tracking and segmentation metrics, due to the dense layout of the cells, where the embedding loss works particularly well.
%In terms of the segmentation metrics our method delivers mixed results.
%A possible reason for that is the relative low image resolution of our network, due to memory restrictions.
%%Furthermore, in contrast to other methods, we do not employ tile-based inference, where parts of the image are cropped and processed one after another before merging them again.
%%Nevertheless, in principle our method would allow tile-based inference, if the clustering part would be adapted accordingly.
%Because of this low network resolution, our method performs worse in datasets with small cells, e.g., Flou-HeLa and Flou-SIM+.
%%Furthermore, we were not able to submit results for the only remaining 2D dataset PhC-PSC, which has the smallest cell sizes.
%Furthermore, we were not able to submit satisfactory results for the only remaining 2D dataset PhC-PSC, as it has the smallest cell sizes.
%%However, despite the low processing resolution, we could improve results by using tile-based inference, where parts of the image are cropped and processed one after another before merging them again, as also used by the authors of the \mbox{U-Net}~\cite{Ronneberger2015}.
%%Additionally, more memory efficient network training could further allow higher resolutions.
%%Nevertheless, in principle our method would allow tile-based inference, if the clustering part would be adapted accordingly.
%
%In future work, we plan to make the network training more memory efficient to allow higher resolutions,
%Furthermore, we could improve results with tile-based processing that splits the input image into multiple smaller parts, as also used in the \mbox{U-Net}~\cite{Ronneberger2015}.
%Furthermore, we plan to extend our framework to 3D for analyzing volumetric celltracking datasets.
%In conclusion, we have proposed a novel framework that effectively combines recurrent neural networks and pixel-wise embeddings for instance segmentation and tracking.
%
%\christian{mention something about clustering, parameters for clustering
%difference to \mbox{U-Net} (if not already clear)
%difference to other embedding losses
%future improvements -> better clustering}
%
%\christian{As our evaluation differs from the standard evaluation protocol of the challenge, we are not able to compare to previously reported results without bias.
%However, as we only want to show a proof-of-concept of our proposed network,}

\bibliographystyle{splncs03}
\bibliography{main}

\begin{appendices}
\renewcommand{\thesection}{\arabic{section}}%

\nociteappendix{*}

\chapter*{Appendix}

\section{Network and Training Parameters}
We set the network parameters as follows:
The weights of each convolution layer of the stacked hourglass network are initialized with the method as described in~\cite{He2015}, the biases with~0.
The networks do not employ any normalization layers or dropout, but use an L2 weight regularization factor of 0.00001.
Due to the demanding training of recurrent neural networks, in terms of both memory and computational requirements, we set the mini-batch size to 1.
We train the recurrent networks for sequences of 10 consecutive frames.
For the non-recurrent neural networks, we use a mini-batch size of 10.
We train all networks with ADAM~\cite{Kingma2015} for total 40000 iterations and a learning rate of 0.0001, while the learning rate is reduced to 0.00001 after 20000 iterations.
Training of a recurrent networks took $\approx12$ hours, training of the non-recurrent networks took $\approx8$ hours on a single NVIDIA Titan Xp with 12 GB.

\section{Data Preprocessing and Augmentation Parameters}

We perform input data augmentation, by changing intensity values and spatial deformations.
First, we change the image intensity values such that the minimum and maximum values are $-1$ and $1$.
As MR and microscopy images may contain outliers in terms of minimum and maximum values, we calculate the minimum value as the median of $i_{\text{min}}$\% of all intensity values of an image, and the maximum as the median of $i_{\text{max}}$\%.
Then, for augmentation, we shift each intensity value randomly by $i_{\text{shift}}$ and scale each intensity by $i_{\text{scale}}$.
For the random spatial deformations in both $x$ and $y$ axes, we translate by $t$ pixels, flip axis with probability $f_p$, rotate by $r$ degrees and scale by $s$.
Furthermore, we employ elastic deformations, by randomly moving points by $b$ pixels on a grid of size $g$ and interpolating with third order splines.
All random augmentations sample from a uniform distribution within the specified intervals.

\noindent\textbf{Left Ventricle Segmentation:}
The augmentation parameters are as follows:
Intensity transformations: $i_{\text{min}}=10\%$, $i_{\text{max}}=10\%$, $i_{\text{shift}} \in [-0.25, 0.25]$, $i_{\text{scale}}\in[0.75, 1.25]$.
Spatial transformations: $t\in[-20, 20]$, $f_p=0$, $r\in[-15, 15]$, $s\in[0.75, 1.25]$, $b=8$, $g\in[-10, 10]$.
%As the datasets consists of MR slices, which vary greatly in the intensity ranges, we a calculate robust minimum and maximum value before normalizing.
%We define the robust minimum as the median of 10\% of pixels with the lowest intensity value, and the robust maximum as the median of 10\% of pixels with the highest intensity value.
%Afterwards, we randomly add a value between $[-0.25, 0.25]$, and multiply with a value between $[0.75, 1.25]$.
%We randomly translate the image by $[-20, 20]$ pixels in $x$ and $y$ direction.
%We randomly rotate the image by $[-15, 15]$ degrees and scale it by $[0.75, 1.25]$.
%For elastic deformations, we employ a $8\times8$ grid and move points on the grid randomly by 10 pixels in $x$ and $y$ direction.
We set default pixel values outside the defined image region to 0.

\noindent\textbf{Leaf Instance Segmentation:}
The augmentation parameters are as follows:
Intensity transformations: $i_{\text{min}}=1\%$, $i_{\text{max}}=1\%$, $i_{\text{shift}} \in [-0.25, 0.25]$, $i_{\text{scale}}\in[0.75, 1.25]$.
Spatial transformations: $t\in[-12, 12]$, $f_p=0.5$, $r\in[-180, 180]$, $s\in[0.75, 1.25]$, $b=8$, $g\in[-10, 10]$.
%We shift the image intensities by -128 and multiply then with $\frac{1}{128}$ such that the values range between $-1$ and $1$.
%Afterwards, we randomly add a value between $[-0.25, 0.25]$, and multiply with a value between $[0.75, 1.25]$.
%We randomly translate the image by $[-12, 12]$ pixels in $x$ and $y$ direction, and randomly flip it with a probability of 50\% in $x$ and $y$ direction.
%We randomly rotate the image by $[-180, 180]$ degrees and scale it by $[0.75, 1.25]$.
%For elastic deformations, we employ a $8\times8$ grid and move points on the grid randomly by 10 pixels in $x$ and $y$ direction.
For each instance $i\in\mathbb{I}$, we define all pixels inside the segmentation mask as $\mathbb{S}^{(i)}$, and all pixels of all other instances as $\mathbb{N}^{(i)}$.
We perform mirror padding for pixels outside the defined image region, but we do not calculate the loss for these pixels.

\noindent\textbf{Cell Instance Tracking:}
Unless otherwise stated, the augmentation parameters for all datasets are as follows:
Intensity transformations: $i_{\text{min}}=20\%$, $i_{\text{max}}=10\%$ (for Fluo-MSC and Fluo-SIM+ we set $i_{\text{max}}=1\%$), $i_{\text{shift}} \in [-0.25, 0.25]$, $i_{\text{scale}}\in[0.75, 1.25]$.
Due to noise in the intensity values, we smooth the images with a Gaussian function with $\sigma=2$ pixel.
Spatial transformations: $t\in[-25, 25]$, $f_p=0.5$, $r\in[-180, 180]$, $s\in[0.75, 1.25]$, $b=8$, $g\in[-10, 10]$.
%
%As all datasets vary greatly in the intensity ranges, we a calculate robust minimum and maximum value for each image before normalizing.
%We define the robust minimum as the median of 20\% of pixels with the lowest intensity value, and the robust maximum as the median of 10\% of pixels with the highest intensity value (for datasets Fluo-MSC and Fluo-SIM+ we set the maximum to 1\%).
%Due to noise in the intensity values, we smooth the images with a gaussian function with $\sigma=2$ pixel.
%Afterwards, we randomly add a value between $[-0.25, 0.25]$, and multiply with a value between $[0.75, 1.25]$.
%We randomly translate the image by $[-25, 25]$ pixels in $x$ and $y$ direction, and randomly flip it with a probability of 50\% in $x$ and $y$ direction.
%We randomly rotate the image by $[-180, 180]$ degrees and scale it by $[0.75, 1.25]$.
%For elastic deformations, we employ a $8\times8$ grid and move points on the grid randomly by 10 pixels in $x$ and $y$ direction.
For each instance $i$, we define all pixels inside the segmentation mask as $\mathbb{S}^{(i)}$, while we set $\mathbb{N}^{(i)}$ to only neighboring instances within a specified radius $r_{\mathbb{N}}$ in pixels.
%We define this radius based on the cell size of each dataset.
For dataset Fluo-MSC we set $r_{\mathbb{N}}=150$, for the dataset Fluo-HeLa we set $r_{\mathbb{N}}=25$. For all other datasets we set $r_{\mathbb{N}}=50$.
For each mini-batch, we use at most 32 different instances for training, to reduce memory consumption.
We perform mirror padding for pixels outside the defined image region, but we do not calculate the loss for these pixels.

\section{Clustering Parameters}

We append the image coordinates scaled with factor $c$ to value of the embeddings as data points for the clustering algorithm.
%We set $t_{\text{match}}=0.25$ and $t_{\text{bac}}=0.95$ for all experiments.
We modify the parameters $c$ and $m_{\text{pts}}$ for each dataset, while we set $m_{\text{clSize}}=m_{\text{pts}}$ and $t_{\text{size}}=\frac{m_{\text{pts}}}{2}$.
%We set the HDBSCAN parameters $m_{\text{pts}}$ and $m_{\text{clSize}}$, as well as $c$ and $t_{\text{size}}$ independently for each dataset.
DIC-HeLa: $m_{\text{pts}}=1000$, $c=0.02$;
Fluo-MSC: $m_{\text{pts}}=500$, $c=0.1$;
Fluo-GOWT1: $m_{\text{pts}}=50$, $c=0.001$;
Fluo-SIM+: $m_{\text{pts}}=100$, $c=0.001$;
Fluo-HeLa: $m_{\text{pts}}=25$, $c=0.01$;
PhC-U373: $m_{\text{pts}}=500$, $c=0.005$;
For the CVPPP dataset we set $m_{\text{pts}}=50$, $c=0.001$.

\begin{figure}[htbp]
\begin{center}
\subfloat[][DIC-HeLa]{\includegraphics[width=0.32\textwidth]{figures/examples/DIC-C2DH-HeLa/input}\hspace{1ex}
\includegraphics[width=0.32\textwidth]{figures/examples/DIC-C2DH-HeLa/emb}\hspace{1ex}
\includegraphics[width=0.32\textwidth]{figures/examples/DIC-C2DH-HeLa/seg}}\\
\subfloat[][Fluo-MSC]{\includegraphics[width=0.32\textwidth]{figures/examples/Fluo-C2DL-MSC/input}\hspace{1ex}
\includegraphics[width=0.32\textwidth]{figures/examples/Fluo-C2DL-MSC/emb}\hspace{1ex}
\includegraphics[width=0.32\textwidth]{figures/examples/Fluo-C2DL-MSC/seg}}\\
\subfloat[][Fluo-GOWT1]{\includegraphics[width=0.32\textwidth]{figures/examples/Fluo-N2DH-GOWT1/input}\hspace{1ex}
\includegraphics[width=0.32\textwidth]{figures/examples/Fluo-N2DH-GOWT1/emb}\hspace{1ex}
\includegraphics[width=0.32\textwidth]{figures/examples/Fluo-N2DH-GOWT1/seg}}
\caption{Example results of the evaluated celltracking datasets. Left: normalized input; middle: three randomly chosen dimensions of the embedding as RGB channels; right: final instance segmentation.}
\label{default}
\end{center}
\end{figure}

\begin{figure}[htbp]
\begin{center}
\subfloat[][Fluo-SIM+]{\includegraphics[width=0.32\textwidth]{figures/examples/Fluo-N2DH-SIM+/input}\hspace{1ex}
\includegraphics[width=0.32\textwidth]{figures/examples/Fluo-N2DH-SIM+/emb}\hspace{1ex}
\includegraphics[width=0.32\textwidth]{figures/examples/Fluo-N2DH-SIM+/seg}}\\
\subfloat[][Fluo-HeLa]{\includegraphics[width=0.32\textwidth]{figures/examples/Fluo-N2DL-HeLa/input}\hspace{1ex}
\includegraphics[width=0.32\textwidth]{figures/examples/Fluo-N2DL-HeLa/emb}\hspace{1ex}
\includegraphics[width=0.32\textwidth]{figures/examples/Fluo-N2DL-HeLa/seg}}\\
\subfloat[][PhC-U373]{\includegraphics[width=0.32\textwidth]{figures/examples/PhC-C2DH-U373/input}\hspace{1ex}
\includegraphics[width=0.32\textwidth]{figures/examples/PhC-C2DH-U373/emb}\hspace{1ex}
\includegraphics[width=0.32\textwidth]{figures/examples/PhC-C2DH-U373/seg}}
\caption{Example results of the evaluated celltracking datasets, continued.}
\label{default}
\end{center}
\end{figure}

\end{appendices}

\bibliographystyleappendix{splncs03}
\bibliographyappendix{appendix}

\end{document}

