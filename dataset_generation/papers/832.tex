\documentclass{bmvc2k}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{eurosym}
\usepackage{calc}
\usepackage{multirow}
\usepackage{adjustbox}
%\usepackage{subcaption}
\newcommand{\R}{\mathbb{R}}

%% Enter your paper number here for the review copyu
%\bmvcreviewcopy{383}

\title{Prototypical Priors: From Improving Classification to Zero-Shot Learning}

% Enter the paper's authors in order
\addauthor{Saumya Jetley}{sjetley@robots.ox.ac.uk}{1}
\addauthor{Bernardino Romera-Paredes}{bernard@robots.ox.ac.uk}{1}
\addauthor{Sadeep Jayasumana}{sadeep@robots.ox.ac.uk}{1}
\addauthor{Philip Torr}{phst@robots.ox.ac.uk}{1}

% Enter the institutions
% \addinstitution{Name\\Address}
\addinstitution{
 University of Oxford\\
 Oxford, UK
}

\runninghead{Jetley et al.}{Prototypical Priors: Classification to Zero-Shot Learning}

% Any macro definitions you would like to include
% These are not defined in the style file, because they don't begin
% with \bmva, so they might conflict with the user's own macros.
% The \bmvaOneDot macro adds a full stop unless there is one in the
% text already.
\def\eg{\emph{e.g}\bmvaOneDot}
\def\Eg{\emph{E.g}\bmvaOneDot}
\def\etal{\emph{et al}\bmvaOneDot}

%-------------------------------------------------------------------------
% Document starts here
\begin{document}

\maketitle

\begin{abstract}
Recent works on zero-shot learning make use of side information such
as visual attributes or natural language semantics to define the relations
between output visual classes and then use these relationships to draw inference
on new unseen classes at test time. In a novel extension to this idea,
we propose the use of visual prototypical concepts as side information. For
most real-world visual object categories, it may be difficult to establish
a unique prototype. However, in cases such as traffic signs, brand
logos, flags, and even natural language characters, these prototypical
templates are available and can be leveraged for an improved recognition
performance. 

% Manually defining an optimal visual atribute set that can uniquely
% identify each of the class in a given learning scenario can be highly
% challenging. The challenge not only depends on the class count. High intra-class
% variation and fine granularity of recognition can also make it
% diffcult to establish relevant attributes. Further, natural language
% semantics are relevant when the word tags are related to the visual
% patterns, which may not always hold true for instance with brand logos
% or country flags. In cases such as these, prototypes provide a useful alternative for 
% defining the output embedding space.

The present work proposes a way to incorporate this prototypical information
in a deep learning framework. Using prototypes as prior information, the deepnet pipeline
learns the input image projections into the prototypical embedding space subject
to minimization of the final classification loss. Based on our
experiments with two different datasets of traffic signs and brand
logos, prototypical embeddings incorporated in a conventional convolutional neural network improve
the recognition performance. Recognition accuracy on the Belga logo dataset is
especially noteworthy and establishes a new state-of-the-art. In zero-shot
learning scenarios, the same system can be directly deployed to draw inference on unseen classes by simply adding the prototypical information 
for these new classes at test time. Thus, unlike earlier approaches, testing
on seen and unseen classes is handled using the same pipeline, and the system
can be tuned for a trade-off of seen and unseen class performance as per 
task requirement. Comparison with one of the latest works in the zero-shot
learning domain yields top results on the two datasets mentioned above.

\end{abstract}

\section{Introduction}

Automatic object recognition has witnessed a huge improvement in recent years due to the successful 
application of convolutional neural networks (CNN). This boost in performance can be explained by 
the replacement of heuristic parts in the previous feature representation approaches by a methodology 
\cite{Lee/icml2009,Krizhevsky/nips2012} based on learning the features straight from the data. 
The learned feature representation, which is tailored to the given learning scenario, generally 
outperforms heuristic approaches provided the training data is sufficient.
When learned over a significant sample variety, this representation captures regularities across samples of a class 
that help distinguish it from all the other classes.      
\begin{figure}
\begin{center}
\includegraphics[scale=0.55]{im_embeddingconcept.pdf}
\end{center}
\protect\caption{\label{fig:scheme} A joint embedding space defined by
the prototypes}
\end{figure}

In an alternative setup, the object recognition problem can be posed as one
in which objects in real images are identified by treating them as imperfect and corrupt copies 
of prototypical concepts. This assumption provides an additional premise that
the different samples of a class are not only similar to each other but also resemble a 
unique prototype. These prototypical concepts are in many cases not
available, for example, there does not exist a chair that contains only the
essence of \textit{chair} and nothing else.
However, there are many scenarios where such prototypical instances do exist. An example of this is traffic 
sign recognition, in which each traffic sign class has its canonical template. Real world images contain 
imperfect instances of it, these imperfections being caused by different viewpoints, light conditions, 
damage to surface, among others. These canonical templates, hereafter
uniformly referred to as prototypes (\textit{an original or first model of
something from which other forms are copied or developed\footnote{Definition taken from Merriam-Webster.com dictionary.%
}}), can play a very important role in recognition.
Conceivably, this prototypical information can benefit by - (a) guiding the learning process and 
(b) establishing an output embedding space where the relationship between
output visual classes can be used to transfer the learned knowledge to unseen
classes directly at test time.

In the present work, we focus on adding this prototypical prior information into convolutional neural networks. 
The underlying idea is that the high-level representation learned by a CNN should be comparable to the 
information extracted from the prototypes. An interpretation of this is that 
layer-by-layer the CNN is able to learn a representation that is invariant to real world
 factors such as light variation, view point distortion, as described in \cite{goodfellow2009measuring}, so that the representation obtained
  at the end of the network is invariant to all factors appearing in real images, and thus 
  comparable to the prototype.  
% The interpretation of this effect is that each layer of the CNN is able to learn a representation that is invariant to a particular factor of the real world, such as light variation, so that the representation obtained at the end of the network is invariant to all factors appearing in the images, and thus comparable to the prototype.

We adjust the traditional CNN pipeline to map both the input and prototypes to a
common feature space with the end goal of minimizing the final recognition error. The idea of
a common space for recognizing the instances by matching them to their
correct prototype is shown in Figure~\ref{fig:scheme}. For current
experiments, this common feature space is defined preemptively by the
prototypes of classes in context. Arguably, the prototypical templates, unaffected by noise
and distortion, are qualified to define an optimal embedding for maximum discrimination of classes.
%Input embeddings are the outcome of a standard deepnet%pipeline that must train to feed into this optimal joint space.

The use of a joint embedding space lends the proposed model an interesting possibility 
of applying it to recognize new classes not present at the training stage. This aligns 
the approach within the areas of zero and one-shot learning. These areas pursue to 
emulate the ability of human beings to extrapolate and draw inference on test samples 
only from a description, or a single instance per class. Indeed, this is a faculty humans 
own, for example when assimilating and recognizing a new character such as \euro, after 
being presented with one instance. 

% Following with the example of traffic sign recognition, the same idea is endorsed by the work on human performance benchmarking for traffic sign recognition {[}manvscomputers{]}. As part of their experimental setup, human subjects are asked to look at real-time traffic sign samples and identify them as corresponding to one of the many standard sign templates (one of each class) displayed on the GUI.%In an alternate, and probably more pertinent setup, each%class could have been represented by a suitable set of exemplars.%Instead, substituting them by one standard template per class implicitly%acknowledges the practical possibility of one-shot learning in recognition%tasks.% a new unseen sign can still be identified if its standard (ideal) template%  exists in the human visual memory. % The main contribution of this paper is the development of a CNN that is able to use prototypical % information to guide its learning process. Two further points follow from this main contribution. % First, the boost in overall classification performance given the prototypical priors. % Second, the application of our model in zero-shot learning scenarios.  

This paper makes the following contributions : (a) development of a CNN that is
able to use prototypical information to guide its learning process, (b) its application to classification
tasks presenting a boost in overall performance, (c) establishment of a new
benchmark in logo recognition (on Belga logo dataset), and (d) the seamless
application of the proposed model in zero-shot learning scenarios, given the
prototypical information of new classes at run time.

% Second, a novel method of tackling the background-class that is consistent with the idea of prototypical mappings as per the proposed approach.

The paper is organized as follows. In Section~\ref{sec:Related-Works} we review related work. 
Section \ref{sec:Proposed-Approach} discusses the proposed approach. Sections
\ref{sec:Implementation-Details} and \ref{sec:Experiments} successively present the implementation details and
our experimental findings. Finally, Section~\ref{sec:Conclusion} concludes the
paper with a discussion about the presented work and a description of future
directions.


\section{\label{sec:Related-Works}Related Works}
Traditional computer vision approaches for classification do not take into
account the relationships there may be between the different output classes.
Arguably, if these relationships were available as side information, they could
be exploited to improve recognition performance.

Recent work focuses on taking advantage of this side information.
A considerable effort has been advocated to attribute learning. In this case,
side information takes the form of a high level description of each class as a list of attributes.
These attributes are often available in real datasets as tags, and have been
popularized within the research community thanks to datasets such as
\cite{Farhadi2009,lampert2009learning,patterson2012sun}.
Another side information that has recently been exploited by several works
\cite{socher2013zero_csm,frome2013devise,norouzi2013zero} is the semantic vector
representation of the name of each class. A semantic space of words can be
learned from a large corpus of text in an unsupervised way, so that
words are mapped to an Euclidean space in which the distance between vectors
depends on the semantic closeness of the words they represent. The vectors corresponding to the 
names of the classes can then be utilized as side information.
%This idea is exploited in \cite{socher2013zero_csm}, where a mapping function%from the input space to the word vector space is learned.

The availability of this side information about the relationship between classes
has led to the development of zero-shot learning, that is, the challenge of
identifying a class at test time without ever having seen samples of that during
training. Over the past few years, this idea has spurred much success,
using both attributes
\cite{pala_geoff2009zsl,akata2013label,romera2015embarrassingly,lampert2009learning},
and word embeddings \cite{norouzi2013zero,socher2013zero_csm}.

The developed approaches vary in the way knowledge is transferred from the training classes 
to the new classes. In \cite{Lampert2014,Suzuki2014} this transfer is done by
means of a cascaded probabilistic framework which determines the most likely class. One drawback of
probabilistic methods is that they make independence assumptions that do not
usually hold in practice. An alternative strategy which bypasses this drawback
has been recently exploited in
\cite{akata2013label,romera2015embarrassingly,weston2011wsabie}, where the
proposed model learns a linear embedding from both instances and attributes to a common space.
This can be seen as a two-layer model that connects the input images to class
labels through a layer containing attribute information. The weights connecting
the input space to the embedding space are learned to minimise the final
classification loss. Our proposed approach builds on this idea, although it presents two
significant differences. Firstly, the side information used consists of a
visual prototype for each class. Secondly, the mapping function from input to
embedding space is not linear, but modeled using a deepnet pipeline.
%WSABIE system proposed by \cite{weston2011wsabie} is one of the first ones to% use a joint feature space to map and match the images and their annotations, while optimising performance over the complete prediction task.%Very recently, output embeddings have been studied for the purposes of zero% shot learning. Intuitively, these output embeddings capture relations between the output classes such that the inference over seen classes Ls can be extended to unseen classes Lu. Works such as \cite{pala_geoff2009zsl, lampert2009learning} define an output embedding space using hand-labelled visual attributes.\iffalse
While we attempt to show how the use of prototypical information can augment deep-learning
performance on recognition tasks, we lay particular emphasis on inference on unseen classes.
This places us under the purview of zero-shot learning.
\fi\iffalse
With datasets such as for traffic signs or brand logos, the classification task amounts to a
fine-grained recognition. In this case, manually identifying visual attributes to uniquely define
each class is not only laborious but almost impossible with very small semantic difference between
some classes as in Fig. Establishing natural language semantics for these datasets is also not trivial. 
In case of brand logos, the class names have an ambiguous, often unrelated, natural language context. 
In case of traffic signs, the road sign phrases are difficult to mine preserving their complete meaning. 
Thus, in a first of its kind, we use canonical templates or prototypes to define the output embedding space.
\fi

Another related area is that of one-shot learning
\cite{bart2005cross,lake2011one,fei2006one}. Similar to zero-shot learning, the objective here is
to transfer the knowledge learned at training stage to distinguish new classes.
The difference is that the information given to the model about the new classes
consists in one, or very few, instances.
One-shot learning is useful in image retrieval, where given an image as a query,
the model returns items that are similar \cite{seanBell2015}. Our work can be considered within this area, with the peculiarity that in
our framework the instance provided to the model is a very special one: it is a prototype. 
In fact, in our model the representation of the prototypes and input images could
be completely different (e.g. having different image size).

% This section on related works starts with a discussion about the different% tasks that could possibly leverage the idea of prototypical mapping,% and the state-of-the-art methods for solving these problems till date.% Further, we elaborate the latest techniques in Zero Shot Learning% which have served to provide a conceptual basis to our proposed approach.%% \subsection{Germane Challenges (Equipped with Prototypes)}%% As discussed before, our proposed approach aims to explore the possible% benefits of introducing prototypical priors in a deepnet setup. Prototypes% are available in a wide variety of cases such as with Traffic Signs,% Brand Logos, and even character sets of language scripts.%%% \subsection{Zero Shot Learning}%% Zero shot learning refers to the challenge of identifying a class% at test time without ever having seen it samples during training.% The idea is to identify each class by a certain set of attributes% such that by learning the image to attribute mapping even an unseen% class can be identified given the semantic knowledge prior i.e. attribute% to class label mapping. Our work is similar to this concept, in the% way that prototypical templates serve as the prior knowledge base% and a feature representation over these prototypes compares to the% attribute set in ZSL.%% Substantial research has been done in ZSL with most of it focused% on learning only the input image representation in the attribute space.% To overcome the possible sub-optimality of such approaches that tackle% the two sub-tasks of image-attribute mapping and attribute-class prediction% independently, {[}akata\_et\_al{]} proposed a two layer model with% end to end learning. Their framework connects features, classes and% attributes in a two linear-layer model with a fixed class to attribute% mapping, and learns to minimize the final classification error. This% work is in turn inspired by the WSABIE system of {[}weston\_et\_al{]}.% {[}weston\_et\_al{]} introduce the concept of embedding both input% and output into a joint feature space and matching them to minimize% the final top K-label predictions. While input embeddings serve to% characterize the image space distribution, even so the system can% leverage the output distribution through the output embedding. In% {[}weston\_et\_al{]} these output embeddings capture the semantic% closeness and relationships of words. In {[}Akata\_et\_al{]} these% embeddings capture the rich attributes as shared by multiple classes.% In our case, these embeddings leverage the prototypical information% base.%% ({[}bernardino\_eszsl{]} discuss.... They express ZSL as a domain% adaptation problem which allows deepnet)...to be discussed!

\section{\label{sec:Proposed-Approach}Proposed Approach}

In the usual image classification setup, given training samples of form
$(x, y)$, where $x \in \mathbb{R}^d$ is an image and $y \in
\left\{1,\ldots,C\right\}$ is the class label of the image, a classifier $h : \mathbb{R}^d \to \{1,\ldots,C\}$ 
is learned to predict the label of an unseen image $x$ as $\hat{y}$.

%consider $m$ training instances in a $d$- dimensional space,%that is, $m$ images where the number of pixels times the channels%of the image amount to $d$. We can arrange them in matrix $X\in\mathbb{R}^{d\times m}$.%At training stage we know the ground truth class they belong to, and%we arrange that information in ,%where $C$ is the number of classes.
If we apply a regular $L$-layer
CNN to this problem, the function that is learned takes the following form:

\begin{equation}
\hat{y}=\operatornamewithlimits{argmax}_{c\in\{1,\ldots,C\}}\enspace
s\left(f_{L}\left(f_{L-1}\left(\ldots f_{2}\left(f_{1}\left(x;\theta_{1}\right);
\theta_{2}\right)\ldots;\theta_{L-1}\right);\theta_{L}\right)\right)_c.\label{eq:CNN}
\end{equation}Here, $f_{l}$, for $l\in\left\{ 1,\ldots ,L\right\} $ represents the function
(e.g. convolution, pooling) applied at layer $l$, and $\theta_{l}$ denotes its
learnable parameters, if any. The last function $f_L$ maps its inputs to
$\mathbb{R}^C$. Finally, $s(.) : \mathbb{R}^C \to [0, 1]^C$ represents the
softmax activation function operating on a vector $z$, as follows: 

\begin{center}
$s(z)_{c}=\frac{\exp\left(z_{c}\right)}{\overset{C}{\underset{j=1}{\sum}}\exp\left(z_{j}\right)}$,
for $c\in\left\{ 1,\ldots ,C\right\} $,
\par\end{center}
where subscripts denote the elements of a vector.

During training, learnable weights $\theta_1, \theta_2, \ldots, \theta_L$ of the model are adjusted 
by backpropagating the negative log-likelihood loss over the ground truth label
${y}$ of a sample $x$, defined as follows:
\begin{center}
$\operatorname{loss}(x; \theta_1, \theta_2, \ldots, \theta_L) =
-\log(s(z)_{{y}}).$
\end{center}

The CNN represented by Equation~\eqref{eq:CNN} does not account for prior
information regarding prototypes of the classes. In order to introduce our
approach, let us assume that a prototype template image $p_{c}$ for each class
$c \in \{1,\ldots,C\}$ is provided. The proposed approach is based upon fixing the parameters of
the last layer of the CNN as a function of the prototype templates $p_c$, given by 
$\phi(p_{c})\in\mathbb{R}^{k}$, for some integer $k$, with $\|\phi(p_c)\|_2$ being constant for all $c
\in \{1,\ldots,C\}$. In practice, $\phi$ can be a feature extractor for the
template $p_c$; for instance, $\phi(p_c)$ can be a $k$-dimensional normalized HOG feature extracted from the
prototypical image $p_c$.

More specifically, we set $f_L : \R^k \to \R^C : f_L(v)_c = \langle\phi(p_c), v\rangle$, 
where $v$ denotes the activations fed into layer $L$ for a certain input
image, the subscript denotes vector elements and $\langle .,.\rangle$ denotes the usual dot product in $\R^k$. 
Since $\|\phi(p_c)\|_2$ is constant, when $c$ is varied for a fixed $v$,
$f_L(v)_c = \langle\phi(p_c), v\rangle$ attains the highest value for the
$\phi(p_c)$ closest to $v$ in the $k$-dimensional feature space.

The modified network can now be described using the following formula:
\begin{equation}
\hat{y}=\operatornamewithlimits{argmax}_{c \in \{1,\ldots,C\}}\enspace
s\left(f_{L}\left(f_{L-1}\left(\ldots
f_{2}\left(f_{1}\left(x\right)\right)\ldots\right)\right)\right)_c =
\operatornamewithlimits{argmax}_{c\in\{1,\ldots,C\}} \langle\phi(p_c),
\psi(x)\rangle,
\label{eq:CNN2}
\end{equation}
where $\psi(.)$ and $\phi(.)$ represent the projections of input images and
output labels into the joint feature space, respectively. An interpretation of this approach is that 
the learnable part of the network,
$\psi : \R^{d} \to \R^{k}: \psi=f_{L-1}\circ\ldots\circ f_{1}$,
learns a non-linear mapping from the original images to a $k$-dimensional
latent space, which in this case is defined by the prototypes. 
\begin{figure}
  \centering
  \includegraphics[width=0.95\columnwidth]{im_cnn_archi.pdf}
%   \vspace{0.1pt}
\caption{\label{fig:network_arch}Network architecture with the introduction
  of prototypical priors. In the current experiments, $k$-dimensional HoG
  features extracted over the prototypical templates are used to define the
  common embedding space.}
\end{figure}
This space contains both the projections of an input image, $\psi(x)$, and the
prototype templates, $\phi(p_{c})$ for all $c\in\left\{ 1,\ldots ,C\right\} $,
such that the similarity between each instance-prototype pair can be computed by means of an inner product.
The use of softmax-loss function leads to a discriminative way to encourage the inner product 
between $\psi(x)$ and $\phi(p_{c})$ to be high if instance $x$ belongs to class
$c$, and to be low otherwise. Thus, we introduce prior
information about the classes directly into the network with the aspiration that the remaining parameters
will adapt themselves to accommodate the fixed last layer in the learning process.

% \textbf{revise this}%  is the learnable part of the network. Note that we have dropped the parameters $\theta$ in %  the notation for clarity. Due to the behaviour of fixed $f_L$ described above, $\hat{y}$ will %  be equal to $c''$ where $c''$ is the class of the prototype $p_{c''}$ that has the feature %  representation $\phi(p_{c''})$ closest to $\psi(x)$.%In a standard deepnet framework, system weights are adjusted to map%the input samples onto rather uninformative albeit correct class labels.%In the current approach, to leverage on the prototypes associated%with each class (or label), both the input samples as well as the%corresponding protoypical templates are transformed to match in a%joint feature space. 

Note that, unlike in other works such as \cite{pala_geoff2009zsl,norouzi2013zero}, at test
time the inference process is exactly the same as in any other CNN. There is no need to 
perform explicit calculations about distances in the embedding space.

This framework easily allows for using new prototypes after
the training stage is finished. This is done by replacing, or adding to the last
layer new weights according to the new prototypes. The resultant network is
potentially capable of distinguishing the new classes because the invariances
learned in $\psi$ are conceivably common to all classes.


In the given framework, both functions $\psi$ and $\phi$ can be learned.
However, for the purpose of current research, we focus on the
case where $\psi$ is learned as part of the traditional CNN pipeline, while $\phi$ is fixed by 
a prescribed function, such as HoG transform.

%Thus, the joint features space is largely defined by the pre-selected% representation and is left only slightly adjustable. % \iffalse% At an intuitive level, pre-fixed prototypical weights in the current% setup can be reasoned to improve deep learning classification performance% in the following two ways:% % First, by guiding the learning process for feature extraction. The sequence of% convolutional layers in a CNN incrementally learns how to% extract/model the regularities observed across different samples of each class.% Traditionally, these convolutional layers need only to adjust their% parameters to represent the commonality across these varying class% samples. With the current addition, they must also adjust to develop% a correlation with the fixed prototypical mappings. The difference% in weight adjustment is also qualitatively analysed as part of section% (). % % Second, by improving the discriminative model. The fully-connected layers of a% CNN learn the decision boundaries between different learned classes. With% the last layer fixed for maximum discrimination between the prototypical% templates (ideally), system weights must align with this maximum discrimination,% pushing for an improved performance. % \fi

\section{\label{sec:Implementation-Details}Implementation Details}
We now detail the architecture of our deep network used to implement the ideas described above. 
The first stage of our network consists of a CNN to enable learning of image features starting from 
original RGB patches of $48\times48$ (size suitable for both traffic-sign and
logo samples in experimental datasets). 

The configuration, as presented in \textit{red} (\textit{light} for grayscale)
in Figure ~\ref{fig:network_arch}, is the same from {\cite{multicolumnarDNN}}
with the exception of a dropout layer after $L_{5}$.
As in a traditional CNN designed for classification, the last few layers are fully-connected, and the 
network is terminated with a layer having the same number of activations as the number of classes $C$. 
A softmax function is applied to the last layer to obtain a probability distribution over the output class 
labels. 

In the proposed approach, prototypical information is introduced by wedging a layer before the output 
layer, fully connected to the $C$ output neurons using the fixed weights $\phi(p_c)\in \mathbb{R}^k$ for 
all $c\in\left\{ 1,\ldots,C\right\}$. The new layer and its connections are
shown in \textit{blue} (\textit{dark} for grayscale) in
Figure ~\ref{fig:network_arch}.
Thus, the $k\times C$ weight matrix for the last fully connected layer $f_{L}$
is defined as a set of $k\times1$ vectors $\phi(p_c)$ one for each
$c\in\{{1,\ldots, C}\}$. In Figure ~\ref{fig:network_arch}, we use $\phi_1(p_c),
\phi_2(p_c), \ldots,  \phi_k(p_c)$ to represent the elements of the $k$-dimensional vector $\phi(p_c)$.

In the current work, we fix the embedding space using $k$-dimensional
normalized histograms of oriented gradients (HoG) {\cite{dalal2005histograms}} features
extracted from the prototypical templates. The prototypical images are all resized to a fixed size $s\times s$.
The HOG features are extracted using the standard \textit{extractHoGFeatures} function provided with Matlab. 
In our experiments, we make use of an empirically selected cell size $c=10$, block size $b=2$,  
overlap factor $o=1$ and a bin count $n=12$, for $s=100$, which yields $3888$-dimensional HoG features.
% % Hence, the penultimate layer, representing the embedding space, consists of $k$ neurons. Let $\textbf{a}_x = [a_{x,1}\;a_{x,2}\;\ldots\;a_{x,k}]$ be the activation of this layer % for a given input $x$ to the network, i.e., $\textbf{a}_x = \phi_{l}(x)$. Since each feature % representation $\phi_{p}(p_c)$ is normalized, $\hat{y}_c = s(\textbf{a}_x^T \phi_{p}(p_c))$ % achieves a high value when $\textbf{a}_x$ is close to $\phi_{p}(p_{c'})$ where $c'$ is the correct % class of the sample $x$.% Let us now consider a scenario where the network is to recognize $C$ classes defined by $C$ prototypes $p_1, p_2, \ldots, p_C$, but real world training samples of only $C_1 < C$ classes are available. A practical example for such a scenario would be the design of a traffic sign recognition system, where real images of some rare traffic signs are not available during the training phrase. In this case, during training, we initialize the weights between the last two layers of the network using all prototypes $p_1, p_2, \ldots, p_C$ as described above, but apply the softmax function only across the first $C_1$ classes for which training data are available (without a loss of generality, we assume that these classes are indexed $1, \ldots, C_1$).

\section{\label{sec:Experiments}Experiments}
We explore the above idea of introducing prototypical information during deep learning phase for 
two end goals: (a) Improvement in overall classification performance when all classes are seen during 
training, (b) Improvement in classification performance over unseen classes,
i.e., in a zero-shot learning scenario.

\begin{figure}
  \centering
  \includegraphics[width=0.8\columnwidth]{im_images_sample.pdf}
%   \vspace{0.1pt}
\caption{\label{fig:sampleimages}Sample images of traffic signs (left) show view
point distortion, illumination variation and background clutter while logo
images (right) additionally contain non-planar distortions and high
self-occlusion. }
\end{figure}\subsection{\label{}Datasets}
To analyse the generalisability of the proposed approach we evaluate it on two
separate datasets as described below.

\textit{Traffic Sign Dataset:} We use the German Traffic Sign Recognition
benchmark \cite{stallkamp2012manvscomp}, hereafter referred to as D1. This dataset has a substantial 
sample base of more than 50,000 images spread over 43 traffic sign classes. 
The dataset is divided into 39,209 training samples and 12,630 test samples. 
For experimental purpose, we randomly split the test data into validation and 
test sets of 6,315 samples each. We fine crop the samples using the information provided with the 
dataset. No additional distortion (such as scaling, rotation) is applied at training or testing time. 

\textit{Brand Logo Dataset:} We use the Belga Logos dataset
{\cite{belgajoly2009logo}}, hereafter referred to as D2.
The dataset contains bounding box annotations for 37 logo categories collected from across 10,000 
real images. Out of a total of 9,841 logo samples, 2,697 are marked as `OK' for their ability to be 
recognizable without the image context. We use a subset of 10 logo classes (out of the 37), for 
which the total number of samples per class is at least 100. We set aside 20\% of the samples from each class for validation, and 20\% for testing.
 
Sample images from both the datasets are as shown in Figure
~\ref{fig:sampleimages}.
\begin{table}
\begin{centering}
\begin{tabular}{|c|c|c|}
\hline 
 \shortstack{Dropout \\ Factor}& \shortstack{Case 1 \\ Test accuracy
 (\% )} & \shortstack{Case 2 \\ Test accuracy in (\%
)}\tabularnewline
\hline 
\hline 
0.5 & 96.60 & 97.98\tabularnewline
\hline 
0.6 & 97.18 & 97.53\tabularnewline
\hline 
0.65 & 97.48 & 97.74\tabularnewline
\hline 
\end{tabular}
\vspace{6pt}
\protect\caption{\label{zslconfigs} Consistent boost in classification
performance across different configurations experimented for dataset D1. The
test accuracy compares to \cite{multicolumnarDNN} in the case of no use of data
augmentation during training.}
\end{centering}
\end{table}\begin{table}
\begin{centering}
\begin{tabular}{|c|c|c|}
\hline 
Dataset & \shortstack{Case 1 \\ Test accuracy (\% )}  
& \shortstack{Case 2 \\ Test accuracy in (\%
)}\tabularnewline
\hline 
\hline 
D1 & 97.48 & 97.98\tabularnewline
\hline 
D2 & 93.48 & 93.57\tabularnewline
\hline 
\end{tabular}
\vspace{6pt}
\protect\caption{\label{classperformance} Overall improvement in classification
performance with the use of prototypical information}
\end{centering}
\end{table}\subsection{\label{}Results}\subsubsection{\label{overall}Overall Recognition Performance}
In this setup, all the classes are treated as seen. Classification
results on dataset D1 with comparable configurations of conventional (Case 1)
and proposed (Case 2) deepnet pipeline are shown in Table \ref{zslconfigs}. For
the 3 different configurations, dropout-factor of layer after $L_5$ is varied to be 0.5, 0.6 and 0.65
respectively. 

Top results on D1 and D2, without (Case 1) and with (Case 2) the use of
prototypical information, are shown in Table \ref{classperformance}. For dataset D1, test performance 
without prototypical information is comparable to that presented in
\cite{multicolumnarDNN} for the case when no additional data augmentation technique is employed. Inclusion
of prototypical embedding boosts the performance by 0.5\% leading
to an almost 20\% reduction in the error rate. On dataset D2, the proposed
approach gives a comparable performance to, if not better than, the
baseline.
A possible explanation could be that logo samples display heavy self
occlusion, perspective distortion and general lack of visual quality.
\iffalse
Our understanding is that a
deeper CNN with a greater adaptability to the above variations should give a
higher performance boost while attempting to match the samples to the
prototypes in the embedding space.
\fi\textit{Additional findings:} For both the datasets, we experimented with
grayscale as well as colored (RGB) prototypes. Models using prototypical features extracted from 
colored templates consistently performed lower (by an average margin of 0.1\%)
compared to those using the same features obtained from grayscale templates. 

This suggests that while color coding may be useful in
garnering visual attention, it may not be quintessential for distinguishing the
classes. For traffic sign dataset D1, 12 out of 43 classes
are \textit{Prohibitory} traffic signs with a consistent circular red and white
color coding, while 8 are \textit{Mandatory} signs with a uniform circular blue
and white color coding. Evidently, the main discrimination quotient in traffic signs is added by
the inset depiction. 
On the other hand, for logo dataset D2, samples show significant color
variation within a single class,  as shown in ~\ref{fig:sampleimages}, which
renders the color information quite irrelevant.
\begin{figure}
  \centering
  \includegraphics[width=0.7\columnwidth]{im_compd1_1.pdf}
%   \vspace{0.1pt}
\caption{\label{fig:comparisonD1}Recognition performance on unseen classes of
dataset D1 compared across proposed and baseline \cite{norouzi2013zero} approach}
\end{figure}\begin{figure}
  \centering
  \includegraphics[width=0.7\columnwidth]{im_compd2_1.pdf}
%   \vspace{0.1pt}
\caption{\label{fig:comparisonD2}Recognition performance on unseen classes of
dataset D2 compared across proposed and baseline \cite{norouzi2013zero}
approach}
\end{figure}\subsubsection{\label{}Zero-Shot Learning}\textit{Data setup:}  The 43 classes of dataset D1 are
divided into 33 seen classes (denoted by the set of classes $C_s$), and 10 unseen classes (denoted by the 
set $C_u$). Samples from classes in $C_s$ are used for 
training the model while the remaining 10 classes in $C_u$ are used for testing the model. During test time, all $c \in C_u$ form the output 
label set, that is, the network could predict any label from $C_u$.

Similarly for D2, 10 classes are divided into 7 seen classes (set $C_s$) and 3
unseen classes (set $C_u$). Samples with class labels in $C_s$ are used for
training the model and the 3 classes in $C_u$ are used for testing.

\textit{Comparison:} We compare our approach with the method
of convex combination of embedding vectors, as
discussed in \cite{norouzi2013zero}. In this, new unseen class samples are represented as
weighted combinations of vector embeddings $\phi(p_c)$ of seen classes $c \in
C_s$, where the weights are the probabilistic output of the softmax layer. Top
$T$-predictions are combined to yield the feature representation, where $T$ is a
hyperparameter that can be tuned by means of a validation process. These
representations are compared in the vector space defined by
$\phi(p_c)$, where $c \in C_u$.
The class of the input sample is inferred to be the class of the nearest
prototype in this space.

\textit{Findings and discussion:} We make 10 random selections of $C_s$ and
$C_u$. For the proposed approach, the prototypical
representations of $\phi(p_c),\,c\in C_s$ are used during training, while these are replaced 
by $\phi(p_c),\,c\in C_u$ during testing. The validation
hyperparameter $T$ of \cite{norouzi2013zero} is set to the total
number of seen classes $C_s$ while the proposed approach simply
validates against a set-aside sample set over all the seen classes.
The classification results for unseen classes on datasets D1 and D2 
are compared in Figures ~\ref{fig:comparisonD1} and ~\ref{fig:comparisonD2}
respectively.
The proposed approach outperforms \cite{norouzi2013zero} with an average
accuracy gain of 5.48\% and 10.15\% on datasets D1 and D2 respectively. The
performance gain is statistically significant for D1 at a p-value of 5\% as well
as for D2 although with a p-value of 33\%.
Due to visual similarity, an unseen traffic sign can still be fairly well reproduced by the combination of
related prototypical templates as done in \cite{norouzi2013zero}. The major
benefit of proposed approach is evident in visually dissimilar logo categories
where the zero shot performance is considerably improved.

% while the baseline CNN model can only be optimised for seen classes. Indeed,% this is a highlight of the proposed approach. % \begin{table}% \begin{center}% \begin{adjustbox}{max width=\textwidth}% \begin{tabular}{|c|c|c|}% \hline % \multirow{3}{*}{\shortstack{$Random$ \\ $Trial$}} &% \multicolumn{3}{c|}{\shortstack{$Approach$ $of$ \\ \cite{norouzi2013zero}}} &% \multicolumn{3}{c|}{\shortstack{$Proposed$ \\ $Approach$}}\tabularnewline% % \cline{2-7} % %  & \multirow{2}{*}{$C_s$} & \multicolumn{2}{c|}{$C_u$} & \multirow{2}{*}{$C_s$}% %  & \multicolumn{2}{c|}{$C_u$}\tabularnewline% % \cline{3-4} \cline{6-7} % %  &  & $val$ & $test$ &  & $val$ & $test$\tabularnewline% \hline % 1 & 97.39 & 54.43 & 43.95 & 95.18 & 58.10 & \textbf{57.57}\tabularnewline% \hline % 2 & 97.06 & 45.92 & 38.90 & 95.95 & 65.97 & \textbf{45.23}\tabularnewline% \hline % 3 & 97.76 & 38.22 & 53.68 & 98.16 & 48.81 & \textbf{62.05}\tabularnewline% \hline % 4 & 97.62 & 49.59 & 36.53 & 96.35 & 76.01 & 20.42\tabularnewline% \hline % 5 & 98.16 & 72.03 & 68.12 & 97.20 & 76.08 & \textbf{75.13}\tabularnewline% \hline % Average & 97.60 & 52.045 & 48.24 & 96.57 &% 64.99 & \textbf{52.08}\tabularnewline% \hline % \end{tabular}% \end{adjustbox}% \protect\caption{\label{zslperformanceD1} Recognition performance on both seen% and unseen classes for dataset D1}% \end{center}% \end{table}% \begin{table}% \begin{center}% \begin{adjustbox}{max width=\textwidth}% \begin{tabular}{|c|c|c|}% \hline % \multicolumn{1}{|c|}{\shortstack{$Random$ \\ $Trial$}} &% \multicolumn{1}{c|}{\shortstack{$Approach$ $of$ \\ \cite{norouzi2013zero}}} &% \multicolumn{1}{c|}{\shortstack{$Proposed$ \\ $Approach$}}\tabularnewline% % \cline{2-7} % %  & \multirow{2}{*}{$C_s$} & \multicolumn{2}{c|}{$C_u$} & \multirow{2}{*}{$C_s$}% %  & \multicolumn{2}{c|}{$C_u$}\tabularnewline% % \cline{3-4} \cline{6-7} % %  &  & $val$ & $test$ &  & $val$ & $test$\tabularnewline% \hline % 1 & 97.39 & 54.43 \tabularnewline% \hline % 2 & 97.06 & 45.92 \tabularnewline% \hline % 3 & 97.76 & 38.22 \tabularnewline% \hline % 4 & 97.62 & 49.59 \tabularnewline% \hline % 5 & 98.16 & 72.03 \tabularnewline% \hline % Average & 97.60 & 52.045 \tabularnewline% \hline % \end{tabular}% \end{adjustbox}% \protect\caption{\label{zslperformanceD1} Recognition performance on unseen classes for dataset D1}% \end{center}% \end{table}% % \begin{table}%  \begin{center}% \begin{tabular}{|c|c|c|c|c|c|c|}% \hline % \multirow{3}{*}{\shortstack{$Random$ \\ $Trial$}} &% \multicolumn{3}{c|}{\shortstack{$Approach$ $of$ \\ \cite{norouzi2013zero}}} &% \multicolumn{3}{c|}{\shortstack{$Proposed$ \\ $Approach$}}\tabularnewline% \cline{2-7} %  & \multirow{2}{*}{$C_s$} & \multicolumn{2}{c|}{$C_u$} & \multirow{2}{*}{$C_s$}%  & \multicolumn{2}{c|}{$C_u$}\tabularnewline% \cline{3-4} \cline{6-7} %  &  & $val$ & $test$ &  & $val$ & $test$\tabularnewline% \hline % 1 & 98.14 & 0.00 & 61.22 & 96.05 & 62.25 & \textbf{62.24}\tabularnewline% \hline % 2 & 94.04 & 15.45 & 47.81 & 92.56 & 66.89 & \textbf{71.43}\tabularnewline% \hline % 3 & 95.41 & 72.73 & 11.75 & 86.05 & 31.79 & \textbf{25.25}\tabularnewline% \hline % 4 & 94.04 & 0.00 & 61.76 & 93.10 & 70.75 & 34.11\tabularnewline% \hline % 5 & 93.93 & 49.59 & 31.68 & 87.45 & 72.73 & \textbf{33.59}\tabularnewline% \hline % Average & 95.11 & 27.55 & 42.84 & 91.04 & 60.88 & \textbf{45.32}\tabularnewline% \hline % \end{tabular}% \protect\caption{\label{zslperformanceD2} Recognition performance on both seen% and unseen classes for dataset D2}% \end{center}% \end{table}   \begin{figure}[t]
%   \begin{subfigure}
  	\begin{center}
  	\includegraphics[trim = 0mm 2mm 0mm 0.1mm, clip,scale=0.55]{im_tradeoff1}
  	\end{center}
%   \end{subfigure}
  \caption{\label{tradeoff}Performance trade-off curve 
for seen and unseen classes over a certain trial of D1 and D2 respectively}
\end{figure}
In the approach of \cite{norouzi2013zero}, training and
validation are disconnected steps. The CNN can be trained for maximum
performance only on $C_s$. At validation time parameter $T$, that defines the
number of seen classes used for drawing inference, provides little flexibility for tuning the performance 
on $C_u$. On the contrary, our model can be fine-tuned either for unseen
or seen class performance by validating against the appropriate set. In
the current experiments, we validate against a sample set collected over the
seen classes $C_s$, however it can also contain samples from a few unseen
classes $c\in C_u$ marked as validation classes. CNN training is carried out as
before to get a joint optimization for both seen and unseen class performance.

In our experimental experience we found that the performances of seen and unseen
classes are positively correlated in the initial stages of the training
procedure. However, this happens up to a point, beyond which both performances
appear to be negatively correlated (see Figure \ref{tradeoff} showing the performance
trade-off curve for seen and unseen classes over a certain trial of D1 and D2 respectively using
our approach). The above tests are carried out using a certain random
selection of 5 unseen classes for D1 and 2 unseen classes for D2.

% In this case, we tune our model by following the unseen % classification performance, thus neglecting the seen classification results. If,% however, the proposed model were to be trained and tested for only $C_s$,% results (extrapolated) from the setup of Section~\ref{overall}, would show an improvement over % conventional CNNs.% For a particular case of poor performance, we visualise the embedding% space for both the approaches, in Figure\ldots.% \subsection{DeepNet Design Details}% % For our experimental purposes, we have not made use of any of the% popular deepnet configurations like AlexNet, VGG Net or GoogleNet.% These nets have a higher inference window size of around 224x224.% This is particularly unsuited for our small object datasets where% the object sizes range from 15x15 to 250x250.% % We implement and work with the deep-net configuration presented in% {[}multicolumn\_deepnet{]} for traffic sign recognition (more specifically,% German Traffic Sign Recognition). The architectural details are summarized% in Table (). This net takes an input image of size 48x48 (suitable% for all 3 experimental datasets) and generates a single label inference.% Although not very deep, it has shown to perform exceedingly well at% the task of traffic sign recognition. % % In addition to the basic layers for performing convolution, pooling% and non-linear activation, we also incorporate a drop-out layer after% layer (). This prevents the net from over-fitting to the training% data and also helps achieve robustness to local noises particularly% occlusion. Different values of drop-out factor and their effect on% the network performance is presented in ().% % % \subsection{Proposed Enhancement}% % We propose the use of prototypical information as the set of fixed% weights (fully) connecting the penultimate layer to the last n-way% layer (where n is the number of classification categories) in a given% deepnet. With the deepnet configuration shown in Fig.1(), prototypical% prior has been introduced by wedging a fixed-weight fully-connected% layer () before the output layer. % % Here, the processing of layers ( to ) corresponds to the input embedding% that results in the representation at layer (). The weights connecting% this layer () to () are the output embeddings. The inner product of% layer () and weights () represents the matching between the input% and output embeddings in the abstract joint space. Resulting scores% are fed to the softmax layer:% % ()% % Maximising the score for the correct class using iterative backpropagation% amounts to adjusting the input embeddings to correlates strongly with% the correct class output mappings while de-correlating with the other% class mappings.% % ()% % This implicity ensures that the fully-connected classification layers% learn a strong classwise representation as well as an improved discriminative% model established by the ideal class templates.% % % \section{\label{sec:Performance-Results}Performance Results }% % With the use of propotypical priors, we report an improvement in recognition% performance consistently across three different datasets. We compare% the effectiveness of different common embedding spaces for the input% images and labels. As per intuition, higher level embedding spaces% i.e. HoG features show better performance as opposed to low level% pixel data. % % Also, we analyse the qualitative effects of using prototypes on the% deep-learning experience. This is done by visualizing (using T-sne)% how the features at layer () cluster differently when the net is learned% along with the prototypical mappings. Additionally, a look at the% differently recognized samples can give useful clues about system% learning.% % % \subsection{Quantitative}% % % \subsubsection{German Traffic Sign Recognition Benchmark}% % % \subsubsection{Belga Brand Logo Recognition Benchmark}% % % \subsubsection{ICDAR 2003 Character Recognition Benchmark}% % % \subsection{Qualitative}% % % \section{\label{sec:Wider-Applicability}Wider Applicability }% % Deepnets came back into popularity with their success on object recognition% tasks {[}{]}. Since then, their effectiveness has been confirmed on% challenges such as detection {[}{]} and segmentation {[}{]}. It is% particularly for the latter two, that deepnets need also to accomodate% an additional background class. On face value, our proposed enhancement% doesn't support this requirement. The background class doesn't have% one single consistent definition. It comprises of everything other% than the objects of interest. A prototypical mapping for such a concept% is counter-intuitive. % % Then again, should we necessarily treat background as just-another-object-category.% Given our principled way of incorporating prototypical priors, we% could well argue that the background class has not been appropriately% handled in the current deepnet setups. After all, there is no one% single all-encompassing definition of a background. Training the deepnet,% through a few thousand background patches, as to what a background% must look like is theorectically incorrect. Also, on a practical level,% such a system is not well placed in handling a change of scene/background% content given the same set of object classes. % % This motivated us to....(I think we can remove this section so as% to keep a direct focus on the above approach of using prototypes to% augment deep learning.)% % % \section{\label{sec:Conclusion-and-Future}Conclusion and Future Work}

\section{\label{sec:Conclusion}Conclusion}

In this paper we showed that visual prototypes can be successfully used as side information 
to aid the learning process in traditional classification setup as well as for
zero-shot learning.

We proposed a method for integrating prototypical information in the
successful deep learning framework. Using a conventional CNN stage, the input projection
function that maps input images to a joint prototypical space can be learned for 
maximum similarity between a real-world instance and its prototype, while minimising the
end recognition loss. In the current research, this embedding space is
preemptively fixed by the choice of prototypical representation while the input mapping is
learnable as a complex non-linear function. More generally, however, both the
input and output embeddings can be learned as an end-to-end deepnet pipeline. We
plan to explore this as part of our future work.

As observed on two different datasets of traffic signs and brand logos, results
of the proposed approach are highly promising.
Regarding its application to regular object recognition, we can conclude that constraining the
network to incorporate the given prototypes does not hamper, but on the contrary
 improves the classification performance. With regard to zero-shot learning, our model shows better results than a state-of-the-art competitor~\cite{norouzi2013zero}. Furthermore,  
our model can be flexibly trained for the required trade-off between seen and unseen class performance, 
and inference on new unseen classes simply involves adding their prototypical
information at test time.

%\bibliography{references}
\begin{thebibliography}{23}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akata et~al.(2013)Akata, Perronnin, Harchaoui, and
  Schmid]{akata2013label}
Zeynep Akata, Florent Perronnin, Zaid Harchaoui, and Cordelia Schmid.
\newblock Label-embedding for attribute-based classification.
\newblock In \emph{Computer Vision and Pattern Recognition (CVPR), 2013 IEEE
  Conference on}, pages 819--826. IEEE, 2013.

\bibitem[Bart and Ullman(2005)]{bart2005cross}
Evgeniy Bart and Shimon Ullman.
\newblock Cross-generalization: Learning novel classes from a single example by
  feature replacement.
\newblock In \emph{Computer Vision and Pattern Recognition, 2005. CVPR 2005.
  IEEE Computer Society Conference on}, volume~1, pages 672--679. IEEE, 2005.

\bibitem[Cire{\c{s}}an et~al.(2012)Cire{\c{s}}an, Meier, Masci, and
  Schmidhuber]{multicolumnarDNN}
Dan Cire{\c{s}}an, Ueli Meier, Jonathan Masci, and J{\"u}rgen Schmidhuber.
\newblock Multi-column deep neural network for traffic sign classification.
\newblock \emph{Neural Networks}, 32:\penalty0 333--338, 2012.

\bibitem[Dalal and Triggs(2005)]{dalal2005histograms}
Navneet Dalal and Bill Triggs.
\newblock Histograms of oriented gradients for human detection.
\newblock In \emph{Computer Vision and Pattern Recognition, 2005. CVPR 2005.
  IEEE Computer Society Conference on}, volume~1, pages 886--893. IEEE, 2005.

\bibitem[Farhadi et~al.(2009)Farhadi, Endres, Hoiem, and Forsyth]{Farhadi2009}
a.~Farhadi, I.~Endres, D.~Hoiem, and D.~Forsyth.
\newblock{Describing objects by their attributes}.
\newblock \emph{2009 IEEE Conference on Computer Vision and Pattern
  Recognition}, pages 1778--1785, June 2009.
\newblock \doi{10.1109/CVPR.2009.5206772}.

\bibitem[Fei-Fei et~al.(2006)Fei-Fei, Fergus, and Perona]{fei2006one}
Li~Fei-Fei, Robert Fergus, and Pietro Perona.
\newblock One-shot learning of object categories.
\newblock \emph{Pattern Analysis and Machine Intelligence, IEEE Transactions
  on}, 28\penalty0 (4):\penalty0 594--611, 2006.

\bibitem[Frome et~al.(2013)Frome, Corrado, Shlens, Bengio, Dean, Mikolov,
  et~al.]{frome2013devise}
Andrea Frome, Greg~S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Tomas
  Mikolov, et~al.
\newblock Devise: A deep visual-semantic embedding model.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2121--2129, 2013.

\bibitem[Goodfellow et~al.(2009)Goodfellow, Lee, Le, Saxe, and
  Ng]{goodfellow2009measuring}
Ian Goodfellow, Honglak Lee, Quoc~V Le, Andrew Saxe, and Andrew~Y Ng.
\newblock Measuring invariances in deep networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  646--654, 2009.

\bibitem[Joly and Buisson(2009)]{belgajoly2009logo}
Alexis Joly and Olivier Buisson.
\newblock Logo retrieval with a contrario visual query expansion.
\newblock In \emph{Proceedings of the 17th ACM international conference on
  Multimedia}, pages 581--584. ACM, 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{Krizhevsky/nips2012}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In F.~Pereira, C.J.C. Burges, L.~Bottou, and K.Q. Weinberger,
  editors, \emph{Advances in Neural Information Processing Systems 25}, pages
  1097--1105. Curran Associates, Inc., 2012.

\bibitem[Lake et~al.(2011)Lake, Salakhutdinov, Gross, and
  Tenenbaum]{lake2011one}
Brenden~M Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua~B Tenenbaum.
\newblock One shot learning of simple visual concepts.
\newblock In \emph{Proceedings of the 33rd Annual Conference of the Cognitive
  Science Society}, volume 172, 2011.

\bibitem[Lampert et~al.(2009)Lampert, Nickisch, and
  Harmeling]{lampert2009learning}
Christoph~H Lampert, Hannes Nickisch, and Stefan Harmeling.
\newblock Learning to detect unseen object classes by between-class attribute
  transfer.
\newblock In \emph{Computer Vision and Pattern Recognition, 2009. CVPR 2009.
  IEEE Conference on}, pages 951--958. IEEE, 2009.

\bibitem[Lampert et~al.(2014)Lampert, Nickisch, and Harmeling]{Lampert2014}
Christoph~H Lampert, Hannes Nickisch, and Stefan Harmeling.
\newblock Attribute-based classification for zero-shot visual object
  categorization.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 36\penalty0 (3):\penalty0 453--65, March 2014.
\newblock ISSN 1939-3539.
\newblock \doi{10.1109/TPAMI.2013.140}.

\bibitem[Lee et~al.(2009)Lee, Grosse, Ranganath, and Ng]{Lee/icml2009}
Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew~Y Ng.
\newblock Convolutional deep belief networks for scalable unsupervised learning
  of hierarchical representations.
\newblock In \emph{Proceedings of the 26th Annual International Conference on
  Machine Learning}, pages 609--616. ACM, 2009.

\bibitem[Norouzi et~al.(2013)Norouzi, Mikolov, Bengio, Singer, Shlens, Frome,
  Corrado, and Dean]{norouzi2013zero}
Mohammad Norouzi, Tomas Mikolov, Samy Bengio, Yoram Singer, Jonathon Shlens,
  Andrea Frome, Greg~S Corrado, and Jeffrey Dean.
\newblock Zero-shot learning by convex combination of semantic embeddings.
\newblock \emph{arXiv preprint arXiv:1312.5650}, 2013.

\bibitem[Palatucci et~al.(2009)Palatucci, Pomerleau, Hinton, and
  Mitchell]{pala_geoff2009zsl}
Mark Palatucci, Dean Pomerleau, Geoffrey~E Hinton, and Tom~M Mitchell.
\newblock Zero-shot learning with semantic output codes.
\newblock In \emph{Advances in neural information processing systems}, pages
  1410--1418, 2009.

\bibitem[Patterson and Hays(2012)]{patterson2012sun}
Genevieve Patterson and James Hays.
\newblock Sun attribute database: Discovering, annotating, and recognizing
  scene attributes.
\newblock In \emph{Computer Vision and Pattern Recognition (CVPR), 2012 IEEE
  Conference on}, pages 2751--2758. IEEE, 2012.

\bibitem[Romera-Paredes et~al.(2015)Romera-Paredes, OX, and
  Torr]{romera2015embarrassingly}
Bernardino Romera-Paredes, ENG OX, and Philip~HS Torr.
\newblock An embarrassingly simple approach to zero-shot learning.
\newblock In \emph{Proceedings of The 32nd International Conference on Machine
  Learning}, pages 2152--2161, 2015.

\bibitem[Sean~Bell(2015)]{seanBell2015}
Kavita~Bala Sean~Bell.
\newblock Learning visual similarity for product design with convolutional
  neural networks.
\newblock \emph{ACM Transactions on Graphics (SIGGRAPH 2015)}, 2015.

\bibitem[Socher et~al.(2013)Socher, Ganjoo, Manning, and
  Ng]{socher2013zero_csm}
Richard Socher, Milind Ganjoo, Christopher~D Manning, and Andrew Ng.
\newblock Zero-shot learning through cross-modal transfer.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  935--943, 2013.

\bibitem[Stallkamp et~al.(2012)Stallkamp, Schlipsing, Salmen, and
  Igel]{stallkamp2012manvscomp}
Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel.
\newblock Man vs. computer: Benchmarking machine learning algorithms for
  traffic sign recognition.
\newblock \emph{Neural networks}, 32:\penalty0 323--332, 2012.

\bibitem[Suzuki et~al.(2014)Suzuki, Sato, Oyama, and Kurihara]{Suzuki2014}
Masahiro Suzuki, Haruhiko Sato, Satoshi Oyama, and Masahito Kurihara.
\newblock{Transfer learning based on the observation probability of each
  attribute}.
\newblock \emph{2014 IEEE International Conference on Systems, Man, and
  Cybernetics (SMC)}, pages 3627--3631, October 2014.
\newblock \doi{10.1109/SMC.2014.6974493}.

\bibitem[Weston et~al.(2011)Weston, Bengio, and Usunier]{weston2011wsabie}
Jason Weston, Samy Bengio, and Nicolas Usunier.
\newblock Wsabie: Scaling up to large vocabulary image annotation.
\newblock In \emph{IJCAI}, volume~11, pages 2764--2770, 2011.

\end{thebibliography}


\end{document}
