\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage[numbers]{natbib}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{color}
\usepackage[it,small]{caption}
\usepackage{subcaption}
\usepackage{dsfont}
\usepackage{enumitem}
\usepackage{authblk}

\newcommand{\ve}[1]{\mathbf{#1}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\rc}[1]{\textcolor{red}{#1}}
\newcommand{\argmax}{\operatorname{arg\,max}}
\newcommand{\argmin}{\operatorname{arg\,min}}
\newcommand{\todo}[1]{\textcolor{blue}{\textbf{#1}}}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\def\cvprPaperID{1033} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\cvprfinalcopy

\pagenumbering{gobble}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi

% Some illegal space-saving macros\parskip=3pt
  \abovedisplayskip 3.0pt plus2pt minus2pt%\belowdisplayskip\abovedisplayskip\renewcommand{\baselinestretch}{0.97}\newenvironment{packed_enum}{
\begin{enumerate}
  \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{enumerate}}\newenvironment{packed_item}{
\begin{itemize}
  \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}\newlength\savedwidth\newcommand\whline[1]{\noalign{\global\savedwidth\arrayrulewidth
								\global\arrayrulewidth #1} %
					   \hline
					   \noalign{\global\arrayrulewidth\savedwidth}}\renewcommand\multirowsetup{\centering}\linespread{0.95}\selectfont\newlength{\sectionReduceTop}\newlength{\sectionReduceBot}\newlength{\subsectionReduceTop}\newlength{\subsectionReduceBot}\newlength{\abstractReduceTop}\newlength{\abstractReduceBot}\newlength{\captionReduceTop}\newlength{\captionReduceBot}%\newlength{\nameReduceTop}\newlength{\subsubsectionReduceTop}\newlength{\subsubsectionReduceBot}\newlength{\horSkip}\newlength{\verSkip}\newlength{\figureHeight}\setlength{\figureHeight}{1.7in}%\newlength{\figureFraction}\setlength{\horSkip}{-.09in}\setlength{\verSkip}{-.1in}%\setlength{\figureFraction}{.195}%\setlength{\subsectionReduceTop}{-0.08in}\setlength{\subsectionReduceBot}{-0.05in}\setlength{\sectionReduceTop}{-0.08in}\setlength{\sectionReduceBot}{-0.10in}\setlength{\subsubsectionReduceTop}{-0.06in}\setlength{\subsubsectionReduceBot}{-0.05in}%%%\setlength{\figureHeight}{1.5in}\setlength{\abstractReduceTop}{-0.15in}\setlength{\abstractReduceBot}{-0.05in}%%%\setlength{\nameReduceTop}{-0.05in}\setlength{\captionReduceTop}{-0.09in}\setlength{\captionReduceBot}{-0.2in}
\begin{document}

\title{ Supplementary for\\  Structural-RNN: Deep Learning on Spatio-Temporal Graphs}
\author[1,2]{Ashesh Jain}
\author[2]{Amir R. Zamir}
\author[2]{Silvio Savarese}
\author[3]{Ashutosh Saxena}
\affil[ ]{Cornell University$^1$, Stanford University$^2$, Brain Of Things Inc.$^3$}
\affil[ ]{{ashesh@cs.cornell.edu, \{zamir,ssilvio,asaxena\}@cs.stanford.edu}}

\iffalse
\title{ Supplementary for\\ Structural-RNN: Deep Learning on Spatio-Temporal Graphs}
\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}
\fi

\maketitle






\setcounter{section}{3}
\section{Experiments}
\subsection{Human motion modeling and forecasting}
\noindent \textbf{User study.}  We randomly sampled three seed motions from each of the four activities (walking, eating, smoking, and discussion), giving a total of 12 seed motions. We forecasted human motion from the seeds using S-RNN, LSTM-3LR and ERD, resulting in total of 36 forecasted motions -- equally divided across algorithms and activities. We asked five users to rate the forecasted motions on a Likert scale of 1 -- 3, where a score of 1 is bad, 2 is neutral, and 3 is good. The users were instructed to rate  based on how human like the forecasted motion appeared. In order to calibrate, the users were first shown many examples of ground truth motion capture videos.

Figure~\ref{fig:user_study} shows the number of examples that obtained bad, neutral, and good scores for each algorithm. Majority of the motions generated by S-RNN were of high-quality and resembled human like motion. On the other hand, LSTM-3LR generated reasonable motions most of the times, however they were not as good as the ground truth. Finally, the motions forecasted by ERD were not human like for most of the aperiodic activities (eating, smoking, and discussion). On the walking activity, all algorithms were competitive and users mostly gave a score of 3 (good). Hence, through the user study we validate that S-RNN generates most realistic human motions majority of the times. Look at the supplementary video for more details. 

\noindent \textbf{Training S-RNN for motion forecasting.} We closely follow the training procedure by Fragkiadaki et al.~\citep{Fragkiadaki15}. We cross-validate over the hyperparameters on the validation set and set them to the following values:
\begin{itemize}
\item Back propagation through 100 time steps.
\item Mini-batch size of 100 sequences.
\item We use SGD and start with the step-size of $10^{-3}$. We decay the step-size by 0.1 when the training error plateaus. 
\item We clip the L2-norm of gradient to 25.0, and clip each dimension to [-5.0, 5.0]
\item We gradually add Gaussian noise to the training data following the schedule: at iterations \{250, 500, 1000, 1300, 2000, 2500, 3300\} we add noise with standard deviation \{0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 0.7\}. As also noted by Fragkiadaki et al.~\citep{Fragkiadaki15}, adding noise is very important during training in order to forecast motions that lie on the manifold of human-like motions. 
\end{itemize}

Figure~\ref{fig:loss} examines the test and train error with iterations. Both S-RNN and ERD converge to similar training error, however S-RNN generalizes better with a smaller test error for next step prediction. The number of parameters in S-RNN are marginally more than ERD. S-RNN have more LSTMs than ERD, but each LSTM in S-RNN is half the size of the LSTMs used in ERD. For ERD we used the best set of parameters described in~\citep{Fragkiadaki15}. There, the authors cross-validated over model parameters. In the plot, the  jump in error around iteration 1500 corresponds to the decay in step size. Due to addition of noise the test error of S-RNN exhibits a small positive slope, but it always stays below ERD.

\begin{figure}[t]
\centering
%\vspace{\sectionReduceTop}
\includegraphics[width=.8\linewidth]{../Figs/user_study.pdf}
\caption{\textbf{User study} with five users. Each user was shown 36 forecasted motions equally divided across four activities (walking, eating, smoking, discussion) and three algorithms (S-RNN, ERD, LSTM-3LR). The plot shows the number of bad, neutral, and good motions forecasted by each algorithm.}
\label{fig:user_study}
\end{figure}


\begin{figure}[t]
\centering
\includegraphics[width=0.8\linewidth]{../Figs/lossPlot.pdf}
\caption{ \textbf{Train and test error}. S-RNN generalizes better than ERD with a smaller test error. }
%Structural nature of our architecture allows us to exchange nodeRNNs between S-RNN architectures trained on different motion styles. 
\label{fig:loss}
\end{figure}



\setcounter{subsection}{3}

\begin{table*}[t]
\centering
\caption{\footnotesize{\textbf{Maneuver Anticipation on 1100 miles of real-world driving data}. S-RNN is derived from the st-graph shown in Figure~\ref{fig:stgraphexp}\rc{c}. Jain et al.~\cite{Jain15} use the same st-graph but models it in a probabilistic frame with AIO-HMM. The table shows average \textit{precision}, \textit{recall} and \textit{time-to-maneuver}. Time-to-maneuver is the interval between the time of algorithm's prediction and the start of the maneuver. Algorithms are compared on the features from~\cite{Jain15}. }}
\vspace{1\sectionReduceBot}
\resizebox{0.9\linewidth}{!}{
\centering
\begin{tabular}{cr|ccc|ccc|ccc}
%\hline
&  &\multicolumn{3}{c}{Turns}&\multicolumn{3}{|c}{Lane change}&\multicolumn{3}{|c}{All maneuvers}\\
\cline{1-11}
\multicolumn{2}{c|}{\multirow{2}{*}{Method}} & \multirow{2}{*}{$Pr$ (\%)}  & \multirow{2}{*}{$Re$ (\%)} & Time-to-  & \multirow{2}{*}{$Pr$ (\%)} & \multirow{2}{*}{$Re$ (\%)} & Time-to-  & \multirow{2}{*}{$Pr$ (\%)} & \multirow{2}{*}{$Re$ (\%)}  & Time-to- \\ 
& & & &  maneuver (s) &  & &  maneuver (s) &  & & maneuver (s)\\\hline
&SVM 		&	64.7 &	47.2 &	2.40 	&	73.7 &	57.8 &	2.40		&	43.7 &	37.7 & 1.20\\
&AIO-HMM	~\cite{Jain15}	 		&	{80.8}	&		75.2	&	4.16 &	{83.8}	&	79.2	&	3.80 		&	{77.4}	&	{71.2}	&	3.53 \\
 & S-RNN w/o edgeRNN  & 75.2 & 75.3  & 3.68 & 85.4  & \textbf{86.0} & 3.53 & 78.0 & 71.1 & 3.15 \\
&(Ours) S-RNN  & \textbf{81.2}  & \textbf{78.6} & 3.94 & \textbf{92.7} & 84.4  & 3.46 & \textbf{82.2}  & \textbf{75.9} & 3.75 \\\hline
%\textit{Methods}&S-RNN with EL & 88.2 $\pm$ 1.4 & \textbf{86.0} $\pm$ 0.7 & 3.42 & \textbf{83.8} $\pm$ 2.1 & \textbf{79.9} $\pm$ 3.5 & 3.78 & \textbf{84.5} $\pm$ 1.0 & \textbf{77.1} $\pm$ 1.3 & 3.58\\
%\hline
\end{tabular}
}
\vspace{2\sectionReduceTop}
\label{tab:maneuver}
\end{table*}

\subsection{Driving maneuver anticipation}

We now present S-RNN for another application which involves anticipating maneuvers several seconds before they happen. For example, anticipating a future lane change maneuver several seconds before the wheel touches the lane markings. This problem requires spatial and temporal reasoning of the driver, and the sensory observations from inside and outside of the car. Jain et al.~\cite{Jain15} represent this problem with the st-graph shown in Figure~\ref{fig:stgraphexp}\rc{c}. They model the st-graph as a probabilistic Bayesian network called AIO-HMM. The st-graph represents the interactions between the observations outside the vehicle (eg. the road features), the driver's maneuvers, and the observations inside the vehicle (eg. the driver's facial features). We model the same st-graph with S-RNN architecture using the node and edge features from Jain et al.~\cite{Jain15}.

The nodeRNN models the driver, and the two edgeRNNs model the interactions between the driver and the observations inside the vehicle, and the observations outside the vehicle. The driver node is labeled with the future maneuver and, the observation nodes do not carry any label. The output of the driver nodeRNN is softmax probabilities of the following five maneuvers: \{\textit{Left lane change, right lane change, left turn, right turn, straight driving}\}.  Our nodeRNN architecture is RNN(64)-softmax(5), and edgeRNN is LSTM(64).

\begin{figure}[t]
\centering
%\vspace{\sectionReduceTop}
\includegraphics[width=.9\linewidth]{../Figs/stgraph_examples_exp.pdf}
\caption{\footnotesize{\textbf{Diverse spatio-temporal tasks}. We apply S-RNN to the following three diverse spatio-temporal problems. (View in color)}}
\label{fig:stgraphexp}
\end{figure}

We train S-RNN on the features provided by Jain et al.~\cite{Jain15} on their 1100 miles of natural driving data set. The algorithms are evaluated on their precision and recall in anticipating maneuvers  under the following three prediction
settings: (i) Lane change: algorithms only anticipate lane changes. This setting is relevant for freeway driving; (ii) Turns: algorithms only anticipate turns; and (iii) All maneuvers: algorithms anticipate all five maneuvers. Table~\ref{tab:maneuver} shows the performance of different algorithms on this task.  S-RNN performs better than the previous state-of-the-art AIO-HMM~\cite{Jain15} in every setting.  It improves the precision  by 5\% and recall by 4\% with predicting all five maneuvers. Both AIO-HMM and S-RNN model the same st-graph but using different techniques. The table also shows that the performance decreases if we remove edgeRNNs and simply feed the concatenation of edge features into the nodeRNN. This emphasizes importance of the edgeRNNs,  and the need for separately modeling different kinds of edge interactions. 



\iffalse
\begin{table*}[t]
\centering
\caption{\footnotesize{\textbf{Detection and anticipation results on
CAD-120~\cite{Koppula13b}}. S-RNN architecture derived from st-graph in
Figure~\ref{fig:stgraphexp}b outperforms Koppula et al.~\cite{Koppula13b} which
models the st-graph in a probabilistic framework. S-RNN in multi-task setting
(joint detection and anticipation) further improves the performance. \todo{This
will go to supplementary.}}}
%\resizebox{1\linewidth}{!}{
	\centering
	\begin{tabular}{r|cccc|cccc}
	Methods&\multicolumn{4}{c|}{Detection}&
	\multicolumn{4}{c}{Anticipation}\\\hline
	 & \multicolumn{2}{c}{Sub-activity} & \multicolumn{2}{c|}{Object
	 affordance} & \multicolumn{2}{c}{Sub-activity} &
	 \multicolumn{2}{c}{Object affordance} \\
	 & Micro P/R  & F1-score & Micro P/R  & F1-score & Micro P/R  & F1-score
	 & Micro P/R  & F1-score\\\hline 
	 Koppula et
	 al.~\cite{Koppula13,Koppula13b}&86.0&80.4&91.8&81.5&47.7&37.9&66.1&36.7\\
	 S-RNN w/o edgeRNN &83.5&82.2&90.1&82.1&70.5&64.8&77.0&72.4\\
	 S-RNN &85.6&83.2&92.4&88.7&69.1&62.3&81.9&80.7\\
	 S-RNN (multi-task) &85.7&82.4&92.8&91.1&72.3&65.7&81.5&80.9\\
	 %&activity (\%)&Affordance (\%)&activity (\%)&Affordance (\%)\\\hline
	 %Koppula et
	 %al.~\cite{Koppula13,Koppula13b}&\textbf{86.0}&91.8&47.7&66.1\\
	 %\todo{w/o edgeRNN} &&&&\\
	 %S-RNN &85.5&92.4&68.9&\textbf{81.8}\\
	 %S-RNN (multi-task)&85.7&\textbf{92.8}&\textbf{72.3}&81.5\\\hline
	 \end{tabular}
	 %}
	 \label{tab:cad120-supp}

	 \end{table*}

\fi{\small
\bibliographystyle{ieee}
\bibliography{../COMTools/shortstrings,../COMTools/references}
}


\end{document}


