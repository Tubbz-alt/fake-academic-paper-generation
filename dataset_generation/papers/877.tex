%% bare_jrnl.tex
%% V1.4a
%% 2014/09/17
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8a or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_conf_compsoc.tex,
%%                    bare_jrnl_compsoc.tex, bare_jrnl_transmag.tex
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices and paper sizes can       ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal, onecolumn]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/tex-archive/info/epslatex/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Do not use the stfloats baselinefloat ability as IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/dblfloatfix/




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/endfloat/
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/url/
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
%\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{cite}
\usepackage{subfigure}
\usepackage[fleqn]{amsmath}
\usepackage{url}
\usepackage{color}
%\usepackage{natbib}


\newcommand{\disc}[1]{$\Rightarrow$\textbf{TO-FIX: \textit{#1}}$\Leftarrow$}

\begin{document}

\title{Deep Recurrent Neural Networks for mapping winter vegetation quality coverage via multi-temporal SAR Sentinel-1}


\author{Dinh Ho Tong Minh,
		Dino Ienco,
        Raffaele Gaetano,
        Nathalie Lalande,
        Emile Ndikumana,
        Faycal Osman,
        and Pierre Maurel
        
\thanks{ D. Ho Tong Minh, E. Ndikumana, F. Osman and P. Maurel are with UMR-TETIS laboratory, IRSTEA, Montpellier, France (email: dinh.ho-tong-minh; emile.ndikumana; faycal.osman; pierre.maurel@irstea.fr)}
\thanks{D. Ienco is with UMR-TETIS laboratory, IRSTEA, University of Montpellier, Montpellier, France and with LIRMM laboratory, Montpellier, France (email: dino.ienco@irstea.fr).}%
\thanks{R. Gaetano is with UMR-TETIS laboratory, CIRAD, University of Montpellier, Montpellier, France (email: raffaele.gaetano@cirad.fr).}% <-this % stops a space
\thanks{N. Lalande is with Envylis Cie, Villeneuve-les-Maguelone, France (email: nathalie.lalande@envilys.com).}}


\maketitle


\begin{abstract}
Mapping winter vegetation quality coverage is a challenge problem of remote sensing. This is due to the cloud coverage in winter period, leading to use radar  rather than optical images. The objective of this paper is to provide a better understanding of the capabilities of radar Sentinel-1 and deep learning concerning about  mapping winter vegetation quality coverage. The analysis presented in this paper is carried out on multi-temporal Sentinel-1 data over the site of La Rochelle, France, during the campaign in December 2016. This dataset were processed in order to produce an intensity radar data stack from October 2016 to February 2017. Two deep Recurrent Neural Network (RNN) based classifier methods were employed. We found that the results of RNNs clearly outperformed the classical machine learning approaches (Support Vector Machine and Random Forest). This study confirms that the  time series radar Sentinel-1 and RNNs could be exploited for winter vegetation quality cover mapping.
\end{abstract}

\begin{IEEEkeywords}
SAR, Sentinel-1, multi-temporal, vegetation quality, LSTM, GRU, Recurrent Neural network
\end{IEEEkeywords}


\section{Introduction\label{sec:Introduction}}

\IEEEPARstart{ a}{n} excess of nitrates in drinking water is harmful to human health. They come mainly from surplus agricultural fertilizers leached when it rains on bare soil. Since 2011, the European Nitrates Directive has obliged to reduce diffuse pollution in the perimeters of drinking water collection thanks to a winter vegetation covering of the soils  \cite{Kallis01}. It makes it possible to absorb a part of the surpluses, and above all to limit the leaching of the soil. When spring and summer crops release the soil from any vegetation cover in autumn and early winter, farmers have to establish intermediate crops (cereals, grasslands, fallow land, etc.) and receive aid in return \cite{Buckley13}. The mapping of the winter cover in vulnerable areas helps the services of the State to control the declarations of the farmers but also the local actors to set up innovative action plans to reduce diffuse pollution. Remote sensing satellite imagery is a valuable aid in understanding the level of vegetation cover in winter \cite{Hively09}. Moreover, the high cloud cover levels during the winter season and the dynamic of vegetation justify the use of radar images time series rather than optical ones. Recently,  the Sentinel-1 satellite is operating day and night and performing C-band synthetic aperture radar (SAR) imaging, enabling it to acquire imagery regardless of the weather \cite{TORRES20129}. The Sentinel-1 is systematically acquired data at global scale  in Terrain Observation with Progressive Scan (TOPS) mode with a revisit period of 12 days. This offers a unique source of information to map our vegetation quality cover in winter season.   

In the literature of remote sensing for land cover mapping, most of works are based on standard machine learning approach (i.e., Support vector machine (SVM), Random Forest (RF)). These standard approaches however ignore any temporal dependency of a time series data. Recent advances in machine learning, deep learning approaches, have demonstrated their powerful value. In particular, Recurrent Neural Network (RNN) \cite{BengioCV13} is a family of deep learning methods, which offers a certain model (e.g., Long Short Term Memory (LSTM) \cite{HochreiterS96}) to exploit temporal dependency among data and, in our case, the temporal dependency available in a time series SAR Sentinel-1 data.  

In this paper, we aim to provide a better understanding of the capabilities of SAR Sentinel-1 and deep RNN concerning about  mapping winter vegetation quality coverage. 


\section{Study area \label{sec:area}}

\subsection{La Rochelle site\label{sec:IRF-1} }

The study area (horizontal: 40 km, vertical: 60 km) is located in the department of Charentes-Maritimes, in the west of France, a very vulnerable zone to diffuse pollution. The Re-Sources actions program (2016-2020) aims to improve water quality while maintaining an efficient agricultural both in environmental and economic terms. It is based on a close partnership between the State services, the Water Agency, the Region and local actors (municipalities, professional agricultural organizations and farmers). 

\subsection{Ground data\label{sec:IRF-2} }
A field survey was carried out on 194 plots in December 2016 by the local actors with the help of an environmental engineering company. The following information was collected on each plot: land cover, an estimate of the quality of the vegetative development (5 classes: bare soil, very low, low, average, high), the date of observation and a photograph. Figure \ref{fig:StudyArea} shows the study area and ground  measured data.


\begin{figure}[ht!]
\centering
\includegraphics[scale=0.25]{CIPAN_LaRochelle.png}
\caption{The La Rochelle study site  (horizontal: 40 km, vertical: 60 km) is located in the department of Charentes-Maritimes, in the west of France. The background is the average VV intensity. Color polygons represent field locations. \label{fig:StudyArea}}
\end{figure}


\section{SAR data and processing \label{sec:SAR-data}}

\subsection{SAR data }




The Sentinel-1 single look complex  dataset includes 13 acquisitions in TOPS mode from 07 October 2016 to 28 February 2017, with a temporal baseline of 12 days. This is dual-polarization (VV+VH) data, resulting in 26 images. Figure \ref{fig:sarSignature} summarizes the temporal profiles of the five winter vegetation quality classes per polarization. Each time series has 13 points (one for each acquisition in TOPS mode) ordered considering the temporal dimension. 


\begin{figure}[!ht]
\centering
\includegraphics[scale=0.50]{SAR_signature.png}
\caption{The temporal profiles of the five different classes w.r.t. the polarization VV (top) and the polarization VH (bottom). \label{fig:sarSignature}}
\end{figure}


\subsection{Pre-procesing}

First, a master image was chosen and all images  were coregistered with taking into account of TOPS mode to the  master image \cite{6130599}.  Five-look (5 range looks) intensity  images were generated and radiometrically calibrated for range spreading loss, antenna gain, normalized reference area and the calibration constant that depends on the parameters Sentinel-1 SAR header.  

\subsection{Temporal filtering}

Reliable estimates of the intensity from a distributed target require that the estimated number of looks  is sufficiently large. Speckle filtering is often used to increase the ENL with loss of spatial resolution \cite{Quegan01}. In properly coregistered multitemporal datasets it is possible to employ the technique of temporal filtering, which in principle increases radiometric resolution without degrading spatial resolution.  The temporally filtered images usually are markedly diminished speckle with little or no reduction in spatial resolution. In this paper, We improve the time series SAR Sentinel-1 dataset by  exploiting a temporal filtering developed by \cite{Quegan01} to reduce noise while retaining as much as possible the fine structures present in the images. 

\subsection{Geocoding}

After pre-processing and filtering all the processed images
were in the imaging geometries of the master image. In order to create a unified dataset all image data had to be orthorectified into map coordinates. This was accomplished by creating a simulated SAR image from a SRTM DEM 30m, and using the simulated SAR image to coregister the two image sets. The pixel size of the orthorectified image data is 10 m. After geocoding, all intensity images are transformed to logarithm dB scale, normalized to values between 0-255 (8 bits) and inputted into classifiers. The SAR Sentinel-1 data  were processed by the IRSTEA TomoSAR platform, which offers SAR, interferometry and tomography processing  \cite{DinhLPS2016}. 


\section{Recurrent Neural Network\label{sec:RNN}}
Recurrent Neural Networks are well established machine learning techniques that demonstrate their quality in different domains such as speech recognition, signal processing, and natural language processing \cite{SomaMSFN15,LinzenDG16}. Unlike standard feed forward networks (i.e., Convolutional Neural Networks (CNNs)), RNNs explicitly manage temporal data dependencies since the output of the neuron at time t-1 is used, together with the next input, to feed the neuron itself at time t. A sketch of a typical RNN neuron is depicted in Fig. \ref{fig:RNN}. 

Among the different RNN models, Long-Short Term Memory~\cite{HochreiterS96} (LSTM) and Gated Recurrent Unit~\cite{ChoMGBBSB14} (GRU) are the two most well known RNN units. The main difference between them is related to the number of parameters to learn. Considering the same size of the hidden state, the \textit{LSTM} models has more parameters w.r.t. the \textit{GRU} unit to set.

In the following we briefly describe the two RNN units.
For each of them we supply and discuss the equations that describe its inner behavior. The $\odot$ symbol indicates an element-wise multiplication while $\sigma$ and $\tanh$ represent Sigmoid and Hyperbolic Tangent function, respectively.
The input of a RNN unit is a sequence of variables ($x_1$, ..., $x_N$) where a generic element $x_t$ is a feature vector and $t$ refers to the corresponding timestamp.

\begin{figure}[ht!]
\centering
\includegraphics[height=0.1\textwidth,width=0.30
\textwidth]{RNN.png}
\caption{RNN Unit (on the left) and unfolded structure (on the right). \label{fig:RNN}}
\end{figure}
\subsection{Long-Short Term Memory\label{sec:IRF} }


%The most well-known type of RNN is the Long-Short Term Memory (LSTM)~\cite{HochreiterS96} model. %There are many variants of LSTM network~\cite{GreffSKSS15} but here we refer to the architecture proposed in~\cite{GersSC00}.
The \textit{LSTM} model was mainly introduced with the purpose to learn long term dependencies~\cite{HochreiterS96}, since previous RNN models failed in this task due to the problem of vanishing and exploding gradients. The equations (1), (2), (3), (4), (5) and (6) formally describes the LSTM neuron. 
The LSTM unit is composed of two cell states, the memory $C_t$ and the hidden state $h_t$, and three different gates:  input ($i_t$), forget ($f_t$) and output ($o_t$) gate that are employed to control the flow of information. All the three gates combine the current input $x_t$ with the hidden state $h_{t-1}$ coming from the previous timestamp.
The gates have also two important functions: i) they regulate how much information have to be forgotten/remembered during the process; ii) they deal with the problem of vanishing/exploding gradients. We can observe that the gates are implemented by a sigmoid. This function returns values between 0 and 1.
The LSTM unit also uses a temporary cell state $y_t$ that rescales the current input. 
This temporary cell is implemented by an hyperbolic tangent function that returns values between -1 and 1. Both sigmoid and hyperbolic tangent are applied element-wise.

$i_t$ regulates how much of the current information needs to be maintained ($i_t \odot y_t$) while $f_t$ indicates how much of the previous memory needs to be retained at the current step ($f_t \odot c_{t-1}$). Finally, $o_t$ impacts on the new hidden state $h_t$ deciding how much information of the current memory will be outputted to the next step. The different $W_{**}$ matrices and bias coefficients $b_{*}$ are the parameters learned during the training of the model. Both, the memory $C_t$ and the hidden state $h_t$ are forwarded to the next time step. 

\begin{align*}
i_{t} &= \sigma(W_{ix} x_{t} + W_{ih} h_{t-1} + b_i  ) \tag{1}\\
f_{t} &= \sigma(W_{fx} x_{t} + W_{fh} h_{t-1} + b_f  ) \tag{2}\\
y_{t} &= \tanh(W_{yx} x_{t} + W_{yh} h_{t-1} + b_y  )  \tag{3}\\
c_t &= i_t \odot y_t + f_t \odot c_{t-1}  \tag{4}\\
o_{t} &= \sigma(W_{ox} x_{t} + W_{oh} h_{t-1} + b_o  ) \tag{5}\\
h_t &= o_t \odot \tanh(c_t) \tag{6}
\end{align*}

\subsection{Gated Recurrent Unit}

In~\cite{ChoMGBBSB14} the authors design a new RNN unit with the goal to be much simpler to compute and implement of the \textit{LSTM} model.
The equations (7), (8) and (9) formally describes the \textit{GRU} neuron. 
This unit follows the general philosophy of the \textit{LSTM} model implementing gates and cell states but, conversely to the \textit{LSTM} model, the \textit{GRU} unit has only two gates: update ($z_t$) and reset ($r_t$) gates and one cell state: the hidden state ($h_t$). 
Also in this case, the two gates combine the current input ($x_t$) with the information coming from the previous timestamps ($h_{t-1}$).
The update gate effectively controls the trade off between how much information from the previous hidden state will carry over to the current hidden state and how much information of the current timestamps need to be kept. This acts similarly to the memory cell in the \textit{LSTM}
unit supporting the RNN to remember longterm information.

On the other hand, the reset gate monitors how much information of the previous timestamps needs to be integrated with the current information. 
As each hidden unit has separate reset and update gates, each hidden unit will learn to capture dependencies over different time scales. Those
units that learn to capture short-term dependencies will tend to have reset gates that are frequently active, but those that capture longer-term dependencies will have update gates that are mostly active~\cite{ChoMGBBSB14}.

\begin{align*}
z_{t} &= \sigma(W_{zx} x_{t} + W_{zh} h_{t-1} + b_z  ) \tag{7} \\
r_{t} &= \sigma(W_{rx} x_{t} + W_{rh} h_{t-1} + b_r  ) \tag{8} \\
h_{t} &= z_t \odot h_{t-1} +  \tag{9} \\
& (1-z_t) \odot \tanh( W_{hx} x_{t} + W_{hr} (r_t \odot h_{t-1})+b_h  )
\end{align*}



\subsection{RNN-Based Time Series Classification\label{sec:IRF-3} }
To perform the classification task, for each of the RNN unit, we build a deep architecture stacking together five units. The use of multiple units, similarly to what is commonly done for CNN networks combining together several convolutional layers~\cite{BengioCV13}, will allow to extract high-level non-linear temporal dependencies available in the remote sensing time series.
%As previously underlined, this is done for both \textit{LSTM} and \textit{GRU} 


The RNN model learns a new representation of the input sequences but it does not make any prediction by itself. To this end, a SoftMax layer~\cite{Graves13} is stacked on top of the last recurrent unit to perform the final multi-class prediction. %}
The SoftMax layer has as many neurons as the number of the classes to predict. We choose the SoftMax instead of the Sigmoid function because the value of the SoftMax layer can be seen as a probability distribution over the classes that sum to 1 while each of the Sigmoid neurons can output a value between 0 and 1. This is due to the fact that, for the SoftMax neuron, the values are normalized per layer while no normalization is performed in the case of Sigmoid layer. This is why, in our context (multi-class prediction), we prefer the SoftMax instead of Sigmoid layer since we know that our samples exclusively belong to a single class.
From an architectural point of view, the connection between the RNN model and the SoftMax layer is realized fully connecting the last hidden state vector of the recurrent architecture with the SoftMax neurons. This schema is instantiated for both \textit{LSTM} and \textit{GRU} units coming up with two different classifiers: an \textit{LSTM}-based and a \textit{GRU}-based classification schema.


%\subsubsection{One pipline : intensity }


\section{Experimental Results\label{sec:Results}}


\subsection{Experimental Settings }
We compare the LSTM and the GRU-Based Time Series models with standard machine learning approaches,  Random Forest (RF) and Support Vector Machine (SVM), usually employed in the remote sensing field \cite{Flamary15}. 

For the \textit{RF} model, we set the number of generated trees equals to 400 and we allow a maximum tree depth of 25. For the \textit{SVM} model we use RBF kernel with default gamma and complexity parameter equals to 10$^6$. For Random Forest we used the python implementation supplied by the Scikit-learn library  \cite{scikit-learn} while for \textit{SVM} we use the LibSVM implementation \cite{CC01a}. 

Considering the RNN-Based classifiers, we set the number of hidden dimensions equals to 512. An initial learning rate of $5 \times 10^{-4}$ and a decay of $5 \times 10^{-5}$ is employed. We have implemented the model via the \textit{Keras} python library  \cite{chollet2015} with \textit{Theano} as back end. 
To train the model we have used the \textit{Rmsprop} strategy that is a variant of the Stochastic Gradient Descent \cite{DauphinVCB15}. The loss function being optimized is the categorical cross-entropy that is the standard loss function employed for multi-class classification tasks ~\cite{Zhang16}. The model is trained for 350 epochs with a batch size equals to 64. To validate the different methods, we perform a 5-fold cross validation on the dataset as shown in Tab. \ref{Table:occSolDistrib}.  In order to assess classification performances, we use not only the Global Accuracy and Kappa measures but also average and per-class F-Measure. Experiments are carried out on a workstation Intel(R) Xeon(R) CPU E5-2667 v4@3.20Ghz with 256 GB of RAM and GPU TITAN X.



\subsection{Results and Discussions}
Table ~\ref{Table:VHVV} summarizes the results of the different classification approaches on the SAR Sentinel 1 Time Series data considering the winter vegetation quality coverage task. 
At first glance, we can observe a significant performance gain between RNN-based classification approaches and classical Machine Learning methods (\textit{RF} and \textit{SVM}). The performance gain involves all the different evaluation metrics. In average, both \textit{LSTM} and \textit{GRU} improve the classification performances of more than 7 point of accuracy/F-Measure. Between the two RNNs models, the \textit{GRU}-based method obtains slightly better results than the \textit{LSTM} one.

Figure \ref{fig:perClassF} supplies an F-Measure per class comparison. Here we can have a more precise comprehension of the behavior of the different methods. We can see that the gain supplied by the RNN-based methods involve all the five classes resulting in equally good results on all the them. Conversely, we can note that both \textit{RF} and \textit{SVM} have different behaviors considering different classes. Both classifiers obtain the best performances on the \textit{High} class (4) and the lowest performances on the \textit{Low} class (2). 
This behavior can be explained considering the temporal profiles of both VV and VH presented in Fig.  \ref{fig:sarSignature}. The \textit{Strong} class (depicted with green lines in Fig.  \ref{fig:sarSignature}) has a clear and distinct profile, this facilitate its detection without necessity to consider its temporal correlation. On the other hand, the \textit{Low} class (depicted with black lines in Fig. \ref{fig:sarSignature}) intersects the temporal profiles of all the other classes multiple times. This is probably why, both \textit{RF} and \textit{SVM} approaches are not capable to correctly detect this class since they ignore the temporal correlation of the data. 
On the other hand we can observe that the RNN-Based approaches well discriminate among all the classes since they extract and summarize the important signal portions that support the discriminative task among the different winter quality coverage classes.

Figure \ref{fig:ContiTables} shows the Confusion Matrices for each method. We can observe that the sum of the counts of a matrix is equal to the number of pixels we have in our dataset. To produce the Confusion Matrices, for each classifier, we have aggregated the predictions over the 5-folds. 
We can observe that an heavy misclassification rate happens between the \textit{Low} (2) and \textit{Bare soil} (5) classes. This is true for all the different classifiers. However, for the RNN-based approaches this misclassification error is not so high. Conversely, in the case of both \textit{RF} and \textit{SVM} this misclassification behavior is critical. Another problem we can observe is related to the discrimination between the \textit{Low} and the \textit{Very Low} classes. Also in this case, the standard machine learning approaches suffer and they incur in an high misclassification rate.

The joint optimization of non linear input transformations along with the classifier, proper to all deep learning approaches, provides a valuable strategy to discriminate among the different winter vegetation quality classes. Furthermore, as expected, the ability of RNNs to deal with the temporal correlations, characterizing the SAR Sentinel-1 data, results in a gain of performance on all classes with particular emphasis on such classes that share similar temporal behaviors. All these results indicate that the RNN models (both \textit{LSTM}-based and \textit{GRU}-based) are well suited to detect and exploit temporal dependencies as opposed to common classification approaches that do not explicitly leverage temporal correlations. 


\begin{table}[tp]
\centering
\begin{tabular}{|l|c|c|} \hline
\textbf{ID} & \textbf{Vegetation quality class} & {Number of pixels} \\ \hline
(1) & Very low & 12\,589 \\ \hline
(2) & Low & 15\,000 \\ \hline
(3) & Average & 15\,000 \\ \hline
(4) & High & 15\,000 \\ \hline
(5) & Bare soil & 15\,000 \\ \hline
\end{tabular}
\caption{Distribution of the number of pixel per class. \label{Table:occSolDistrib}}

\end{table}



\begin{table}[tp]
\centering
\begin{tabular}{|c|c|c|c|}
\hline 
Classifier & F-measure & Accuracy & Kappa\tabularnewline
\hline 
\hline 
\textit{RF} & 91.77\% & 91.79\% & 0.897 \\
\hline 
\textit{SVM} & 91.22\% & 91.25\% & 0.890 \\
\hline 
\textit{LSTM} & 98.83\% & 98.83\% & 0.985\\
\hline 
\textit{GRU} & \textbf{99.05}\% & \textbf{99.05}\% & \textbf{0.988} \\
\hline 
\end{tabular}
\caption{5-Fold cross valid on the time series SAR Sentinel-1 data. \label{Table:VHVV}
}
\end{table}


\begin{figure}[!ht]
\centering
\includegraphics[scale=0.7]{vv_vh.png}
\caption{Per Class F-Measure of the different approaches. 
\label{fig:perClassF}}
\end{figure}


\begin{figure}[!ht]
\centering
\subfigure[\label{rf}]{\includegraphics[scale=0.5]{rf_vv_vh.png}}
\subfigure[\label{svm}]{\includegraphics[scale=0.5]{svm_vv_vh.png}}
\subfigure[\label{lstm}]{\includegraphics[scale=0.5]{lstm_vv_vh.png}}
\subfigure[\label{gru}]{\includegraphics[scale=0.5]{gru_vv_vh.png}}
\caption{ Confusion Matrices on the SAR SENTINEL 1 Time Series Data of the different approaches: a) \textit{RF}, b) \textit{SVM}, c) \textit{LSTM} and d) \textit{GRU}. \label{fig:ContiTables} }
\end{figure}





\section{Conclusions\label{sec:conclusion}}


In this letter, we demonstrate the capabilities of SAR Sentinel-1 and deep recurrent neural network concerning the mapping winter vegetation quality coverage. We assess the benefit of using the \textit{LSTM} and the \textit{GRU} RNNs to perform winter vegetation quality coverage prediction on a time series of SAR Sentinel-1 images. 
The performances of RNNs clearly surpass the results of classical machine learning approaches commonly used in the remote sensing. The experiments highlight the appropriateness to use deep learning models (RNNs) that explicitly consider the temporal correlation of the SAR data in order to discriminate between classes that exhibit complex temporal behaviors. 




\section{Acknowledgments}

The authors wish to thank ESA and CNES/TOSCA for supporting this work.
They also thank the Syndicat des Eaux de la Charente Maritime, the water authority of the City of La Rochelle who carried out field surveys, and the Re-Sources program.
The authors acknowledge the National Research Agency in the framework of the program "Investissements d'Avenir" for the GEOSUD project (ANR-10-EQPX-20).


\bibliographystyle{IEEEtran}
\bibliography{paper.bib}

\end{document}
