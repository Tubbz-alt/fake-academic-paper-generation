\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bbm}

\usepackage{bm}% bold math
\usepackage{ifthen}
\usepackage{rays_defs_18}


\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{fact}{Fact}
\newtheorem{condition}{Condition}
\newtheorem{proposition}{Proposition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{problem}{Problem}
\newtheorem{remark}{Remark}
\newtheorem{algorithm}{Algorithm}

\usepackage{hyperref}
\hypersetup{
    bookmarks=true,         % show bookmarks bar?
    unicode=false,          % non-Latin characters in Acrobat‚Äôs bookmarks
    pdftoolbar=true,        % show Acrobat‚Äôs toolbar?
    pdfmenubar=true,        % show Acrobat‚Äôs menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdfauthor={Reinhard Heckel},     % author
    pdfsubject={Subject},   % subject of the document
    pdfcreator={Reinhard Heckel},   % creator of the document
    pdfproducer={Producer}, %!TEX encoding = UTF-8 Unicode: producer of the document
    pdfnewwindow=true,      % links in new window
    colorlinks=true,       % false: boxed links; true: colored links
    linkcolor=red,          % color of internal links
    citecolor=green,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}

\newcommand\sure{\mathrm{sure}}
\newcommand\SUREest{\hat \mU_{\mathrm{SURE}}}
\newcommand\PCAest{\hat \mU_{\mathrm{PCA}}}




\title{SURE Denoising}
\author{Chris, Ali, Reinhard, Richard Baraniuk}
\date{April 2018}


\newcommand\img{\vx}
\newcommand\obs{\vy}
\newcommand\noise{\vw}


\begin{document}

\maketitle

\section{Introduction}
Denoising is a classical problem in image and signal processing.
The goal of denoising is to obtain a good estimate of an image $\img$ from a noisy observation
$\obs = \mH \img + \noise$, using prior knowledge of the signal $\img$, and the measurement operator $\mH$.
Here, $\noise$ is an unknown noise term, which is often assumed to be white Gaussian noise with variance $\sigma$, but can also model structured noise. 
Over the past decades, a large variety of increasingly complex image priors have been considered, ranging from simple subspace models, to sparse models, to deep learning based priors.
The best performing priors for denoising are typically learned from training data.
Once a prior is learned, prior based denoising typically solves an optimization problem in order to find a signal in the range of the prior as close as possible to the noisy observation.
Since that often makes the denoising step computationally expensive, recent works~\cite{zhang_beyond_2017,...} have proposed to learn a denoising directly from data, by treating the denoising problem as a discriminative learning problem, with the goal of separating the noise from the image, for example by training a deep network.

Both approaches rely on clean, noise-free data in order to train the prior or the denoiser. 
However, often we do not have access to noise-free data, for example cameras operating at low-light or other difficult conditions only produce noisy images. Even if we have noise-free images, we might have a large set of noisy ones as well, and would like to use that to refine our denoiser. 
Motivated by those applications, in this paper, we therefore consider the problem of learning to denoise from noisy data.
Towards this goal, we train a denoiser directly on noisy data by minimizing a loss function based on Stein’s unbiased risk estimator (SURE). 
Specifically, [DnCNN for denosing] + [LDAMP for compressive sensing and debluring]





\section{Monte Carlo Generalized Stein's Unbiased Risk Estimator}
\section{Compressed Sensing with LDAMP}
We compare LDAMP trained with SURE, using AMP's estimate of the variance, with LDAMP trained with the generalized SURE loss.
This corresponds to layer-by-layer and end-to-end loss.

Operate in a regime where we can never measure the same thing twice. If we could, there would be better ways to train LDAMP.

\section{Deblurring with DnCNN}
\newpage
\section{Old Work on Denoisers}
\subsection{ML Estimation with Known Subspace}
\textcolor{red}{Constrained maximum likelihood is a thing.}

$x=\Psi\alpha$, $y=x+w$, $w\sim N(0,\Sigma_w)$ 


\begin{align}
    \hat{x}=\Psi(\Psi^t\Sigma_w^{-1}\Psi)^{-1}\Psi^t\Sigma_w^{-1}y
\end{align}

With white noise this simplifies to 

\begin{align}
    \hat{x}=\Psi(\Psi^t\Psi)^{-1}\Psi^ty
\end{align}

\subsection{ Minimum Mean Square Estimation (MMSE) with Known Subspace}
$x=\Psi\alpha$, $y=x+w$, $w\sim N(0,\Sigma_w)$, and $\alpha\sim N(0,\Sigma_\alpha)$


The MMSE solution is given by the Wiener filter:

\begin{align}
    \hat{x}=\Psi \Sigma_\alpha\Psi^t (\Psi\Sigma_\alpha\Psi^t+\Sigma_w)^{-1} y
\end{align}

With white noise and i.i.d.~signal this simplifies to 

\begin{align}
    \hat{x}=\Psi \Psi^t (\Psi\Psi^t+\frac{\sigma_w^2}{\sigma_\alpha^2}\mathbf{I})^{-1} y
\end{align}


\textcolor{red}{RH: Are you sure that this is correct? If let $\sigma_w\to 0$, then $\Psi\Psi^t$ is not even invertible, if $\Psi$
spans a low-dimensional subspace. Also, shouldn't we simple regard $\alpha$ as fixed, and not coming from some distribution?}



\url{https://ac.els-cdn.com/0022247X86900636/1-s2.0-0022247X86900636-main.pdf?_tid=206a3b32-0c86-44be-a6d4-15fe01adc9fe&acdnat=1523395210_563958b533b9077f9deb5b4327c05aea} (Projection Filter, Wiener Filter, and Karhunen-Lo&e Subspaces in Digital Image Restoration).
Alternatively, can derive results for when we have training data. Should be approximately the same.

\subsection{MMSE Estimation of Subspace Data with SURE}
$x=\Psi\alpha$, $y=x+w$, $w\sim N(0,\sigma_w^2\mathbf{I})$, and $\alpha\sim N(0,\sigma_\alpha^2\mathbf{I})$

$\sigma_w^2$ is known, $\Psi$ and $\sigma_\alpha^2$ are not.

\begin{align}
    \text{SURE}(y_1,...y_L)&=\frac{1}{N}\sum_{l=1}^L\|y_l-\mathbf{B}y_l\|_2^2-L\sigma_w^2+\frac{2\sigma_w^2L}{N}\text{tr}(B),\nonumber \\
    &=\frac{1}{N}\|\mathbf{Y}^H-\mathbf{Y}^H\mathbf{B}^H\|_F^2-L\sigma_w^2+\frac{2\sigma_w^2L}{N}\text{tr}(B),
\end{align}
where $\mathbf{Y}=[y_1,y_2,...y_L]$ and $\|\cdot\|_F$ denotes the Frobenius norm.

Need to add PD constraint to $B$.



Minimizing the above quanitity involves trace minimization, which is well studied. See for instance \url{http://proceedings.mlr.press/v32/hsiehb14.pdf} and \url{http://proceedings.mlr.press/v22/dudik12/dudik12.pdf}.

\subsection{Wavelet Sparse Without Training Data (Sure Shrink)}
\subsection{Wavelet Sparse With Training Data (Sure Shrink)}
\subsection{Sparse wrt Unknown Dictionary}

$y_i^{(l)}$ is the $i$th element of the $l$th example

Signal model: $x$ is sparse wrt unknown orthonormal dictionary.

Denoiser structure: $D(y)=\text{ReLu}(\mathbf{W}y+b)$


\textcolor{red}{
RH: Why is that a sensible structure for the denoiser? I thought the idea is to assume a generative prior with unknown coefficients, for example $D(x)=\text{ReLu}(\mathbf{W}x+b)$. 
Then the goal would be to learn the weights $W$. 
But the denoiser, applied to a noisy observation $y=D(x^\ast)+w$, where $w$ is noise, would do the following:
First compute:
\[
\hat x = \arg \min_x \norm[2]{D(x) - y}
\]
And then let the estimate of the image be: $\hat y = D(\hat x)$.
}



\begin{align}
    \arg\min_{\mathbf{W},b} \frac{1}{N} \sum_{l=1}^{L}
    \left(
    \|y^{(l)}-\text{ReLu}(\mathbf{W}y^{(l)}+b)\|_2^2-L\sigma^2+2\sigma^2\sum_{i=1}^{N}\frac{\partial}{\partial y_i}\text{ReLu}(\mathbf{W}y^{(l)}+b)[i] \right)
\end{align}

\begin{align}
\frac{\partial}{\partial y_i}\text{ReLu}(\mathbf{W}y^{(l)}+b)[i]=
    \begin{cases}
        w_{ii}\text{ if }w_i^ty^{(l)}+b_i\geq0,\\
        0\text{ otherwise}
    \end{cases}
\end{align}

\begin{align}
\sum_{i=1}^{N}\frac{\partial}{\partial y_i}\text{ReLu}(\mathbf{W}y^{(l)}+b)[i]=
\text{diag}(\mathbf{W})^t\mathbbm{1}_{\mathbf{W}y^{(l)}+b\geq0},
\end{align}
where $\text{diag}(\mathbf{W})=[w_{11},w_{22},...w_{NN}]^t$ and  $\mathbbm{1}$ denotes the indicator function.

\begin{align}
    \arg\min_{\mathbf{W},b} \frac{1}{N} \sum_{l=1}^{L}\|y^{(l)}-\text{ReLu}(\mathbf{W}y^{(l)}+b)\|_2^2-L\sigma^2+\frac{2\sigma^2}{N}\sum_{l=1}^L\text{diag}(\mathbf{W})^t\mathbbm{1}_{\mathbf{W}y^{(l)}+b\geq0},\nonumber \\
    \arg\min_{\mathbf{W},b} \frac{1}{N} \sum_{l=1}^{L}\|y^{(l)}-\text{ReLu}(\mathbf{W}y^{(l)}+b)\|_2^2-L\sigma^2+\frac{2\sigma^2}{N}\text{diag}(\mathbf{W})^t\sum_{l=1}^L\mathbbm{1}_{\mathbf{W}y^{(l)}+b\geq0}, \nonumber \\
    \arg\min_{\mathbf{W},b} \frac{1}{N} \sum_{l=1}^{L}\|y^{(l)}-\text{ReLu}(\mathbf{W}y^{(l)}+b)\|_2^2-L\sigma^2+\frac{2\sigma^2L}{N}\text{diag}(\mathbf{W})^tp,
\end{align}
where $p$ is the fraction of the time that each activation is on. My regularization penalty is just a weighted trace (if we ignore the fact that $p$ is a function of $\mathbf{W}$ and $b$). 
\textcolor{red}{I think using larger batches would make this approximation more accurate.}
This also goes by the name weighted nuclear norm minimization (when $\mathbf{W}$ is PD).

(The weighted nuclear norm minimization image denoising algorithm may have just learned a linear subspace for the patches in an image.)


Loss function is non-differentiable, therefore we can minimize it by applying stochastic gradient descent to a differentiable surrogate or by using alternative algorithms for training, such as ADMM.


\textcolor{red}{The function is sub-differentiable, so we can simply compute a sub-gradients and apply gradient descent, no need to think about a surrogate.}



\section{Denoising and learning to denoise in linear subspaces}

Consider a signal $\vy^\ast$ that lies in a $d$-dimensional subspace, i.e.,  $\vy^\ast = \mU \vx^\ast$, where $\mU \in \reals^{m\times d}$ is unitary, and $d \ll m$.
Suppose we observe a noisy version of that signal 
$\tilde \vy = \mU \vx^\ast + \vn$, where $\vn \sim \mc N(0, \sigma^2/n \, \mI)$. 
Suppose we want to denoise $\vy$. Then an good denoiser is the least-square estimate
\[
\hat \vy = \mU \pinv{\mU} \vy.
\]
The least square estimate achieve the denoising rate
\[
\norm[2]{ \hat \vy - \vy^\ast}^2 \leq c \frac{d}{m}, 
\]
with high probability. 

\subsection{Learning to denoise}

\textcolor{red}{Can we express this as ML estimation with unknown subspace?}

To denoise we need to know the signal model $\mU$ or an estimate thereof. 
Suppose we have noisy examples:
\[
\tilde \vy_i = \mU \vx_i + \vn_i,
\]
where $\vx_i \sim \mc N(0, 1/d\, \mI)$, independent across $i$. 
This implies that $\tilde \vy_i \sim \mc N( 0, \mSi )$ with 
\[
\mSi = 1/d \, \mU \transp{\mU} + \sigma^2 / m \, \mI.
\]
Thus we can estimate $\mU$ by first computing the sample co-variance matrix
\[
\hat \mSi = \frac{1}{n} \sum_{i=1}^n \vx_i \vx_i^T,
\]
and then computing the leading singular vectors of $\hat \mSi - \sigma^2 / m \mI$, which yields an estimate of the signal subspace $\mU$, which we denote by $\PCAest$.

\subsection{Stein's unbiased risk estimate (SURE) for learning to denoise}

Consider an estimate of $\vy^\ast$ based on the noisy observation $\tilde \vy$. 
Define the risk of the estimator $\hat \vy$ as 
\[
R = \EX{\norm[2]{\hat \vy - \vy^\ast}^2},
\]
where the expectation is over the noise. 
Define 
\[
\hat R( \hat \vy(\vy) ) = - \sigma^2 + \norm{\hat \vy - \vy}^2
+ \frac{2 \sigma^2}{m} \sum_{i=1}^m \frac{\partial \hat y_i(\vy)}{\partial y_i}.
\]
The estimate $\hat R$ is called the SURE, and its significance is that it is an unbiased estimate of the mean square error of the estimate $\hat \vy(\vy)$. 
Specifically, Stein's lemma guarantees that
\[
\EX{\hat R} = R. 
\]
Now consider the estimator 
\[
\hat \vy(\vy) = \mU \pinv{\mU} \vy.
\]
For this estimator, we can define a sure loss function
\[
\hat R( \hat \vy(\vy) ) =  \norm[2]{ (\transp{\mU} \pinv{\mU} - \mI)\vy}^2
+ ...
\]
We can then train a model by minimizing
\[
\SUREest \in \arg \min_\mU 
\sum_{i=1}^n \hat R( \hat \vy(\vy_i) )
\]
The questions is, for a given number of training examples, which estimate works better when used in the denoiser?

Also, we can probably show show a result like:
\begin{theorem}
The SURE estimator obeys 
\[
\norm{\SUREest - \mU^\ast} \leq f(n,d,m),
\]
where $f$ is a function that depends on $(n,d,m)$.
\end{theorem}

That can then be used to make a statement about the performance of the `learned' denoiser.


As a next step, it is probably best to simply simulate and compare the two approaches and then, if the SURE approach works better further compare.

If the PCA based approach works better, we should try it on samples where $\vx_i$ is drawn from a different distribution, since the SURE approach does not rely on that assumption, which is a plus.






\end{document}
