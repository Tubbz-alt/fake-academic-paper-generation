\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{hyperref}
 \usepackage{color}
 \usepackage{xcolor}


% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

 \iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{1237} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\definecolor{gray}{rgb}{0.1,0.1,0.5}


% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE

% \title{Recurrent Network Models for Kinematic Tracking}
\title{Recurrent Network Models for Human Dynamics}
\author{Katerina Fragkiadaki \quad \quad Sergey Levine \quad \quad Panna Felsen \quad \quad Jitendra Malik\\
University of California, Berkeley\\
Berkeley, CA \\
{\tt\small \{katef,svlevine@eecs,panna@eecs,malik@eecs\}.berkeley.edu}
}

\pdfoutput=1

\maketitle
%\thispagestyle{empty}
%trajectory\newcommand{\mocap}{\theta}\newcommand{\ea}{\text{\textit{et al.}}}\newcommand{\z}{\mathrm{z}}\newcommand{\tr}{\mathrm{tr}}\newcommand{\mycolor}{gray}% \newcommand{\ntr}{n_{tr}}\newcommand{\ntr}{n_T}\newcommand{\auto}{\text{ERD}}\newcommand{\ERD}{\text{ERD}}%\newcommand{\ntr}{|\mathcal{T}|}% \newcommand{\ntr}{n_T}\newcommand{\data}{x}\newcommand{\seq}{\mathbf{x}}\newcommand{\WW}{\mathbf{W}}\newcommand{\xex}{\mathbf{x}}\newcommand{\Lo}{\mathcal{L}}\newcommand{\thetanet}{\boldsymbol \theta}\newcommand{\LL}{\mathbf{L}}\newcommand{\clcl}{\mathrm{cl}^2}%detection response\newcommand{\dl}{\mathrm{dl}}%\newcommand{\ndl}{n_{\dl}}\newcommand{\ov}{s}%\newcommand{\ndl}{{|\mathcal{D}|}}% \newcommand{\ATsteer}{\mathrm{steer}(\AT;\yf)}\newcommand{\ATsteer}{\W^{\mathrm{steer}}_{T}}\newcommand{\Csteer}{\C^{\mathrm{steer}}}\newcommand{\ndl}{n_D}\newcommand{\np}{n_P}% \newcommand{\salgroup}{\mathrm{salient\text{\textbf{-}}group}}\newcommand{\salgroup}{\mathrm{ncut}}\newcommand{\cnfd}{\mathrm{confidence}}\newcommand{\thresh}{\mathrm{th}}%detector response\newcommand{\dr}{\mathrm{r}}%detectlet confidence score% \newcommand{\dc}{\mathbf{f}}\newcommand{\dc}{\ce}\newcommand{\confcf}{\mathbf{confidence}}\newcommand{\aligncf}{\mathrm{align}}% \newcommand{\align}{ \align(X_k,Y_k)>\thresh%detectlet afinities\newcommand{\AD}{\mathbf{A}_D}\newcommand{\W}{\mathbf{W}}\newcommand{\eig}{\mathrm{eig}}% \newcommand{\pbth}{\mathrm{pb}}\newcommand{\pbth}{\beta}\newcommand{\discthresh}{\gamma}%\newcommand{\Asteers}{\mathbf{A}_\mathrm{steer}}% \newcommand{\Asteer}{\mathrm{steer}(\A;h)}\newcommand{\Asteer}{\W^\mathrm{steer}_T}\newcommand{\Asteernoh}{\mathbf{A}_\mathbf{steer}}\newcommand{\Aco}{\mathbf{A}^{\mathrm{c}}}%detectlet repulsions\newcommand{\RD}{\mathbf{R}_D}\newcommand{\dlsel}{h}\newcommand{\RDF}{\mathbf{R}_D^\dlsel}\newcommand{\R}{\mathcal{R}}%trajectet affinities% \newcommand{\AT}{\mathbf{A}_T}\newcommand{\AT}{\mathbf{A}}% \newcommand{\AT}{\mathbf{W}}%\newcommand{\cf}{\mathbf{v}}\newcommand{\f}{\mathrm{f}}\newcommand{\trc}{\mathrm{C}}%trajectlet repulsions\newcommand{\RT}{\mathbf{R}_T}%set of foreground trajectlets\newcommand{\Tf}{\mathcal{T}^\mathbf{f}}%set of foreground detectlets% \newcommand{\Df}{\mathcal{D}^\mathbf{f}}\newcommand{\st}{\mathrm{subject \text{ } to}}%associations\newcommand{\vc}{\mathrm{vec}}\newcommand{\Wtilde}{\tilde{\mathbf{W}}}\newcommand{\thr}{\mathrm{th}}\newcommand{\maxmarg}{\mathrm{mxmr}}\newcommand{\trd}{\tr^D}\newcommand{\C}{\mathbf{C}}\newcommand{\ns}{n_S}\newcommand{\nss}{m}%trajectlet cluaster to detectlets associations\newcommand{\Cg}{\mathbf{C}_G}\newcommand{\Prob}{\mathrm{P}}%detection compatibility\newcommand{\D}{\mathbf{D}}%trajectlet cluster\newcommand{\g}{\mathrm{G}}% detection score\newcommand{\score}{c}% bounding box\newcommand{\bbox}{\mathrm{box}}\newcommand{\X}{\mathrm{X}}\newcommand{\T}{\mathcal{T}}\newcommand{\diag}{\mathrm{Diag}}\newcommand{\Dcap}{\mathcal{D}}\definecolor{lightgray}{gray}{0.9}\newcommand{\todo}{\textcolor{red}{TODO: }}% \DeclareMathOperator*{\union}{\bigcup}% \DeclareMathOperator*{\maximize}{\max \text{.}}% \DeclareMathOperator*{\median}{\text{median}}% \DeclareMathOperator*{\minimize}{\min \text{.}}% \DeclareMathOperator*{\argmin}{arg\,min}% \DeclareMathOperator*{\argmax}{arg\,max}%\DeclareMathOperator*{\mean}{\mathrm{mean}}\newcommand{\velocity}{\mathrm{vel}}\newcommand{\disparity}{\mathrm{dsp}}\newcommand{\trajdist}{\mathrm{dst}}\newcommand{\Gup}{\mathrm{G}}\newcommand{\detection}{R}% \newcommand{\median}{\mathrm {median}}\newcommand{\na}{n(\allX)}\newcommand{\location}{\mathrm {loc}}\newcommand{\Tr}{\mathrm {Tr}}\newcommand{\YY}{\mathcal{Y}}%\newcommand{\yf}{\dlsel_{\Dcap}}\newcommand{\sel}{\mathbf {s}}% \newcommand{\yf}{h^\sel}% \newcommand{\yf}{\sel}\newcommand{\yf}{y}\newcommand{\ATDh}{\mathbf{A}_T^{\yf}}% \newcommand{\Ch}{\mathbf{C}^{\yf}}\newcommand{\Ch}{\mathrm{steer}(\C;h)}\newcommand{\RTh}{\mathbf{R}_T^{\yf}}% \newcommand{\ATD}{\tilde {\mathbf{A}}_T}\% \newcommand{\ATD}{\mathbf{A}_T}\newcommand{\AR}{\mathbf{A}_R}% \newcommand{\pn}{\textbf{Spec.Part.}}\newcommand{\pn}{\mathrm{Partition}}\newcommand{\ptwisemul}{\bullet}\newcommand{\modify}{M}\newcommand{\zeroone}{\{0, 1\}}\newcommand{\ones}{\mathbf 1}\newcommand{\suchthat}{\mathrm{s.t.}}\newcommand{\A}{\mathbf{A}}\newcommand{\GF}{\mathbb{G}}\newcommand{\e}{e}\newcommand{\nmultseg}{|\mathrm{Seg}|}\newcommand{\cut}{\mathrm{cut}}\newcommand{\degree}{\mathrm{degree}}% \newcommand{\pool}{\mathcal P}\newcommand{\npool}{n(\allX)}%\newcommand{\npool}{| \mathcal P |}%\newcommand{\npool}{|\allX|}% \newcommand{\gscal}{\mathrm{g}}\newcommand{\gscal}{\pbth}\newcommand{\Dr}{\mathbf{D}}\newcommand{\allX}{\mathbf X}\newcommand{\allY}{\mathbf Y}% \newcommand{\dith]{\gamma}\newcommand{\footnoteremember}[2]{
\footnote{#2}
\newcounter{#1}
\setcounter{#1}{\value{footnote}}
}\newcommand{\footnoterecall}[1]{
% \footnotemark[\value{#1}]
}%trajectory\newcommand{\p}{\mathbf{p}}% \newcommand{\P}{\mathbf{P}}%\newcommand{\ntr}{n_{\tr}}%\newcommand{\ntr}{|\mathcal{T}|}\newcommand{\link}{\mathrm{links}}\newcommand{\vol}{\mathrm{vol}}\newcommand{\ncut}{\mathrm{ncut}}\newcommand{\AF}{\mathbb{A}}\newcommand{\BF}{\mathbb{B}}\newcommand{\VF}{\mathbb{V}}\newcommand{\px}{\mathrm{p}}\newcommand{\Edge}{E^D}\newcommand{\nr}{n_R}\newcommand{\xx}{\mathbf{x}}\newcommand{\ww}{\mathbf{w}}\newcommand{\wwp}{\mathbf{w}^{D}}\newcommand{\aex}{a}\newcommand{\ca}{c}\newcommand{\nd}{n_D}% \newcommand{\Wbar}{\mathbf{\hat{W}}}% \newcommand{\Wbar}{\mathbf{\hat{A}}_T}% %\newcommand{\Asteers}{\mathbf{A}_\mathrm{steer}}% % \newcommand{\Asteer}{\mathrm{steer}(\A;h)}% \newcommand{\Asteer}{\W^\mathrm{steer}}% \newcommand{\Asteernoh}{\mathbf{A}_\mathbf{steer}}% \newcommand{\Aco}{\mathbf{A}^{\mathrm{c}}}% %detectlet repulsions% \newcommand{\RD}{\mathbf{R}_D}% \newcommand{\dlsel}{h}% \newcommand{\RDF}{\mathbf{R}_D^\dlsel}\newcommand{\RSPX}{\mathbf{R}_R}% %trajectet affinities% \newcommand{\AT}{\mathbf{A}_T}% % %trajectlet repulsions% \newcommand{\RT}{\mathbf{R}_T}% %set of foreground trajectlets% \newcommand{\Tf}{\mathcal{T}^\mathbf{f}}% %set of foreground detectlets% \newcommand{\Df}{\mathcal{D}^\mathbf{f}}%associations% \newcommand{\C}{\mathbf{C}}% % %trajectlet cluaster to detectlets associations% \newcommand{\Cg}{\mathbf{C}_G}% % %detection compatibility% \newcommand{\D}{\mathbf{D}}\newcommand{\DD}{\mathcal D}%trajectlet cluster% \newcommand{\g}{\mathrm{G}}% % % detection score% \newcommand{\score}{f}\newcommand{\nS}{n_{S}}\newcommand{\den}{\mathrm{\rho}}\newcommand{\spx}{\mathrm{r}}\newcommand{\prt}{\mathrm{d}}\newcommand{\Seg}{\mathbf{S}}% \newcommand{\aff}{\alpha}\newcommand{\aff}{\ww}\newcommand{\affr}{\ww^R}% bounding box% \newcommand{\bbox}{\mathrm{box}}% \newcommand{\T}{\mathcal{T}}\newcommand{\Diag}{\mathrm{Diag}}% \newcommand{\Dcap}{\mathcal{D}}\newcommand{\conf}{\mathbf{c}}\newcommand{\stab}{\mathbf{ncut}}\newcommand{\cmtb}{\mathbf{cmtb}}\newcommand{\pset}{\mathcal{P}}% \newcommand{\todo}{\textcolor{red}{TODO: }}\newcommand{\attr}{\mathrm{att}}% \newcommand{\na}{n(\allX)}% \newcommand{\location}{\mathrm {loc}}% % \newcommand{\Tr}{\mathrm {Tr}}% % %\newcommand{\yf}{\dlsel_{\Dcap}}% \newcommand{\sel}{\mathbf {s}}% % \newcommand{\yf}{h^\sel}% % \newcommand{\yf}{\sel}% % \newcommand{\yf}{y_D}% \newcommand{\ATDh}{\mathbf{A}_T^{\yf}}% % \newcommand{\Ch}{\mathbf{C}^{\yf}}% \newcommand{\Ch}{\mathrm{steer}(\C;h)}% %\newcommand{\RSPXst}{\mathrm{steer}(\A;h)}% \newcommand{\RTh}{\mathbf{R}_T^{\yf}}% % \newcommand{\ATD}{\tilde {\mathbf{A}}_T}\% \newcommand{\ATD}{\mathbf{A}_T}% \newcommand{\Ss}{\mathbf{S}(\PP)}% \newcommand{\RR}{\mathcal{R}}% % \newcommand{\ASPXI}{\mathbf{A}}\newcommand{\V}{\mathbf{V}}\newcommand{\Y}{\mathrm{Y}}\newcommand{\rr}{\mathrm{r}}\newcommand{\pa}{p}\newcommand{\EV}{\mathbf{\Lambda}}\newcommand{\WE}{\mathbf{W}}\newcommand{\K}{K}\newcommand{\ce}{\mathbf{c}}\newcommand{\width}{\mathrm{w}}\newcommand{\height}{\mathrm{h}}\newcommand{\nf}{n_F}\newcommand{\sss}{\tilde{s}}\newcommand{\PS}{\mathbf{P}}\newcommand{\SSp}{\mathcal{S}}\newcommand{\N}{\mathcal{N}}% \newcommand{\DT}{\mathcal{D}}% \newcommand{\DT}{G^D}\newcommand{\DT}{G}% \newcommand{\di}{\mathbf{S}}\newcommand{\di}{\mathbf{d}}% \newcommand{\den}{\mathrm{\delta}}\newcommand{\edge}{\mathrm{e}}\newcommand{\ASPX}{\mathbf{A}_{R}}% \newcommand{\ASPX}{\mathbf{A}}% % \newcommand{\ASPXst}{\mathbf{A}_{R}^{\mathrm{steer}}}% % \newcommand{\ASPXst}{\mathbf{W}_{J}}%  \newcommand{\ASPXst}{\mathbf{A}^{\mathrm{steer}}}\newcommand{\ASPXst}{\mathbf{W}^{\mathrm{steer}}}\newcommand{\PP}{\mathcal{D}}\newcommand{\bo}{t_f}% % \newcommand{\pn}{\textbf{Spec.Part.}}% \newcommand{\pn}{\mathrm{Partition}}% \newcommand{\ptwisemul}{\bullet}% % \newcommand{\ptwisemul}{\bullet}% % \newcommand{\modify}{M}% % \newcommand{\ones}{\mathbf 1}% \newcommand{\suchthat}{\mathrm{s.t.}}% % \newcommand{\e}{e}% \newcommand{\nmultseg}{|\mathrm{Seg}|}% % \newcommand{\pool}{\mathcal P}% \newcommand{\npool}{n(\allX)}% %\newcommand{\npool}{| \mathcal P |}% %\newcommand{\npool}{|\allX|}% \newcommand{\Dr}{\mathbf{D}}% \newcommand{\allX}{\mathbf X}% \newcommand{\Diag}{\mathrm Diag}% \newcommand{\allY}{\mathbf Y}\newcommand{\s}{\mathcal S (\mathcal D)}%  \newcommand{\nd}{n_D}% \newcommand{\footnoteremember}[2]{% \footnote{#2}% \newcounter{#1}% \setcounter{#1}{\value{footnote}}% }% \newcommand{\footnoterecall}[1]{% \footnotemark[\value{#1}]% }
 %\input{abstract.tex}
 %what we do
%  
 %\input{abstract3.tex}
  
  
  
  \begin{abstract}
We propose the Encoder-Recurrent-Decoder (ERD) model for recognition and prediction of human body pose in videos and motion capture. The ERD model is a  recurrent neural network that incorporates nonlinear encoder and decoder networks before and after  recurrent layers. 
We test instantiations of ERD architectures in the tasks of motion capture (mocap) generation, body pose labeling and body pose forecasting in videos. Our model handles mocap training data across multiple subjects and activity domains, and  synthesizes novel motions while avoiding drifting for long periods of time.   
 For human pose labeling, ERD outperforms a per frame body part detector by resolving left-right body part confusions. 
For video pose forecasting, ERD  predicts body joint displacements across a temporal horizon of 400ms and outperforms a first order motion model based on optical flow. 
ERDs extend previous  Long Short Term Memory (LSTM) models  in the literature to jointly learn representations and their dynamics. Our experiments show such representation learning is crucial for both labeling and prediction in space-time. We find this is a distinguishing feature between the spatio-temporal visual domain in comparison to  1D text, speech or  handwriting, where straightforward hard coded representations have shown excellent results when directly combined with recurrent units \cite{DBLP:conf/nips/SutskeverVL14} .   

\end{abstract}%%\begin{abstract}%We propose the Encoder-Recurrent-Decoder (ERD) model for recognition and prediction of human body pose in videos and motion capture. The ERD model is a  recurrent neural network that incorporates nonlinear encoder and decoder networks before and after  recurrent layers. %We test instantiations of ERD architectures in the tasks of motion capture (mocap) generation, body pose labeling and body pose forecasting in videos. Our model handles mocap training data across multiple subjects and activity domains, and  synthesizes novel motions while avoid drifting for long periods of time.   % For human pose labeling, ERD outperforms a per frame body part detector by resolving left-right body part confusions. %For video pose forecasting, ERD  predicts body joint displacements across a temporal horizon of 400ms and outperforms a first order motion model based on optical flow. %ERDs extend previous  Long Short Term Memory (LSTM) models  in the literature to jointly learn representations and their dynamics. Our experiments show such representation learning is crucial for both labeling and prediction in space-time. We find this is a distinguishing feature between the spatio-temporal visual domain in comparison to  1D text, speech or  handwriting, where straightforward hard coded representations have shown excellent results when directly combined with recurrent units.   
  
  

 % \input{introduction.tex}
   




 
%    \input{related_work.tex}
   \section{$\auto$s for recurrent kinematic tracking and forecasting}%We build a model that  predicts a distribution over plausible poses in the next frame given a human pose sequence so far.  Figure \ref{fig:autornn} illustrates   $\auto$ models for recurrent kinematic tracking and forecasting.    
%It consists of %for human motion prediction and generation has %encoder, decoder and  recurrent subnetwork. %s, each of the three being a multilayer etwork with intrleaved nonilearities. 
At  each time step $t$, vector $\data_t$ of a sequence  $\seq=(\data_1, \cdots, \data_T)$ passes through the encoder, the recurrent layers, and the decoder network, producing the output $y_t$. 
In general, we are interested in estimating some function $f(x)$ of the input $x$ at the current time step, or at some time in the future. For example, in the case of motion capture, we are interested in estimating the mocap vector at the next frame. Since both the input and output consists of mocap vectors, $f$ is the identity transformation, and the desired output at step $t$ is $f(x_{t+1})$.
In case of video pose labeling and forecasting, $f(x)$ denotes body joint locations corresponding to the image in the current bounding box $x$. At step $t$, we are interested in estimating either $f(x_t)$ in the case of labeling, or $f(x_{t+H})$ in the case of forecasting, where $H$ is the forecast horizon.

%showrt about the recurrent layer% The architecture of the recurrent component of our model is similar to the one proposed in \cite{DBLP:journals/corr/Graves13}. % The two recurrent layers shown in the $2^\text{nd}$ column of Figure \ref{fig:autornn} both receive inputs from the encoder and are directly connected to the decoder. % For mocap prediction, video pose labeling and forecasting (shown in the $3^\text{rd}$ and $4^\text{th}$ columns), the recurrent network has only one layer. %%SL: this is a bit strange, no? are there many more units here for some reason? isn't it a bit weird that we had overfitting with video (which seems more complex) but not mocap?
The units in each recurrent layer  implement the Long Short Term Memory functions \cite{Hochreiter:1997:LSM:1246443.1246450}, where writing, resetting, and reading a value from each recurrent hidden unit is explicitly controlled via gating units, as described by Graves \cite{DBLP:journals/corr/Graves13}. Although LSTMs have four times more parameters than regular RNNs, they  facilitate long term storage of task-relevant data. %, as described by Graves \cite{DBLP:journals/corr/Graves13}. 
In Computer Vision, LSTMs have been used  so far for image captioning  \cite{DBLP:journals/corr/VinyalsTBE14} and action classification in videos \cite{DBLP:journals/corr/DonahueHGRVSD14}.   
%The units in each recurrent layer implement the Long Short Term Memory functions \cite{Hochreiter:1997:LSM:1246443.1246450}, and particularly their variants   described in  Graves  \cite{DBLP:journals/corr/Graves13}. Writing, reseting and reading  a value to and from each hidden recurrent unit is explicitly controlled using non linear equations, as described in \cite{DBLP:journals/corr/Graves13}. %While LSTMs have four times more parameters than regular RNNs, they facilitate long term storage of task relevant input data.  %The equations for the recurrent layer computations are included in the supplementary material for completeness. % the following equations   take as input the output of the encoder as well as the activations of the recurrent layer from the previous time step. % % % % The recurrent layers closely follow the long-short term memory architecture of Graves  \cite{} where The writing, reseting and outputing of a value in each hidden unit is explicitly controlled using the following equations:% %e.g., a distribution over mocap poses in the next frame given a mocap sequence so far, or a prediction of body joints for the next video frame given a video sequence so far.%The decoder's output $y_t$ may be deterministic (a single prediction ) or probabilistic in which case  $y^t$  parametrizes %a Gaussian Mixture Model over outputs $\o^{t+1}$. % For our mocap prediction task, the input and prediction spaces of the  model coincide, that is $f$ is the identity transformation. % If that case, vector sequences can be generated by iteratively querying the model and feeding its prediction as input.  By sampling from the GMM, diverse sequences can be generated, in contrast to the deterministic setup. %We consider a predictive and a labelling setup:% \begin{itemize}%  \item In the predictive setup, $y_t$  corresponds to a prediction regarding properties $f$ of the input at time $t+1$ $y^t->f(\seq^{t+1})$.  e.g., a distribution over mocap poses in the next frame given a mocap sequence so far, or a prediction of body joints for the next video frame given a video sequence so far. The decoder's output $y_t$ may be deterministic (a single prediction ) or probabilistic in which case  $y^t$  parametrizes % a Gaussian Mixture Model over outputs $\o^{t+1}$. For our mocap prediction task, the input and prediction spaces of the  model coincide, that is $f$ is the identity transformation. % If that case, vector sequences can be generated by iteratively querying the model and feeding its prediction as input.  By sampling from the GMM, diverse sequences can be generated, in contrast to the deterministic setup. %\item %INTRO%The expectation is that AutoRNN will outperform the per frame or window model by exploiting variable length temporal window of potentially deformable temporal context, e.g., correct the left right confusion of a per frame body part detector.%\end{itemize}% \begin{enumerate}%  \item bodyjoint tracking: yt then % \end{enumerate}%For a labellign %multilayer encoder network, a stack of recurrently connected  layers and a multilayer % An input vector sequence $\seq=(\seq_1, \cdots, \seq_T)$  is passed through the encoder %  to a stack of recurrently connected  layers to % compute first the  vector sequences \mathbf{\rr}^n = (h n 1 , . . . , h nT ), then through the decoder produce % output vector sequence y = (y 1 , . . . , y T ).% Given an input sequence $\seq : \{ \seq^t \in \mathcal{S}, t=1 \cdots T \}$, at each time step $t$, the encoder receives as input $\seq_t$. % and the  % The output of the decoder  $y^t$ corresponds to the prediction $\o^{t+1}=f(\seq^{t+1})$ in time step $t+1$, where $f$ a function of the input vector sequence. % For our mocap prediction setup, the input and output spaces of the  model coincide, that is $f$ is the identity transformation.   For our video prediction setup, the input is a video pixel  sequence depicting a person and the output is heat maps over the person's body joints. % If the input and output spaces of the predictive model coincide, sequences can be generated by iteratively querying the model and feeding its prediction as input. The decoder's output may be deterministic (a single prediction ) or probabilistic in which case  $y^t$  parametrizes % %that is $f$ is the identity transformation,% % them back as input to the model% % the per frame predictions  can be fed back as input to the RNN for synthesizing whole sequence rather than per frame predictions. % % This is the case for our motion capture setup, as well as the well known examples of  text and handwriting  generation. % %For our pose prediction in video, the input sequence is a bounding box pixel It may be a single body pose in case of deterministic prediction or it may parametrize % %In the case of deterministic prediction,  $y^t$ or parametrizes % a Gaussian Mixture Model over outputs $\o^{t+1}$. By sampling from the GMM, diverse sequences can be generated. % in case of probabilistic prediction. (deterministic or samples from the GMM) %into the model as input per frame predictive model is  used to synthesize full mocap pose sequences conditioning on some mocap prefix by feeding the predictions of the model at each frame as input in the next frame. % ++REPETITION?shall i delete this paragraph?% The non-linear encoder and decoder networks  distinguishes $\auto$ from previous RNN models in the literature, notably those for text or handwriting generation \cite{DBLP:journals/corr/Graves13}. % %The encoder and decoder architectures for our different tasks is illustrated in Figure \ref{fig:autornn}. % Intuitively, the encoder transforms the input sequence into a feature representation where prediction and/or temporal reasoning  is easy. The decoder transforms the output of the recurrent layer to the desired  format. We learn the (non linear) embedding suitable for the prediction task by jointly training the encoder, decoder  and recurrent layers under the prediction or labeling loss at hand. %This joint training is in contrast to \cite{DBLP:conf/slt/MikolovZ12,DBLP:conf/interspeech/MikolovKBCK10} where a skipgram model \cite{DBLP:journals/corr/abs-1301-3781}learns a word embedding, treated as fixed input to a predictive RNN for text generation.  % Omitting  encoder and decoder networks caused % %and rather passing the input  sequence with linear transformations to and from the recurrent layer  % underfiting for both mocap prediction and video labeling and forecasting tasks. Figure \ref{fig:mocapgeneration}e,f  shows results  of such underfit networks. % %problems in our tasks. % This can be explained from the complexity of the mocap and video domain in comparison to text or pen stroke $(x,y)$  locations. Notably,  word embeddings were not crucial for RNNs to do well in text generation or machine translation; the standard one hot encoding vocabulary representation has also  shown excellent results \cite{DBLP:conf/nips/SutskeverVL14}. % % Our $\auto$ architecture extends prior work on LSTMs by augmenting them with encoder and decoder networks.% As shown in Figure \ref{fig:mocapgeneration}, % %Section~\ref{sec:some_experimental_section},% omitting the encoder and decoder networks and instead using linear mappings between the input, recurrent state, and output% %and rather passing the input  sequence with linear transformations to and from the recurrent layer  % causes severe underfitting on all three of our tasks.% %problems in our tasks. % This can be explained by the complexity of the mocap and video input in comparison to the words or pen stroke 2D $(x,y)$ locations considered in prior work \cite{DBLP:journals/corr/Graves13}. For example, word embeddings were not crucial for RNNs to do well in text generation or machine translation, and the standard one hot encoding vocabulary representation also showed excellent results \cite{DBLP:conf/nips/SutskeverVL14}.% $\auto$ architecture extends prior work on LSTMs by augmenting the model with encoder and decoder networks.
%As shown in Figure \ref{fig:mocapgeneration}, %Section~\ref{sec:some_experimental_section},
Omitting the encoder and decoder networks and instead using linear mappings between the input, recurrent state, and output 
%, as proposed in prior work \cite{DBLP:journals/corr/Graves13}, %and rather passing the input  sequence with linear transformations to and from the recurrent layer  
caused underfitting on all three of our tasks. 
%%SL: but fig:mocapgeneration only addresses the mocap task -- do you want to refer to the experiments section in general, or reference another figure here? do you even test the Graves model on the video task?%problems in our tasks. 
This can be explained by the complexity of the mocap and video input in comparison to the words or pen stroke 2D  locations considered in prior work \cite{DBLP:journals/corr/Graves13}. For example, word embeddings were not crucial for RNNs to do well in text generation or machine translation, and the standard one hot encoding vocabulary representation also showed excellent results \cite{DBLP:conf/nips/SutskeverVL14}.

%\vspace{-0.08in}\subsection{Generating Motion Capture}%\vspace{-0.08in}% Given sequences of motion capture % %that capture  the positions of the body joints of a human performer in time, % we want to learn a model that predicts a distribution over plausible poses in the next frame given a sequence  far. % Given a mocap sequence, we want to predict the mocap vector in the next frame.  % Each mocap  vector is comprised of set of 3D body joint angles in a kinematic chain representation: the angle of each body part is measured in the coordinate system of its parent. Thus, % %We use angles  converted in the exponential map representation that does not suffer from gimbal lock apart from one point+++% angles of parts along kinematic chains are highly interdependent. 

Our goal is to predict the mocap vector in the next frame, given a mocap sequence so far. Since the output $y_{t}$ has the same format as  the input $x_{t+1}$, if we can predict $x_{t+1}$, we can ``play'' the motion forward in time to generate a novel mocap sequence by feeding the output at the preceding time step as the input to the current one.

% Since the output $y_{t}$ has the same format as in the input $x_{t+1}$, if we can predict $x_{t+1}$, we can ``play'' the motion forward in time to generate a novel mocap sequence. % Each mocap  vector is comprised of a set of 3D body joint angles in a kinematic tree representation. % % with the angle of each body part specified in the coordinate frame of its parent. Thus, % % %We use angles  converted in the exponential map representation that does not suffer from gimbal lock apart from one point+++% % angles of body parts in the kinematic tree are highly interdependent. % We represent the orientation of each joint by an exponential map in the coordinate frame of its parent, corresponding to 3 degrees of freedom per joint. The global position of the body in the x-y plane and the global orientation about the vertical z axis are predicted relative to the previous frame, since each clip has an arbitrary global position. This is similar to the approach taken in previous work \cite{thr-mhmub-06}. We standardize our input by mean subtraction and division with the standard deviation of each feature dimension. Each mocap  vector consists of a set of 3D body joint angles in a kinematic tree representation. 
% with the angle of each body part specified in the coordinate frame of its parent. Thus, % %We use angles  converted in the exponential map representation that does not suffer from gimbal lock apart from one point+++% angles of body parts in the kinematic tree are highly interdependent. 
We represent the orientation of each joint by an exponential map in the coordinate frame of its parent, corresponding to 3 degrees of freedom per joint. The global position of the body in the x-y plane and the global orientation about the vertical z axis are predicted relative to the previous frame, since each clip has an arbitrary global position. This is similar to the approach taken in previous work \cite{thr-mhmub-06}. We standardize our input by mean subtraction and division by the standard deviation along each dimension. 



We consider both deterministic and probabilistic predictions. In the deterministic case, the decoder's output $y_t$ is a single mocap vector.  In this case, we train our model by minimizing the Euclidean loss between target and predicted body joint angles. In the probabilistic case, $y_t$  parametrizes  a Gaussian Mixture Model (GMM) over mocap vectors in the next frame.  We then  minimize the GMM negative log-likelihood during training:
\begin{equation}
 \mathcal{L}(\seq) = - \displaystyle\sum_{t=1}^T\mathrm{log}\mathrm{Pr}(x_{t+1}|y_t)
\end{equation}
We use five mixture components and diagonal covariances. The variances are outputs of  exponential layers to ensure  positivity, and the mixture component probabilities are outputs of a softmax layer, similar to \cite{DBLP:journals/corr/Graves13}. During training, we  pad the variances in each iteration by a fixed amount to ensure they do not collapse around the mixture means.
   %For the GMM output parametrization, we  pad the variances in each iteration by a fixed amount to ensure they do not collapse around the mixture means. 
   Weights are initialized randomly. We experimented with initializing the encoder and decoder networks of the mocap $\auto$ from the (first two layers of) encoder and (last two layers of) decoder of a) a ten layer autoencoder trained for dimensionality reduction of mocap  vectors \cite{citeulike:778023}, 
  b) a ``skip'' autoencoder  trained to reconstruct the  mocap vector in few frames in the future given the current one. In both cases, we did not observe improvement over random weight initialization.  We train our $\auto$ model with stochastic gradient descent and backpropagation through time \cite{Williams95gradient-basedlearning} with momentum and gradient clipping at 25, using the publicly available Caffe package \cite{Jia13caffe} and the LSTM layer  implementation from \cite{DBLP:journals/corr/DonahueHGRVSD14}. 
  
  %Detailed derivations of the gradients for the GMM parameters are included in the supplementary material.%  We jointly finetuning encoder, recurrent and decoder networks. This is necessary since the  recurrent layer has sigmoidal outputs while our autoencoders have rectified linear units, thus the range of input values for the  decoder  of the autoencoders  and the decoder of $\auto$ may differ significantly. %  We train our $\auto$ model using stochastic gradient descent and backpropagation through time \cite{Williams95gradient-basedlearning} with momentum and gradient clipping at 25, in  Caffe \cite{Jia13caffe}, a publicly available deep learning package.  In the case of GMM parametrization, we  augment the variances in each iteration by a fixed amount to ensure they do not collapse around the mixture means. Please see the supplementary material for detailed derivations of the gradients for the GMM parameters.% We regularize both our autoencoders and $\auto$ by providing  noisy mocap vectors corrupted with zero mean Gaussian noise \cite{Vincent:2010:SDA:1756006.1953039} and asking the model to predict the correct, uncorrupted  mocap vector in the next frame. We found valuable to progressively enlarge the noise standard deviation, learning from non-corrupted  examples first, implementing a type of curriculum learning, as also noted in \cite{DBLP:journals/corr/Graves13}. %, implementing a type of curriculum learning, as also noted in   % %inputs, that is, we add random gaussan noise to the body joint angles and asking to reconstruct the uncorrupted input vector, thus the term denoising autoencoder. % %Denoising is a form of data augmentation that has shown in \cite{} to have improved performance of autoencoders.  In our case, % Denoising \cite{Vincent:2010:SDA:1756006.1953039} is necessary for preventing drifting during motion synthesis: at test time, we run the model forward by feeding its predictions as input to the next time step.% %model's predictions are fed back to the model as input. % However, in this way, errors can accumulate, and eventually produce poses that the model has not seen during training and cause it to produce garbage predictions. Denoising ensures  corrupted mocap data are shown to the network during training so that it learns to be robust to  prediction errors  at test time.% We regularize  our mocap $\auto$ with denoising: we provide   mocap vectors corrupted with zero mean Gaussian noise \cite{Vincent:2010:SDA:1756006.1953039} and have the model  predict the correct, uncorrupted  mocap vector in the next frame. We found it valuable to progressively increase the noise standard deviation, learning from non-corrupted  examples first. This corresponds to a type of curriculum learning. %, as also noted by Graves \cite{DBLP:journals/corr/Graves13}.%%SL: did Graves do exactly the same thing? try to be clear about what is derived from previous work here and what is novel...%, implementing a type of curriculum learning, as also noted in   %inputs, that is, we add random gaussan noise to the body joint angles and asking to reconstruct the uncorrupted input vector, thus the term denoising autoencoder. %Denoising is a form of data augmentation that has shown in \cite{} to have improved performance of autoencoders.  In our case, 
At test time, we  run the model forward by feeding the predictions as input to the model in the following time step. Without  denoising, this kind of forward unrolling  suffers from accumulation of small prediction mistakes at each frame, and the model  
%lead  drift %  \cite{Vincent:2010:SDA:1756006.1953039} %and 
quickly falls into unnatural regions of the state space. 
%%SL: not sure what that citation actually says, but perhaps you could make sure it is logical after my change%model's predictions are fed back to the model as input. %The errors caused by this drift can accumulate, and produce poses that the model has not seen during training and resulting in unnatural predictions. 
Denoising ensures that corrupted mocap data are shown to the network during training so that it learns to  correct small amounts of drift and stay close to the manifold of natural poses.

%\vspace{-0.03in}\subsection{Labeling and forecasting video pose}
In the previous section, we described how the $\auto$ model can be used to synthesize naturalistic human motion by training on  motion capture datasets. 
%In this section, we extend this model to predict human poses directly from pixels in a video. We consider one labeling and one predictive setup. In the former, given a bounding box sequence depicting a person, we want to estimate body joint locations for the current frame, given the box sequence so far. In the latter, we want to estimate body joint locations for a specific future time instance instead.   
In this section, we extend this model to identify human poses directly from pixels in a video. We consider a pose labeling task and a pose forecasting task. In the labeling task, given a bounding box sequence depicting a person, we want to estimate body joint locations for the \textit{current} frame, given the sequence so far.
%%SL: "box sequence" sounds weird
In the forecasting task, we want to estimate body joint locations for a specific future time instance instead.   

% \subsection{Video pose labelling}%  We want to estimate the body joint locations of the \textit{current} frame given a video sequence so far. Our setup is similar to the previous section, except that the target heat maps concern the current, rather than a future frame.%  We assume a causal tracking setup, where future is not known ahead of time, thus bidirectional time context is not available. %If causality is not imposed, bidirectional neural networks could % % % Given a bounding box sequence depicting a person, we want to predict her body joint locations in the next frame. %in the next video frame given a video sequence so far. % video sequence of bounding boxes containing a person.  Given a video sequence we consider the tasks of body joint tracking and prediction. %Our video sequence consists of a bounding box tracklet around a person. %We want to predict how the person will move, what his body pose will be in the near future given its tracklet so far. %We assume a bounding box sequence around a person. We represent $K$ body joint locations as a set of $K$$N \times N$ heat maps over the person's bounding box, that represent likelihood for each joint to appear in each of the $N^2$ grid locations, similar to \cite{vpsKpsTulsianiM14}.  %that are produces by the same network architecture that operates in%Heat map estimation, as opposed to x,y pixel coordinate regression,  %Body joint heat maps is a more natural parametrization for the body part locations than body joint pixel coordinates. %In recent literature, it has been shown that deep part detection outperforms body joint location regression for human pose estimation  \cite{DBLP:conf/nips/TompsonJLB14,DBLP:journals/corr/ToshevS13}, suggesting that heat maps of body joints are a more natural parametrization for learning than $x,y$ pixel coordinates. 
Predicting heat maps naturally incorporates uncertainty over  body joint locations, as opposed to predicting body joint pixel coordinates. 
 
  %We consider grid maps of two resolutions, 6 $\times$ 6 and 12 $\times$ 12.  We chose this representation that has been top shown to give state-of-the-art results in pose estimation of pedestrians in PASCAL VOC in \cite{}. Figure \ref{fig:autornn}\textit{right} illustrates our $\auto$ architecture for video pose labeling and forecasting.  The encoder is a five layer convolutional network with architecture similar to Krizhevsky $\ea$\cite{NIPS2012_0534}. %, the $6$th layer produces heat maps over body joint locations. 
Our decoder is a two layer network with fully connected layers interleaved with  rectified linear unit layers.  
%We represent $K$ body joint locations as  $K$ $N \times N$ heat maps over the person's bounding box, that encode confidence for each joint to appear in each of the $N^2$ grid locations, similar to \cite{vpsKpsTulsianiM14}.   %that are produces by the same network architecture that operates in%Heat map estimation, as opposed to x,y pixel coordinate regression,  %Body joint heat maps is a more natural parametrization for the body part locations than body joint pixel coordinates. %In recent literature, it has been shown that deep part detection outperforms body joint location regression for human pose estimation  \cite{DBLP:conf/nips/TompsonJLB14,DBLP:journals/corr/ToshevS13}, suggesting that heat maps of body joints are a more natural parametrization for learning than $x,y$ pixel coordinates. 
 The output of the decoder  is    body joint heat maps over the person bounding box in the current frame for the labeling task, or body joint heat maps at a specified future time instance for the forecasting  task. 
 %Heat maps naturally incorporate uncertainty over  body joint locations as opposed to  body joint pixel locations. 

We train both our pose labeler and forecaster $\auto$s under a Euclidean loss between estimated and target heat maps. 
We initialize the weights of the encoder from a   six layer convolutional network trained for per frame body part detection, in which the final CONV6 layer corresponds to the body joint heat maps.  %Such weight initialization is crucial for performance, as shown in the experimental section. 

Empirically, we found it valuable to input to the recurrent layer not the per frame estimated heat maps (CONV6), but rather the preceding CONV5  feature maps.   These feature maps capture rich appearance information, rather than merely  body joint likelihood.  
%$\auto$s have the capacity to take advantage of such rich feature representation, since the decoder and encoder are discriminatively finetuned together with the recurrent weights to optimize performance on the labeling or forecasting task.
Rich appearance information  assists the network in discriminating between different actions and pose dynamics without  explicit switching across activity domains, as previous switching dynamical linear systems  or HMMs \cite{prm-lslmh-00}.


We use two  networks on different image scales for our per frame pose detector and $\auto$: one where the output layer  resolution is 6$\times$6 and one that works on double  image size and has output resolution of 12$\times$12. The  heat maps of the coarser scale are upsampled and added to the finer scale to provide the final combined 12$\times$12 heat maps. Multiple scales have shown to be beneficial  for static pose estimation  in \cite{vpsKpsTulsianiM14,DBLP:conf/nips/TompsonJLB14,DBLP:journals/corr/ToshevS13}.
  
  
%. In contrast, previous parametric smoothers or filters, carry out their temporal reasoning over the  space of body joint locations \cite{DBLP:conf/nips/WuBGBSSD02}, outputs of an observation model, and fail to capture useful appearance information for fine-grain labeling or prediction. %%SL: I think I made a comment about this before... I still think the last sentence needs a bit of work to make it less harsh, it comes across as rather aggressive right now. It currently makes it sound like all prior work fails at labeling or prediction of body parts, which is not really true.% \begin{figure*}[ht!]% \begin{center}% \includegraphics[trim=1in 0in 0in 0.3in, scale=0.21]{figs/mocapres2.pdf}
   %\vspace{-0.08in}\section{Experiments}%\vspace{-0.08in}
We test our method on the H3.6M dataset of Ionescu $\ea$\cite{h36m_pami}, which is currently the largest video pose dataset. It consists of 15  activity scenarios, %such as,  smoking, walking, sitting down, eating, giving directions etc. Actions are 
performed by seven  different professional  actors and  recorded from four static cameras. For each activity scenario, subject, and camera viewpoint, there are two video sequences, each  between 3000 and 5000 frames. Each activity scenario features rich gestures, pose variations and interesting subactions  performed by the actors. For example, the walking activity includes holding hands, carrying a heavy load, putting hands in the pockets,  looking around etc. %The actors are particularly animated on purpose. 
The activities are recorded using a Vicon motion capture system that tracks markers on actors' body joints and provides  high quality 3D body joint locations. 
%The dataset provides highly accurate  3D and 2D body joint annotations for each subject obtained by tracking markers on actors body joints with vicon technology. 
2D body joints locations are obtained by projecting the 3D positions onto the image plane using the  known camera calibration and viewpoint. For all our experiments, we treat subject 5 as the test subject and all others as our training subjects. 

%\vspace{-0.08in}\paragraph{Motion capture generation}\begin{figure}[h!]
\begin{center}
\includegraphics[trim=0.0in 4in 0in 0.0in, scale=0.28]{figs/mocap_comparisons_extended.pdf}
\end{center}
%\vspace{-0.15in}
\caption{ \textbf{Motion synthesis.}  
LSTM-3LR and  CRBMs \cite{thr-mhmub-06} provide  smooth short-term motion completions  (for up to 600msecs), mimicking well novel styles of motion, (e.g., here, walking with upright back).  However,  $\auto$  generates realistic motion for  longer periods of time while LSTM-3LR soon converges to the mean pose and  CRBM  diverges to implausible motion.  NGRAM  has a non-smooth transition from conditioning to generation. Per frame mocap vectors  predicted by GPDM \cite{Wang06gaussianprocess}  look plausible,  but their temporal evolution is far from realistic.  You can watch the corresponding video results  at {\tt https://sites.google.com/site/motionlstm/} }.
\label{fig:crbm}
\end{figure}

We compare our $\auto$ mocap generator with  a) an LSTM recurrent neural network with linear encoder and decoders that has 3 LSTM layers of 1000 units each (architecture found through experimentation to work well), b) Conditional Restricted Boltzmann Machines (CRBMs) of Taylor \textit{et al.}\cite{thr-mhmub-06}, c)  Gaussian Process Dynamic Model (GPDM) of Wang \ea\cite{Wang06gaussianprocess}, and d) a nearest neighbor N-gram model (NGRAM).   For CRBM and GPDM, we used the code made publicly available by the authors. For the nearest neighbor N-gram model, we used a frame window of length $N=6$ and Euclidean distance on 3D angles between the conditioning prefix and our training set,  and copy past the subsequent frames of the best matching training subsequence.  
We applied denoising during training to regularize both the $\auto$ and the LSTM-3LR.  For all models, the mocap frame sequences were subsampled by two. $\auto$, LSTM-3LR and CRBM are trained on multiple activity scenarios (Walking, Eating and Smoking).  GPDM is trained on Walking activity only, because   its  cubic complexity prohibits its training on a large number of  sequences.  
Our comparison focuses  on motion forecasting (prediction) and synthesis, conditioning on motion prefixes of our test subject.  Mocap in-filling and denoising  are nontrivial with our current  model but developing this functionality is an interesting avenue for future work. 

%
 
We show  qualitative motion synthesis results in Figure \ref{fig:crbm}  and  quantitative motion prediction errors in Table \ref{tab:predictionerror}.  In Figure \ref{fig:crbm},  the conditioning motion prefix from our test subject is shown in green  and the generated motion is shown in blue. 
In Table \ref{tab:predictionerror}, we show Euclidean norm between the synthesized motion  and ground-truth   motion for our test subject  for different temporal horizons past the conditioning motion prefix, the largest being 560msecs, averaged across 8 different prefixes. %Within  small temporal horizons, human motion is  close to deterministic.  
The stochasticity of human motion prevents a metric evaluation for longer temporal horizons, thus all comparisons in previous literature are qualitative.  
%You can watch video comparisons at { \tt https://sites.google.com/site/motionlstm/ }%\href{https://sites.google.com/site/motionlstm/}{https://sites.google.com/site/motionlstm/}. %\href{https://youtu.be/CvaKD1NGcBk}{online motion completion video} . 
LSTM-3LR dominates the short-term motion generation, yet soon converges to the mean pose, as shown in Figure \ref{fig:crbm}. CRBM also provides smooth short term motion completions,  yet quickly drifts to unrealistic motions. $\auto$ provides slightly less smooth completions, yet can generate realistic  motion for long periods of time. For $\auto$, the smallest error was always produced by the most probable GMM sample, which was similar to the output of an ERD trained under a standard Euclidean loss. N-gram model exhibits a sudden change of style during transitioning from the conditioning prefix to the first generated frame, and cannot generate anything outside of the training set. Due to low-dimensional embedding, GPDM cannot adequately handle the breadth of styles in the training data, and produces unrealistic temporal evolution.  


The quantitative and qualitative motion generation results of $\auto$ and LSTM-3LR suggest an interesting trade-off between smoothness of motion completion (interesting motion extrapolations) and stable long-term motion generation. Generating short-term motion that mimics the style of the test subject is possible with LSTM-3LR, yet, since the network has not encountered similar  examples during  training, it is unable to correctly generate motion for longer periods of time. In contrast, $\auto$ gears the  generated motion towards similarly moving training examples. $\auto$  though cannot really extrapolate, but rather interpolate among the training subjects. It does   provides much smoother motion completions than the N-gram baseline. Both setups are interesting and useful in different applications, and in between architectures potentially lie somewhere in between the two ends of that spectrum. Finally, it is surprising that LSTM-3LR outperforms CRBMs given its simplicity during  training and testing, not requiring inference over latent variables. 

\begin{table}[h]{\small
%\centering
\setlength{\tabcolsep}{2.5pt}
\begin{tabular}{|p{1.52cm}||p{0.75cm}|p{0.75cm}|p{0.75cm}|p{0.75cm}|p{0.75cm}|p{0.75cm}|p{0.75cm}|}
%\begin{tabular}{|p{1.52cm}||c|c|c|c|c|c|c|}
\cline{1-8}
                                             & {80}             & {160}       & {240}       &{320}       & {400} & {480} & {560} \\ \hline 
 ERD                                      & 0.89            & 1.39        & 1.93         & 2.38        & 2.76  &  3.09       &  3.41               \\ \hline 
 LSTM-3LR                           & \textbf{0.41} & \textbf{0.67} & \textbf{1.15} & \textbf{1.50} & \textbf{1.78} & \textbf{2.02} & \textbf{2.26}\\   \hline 
CRBM \cite{thr-mhmub-06} & 0.68              &         1.13      & 1.55             & 2.00             & 2.45              & 2.90              & 3.34\\\hline 
6GRAM &                              1.67                & 2.36              & 2.94              & 3.43             & 3.83             &4.19               &4.53  \\\hline 
GPDM \cite{Wang06gaussianprocess} & 1.76 & 2.5 & 3.04 & 3.52 & 3.92 & 4.28 & 4.61\\   \hline 
\end{tabular}
%\vspace{-0.0in}
}
 \caption{ \textbf{Motion prediction error} during 80, 160, 240, 320, 400, 480 and 560 msecs past the conditioning prefix for our test subject during Walking activity.  %We show  Euclidean norm of the 3D angle error averaged across 8 motion completion examples.  
 %LSTM-3LR is the most competitive model.    
 Quantitative evaluation for longer temporal horizons is not possible due to stochasticity of human motion. }
\label{tab:predictionerror}
\end{table}\begin{figure}[ht]
\begin{center}
\includegraphics[trim=0in 0.3in 0in 1in, scale=0.2]{figs/pretraining.pdf}
%\includegraphics[trim=0in 0.2in 0in 1in, scale=0.3]{figs/pretraining.png}
\end{center}
%\vspace{-0.2in}
\caption{\textbf{Pretraining.}  %We compare training (left) and test (right) loss with initialization of the CNN encoder from a pose estimator versus initialization from random weights. 
%With random weight initialization (we used similar weight filling as Alexnet) the optimization is not successful. 
Initialization of the CNN encoder with the weights of a body pose detector leads to a much better solution than random weight initialization. For motion generation, we did not observe this  performance gap between pertaining and random initialization, potentially due to much shallower encoder and  low dimensionality of the mocap data.}
\label{fig:pretraining}
\end{figure}%\begin{figure*}[ht]
\begin{center}
\includegraphics[trim=0in 0.2in 0in 0in, scale=0.23]{figs/curves_video_labelling2.pdf}
%\includegraphics[trim=0in 0.0in 0in 0in, scale=0.68]{figs/curves_video_labelling.png}
\end{center}
%\vspace{-0.2in}
\caption{\textbf{Video pose labeling in H3.6M.}  Quantitative comparison of  a per frame CNN body part detector  of \cite{vpsKpsTulsianiM14} (PF), dynamic programming for temporal coherence of the body pose sequence in the spirit of \cite{DBLP:conf/iccv/ParkR11,Batra:2012:DMS:2403138.2403140} (VITERBI), and  $\auto$ video pose labeler.  $\auto$  outperforms the per frame detector as well as the dynamic programming baseline.   Oracle curve shows the performance upper-bound  imposed by our grid resolution of 12x12.}
\label{fig:bjtquant}
\end{figure*}\begin{figure*}[ht]
 \begin{center}
 %\includegraphics[trim=0.3in 2.6in 0in 0.5in, scale=0.27]{figs/bodyjointtracking_qualit.pdf}
 \includegraphics[trim=0.3in 0.8in 0in 0.5in, scale=0.67]{figs/bodyjointtracking_qualit.png}
 \end{center}
 %\vspace{-0.0in}
 \caption{\textbf{Left-right disambiguation.} $\auto$ corrects  left-right confusions of the per frame CNN detector   by  aggregating   appearance  features (CONV5) across long temporal horizons.}
 \label{fig:leftright}
 \end{figure*}%\vspace{-0.08in}\paragraph{Video pose labeling}
  Given a person bounding  box sequence, we want to label 2D pixel locations of the person's body joint locations. 
  %This is the standard setup for body pose estimation in videos \cite{DBLP:conf/iccv/ParkR11,conf/cvpr/SappWT11}.  
  Both occluded and non-occluded body joints are required to be detected correctly: the occluder's appearance often times contains useful information regarding the location of an occluded body joint \cite{DBLP:conf/eccv/DesaiR12}. 
  Further, for transcribing 2D to 3D pose, all body joints are required  \cite{Taylor:2000:RAO:364058.364079}. 
  


We compare our $\auto$ video labeler against two baselines: a per frame CNN pose detector (PF) used as the encoder part of our $\auto$ model, and a dynamic programming approach over multiple body pose hypotheses per frame (VITERBI) similar in spirit to \cite{DBLP:conf/iccv/ParkR11,Batra:2012:DMS:2403138.2403140}. For our VITERBI baseline, we consider for each body joint in each frame all possible grid locations and encode temporal smoothness as the negative exponential of the Euclidean distance between the locations of the same body joint  across consecutive frames.  The intuition behind VITERBI is that temporal smoothness will help rule out isolated, bad pose estimates, by promoting ones that have lower per frame scores, yet are more temporally coherent.   

%We use such an approach as one of our baselines (VITERBI). We further compare against a per frame CNN pose detector (PF), used as the encoder part of our $\auto$ model. 
We evaluate our model and baselines by recording the highest scoring pixel location for each frame and  body joint. We compute the percentage of detected joints within a tolerance radius of a circle centered at the ground-truth body joint locations, for various  tolerance thresholds. We normalize the tolerance radii with the distance between left hip and right shoulder. This is the standard evaluation metric for static image pose labeling \cite{MODEC}. We show pose labeling performance curves  in Figure \ref{fig:bjtquant}. For a video comparison between ERD and the per frame CNN detector, please see the video at { \tt https://sites.google.com/site/motionlstm/}.
% \href{https://youtu.be/E6oMStetO7E}{online video labeling video}.%Our $\auto$ pose labeler  outperforms  both the per frame pose estimator (PF), as well as the non discriminative  dynamic programming pose smoother (VITERBI). This shows the importance of 
discriminatively learning to integrate temporal information  for body joint tracking, instead of employing generic motion smoothness priors.  %Previous pose smoothers do not look at the pixel information during temporal pose integration but rather at the pose output samples, same as our VITERBI baseline. $\auto$'s performance boost stems from correcting left and right confusions of the per frame part detector, as Figure \ref{fig:leftright} qualitatively illustrates.  
Left and right confusion is a major challenge for per frame part detectors, to the extent that certain works measure their performance in image centric coordinates, rather than object centric 
%, and do not attempt to  estimate  correct body orientation \cite{YangR_CVPR_2011,MODEC}. Last, VITERBI is marginally better than the per frame CNN detector. While motion coherence proved important when combined with shallow and inaccurate per frame body pose detectors \cite{DBLP:conf/iccv/ParkR11,Batra:2012:DMS:2403138.2403140}, it does not improve much upon stronger  multilayer CNNs.

\begin{figure*}[ht]
\begin{center}
\includegraphics[trim=0.0in 1in 0in 0.5in, scale=0.29]{figs/Forecasting2.pdf}
\end{center}
\caption{ \textbf{Video pose forecasting.} Quantitative comparison between the $\ERD$ model, a zero motion (NM), and constant velocity (OF) models. $\ERD$ outperforms the baselines for the lower body limbs, which are frequently occluded and thus their per frame motion is not frequently observed using optical flow. }
\label{fig:forecasting}
\end{figure*}Figure \ref{fig:pretraining} compares ERD training and test losses during finetuning  the encoder from (the first five layers of) our  per frame CNN pose detector, versus training the encoder from scratch (random weights). CNN encoder's  initialization is crucial to reach a good solution.  %This is not the case for our mocap synthesis $\auto$, where random weight  initialization was shown to be equally effective to initializing from mocap autoencoder weights.  We further compare our video labeler in a subset of 200 video sequences of around 50 frames each from the FlicMotion dataset of \cite{DBLP:conf/accv/JainTLB14,MODEC} that we annotated  densely in time with person bounding boxes. We used 170 video sequences for training and 30 for testing. We show performance curves for the upper body joints in Figure \ref{fig:flic}. VITERBI has similar performance as in H3.6M, marginally exceeding the per frame CNN detector. However $\auto$ does much worse since the training set is too small to learn effectively. Finetuning from the model learnt from H3.6M did not help since H3.6M concerns full body motion while FlicMotion captures upper body only.   We did not change the architecture in comparison to the ERD used in H3.6M. It is probable that a smaller recurrent layer and decoder would improve performance preventing overfitting. Large training sets such as in H3.6M allow high capacity discriminative temporal smoothers as our video labelled $\auto$ to outperform generic motion smoothness priors  for human dynamics. 

\begin{figure}[ht]
\begin{center}
\includegraphics[trim=1in 0in 0in 0in, scale=0.2]{figs/flicmotion.pdf}
\end{center}
%\vspace{-0.2in}
\caption{\textbf{Video pose labeling in FlicMotion.} 
$\auto$ does not succeed in learning effectively from the small set of 170 videos of about 50 frames each. Large training sets, such as those provided in H3.6M, are necessary for $\auto$ video labeler to outperform generic motion smoothness priors. }
\label{fig:flic}
\end{figure}% \begin{figure}[ht]
 \begin{center}
 \includegraphics[trim=0.0in 0.6in 0in 0in, scale=0.33]{figs/videoposeprediction.pdf}
 \end{center}
 %\vspace{-0.0in}
 \caption{\textbf{Video pose forecasting}  400ms in the future. \textit{Left:} the prediction of the body part detector 400ms before superimosed on the frame to predict pose for (zero motion model). \textit{MiddleLeft:} Predictions of the $\auto$. The body joints have been moved towards their correct location. \textit{MiddleRight:} The current and 400ms ahead frame superimposed. \textit{Right:} Ground-truth body joint location (discretized in a $N \times N$ heat map grid). In all cases we show the highest scoring heat map grid location. }
 \label{fig:videoprediction}
 \end{figure}%\vspace{-0.08in}\paragraph{Video pose forecasting}
We predict 2D  body joint locations  400ms ahead of the current frame. 
Figure \ref{fig:forecasting} shows pose forecasting performance curves for our $\ERD$ model, a model that assumes zero object and camera motion  (NoMotion-\textit{NM}), and a model that assumes constant optical flow within the prediction horizon (\textit{OF}).  
$\ERD$ carries out more accurate predictions than the zero order and first order motion baselines, as also shown qualitatively in Figure \ref{fig:videoprediction}.  
Optical flow based motion models cannot make reasonable predictions for occluded body joints, since their frame to frame displacements are not observed.  
%Since lower body limbs are very frequently occluded, optical flow based filters or smoothers cannot exploit long range temporal horizon, as well as 
Further, standard motion models suffer from separation of the observation model (part detector) and temporal aggregation, which $\auto$ combines into a single network.  

\paragraph{Discussion}
Currently, the mocap ERD performs  better on periodic activities (walking, smoking etc) in comparison to non periodic ones (sitting etc.).  Interesting directions for future research is predicting 3D angle differences from frame to frame as opposed to angles directly. Such transformation prediction may generalize better to new subjects, focusing more on  motion rather than  appearance of the skeleton. We are also investigating using large frame volumes as input to our video prediction ERDs with spatio-temporal convolutions in CONV1 as opposed to a single frame LSTM, in order to exploit short temporal horizon more effectively. 
%Different pose encoders in place of our 2 scale CNN are also worth investigating
    \section{Conclusion}%\vspace{-0.03in}
We have presented end-to-end  discriminatively trained encoder-recurrent-decoder models for modeling human kinematics in videos and motion capture. $\auto$s   learn the representation for recurrent prediction or labeling, as well as its dynamics, by jointly training encoder recurrent and decoder networks. 
%We have shown such representation learning is crucial for empirical performance of RNNs in the spatio-temporal visual domain. 
Such expressive models of human dynamics come at a cost of increased need for training examples. In future work, we plan to explore semi-supervised models in this direction, as well learning human dynamics   in multi-person interaction scenarios.   

%High capacity models for capturing human dynamics come at a cost of large number of training examples %and, and we hope to see more video dataset efforts in this direction. %Though the proposed models are motivated for person tracking,%we expect that similar models can be employed in a wide variety of visual tracking and forecasting tasks, and plan to explore wider applications of the ERD model in future work.% We have presented end-to-end  discriminatively trained encoder-recurrent-decoder models for tracking and forecasting human kinematics in videos and motion capture. $\auto$s  jointly learn the representation for recurrent prediction or labelling, as well as its dynamics by jointly training encoder recurrent and decoder networks. We have shown such representation learning is crucial for empirical performance of RNNs in the spatio-temporal visual domain, for tasks of motion generation and body pose labeling and forecasting in videos. This finding distinguishes the visual domain from the 1D language,text or speech domains, where straightforward representations, such as one hot encoding word vectors, have shown excellent performance when combined with RRNs, e.g., int he tasksfor machine translation or text prediction. % Though the proposed models are motivated for person tracking, similar models can be employed in any visual tracking or forecasting tasks with different type of landmarks.%marries recurrence and dimensionality reduction by incorporating encoder and decoder parts with a recurrent layer jointly finetuned to the task at hand. We show recurrent kinematic tracking  improves upon per frame CNN body part predictor by disambiguating object centric left and right arrangements. We show we can generate mocap sequences with variety of activity and style of execution. Last, we show we can predict body joint in video, necessary for timely human computer interaction. \paragraph{Acknowledgements}
We would like to thank Jeff Donahue and Philipp Kr\"ahenb\"uhl for useful discussions. 
We  gratefully acknowledge NVIDIA corporation for the donation of K40 GPUs for this research. This research was funded by ONR MURI N000014-10-1-0933.
 




{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
