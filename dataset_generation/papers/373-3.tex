\documentclass[../main.tex]{subfiles}
\begin{document}

\onecolumn
\noindent{\Huge Supplementary Material}
\section{Crowd Generated Hypothesis Space}
In the main paper, our \EXPLAIN algorithm using the CNN generated hypothesis space performed poorly on the Chinese Characters dataset.
To address this problem, we collected an alternative data embedding that better matches the learner's notion of similarity. 
Using a similar grid-based interface to \cite{gomes2011crowdclustering,kim2018context} with a $4\times6$ image grid, workers on Mechanical Turk were asked to group images of the Chinese Characters based on their appearance.
They were not given any information regarding the ground truth class labels. 
We then converted these groupings into a set of triplet constraints \ie image A is closer to B, compared to C, if A and B were placed in the same cluster by a given worker and C was in a different cluster.
In total, this resulted in 414,160 triplets from 51 workers. 
We used t-STE~\cite{van2012stochastic} to generate a 5D embedding of the images which was then used to construct our hypothesis space. 
This 5D space is used as an alternative to the CNN features from the main paper.
For the hypothesis generation, we used the same strategy outlined in the main paper, along with the same values for the hyperparameters $\alpha$, $\beta$, and $\gamma$, the same duration of teaching and testing, and the same train and test split.
For visualization purposes, a 2D embedding of the images can be viewed in \figref{fig:chinese_embed}.

Another potential weakness of our fully automatic approach is the automatic estimation of difficulty for each explanation $e$. 
In the main paper, we proposed using the entropy of the explanation image as a proxy for the difficulty, $\diff(e)$, that a learner may have in interpreting it.
As an alternative, here, we manually inspected each of the 717 explanations for the Chinese Characters dataset and labeled 589 of them as being `good' explanations and the rest as `bad'.
An explanation was deemed `bad' if it did not clearly highlight the discriminative regions for that particular class \eg the first image in \figref{fig:chinese_examples} A).
In practice, we set a difficulty value of $0$ for `good' explanations and $1000$ for `bad' ones. 
\EXPLAIN can still potentially select images with bad explanations, but the high cost discourages their selection. 

We ran an additional experiment on Mechanical Turk with the crowd generated hypothesis space and expert curated explanation difficulties. 
For comparison, we also measured the performance of \STRICT using the same hypothesis space.
As \RANDIM and \RANDEXP do not depend on the hypothesis space the results from the main paper for these strategies can be directly compared.  
From a total of 81 learners, where each individual was randomly assigned to one of the two strategies, we observed a test time average accuracy of 65.44\% for \EXPLAIN and 53.06\% for \STRICT.
This is an improvement for \EXPLAIN compared to the 46.35\% test accuracy achieved with the hypothesis space generated from CNN features.
Again, we qualitatively observe that \STRICT has a tendency to sometimes select outliers, but it still performs better than random image selection.


\begin{figure*}[h!]
    \centering
    \subfigure[CNN Embedding]{  
        \centering
        \includegraphics[trim={10px 0px 10px 25px},clip,width=.32\textwidth]{figures/cc_cnn.pdf}
    }~
    \subfigure[Crowd Embedding]{
        \centering
        \includegraphics[trim={10px 0px 10px 25px},clip,width=.32\textwidth]{figures/cc_crowd.pdf}
    }~
     \subfigure[Explanation Difficulty]{
       \centering
        \includegraphics[trim={10px 0px 10px 25px},clip,width=.32\textwidth]{figures/cc_difficult.pdf}
    }
    \caption{CNN versus crowd embeddings for the Chinese Characters dataset. A) 2D embedding generated with t-SNE~\cite{maaten2008visualizing} of our 64D CNN features. B) Embedding generated with t-STE~\cite{van2012stochastic} from crowd triplet based similarity constraints. C) The red circles depict manually filtered `bad' explanations. We observe that these bad explanations are not necessarily correlated with difficult examples \ie, examples that are located between classes.}
    \label{fig:chinese_embed}
\end{figure*}





%
%
%
\section{Sample Images and Explanations}
In Figs. \ref{fig:butterflies}, \ref{fig:oct}, and \ref{fig:chinese_examples} we display some randomly selected images along with their corresponding explanations for each of the three datasets used in the main paper.


\begin{figure*}
\centering

    \subfigure[Monarch]{  
        \centering
        \includegraphics[width=0.95\textwidth]{supp_figures/Monarch.png}
    }\\
    \subfigure[Viceroy]{
        \centering
        \includegraphics[width=0.95\textwidth]{supp_figures/Viceroy.png}
    }\\
     \subfigure[Queen]{
       \centering
        \includegraphics[width=0.95\textwidth]{supp_figures/Queen.png}
    }\\
        \subfigure[Red Admiral]{
        \centering
        \includegraphics[width=0.95\textwidth]{supp_figures/Red_Admiral.png}
    }\\
    \subfigure[Cabbage White]{
        \centering
        \includegraphics[width=0.95\textwidth]{supp_figures/Cabbage_White.png}
    }


\caption{Randomly selected images and their corresponding explanations for each class from the Butterflies dataset. The second last Viceroy image depicts a poor explanation as the butterfly is in an unusual pose.}
\label{fig:butterflies}
\end{figure*}



\begin{figure*}
\centering

    \subfigure[Macular Edema]{  
        \centering
        \includegraphics[width=0.95\textwidth]{supp_figures/Macular_edema.png}
    }\\
    \subfigure[Normal]{
        \centering
        \includegraphics[width=0.95\textwidth]{supp_figures/Normal_drusen.png}
    }\\
     \subfigure[Subretinal Fluid]{
       \centering
        \includegraphics[width=0.95\textwidth]{supp_figures/Subretinal_fluid.png}
    }

\caption{Randomly selected images and their corresponding explanations for each class from the OCT Eyes dataset.}
\label{fig:oct}
\end{figure*}



\begin{figure*}
\centering

    \subfigure[Grass]{  
        \centering
        \includegraphics[width=0.95\textwidth]{supp_figures/grass.png}
    }\\
    \subfigure[Mound]{
        \centering
        \includegraphics[width=0.95\textwidth]{supp_figures/mound.png}
    }\\
     \subfigure[Stem]{
       \centering
        \includegraphics[width=0.95\textwidth]{supp_figures/stem.png}
    }

\caption{Randomly selected images and their corresponding explanations for each class from the Chinese Characters dataset. We can see that the CNN generated explanations tend to point to the same part of the image for each class. The first image for Grass is an interesting failure case. This particular character is quite different from the other instances of the class and as a result the explanation is poor.}
\label{fig:chinese_examples}
\end{figure*}



\end{document}
