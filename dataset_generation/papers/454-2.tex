\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
% \usepackage{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{booktabs}       % professional-quality tables
\usepackage{multirow}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{algpseudocode,algorithm,algorithmicx}
\usepackage{xspace}
\usepackage{pifont}
\usepackage{graphicx}
\usepackage{wrapfig}
\graphicspath{{figures/}}
\usepackage[dvipsnames]{xcolor}
\usepackage{lipsum}

\newcommand{\cmark}{{\color{ForestGreen}\ding{51}}}
\newcommand{\xmark}{{\color{BrickRed}\ding{55}}}

% declare last
\usepackage{url}            % simple URL typesetting
\usepackage{hyperref}       % hyperlinks
\hypersetup{breaklinks=true,colorlinks}

\newcommand{\SK}[1]{{\color{NavyBlue}{[@Seung: #1]}}}
\newcommand{\MT}[1]{{\color{RoyalPurple}{[@Makarand: #1]}}}
\newcommand{\SF}[1]{{\color{BrickRed}{[@Sanja: #1]}}}

\def\E{\mathcal{E}}
\def\Mobj{M_\mathrm{obj}}
\def\Matt{M_\mathrm{att}}
\def\Mrel{M_\mathrm{rel}}
\def\Mcnt{M_\mathrm{cnt}}
\def\Mcap{M_\mathrm{cap}}
\def\Mvqa{M_\mathrm{vqa}}
\newcommand{\rMrel}[1]{M_{\mathrm{rel}_{#1}}}
\newcommand{\rMcnt}[1]{M_{\mathrm{cnt}_{#1}}}
\newcommand{\rMcap}[1]{M_{\mathrm{cap}_{#1}}}
\newcommand{\rMvqa}[1]{M_{\mathrm{vqa}_{#1}}}


%%%% COPIED FROM CVPR/ICCV STYLES %%%%
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Progressive Multi-task Learning by Module Communication\\
\textsc{Supplementary Material}}

\author{
  Seung Wook Kim \hspace{0.5cm} Makarand Tapaswi \hspace{0.5cm} Sanja Fidler \\
  Department of Computer Science\\
  University of Toronto\\
  \texttt{\{seung,makarand,fidler\}@cs.toronto.edu} \\
}

\begin{document}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%!TEX root = paper.tex\section{Module Architectures}\label{sec:appendix_module_details}
We discuss the detailed architecure of each module. We first describe the shared environment and soft attention mechanism architecture.

\textbf{Environment.}\hspace{2mm}
The sensory input that form our environment $\E$ consists of:
{\bf (i)}\emph{image regions}: $N$ image regions $X=[X_1, \ldots, X_N]$, each $X_i \in \mathbb{R}^{d}$ with corresponding bounding box coordinates $\mathbf{b}=[b_1, \ldots, b_N]$ extracted from Faster R-CNN~\citep{ren15}; and
{\bf (ii)}\emph{language}: vector representation of a sentence $S$ (in our example, a question).
$S$ is computed through a one layer GRU by feeding in the embedding of each word $[w_1,\ldots,w_T]$ at each time step.
For (i), we use a pretrained model from~\citet{anderson17} to extract features and bounding boxes.

\textbf{Soft attention.}\hspace{2mm}
For all parts that use soft-attention mechanism, an MLP is emloyed.
Given some \emph{key vector}$k$ and a set of data to be attended $\{d_1,\ldots,d_N\}$, we compute
\begin{equation}
\mathrm{attention\_map} = (z(f(k)\cdot g(d_1)), \ldots, z(f(k)\cdot g(d_N)))
\end{equation}
where $f$ and $g$ are a sequence of linear layer followed by ReLU activation function that project $k$ and $d_i$ into the same dimension, and $z$ is a linear layer that projects the joint representation into a single number.
Note that we do not specify softmax function here because sigmoid is used for some cases.

\subsection{Object and Attribute Classification (Level 0)}
The input to both modules $\Mobj, \Matt$ is a visual descriptor for a bounding box $b_i$ in the image, \ie,~$q_\mathrm{obj} = X_i$.
$\Mobj$ and $\Matt$ projects the visual feature $X_i$ to a 300-dimensional vector through a single layer neural network followed by $\mathrm{tanh}()$ non-linearity.
We expect this vector to represent the name and attributes of the box $b_i$.
%\SK{add a sentence like: We expect this vector to represent the word embedding of the class labels.%Is the classifier layer made up of the GloVe embeddings? That ensures it, would be nice to say. This is also how a vector output from these modules can still be converted to a word easily.}% - the embedding layer is not used anymore.\subsection{Image Captioning (Level 1)}$\Mcap$ takes zero vector as the model input and produces natural language sentence as the output based on the environment $\E$ (detected image regions in an image).
It has $\mathcal L_\mathrm{cap} = [\Omega_{\mathrm{cap}}, \Mobj, \Matt, \Delta_{\mathrm{cap}}]$ and goes through maximum of $T_\mathrm{cap}=16$ time steps or until it reaches the end of sentence token.
$\Mcap$ is implemented similarly to the captioning model in ~\citet{anderson17}.
We employ two layered GRU~\citep{cho14} as the recurrent state update function $U_\mathrm{cap}$ where $s^t = (h^t_1, h^t_2)$ with hidden states of the first and second layers of $U_\mathrm{cap}$. Each layer has 1000-d hidden states.


The state initializer $I_\mathrm{cap}$ sets the initial hidden state of $U_\mathrm{cap}$, or the model state $s^t$, as a zero vector.
For $t$ in $T_{\mathrm{cap}}=16$, $M_\mathrm{cap}$ does the following four operations:
\vspace{-2mm}\begin{enumerate}[\hspace{0pt}(1)]
% \setlength\itemsep{-1mm}
\item{
The importance function $G_\mathrm{cap}$ is executed. It is implemented as a linear layer $\mathbb{R}^{1000} \to \mathbb{R}^4$ (for the four modules in $\mathcal L_\mathrm{cap}$) that takes $s^t$, specifically $h_1^t \in s^t$ as input.
} 

\item{
$Q_{\mathrm{cap}\rightarrow \mathrm{\Omega}}$ passes $h_1^t$ to the attention module $\Omega_\mathrm{cap}$ which attends over the image regions $X$ with $h_1^t$ as the key vector.
$\Omega_\mathrm{cap}$ is implemented as a soft-attention mechanism so that it produces attention probabilities $p_i$ (via softmax) for each image feature $X_i \in \E$.
The returned attention map $v_\mathrm{\Omega}$ is added to the scratch pad $V$.
} 

\item{
$Q_{\mathrm{cap}\rightarrow\mathrm{obj}}$ and $Q_{\mathrm{cap}\rightarrow\mathrm{att}}$ pass the sum of visual features $X$ weighted by $v_\mathrm{\Omega} \in V$ to the corresponding modules.
$\Delta_\mathrm{cap}$ is implemented as an MLP.
The receivers project the outputs into 1000 dimensional vectors $v_\mathrm{obj}$, $v_\mathrm{att}$, and $v_\mathrm{\Delta}$ through a sequence of linear layers, batch norm, and $\mathrm{tanh}()$ nonlinearities.
They are added to $V$.
} 

\item{
As stated above, $U_\mathrm{cap}$ is a two-layered GRU.
At time $t$, the first layer takes input the average visual features from the environment $\E$, $\frac{1}{N}\sum_{i}X_i$, embedding vector of previous word $w_{t-1}$, and $h^t_2$.
For time $t=1$, \emph{beginning-of-sentence} embedding and zero vector are inputs for $w_{1}$ and $h^{1}_1$, respectively.
The second layer is fed $h^t_1$ as well as the information from other modules,
\begin{equation}
\rho = \sum (\mathrm{softmax}(g_\mathrm{obj}, g_\mathrm{att}, g_{\Delta}) \cdot (v_\mathrm{obj}, v_\mathrm{att}, v_\mathrm{\Delta}))
\end{equation}
which is a gated summation of outputs in $V$ with softmaxed importance scores.
We now have a new state $s^{t+1} = (h^{t+1}_1, h^{t+1}_2)$.
} 
\end{enumerate}

The output of $\Mcap$, $o_\mathrm{cap}$, is a sequence of words produced through $\Psi_\mathrm{cap}$ which is a linear layer projecting each $h^t_2$ in $s^t$ to the output word vocabulary.

\subsection{Relationship Detection (Level 1)}
Relationship detection task requires one to produce triplets in the form of ``subject - relationship - object''~\citep{lu2016visual}.
We re-purpose this task as one that involves finding the relevant item (region) in an image that is related to a given input through a given relationship.
The input to the module is $q_\mathrm{rel}=[b_i, r]$ where $b_i$ is a one-hot encoded input bounding box (whose $i$-th entry is 1 and others 0) and $r$ is a one-hot encoded relationship category (\eg~above, behind).
$\Mrel$ has $\mathcal L_\mathrm{rel} = [\Mobj, \Matt, \Delta_{\mathrm{rel}}]$ and goes through $T_{rel}=N$ steps where $N$ is the number of bounding boxes (image regions in the environment).
So at time step $t$, the module looks at the $t$-th box.
$\Mrel$ uses $\Mobj$ and $\Matt$ just as feature extractors for each bounding box.
Therefore, it does not have a complex structure.

The state initializer $I_\mathrm{rel}$ projects $r$ to a 512 dimensional vector with an embedding layer, and the resulting vector is set as the first state $s^1$.

For $t$ in $T_{\mathrm{rel}}=N$, $M_\mathrm{rel}$ does the following three operations:

\begin{enumerate}[\hspace{0pt}(1)]

\item{
$Q_{\mathrm{rel}\rightarrow\mathrm{obj}}$ and $Q_{\mathrm{rel}\rightarrow\mathrm{att}}$ pass the image vector corresponding to the bounding box $b_t$ to $\Mobj$ and $\Matt$.
$R_{\mathrm{obj}\rightarrow\mathrm{rel}}$ and $R_{\mathrm{att}\rightarrow\mathrm{rel}}$ are identity functions, \ie, we do not modify the object and attribute vectors.
The outputs $v_\mathrm{obj}$ and $v_\mathrm{att}$ are added to $V$.
} 

\item{
$\Delta_\mathrm{rel}$ projects the coordinate of the current box $b_t$ to a 512 dimensional vector.
This resulting $v_\mathrm{\Delta}$ is added to $V$.
} 

\item{
$U_\mathrm{rel}$ concatenates the visual feature $X_t$ with $v_\mathrm{obj}, v_\mathrm{att}, v_\mathrm{\Delta}$ from $V$.
The concatenated vector is fed through a MLP resulting in 512 dimensional vector.
This corresponds to the new state $s^{t+1}$.
} 
\end{enumerate}
After $N$ steps, the prediction function $\Psi_\mathrm{rel}$ does the following operations:
\\
The first state $s^1$ which contains relationship information is multiplied element-wise with $s^{i+1}$ (Note: $s^{i+1}$ corresponds to the input box $b_i$).
Let such a vector be $l$.
It produces an attention map $b_\mathrm{out}$ over all bounding boxes in $b$.
The inputs to the attention function are $s^2,\ldots,s^{T_\mathrm{rel}}$ (i.e. all image regions) and the key vector $l$.
$o_\mathrm{rel} = b_\mathrm{out}$ is the output of $\Mrel$ which represents an attention map indicating the bounding box that contains the related object.

\subsection{Counting (Level 2)}
Given a vector representation of a natural language question (\eg~how many cats are in this image?), the goal of this module is to produce a count.
The input $q_\mathrm{cnt} = S \in \E$ is a vector representing a natural language question.
When training $\Mcnt$, $q_\mathrm{cnt}$ is computed through a one layer GRU with hidden size of 512 dimensions.
The input to the GRU at each time step is the embedding of each word from the question.
Word embeddings are initialized with 300 dimensional GloVe word vectors~\citep{pennington14} and fine-tuned thereafter.
Similar to visual features obtained through CNN, the question vector is treated as an environment variable.
$\Mcnt$ has $\mathcal L_\mathrm{cnt} = [\Omega_\mathrm{cnt}, \Mrel]$ and goes through only one time step.

The state initializer $I_\mathrm{cnt}$ is a simple function that just sets $s^1 = q_\mathrm{cnt}$.

For $t$ in $T_{\mathrm{cnt}}=1$, $M_\mathrm{cnt}$ does the following four operations:

\begin{enumerate}[\hspace{0pt}(1)]
\item{
The importance function $G_\mathrm{cnt}$ is executed. It is implemented as a linear layer $\mathbb{R}^{512} \to \mathbb{R}^2$ (for the two modules in $\mathcal L_\mathrm{cnt}$) that takes $s^t$ as input.
} 

\item{
$Q_{\mathrm{cnt}\rightarrow \mathrm{\Omega}}$ passes $s^t$ to the attention module $\Omega_\mathrm{cnt}$ which attends over the image regions $X$ with $s^t$ as the key vector.
$\Omega_\mathrm{cnt}$ is implemented as an MLP that computes a dot-product soft-attention similar to ~\citet{yang16san}. The returned attention map $v_\mathrm{\Omega}$ is added to the scratch pad $V$.
} 

\item{
$Q_{\mathrm{cnt}\rightarrow \mathrm{rel}}$ produces an input tuple $[b, r]$ for $\Mrel$.
The input object box $b$ is produced by a MLP that does soft attention on image boxes, and the relationship category $r$ is produced through a MLP with $s^t$ as input.
$\Mrel$ is called with $[b, r]$ and the returned map $v_\mathrm{rel}$ is added to $V$.
} 

\item{
$U_\mathrm{cnt}$ first computes probabilities of using $v_\mathrm{\Omega}$ or $v_\mathrm{rel}$ by doing a softmax over the importance scores.
$v_\mathrm{\Omega}$ and $v_\mathrm{rel}$ are weighted and summed with the softmax probabilities resulting in the new state $s^2$ containing the attention map.
Thus, the state update function chooses the map from $\Mrel$ if the given question involves in relational reasoning.
} 

\end{enumerate}

The prediction function $\Psi_\mathrm{cnt}$ returns a count vector.
The count vector is computed through the counting algorithm by ~\citet{zhang18}, which builds a graph representation from attention maps to count objects.
The method uses $s^2$ through a sigmoid and bounding box coordinates $b$ as inputs.
~\citet{zhang18} is a fully differentiable algorithm and the resulting count vector corresponds to one-hot encoding of a number.
We let the range of count be 0 to 12 $\in \mathbb{Z}$.
Please refer to~\citet{zhang18} for details of the counting algorithm.

\subsection{Visual Question Answering (Level 3)}
The description for the VQA task (Sec.~\ref{sec:example_vqa}) is included here again for completeness.
The input $q_\mathrm{vqa}$ is a vector representing a natural language question (\ie. the sentence vector $S \in \E$).
The state variable $s^t$ is represented by a tuple $(q_{\mathrm{vqa}}^t, k^{t-1})$ where $q_{\mathrm{vqa}}^t$ represents query to ask at time $t$ and $k^{t-1}$ represents knowledge gathered at time $t-1$.
The state initializer $I_\mathrm{vqa}$ is composed of a GRU with hidden state dimension 512.
The first input to GRU is $q_\mathrm{vqa}$, and $I_\mathrm{vqa}$ sets $s^1 = (q_\mathrm{vqa}^1, \textbf{0})$ where $q_\mathrm{vqa}^1$ is the first hidden state of the GRU and $\textbf{0}$ is a zero vector (no knowledge at first).

For $t$ in $T_{\mathrm{vqa}}=2$, $M_\mathrm{vqa}$ does the following seven operations:
\vspace{-2mm}\begin{enumerate}[\hspace{0pt}(1)]
% \setlength\itemsep{-1mm}
\item{
The importance function $G_\mathrm{vqa}$ is executed. It is implemented as a linear layer $\mathbb{R}^{512} \to \mathbb{R}^7$ (for the seven modules in $\mathcal L_\mathrm{vqa}$) that takes $s^t$, specifically $q_\mathrm{vqa}^t \in s^t$ as input.
} 

\item{
$Q_{\mathrm{vqa}\rightarrow \mathrm{\Omega}}$ passes $q_{\mathrm{vqa}}^t$ to the attention module $\Omega_\mathrm{vqa}$ which attends over the image regions $X$ with $q_{\mathrm{vqa}}^t$ as the key vector.
$\Omega_\mathrm{vqa}$ is implemented as an MLP that computes a dot-product soft-attention similar to ~\citet{yang16san}. The returned attention map $v_\mathrm{\Omega}$ is added to the scratch pad $V$.
} 

\item{
$Q_{\mathrm{vqa}\rightarrow \mathrm{rel}}$ produces an input tuple $[b, r]$ for $\Mrel$.
The input object box $b$ is produced by a MLP that does soft attention on image boxes, and the relationship category $r$ is produced through a MLP with $q_{\mathrm{vqa}}^t$ as input.
$\Mrel$ is called with $[b, r]$ and the returned map $v_\mathrm{rel}$ is added to $V$.
} 
%\SK{hm.. $o_{rel}$? Either is ok, but let's stick to same everywhere. In that case, need to change $m_\Omega$ to $o_\Omega$ too}

\item{
$Q_{\mathrm{vqa}\rightarrow\mathrm{obj}}$, $Q_{\mathrm{vqa}\rightarrow\mathrm{att}}$, and $Q_{\mathrm{vqa}\rightarrow\mathrm{\Delta}}$ first compute a joint attention map $m$ as summation of $(v_\mathrm{\Omega}, v_\mathrm{rel})$ weighted by the softmaxed importance scores of $(\Omega_\mathrm{vqa}, \Mrel)$,
and they pass the sum of visual features $X$ weighted by $m$ to the corresponding modules.
$\Delta_\mathrm{vqa}$ is implemented as an MLP.
The receivers project the outputs into 512 dimensional vectors $v_\mathrm{obj}$, $v_\mathrm{att}$, and $v_\mathrm{\Delta}$ through a sequence of linear layers, batch norm, and $\mathrm{tanh}()$ nonlinearities. They are added to $V$.
%The receivers project $o_\mathrm{obj}$, $o_\mathrm{att}$, and $\delta_\mathrm{vqa}$ into 512 dimensional vectors $v_\mathrm{obj}$, $v_\mathrm{att}$, and $v_\mathrm{\Delta}$ through a sequence of linear layers, batch norm, and $\mathrm{tanh}()$ nonlinearities. They are added to $V$.
} 

\item{
$Q_{\mathrm{vqa}\rightarrow \mathrm{cnt}}$ passes $q_\mathrm{vqa}^t$ to $\Mcnt$ which returns $o_\mathrm{cnt}$.
$R_{\mathrm{cnt}\rightarrow\mathrm{vqa}}$ projects the count vector $o_\mathrm{cnt}$ into a 512 dimensional vector $v_{cnt}$ through the same sequence of layers as above.
$v_\mathrm{cnt}$ is added to $V$.
} 

\item{
$\Mvqa$ calls  $\Mcap$ and
% $Q_{\mathrm{vqa}\rightarrow\mathrm{cap}}$ calls $\Mcap$.
$R_{\mathrm{cap}\rightarrow\mathrm{vqa}}$ receives natural language caption of the image.
It converts words in the caption into vectors $[w_1,\ldots,w_T]$ through an embedding layer.
The embedding layer is initialized with 300 dimensional GloVe vectors~\citep{pennington14} and fine-tuned.
It does softmax attention operation over $[w_1,\ldots,w_T]$ through a MLP with $q_{\mathrm{vqa}}^t \in s_t$ as the key vector, resulting in word probabilities $p_1,\ldots,p_T$.
The sentence representation $\sum_{i}^{T} p_i \cdot w_i$ is projected into a 512 dimensional vector $v_\mathrm{cap}$ using the same sequence as $v_\mathrm{cnt}$. $v_\mathrm{cap}$ is added to $V$.
} 

\item{
The state update function $U_\mathrm{vqa}$ first does softmax operation over the importance scores of $(\Mobj$, $\Matt$, $\Delta_\mathrm{vqa}$, $\Mcnt$, $\Mcap)$.
We define an intermediate knowledge vector $k^t$ as the summation of $(v_\mathrm{obj}, v_\mathrm{att}, \delta_\mathrm{vqa}, v_\mathrm{cnt}, v_\mathrm{cap})$ weighted by the softmaxed importance scores.
$U_\mathrm{vqa}$ passes $k^t$ as input to the GRU initialized by $I_\mathrm{vqa}$, and we get $q_\mathrm{vqa}^{t+1}$ the new hidden state of the GRU.
The new state $s^{t+1}$ is set to $(q_\mathrm{vqa}^{t+1}, k^t)$.
This process allows the GRU to compute new question and state vectors based on what has been \emph{asked} and \emph{seen}.
} 
\end{enumerate}
After $T_{\mathrm{vqa}}$ steps, the prediction function $\Psi_\mathrm{vqa}$ computes the final output based on the initial question vector $q_\mathrm{vqa}$ and all knowledge vectors $k^t \in s^t$.
Here, $q_\mathrm{vqa}$ and $k^t$ are fused with gated-tanh layers and fed through a final classification layer similar to ~\citet{anderson17}, and the logits for all time steps are added.
The resulting logit is the final output $o_\mathrm{vqa}$ that corresponds to an answer in the vocabulary of the VQA task.

\newpage\section{Additional Experimental Details}
In this section, we provide more details about datasets and module training.
% We also give more examples of execution traces of PMN on the visual question answering task.\label{sec:appendix_exp_detail}\subsection{Datasets}\label{subsec:appendix_datasets}
We extract bounding boxes and their visual representations using a pretrained model from~\citet{anderson17}which is a Faster-RCNN~\citep{ren15} based on ResNet-101~\citep{he16}.
It produces 10 to 100 boxes with 2048-d feature vectors for each region.
To accelerate training, we remove overlapping bounding boxes that are most likely duplicates (area overlap IoU > 0.7) and keep only the 36 most confident boxes (when available).

\textbf{MS-COCO}
contains $\sim$100K images with annotated bounding boxes and captions.
It is a widely used dataset used  for benchmarking several vision tasks such as captioning and object detection.
%, and many annotations are based on these images.\textbf{Visual Genome}
is collected to relate image concepts to image regions.
It has over 108K images with annotated bounding boxes containing 1.7M visual question answering pairs, 3.8M object instances, 2.8M attributes and 1.5M relationships between two boxes.
Since the dataset contains MS-COCO images, we ensure that we do not train on any MS-COCO validation or test images.

\textbf{VQA 2.0}
is the most popular visual question-answering dataset, with 1M questions on 200K natural images.
Questions in this dataset require reasoning about objects, actions, attributes, spatial relations, counting, and other inferred properties; making it
%This makes VQA 2.0
an ideal dataset for our visual-reasoning PMN.

\subsection{Training}\label{sec:appendix_training}
Here, we give training details of each module.
We train our modules sequentially, from low level to high level tasks, one at a time.
When training a higher level module, internal weights of the lower level modules are not updated, thus preserving their performance on the original task. We do train the weights of the residual module $\Delta$ and the attention module $\Omega$.
We train $I$, $G$, $Q$, $R$, $U$, %residue computer $\Delta$,
and $\Psi$, by allowing gradients to pass through the lower level modules.
Thus, while the existing lower modules are held fixed, the new module learns to communicate with them via the query transmitter $Q$ and receiver $R$.

\textbf{Object and attribute classification.}$\Mobj$ is trained to minimize the cross-entropy loss for predicting object class by including an additional linear layer on top of the module output.
$\Matt$ also include an additional linear layer on top of the module output, and is trained to minimize the binary cross-entropy loss for predicting attribute classes since one detected image region can contain zero or more attribute classes. We make use of 780K/195K train/val object instances paired with attributes from the Visual Genome dataset.
They are trained with the Adam optimizer at learning rate of 0.0005 with batch size 32 for 20 epochs.

\textbf{Image captioning.}$\Mcap$ is trained using cross-entropy loss at each time step (maximum likelihood).
Parameters are updated using the Adam optimizer at learning rate of 0.0005 with batch size 64 for 20 epochs.
We use the standard split of MS-COCO captioning dataset.

\textbf{Relationship detection.}$\Mrel$ is trained using cross-entropy loss on ``subject - relationship - object'' pairs with Adam optimizer with learning rate of 0.0005 with batch size 128 for 20 epochs. The pairs are extracted from the Visual Genome dataset that have both subject and object boxes overlapping with the ground truth boxes (IoU $>$ 0.7), resulting in 200K/38K train/val tuples.

\textbf{Counting.}$\Mcnt$ is trained using cross-entropy loss on questions starting with `how many' from the VQA 2.0 dataset. We use Adam optimizer with learning rate of 0.0001 with batch size 128 for 20 epochs.
As stated in the experiments section, we additionally create $\sim$89K synthetic questions to increase our training set by counting the object boxes and forming `how many' questions from the VG dataset (\eg~(Q: how many dogs are in this picture?, A:3) from an image containing three bounding boxes of dog).
We also sample relational synthetic questions from each training image from VG that are used to train only the module communication parameters when the relationship module is included.
We use the same 200K/38K split from the relationship detection task by concatenating `how many'+subject+relationship' or `how many'+relationship+object (\eg~how many plates on table?, how many behind door?).
The module communication parameters for $\Mrel$ in this case are $Q_{\mathrm{cnt}\rightarrow\mathrm{rel}}$ which compute a relationship category and the input image region to be passed to $\Mrel$.
To be clear, we supervise $q_\mathrm{rel}=[b_i,r]$ to be sent to $\Mrel$ by reducing cross entropy loss on $b_i$ and $r$.

\textbf{Visual Question Answering.}$\Mvqa$ is trained using binary cross-entropy loss on $o_\mathrm{vqa}$ with Adam optimizer with learning rate of 0.0005 with batch size 128 for 7 epochs. We empirically found binary cross-entropy loss to work better than cross-entropy which was also reported by ~\cite{anderson17}.
Unlike other modules whose parameters are fixed, we \emph{fine-tune} only the counting module because counting module expects the same form of input - embedding of natural language question.
The performance of counting module depends crucially on the quality of attention map over bounding boxes.
By employing more questions from the whole VQA dataset, we obtain a better attention map, and the performance of counting module increases from 50.0\% to 55.8\% with finetuning.
Since $\Mvqa$ and $\Mcnt$ exepect the same form of input, the weights of attention modules $\Omega_\mathrm{\{vqa,cnt\}}$ and query transmitters for the relationship module $Q_\mathrm{\{vqa,cnt\} \rightarrow rel}$ are shared.

\newpage\section{PMN Execution Illustrated}\label{sec:appendix_pmn_exec}
We provide more examples of the execution traces of PMN on the visual question answering task in Figure~\ref{fig:appendixb2}.
Each row in the figure corresponds to different examples. For each row in the figure, the left column shows the environment $\E$, the middle column shows the final answer \& visualizes step 3 in the execution process, and
the right column shows computed importance scores along with populated scratch pad.
\begin{figure}[!htb]
\includegraphics[width=\linewidth]{pmv-vqa-examples.pdf}
\vspace{-15mm}
\caption{
\small Example of \PMN's module execution trace on the VQA task.
Numbers in circles indicate the order of execution.
Intensity of gray blocks represents depth of module calls.
All variables including queries and outputs stored in $V$ are vectorized to allow gradients to flow
(\eg, caption is composed of a sequence of softmaxed $W$ dimensional vectors for vocabulary size $W$).
For $\Mcap$, words with higher intensity in red are deemed more relevant by $R_\mathrm{vqa}^\mathrm{cap}$.
}
\label{fig:appendixb2}
\vspace{-2mm}
\end{figure}\newpage\section{Examples of PMN's Reasoning}\label{sec:appendix_pmn_reasoning}
We provide more examples of the human evaluation experiment on interpretability of PMN compared with the baseline model in Figure~\ref{fig:appendixc}.

\begin{figure}[!htb]
\vspace{-2mm}
\includegraphics[width=\linewidth,trim=0 0 0 0]{supp/pmn_reasoning-crop.pdf}
\caption{\small Example of PMN's reasoning processes compared with the baseline given the question on the left. \cmark and \xmark~denote correct and wrong answers, respectively.}
\label{fig:appendixc}
\vspace{-2mm}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
{\small
\bibliographystyle{ieee}
\bibliography{refs}
}

\end{document}

