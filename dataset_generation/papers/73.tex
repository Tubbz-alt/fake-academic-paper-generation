
%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal, 12pt, onecolumn,draftclsnofoot]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.
\usepackage{color}
\usepackage{threeparttable}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{amsmath}
\interdisplaylinepenalty=2500
\usepackage{amssymb}
\usepackage{stfloats}
\usepackage{float}
\usepackage{lipsum}
\usepackage{multirow}
% \usepackage[pdftex,colorlinks,bookmarksopen,bookmarksnumbered,citecolor=red,urlcolor=red]{hyperref}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows, positioning}





% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
% \title{Deep Learning Based Damage Detection on Post-Hurricane Satellite Imagery}
\title{Detecting Damaged Buildings on Post-Hurricane Satellite Imagery Based on Customized Convolutional Neural Networks}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Quoc Dung Cao
        and~Youngjun~Choe$^{*}$% <-this % stops a space
\thanks{$^{*}$Corresponding author (Postal mailing address: 3900 E Stevens Way NE,
Seattle, WA 98195, USA; Telephone: +1 (206) 543-1427; FAX: +1 (206) 685-3072; E-mail address: ychoe@u.washington.edu).}
\thanks{Quoc Dung~Cao and Youngjun~Choe are with the Department of Industrial and Systems Engineering, University of Washington, Seattle, WA, USA.}}% <-this % stops a space
%\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}

% Christopher Haberland <haberc@uw.edu>
% to:	Tessa Schneider <tessajeanschneider@gmail.com>,
% Sean Andrew Chen <sean.a.chen@gmail.com>,
% Andrew Escay <andrew.escay@gmail.com>,
% An Yan <yanan15@uw.edu>,
% Valentina Staneva <vms16@uw.edu>
% ychoe@uw.edu

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers 
% comment to archive arxiv
% \markboth{IEEE Transactions on Geoscience and Remote Sensing,~Vol.~xx, No.~x, October~2018}%
% {Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
After a hurricane, damage assessment is critical to emergency managers and first responders so that resources can be planned and allocated appropriately. One way to gauge the damage extent is to detect and quantify the number of damaged buildings, which is traditionally done through driving around the affected area. This process can be labor intensive and time-consuming. In this paper, utilizing the availability and readiness of satellite imagery, we propose to improve the efficiency and accuracy of damage detection via image classification algorithms. From the building coordinates, we extract their aerial-view windows of appropriate size and classify whether a building is damaged or not. We demonstrate the result of our method in the case study of 2017 Hurricane Harvey.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
image classification, neural networks, emergency services, buildings %disaster, image classification, neural network, first response, emergency management
\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
\IEEEPARstart{W}{hen} a hurricane makes landfall, situational awareness is one of the most critical needs that emergency managers face before they can respond to the event. To assess the situation and damage, the current practice largely relies on emergency response crews and volunteers to drive around the affected area, which is also known as windshield survey. Another way to assess hurricane damage level is flood detection through synthetic aperture radar (SAR) images such as works at the Darthmouth Flood Observatory \cite{dfo}, or the damage proxy map to identify structures that may have been damaged at the Advanced Rapid Imaging and Analysis (ARIA) Project by Caltech and NASA \cite{aria}. SAR imagery is useful in terms of mapping different surface features, texture, or roughness pattern but is harder for laymen to interpret than optical sensor imagery. %It is also harder to collect SAR imagery at a large scale due to the requirement of specialized equipment. Hence, 
In this paper, we focus on using optical sensor imagery as a more intuitive way to analyze hurricane damage. From here onwards, we will refer to optical sensor imagery as `imagery'.

Recently, imagery produced by drones and satellites have started to help improve situational awareness from a bird's eye view, but the process still relies on human visual inspection which is generally time-consuming and unreliable during an evolving disaster. Computer vision techniques, therefore, prove to be particularly useful. Given the available imagery, our proposed method can automatically detect \textit{`Flooded/Damaged Building' vs `Undamaged Building'} on satellite imagery of an area affected by a hurricane. This could give the stakeholders useful information about the severity of the damage to plan for and organize the necessary resources. With decent accuracy and quick enough runtime, this process is expected to significantly reduce the time for building situational awareness and responding to hurricane-induced emergencies. 

The satellite imagery data used in the paper was captured by optical sensors with sub-meter resolution and preprocessed for orthorectification, atmospheric compensation, and pansharpening from the Greater Houston area before and after Hurricane Harvey in 2017 (Figure~\ref{fig:Harvey}). The damaged buildings were labeled by volunteers through crowd-sourcing. We then process, filter, and clean the dataset to ensure that it has higher quality and can be learned appropriately by the deep learning algorithm. 

% \begin{figure}[h]
% {\centering
% \includegraphics[height=1.75in]{images/Harvey2017.png} \\
% \centering\caption{\small{Affected areas during Hurricane Harvey in 2017. The green dots are coordinates with damaged buildings/roads tagged by volunteers.}}
% \label{fig:Harvey}
% }
% \end{figure}

% below is for submission only, use above for publication
\begin{figure}[h]{\centering
\includegraphics[height=2.5in]{Harvey2017.png} \\
\centering\caption{\small{Affected areas during Hurricane Harvey in 2017. The green dots are coordinates with damaged buildings/roads tagged by volunteers.}}
\label{fig:Harvey}
}
\end{figure}

Through this paper, we hope that other researchers can use the dataset to study and experiment with different uses of satellite imagery in disaster response. In addition, our framework can be applied to future hurricane events to improve damage assessment and resource planning. We also provide a pre-trained deep-learning architecture that achieves satisfactory result in terms of classification accuracy. It can facilitate transfer learning either in feature extraction, fine-tuning, or as a baseline model to speed up the learning process in future development/events with similar properties. 

The remaining of the paper is organized as follows. In Section ~\ref{sec:background}, we present a brief review of convolutional neural networks, machine learning-based damage detection work on post-hurricane satellite imagery, and challenges in processing satellite imagery. Section ~\ref{sec:method} describes our framework from collecting data to damage detection. Detailed implementation and discussion of results are shown in Section ~\ref{sec:case_study}. Finally, Section ~\ref{sec:conclusion} concludes our work and draws some future research directions. 
% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol




% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.

% \vspace{-.3cm}

\section{Background}\label{sec:background}
\subsection{Convolutional neural network}
Object detection is a ubiquitous topic in computer vision, thanks to the development of convolutional neural network (CNN) \cite{cnn}. CNNs have proved to yield outstanding results over other algorithms in computer vision tasks such as natural language processing \cite{cnn-nlp}, object categorization \cite{cnn-object}, image classification \cite{cnn-image,cnn-imagenet}, or traffic sign recognition \cite{trafficSign}. Variations of CNN have been applied in remote sensing image processing tasks \cite{Zhang2016} such as aerial scene classification \cite{aid, aerial-label, scene-multiscale}, SAR imagery classification \cite{sar-cnn}, or object detection in unmanned aerial vehicle imagery \cite{Bazi2018}.

Structurally, CNN is a feed-forward network that is particularly powerful in extracting hierarchical features in images. The common structure of CNN has three components: the convolutional layer, the sub-sampling layer, and the fully connected layer as demonstrated in Figure~\ref{fig:cnn-archi}. 

\begin{figure*}[h]{\centering
\includegraphics[width=7in]{cnn-archi.png} \\
\centering\caption{A sample structure of convolutional neural networks, inspired by LeNet-5 architecture in \cite{cnn} ; C: Convolutional layer, S: Sub-sampling layer, F: Fully connected layer; 32@(148x148) means there are 32 filters to extract the features from the input image, and the original input size of 150x150 is reduced to 148x148 since no padding is added around the edges during $3\times3$ convolution operations so 2 edge rows and 2 edge columns are lost; 2x2 Max-pooling means the data will be reduced by a factor of 4 after each operation; Output layer has 1 neuron since the network outputs the probability of one class (`Flooded/Damaged Building'), and the other probability is just 1-$\mathbb{P}$r(`Flooded/Damaged Building').}
\label{fig:cnn-archi}
}
\end{figure*}

In the convolutional layer (C in Figure~\ref{fig:cnn-archi}), each element (or neuron) of the network in a layer receives information from a small region of the previous layer. A 3x3 convolutional filter will take a dot product of 9 weight parameters with 9 pixels (3x3 patch) of the input, transform the value by an activation function, and become a neuron value in the next layer. The same region can yield many information maps to the next layer through many convolutional filters. In Figure~\ref{fig:cnn-archi}, at convolutional layer C1, we have 32 filters that represent 32 ways to extract features from the previous layers and form a stack of 32 feature matrices. Another interesting property of CNN is its robustness to shifts and distortion of the input images \cite{shift-invariant}. This is crucial since in many datasets, the objects of interest are usually not positioned right at the center of the images and we want to learn the features, not their position. 

In the sub-sampling layer (S in Figure~\ref{fig:cnn-archi}), the network performs either local averaging or max pooling over a patch of the input. If the sub-sampling layer size is 2x2 such as S2, local averaging will produce the mean of the 4 nearby convoluted pixel values, whereas max pooling will produce the maximum number among them. Essentially, this operation reduces the input feature matrix to half its number of columns and rows, which helps to reduce the resolution by a factor of 4 and the network's sensitivity to shift and distortion. 

After the features are extracted and resolution reduced, the network will flatten the final stack of feature matrices into a feature vector and pass it through a sequence of fully connected layers (F in Figure~\ref{fig:cnn-archi}). Each subsequent layer's output neuron is a dot product between the feature vector and a weight vector, transformed by a non-linear activation function. In this paper, the last layer has only 1 neuron, which is the probability of a reference class (`Flooded/Damaged building'). 

As mentioned, the dot products are transformed by an activation function. This gives a neural network, with adequate size, the ability to model any function. Some common activation functions include sigmoid $f(x) = \frac{1}{1+e^{-x}}$, rectified linear unit (ReLU) $f(x) = max(0, x)$, or leaky ReLu $f(x) = max(\alpha x, x)$, with $0 < \alpha \ll 1$. There is no clear reason to choose any specific function over the others to improve performance of the network. However, using ReLU may speed up the training without affecting performance \cite{relu}.

\subsection{Machine learning-based damage detection on post-hurricane satellite imagery}
% Within the area of remote sensing, object detection on satellite imagery is a well-established research area. Variations of CNNs and other approaches were applied in different tasks and dataset, although the existing studies focus on detecting roads, buildings, trees, vehicles, ships, airplanes, or airports \cite{Cheng2016,Zhang2016}. Recurrent neural network \cite{Zhu2017} was used to classify different infrastructure types and agriculture crops. Xia et al. constructed the Aerial Image Dataset (AID), a large-scale dataset so that other researchers can benchmark their scene classification algorithms \cite{aid}. CNN and support vector machine were also applied to to unmanned aerial vehicle imagery, e.g. captured by drone \cite{Bazi2018}, in which a custom CNN was shown to outperform a pre-trained network. 

Within the area of remote sensing, object detection based on satellite imagery is a well-established research area. Nevertheless, few studies have investigated machine learning based damage detection on post-hurricane satellite imagery. A small project studied detecting \textit{flooded roads} by comparing pre-event and post-event satellite imagery \cite{Jack2017} but the method is not applicable to other types of damages. Two commercial vendors of satellite imagery also separately developed unsupervised algorithms to detect flooded area using spectral signature of impure water (which is not available from the pansharpened satellite images in our data) \cite{planet, gbd}. Before deep learning era, a method using a pattern recognition template set was applied to detect hurricane damages in \textit{multispectral} images \cite{Barnes2007} but the method is not applicable to our pansharpened images. 

\subsection{Challenges in damage detection from satellite imagery}
There are multiple challenges in damage detection from satellite imagery. First, the satellite imagery resolution of the object of interest is not as high as the various benchmark datasets commonly used to train neural networks (NNs) such as common objects (e.g ImageNet \cite{cnn-imagenet} or traffic signs \cite{trafficSign}. Dodge \& Karam studied the performance of NNs under quality distortions and highlighted that NNs could be prone to errors in blurry and noisy images \cite{Dodge2016}. Although our dataset is of relatively high quality, e.g., one of the satellites capturing the imagery is the GeoEye-1, which has 46cm panchromatic resolution \cite{geoeye}, it is still far from the resolution in animal or vehicle detection datasets. In fact, the labeling task is hard even with human visual inspection, which leads to another challenge. The volunteers' annotation could be erroneous. To limit this, the imagery provider has a proprietary system that computes the agreement score of each label. In this paper, we ignore this information to gather as many labels as possible and take the given labels as ground truth since limited size of training data could be a critical bottle-neck for models with many parameters to learn such as NNs. Third, there are some inconsistencies in image quality. Since the same region can be captured multiple times in different days, the same coordinate may have different quality and orthorectification, as shown in Figure~\ref{fig:orthorectification}. 

\begin{figure}[h]{\centering
\subfigure[Not orthorectified correctly]{\includegraphics[width=1.5in]{orthor1-1.png}}
\subfigure[Orthorectified correctly]{\includegraphics[width=1.5in]{orthor1-2.png}}
\subfigure[More blurry]{\includegraphics[width=1.5in]{orthor2-1.png}}
\subfigure[Less blurry]{\includegraphics[width=1.5in]{orthor2-2.png}}
\\
\caption{\small{Different orthorectification and processing quality  of the same location in different days}}
\label{fig:orthorectification}
}
\end{figure}
% \vspace{-.3cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Methodology
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}\label{sec:method}
In this section, we describe our end-to-end framework from collecting, processing, featurizing data to building the convolutional neural network to classify and detect damage. 

\subsection{Data description}
\begin{enumerate}
    \item The raw imagery data covering the Greater Houston area was captured in about four thousand strips ($\sim$ 400 million pixels ($\sim$ 1GB) with RGB bands per strip) in different days. Hence, some strips can overlap, leading to some images blacked out at the boundaries. In some days, the images are also covered fully or partially by clouds.
Figure~\ref{fig:strip} shows a typical strip in the data set and Figure~\ref{fig:bad_images} shows some examples of low quality images in 128x128 pixels that we choose to discard.

% Due to the big volume, it is not feasible to store the data in a local computer. The raw imagery data was downloaded and stored on a high performance computing cluster.

\begin{figure}[h]{\centering
\includegraphics[height=3.2in]{strip.png} \\
\centering\caption{\small{A typical strip of image}}
\label{fig:strip}
}
\end{figure}

% \begin{figure}[h]
% {\centering
% \subfigure[Blacked out partially]{\includegraphics[width=1.2in]{bad_images/bad1.jpeg}}
% \subfigure[Covered by cloud partially]{\includegraphics[width=1.2in]{bad_images/bad2.jpeg}}
% \subfigure[Covered by cloud mostly]{\includegraphics[width=1.2in]{bad_images/bad3.jpeg}}
% \subfigure[Covered by cloud totally]{\includegraphics[width=1.2in]{bad_images/bad4.jpeg}}
% \\
% \caption{\small{Examples of 128x128-pixel low quality images}}
% \label{fig:bad_images}
% }
% \end{figure}

\begin{figure}[h]{\centering
\subfigure[Blacked out partially]{\includegraphics[width=1.4in]{bad1.jpeg}}
\subfigure[Covered by cloud partially]{\includegraphics[width=1.4in]{bad2.jpeg}}
\subfigure[Covered by cloud mostly]{\includegraphics[width=1.4in]{bad3.jpeg}}
\subfigure[Covered by cloud totally]{\includegraphics[width=1.4in]{bad4.jpeg}}
\\
\caption{\small{Examples of 128x128-pixel low quality images}}
\label{fig:bad_images}
}
\end{figure}

    \item The raw data is in geoTIFF format
%     , which allows georeference information to be embedded within a TIFF file.
    \item \textit{Flooded/Damaged} buildings (or \textit{Damaged}) are annotated with labels and coordinates given in GeoJSON format. Using the coordinates, we extract the images of damaged buildings in JPEG format from the geoTIFF \textit{post-event} imagery. 
    \item \textit{Undamaged buildings} (or \textit{Undamaged}) are extracted in JPEG format directly from the geoTIFF \textit{pre-event} imagery at the same coordinates.
\end{enumerate}

\subsection{Damage annotation}\label{sec:annotation}
We present here a framework (Figure~\ref{fig:framework}) from raw data to damage annotation. Since there is no readily available data for model training, the first obvious step is to generate the data. We adopt a cropping window approach. Essentially, the building coordinates, which are either easily obtained publicly or available from crowd-sourcing projects, are used as the center of a window. The window is then cropped from the raw satellite imagery to create a data sample. The optimal window size will depend on various factors including the image resolution and building footprint sizes.. Too small windows may limit the background information contained in each sample, whereas too large ones may introduce unnecessary noise. We keep the window size as a tuning hyper-parameter in the model. A few sizes are considered such as 400x400, 128x128, 64x64, and 32x32. The cropped images are then manually filtered to ensure the high quality of the dataset. To let the model generalize well, we only discard obviously flawed images, as shown in Figure~\ref{fig:bad_images}. The clean images are then split into training, validation, and test sets and fed to a convolutional neural network for damage annotation as illustrated in Figure~\ref{fig:framework}. Validation accuracy is monitored to tune the necessary hyper-parameters and the window size.

\begin{figure}
\begin{center}
% Define block styles
\tikzstyle{decision} = [diamond, draw, fill=blue!20, 
    text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, rectangle, minimum width=1.5cm, minimum height=1cm, text centered, text width=1.5cm, draw=black, fill=red!20]
    
% \begin{tikzpicture}[node distance = 2cm, auto]
%     % Place nodes
%     \node [block] (input) {Raw imagery};
%     \node [cloud, left of=input] (coordinate) [left = 0.3cm]{coordinates and labels};
%     \node [cloud, right of=input] (size) [right = 0.3cm]{window size};
%     \node [block, below of=input] (label) {Labeled images};
%     \node [cloud, left of=label] (split) [left = 0.3cm] {train-validation-test split};
%     \node [cloud, right of=label] (filter) {manual filter};
%     \node [block, below of=label] (clean) {Clean dataset};
%     \node [cloud, left of=clean] (hyper) [left = 0.3cm] {hyper-parameters};
%     \node [decision, below of=clean] (decide) {Damage annotation};
%     % Draw edges
%     \path [line] (input) -- (label);
%     \path [line] (label) -- (clean);
%     \path [line] (clean) -- (decide);
%     \path [line,dashed] (coordinate) -- (label);
%     \path [line,dashed] (size) -- (label);
%     \path [line,dashed] (split) -- (clean);
%     \path [line,dashed] (filter) -- (clean);
%     \path [line,dashed] (hyper) -- (decide);
%     \path [line,dashed] (decide) -| node [near start] {update} (hyper);
%         \path [line,dashed] (decide) -| node [near start] {update} (size);
%     %\path [line,dashed] (system) |- (evaluate);
% \end{tikzpicture}
% \end{center}
% \caption{Damage annotation framework} \label{fig:framework}
% \end{figure}



\begin{tikzpicture}[node distance = 2.8cm, auto, cloud/.style={draw,
fill=pink,
rectangle, 
minimum width={width("hyper-paramters")+2pt}}]
    % Place nodes
    \node[block] (input) {Raw imagery};
    \node[cloud, left of=input] (coordinate) [left = 1cm]{coordinates and labels};
    \node[cloud, right of=input] (size) [right = 1cm]{window size};
    \node[block, below of=input] (label) {Labeled images};
    \node[cloud, left of=label] (split) [left = 1cm] {train-validation-test split};
    \node[cloud, right of=label] (filter) {manual filter};
    \node[block, below of=label] (clean) {Clean dataset};
    \node[cloud, left of=clean] (hyper) [left = 1cm] {hyper-parameters};
    \node[decision, below of=clean] (decide) {Damage annotation};
    % Draw edges
    \path[line] (input) -- (label);
    \path[line] (label) -- (clean);
    \path[line] (clean) -- (decide);
    \path[line,dashed] (coordinate) -- (label);
    \path[line,dashed] (size) -- (label);
    \path[line,dashed] (split) -- (clean);
    \path[line,dashed] (filter) -- (clean);
    \path[line,dashed] (hyper) -- (decide);
    \path[line,dashed] (decide) -| node [near start] {update} (hyper);
        \path[line,dashed] (decide) -| node [near start] {update} (size);
    %\path [line,dashed] (system) |- (evaluate);
\end{tikzpicture}
\end{center}

\caption{Damage annotation framework} \label{fig:framework}

\end{figure}

\subsection{Data processing}\label{sec:collection}
% \begin{enumerate}
As mentioned above, the data generation starts from a building coordinate. Since there are many raw geoTIFF files containing the same coordinates, there are many duplicate images with different quality. This can potentially inflate the prediction accuracy as the same coordinate may both appear in the training and test sets. We maintain an unordered set of the available coordinates and make sure each coordinate yields a unique, ``good-quality" image in the dataset through a semi-automated process. We first automatically discard the totally blacked out images for each coordinate, and keep the first image we encounter that is not totally black. The remaining images are manually filtered to eliminate images that are partially black, and/or covered by clouds. 
%     \item Many images are just blank due to the original raw image. We need to discard those and keep a valid image for each coordinate. Pick the first one that is not blank in the list of duplicates
%     \item Manually pick the images of good quality. It takes hours. 
%     \item Manually pick the training, validation, and test set to make sure the representation is good, since they can be close to one another and "almost" repeat themselves. 
% \end{enumerate}

% We manually filter the bad-quality images and remove duplicates to make sure the dataset contains only one good-quality image at every unique coordinate.


\subsection{Data featurization}\label{sec:featurization}
Since we control the window size through physical distance, there could be round off errors when converting the distance to the number of pixels. When we featurize the images, we project them into the same feature dimension. For instance, 128x128 images are projected into 150x150 dimension. The images are then fed through a CNN to further extract the right set of features, such as edge extraction in Figure~\ref{fig:one_filter}.

\begin{figure}[h]{\centering
\subfigure[Original image (Damaged)]{\includegraphics[width=1.5in]{filter-0.png}}
\subfigure[After $1^{st}$ layer]{\includegraphics[width=1.5in]{filter-1.png}}
\subfigure[After $2^{nd}$ layer]{\includegraphics[width=1.5in]{filter-2.png}}
\subfigure[After $3^{rd}$ layer]{\includegraphics[width=1.5in]{filter-3.png}}
\\
\caption{\small{Information flow within one filter}}
\label{fig:one_filter}
}
\end{figure}

How to construct the most suitable CNN architecture is an ongoing research problem. The practice is usually starting with an existing architecture and fine-tune further from there. We experiment with a well-known architecture, VGG-16 \cite{vgg16}, and modify the first layer to suit our input dimension. VGG-16 can perform extremely well on the ImageNet dataset for object detection. 

However, realizing the crucial differences between common objects detection and damage building annotation tasks, we also build our own network from scratch with proper hyper-parameters. Our basis for determining the size and depth of a customized network is to monitor the information flow through the network and stop appropriately when there are too many ``dead" filters. Due to the nature of the rectified linear unit (ReLU) which is defined as $max(0,x)$, there will be many hard 0 in the hidden layer. Although sparsity in the layer will promote the model to generalize better, it can potentially cause the problem to gradient computation at 0 and hurt performance \cite{relu,leaky}. We see that in Figure ~\ref{fig:flow} after four convolutional layers, about 30\% of the filters are ``dead" and will not carry further information to the subsequent layers. This is a significant stopping criterion since we can avoid a deep network such as VGG-16 to save the computational time and safeguard satisfactory information flow in the network at the same time. 

We present our network architecture that achieves the best result in Table \ref{tab:architecture}. An example of how the number of parameters is calculated is as follows: in the first 2-D convolutional layer, there will be 32 convolutional filters, each of size (3x3), for each of the 3 RGB channels of the input image. Including 1 more bias parameter for each filter, we have: \\
\[
[(3 \times 3) \times 3 + 1] \times 32 = 896
\]
In our CNN structure, with four convolutional layers and two fully connected layers, there are already about $3.5$ million parameters to train for, given $67,500$ pixels as an input vector for each image. The VGG-16 structure (not shown here explicitly), with thirteen convolutional layers, has almost $15$ million trainable parameters, which can easily over-fit, be more resources-consuming and reduce generalization performance on the testing data. 

\begin{table}[t]
\caption{Convolutional neural network architecture which achieves the best result}
\begin{center}
\begin{tabular}{lllll}
\multicolumn{1}{c}{\bf Layer type}  &\multicolumn{1}{c}{\bf Output shape}
   &\multicolumn{1}{c}{\bf \begin{tabular}{@{}c@{}}Number of \\ trainable parameters \end{tabular}}

\\ \hline 
Input                       &3@(150x150)     &0  \\
2-D Convolutional 32@(3x3)        &32@(148x148)       &896   \\
2-D Max pooling (2x2)        &32@(74x74)       &0   \\
2-D Convolutional 64@(3x3)        &64@(72x72)       &18,496  \\
2-D Max pooling (2x2)         &64@(36x36)     &0   \\
2-D Convolutional 128@(3x3)        &128@(34x34)       &73,856   \\
2-D Max pooling (2x2)        &128@(17x17)       &0   \\
2-D Convolutional 128@(3x3)        &128@(15x15)       &147,584   \\
2-D Max pooling (2x2)        &128@(7x7)       &0   \\
Flattening      &1x6272       &0  \\
Dropout         &1x6272       &0  \\
Fully connected layer &1x512       &3,211,776  \\
Fully connected layer &1x1       &513  \\
\hline
\end{tabular}
\end{center}
Note:\\
Total number of trainable parameters: 3,453,121\\
C@($A\times B$) is interpreted as there are a total of C matrices of shape ($A\times B$) stacked on top of one another to form a three dimensional tensor.\\
2-D Max pooling layer with ($2\times 2$) pooling size means the input tensor's size will be reduced by a factor of 4. \\
\label{tab:architecture}
\end{table}

% \begin{figure}[h]
% {\centering
% \subfigure[After $1^{st}$ layer]{\includegraphics[width=2.2in]{images/flow1.png}}
% \subfigure[After $2^{nd}$ layer]{\includegraphics[width=2.2in]{images/flow2.png}}
% \subfigure[After $3^{rd}$ layer]{\includegraphics[width=2.2in]{images/flow3.png}}
% \subfigure[After $4^{th}$ layer]{\includegraphics[width=2.2in]{images/flow4.png}}
% \\
% \caption{\small{Information flow in all filters after each layer}}
% \label{fig:flow}
% }
% \end{figure}

\begin{figure}[h]{\centering
\subfigure[After $1^{st}$ layer]{\includegraphics[width=2.5in]{flow1.png}}
\subfigure[After $2^{nd}$ layer]{\includegraphics[width=2.5in]{flow2.png}}
\subfigure[After $3^{rd}$ layer]{\includegraphics[width=2.5in]{flow3.png}}
\subfigure[After $4^{th}$ layer]{\includegraphics[width=2.5in]{flow4.png}}
\\
\caption{\small{Information flow in all filters after each layer}}
\label{fig:flow}
}
\end{figure}

% Our network is shallower but can outperform the pre-built network. 


% \begin{enumerate}
%     \item Project to feature space of unequal image sizes
%     \item Convolutional + Max pooling hyper parameter
%     \item Unfeaturized image trial if time allows
% \end{enumerate}

\subsection{Image classification}\label{sec:classification}
Due to the limited availability of pre-event images and the presence of flawed images in the \textit{Damaged} and \textit{Undamaged} categories, we experience an unbalanced dataset with the majority class being \textit{Damaged}. As a result, the following method for splitting training, validation, and test datasets is adopted. We keep the training and validation sets balanced and leave the remaining data to construct 2 test sets, a balanced set and an unbalanced (with a ratio of 1:8) set. 

The first performance metric is the classification accuracy. Based on the unbalanced test set, the baseline performance is determined by annotating all buildings as the majority class \textit{Damaged} with $\frac{8}{9}=88.89\%$ accuracy. To be comprehensive, we also monitor the area under the receiver operating characteristic curve (AUC).



% Leaky ReLu may be better than ReLu for satellite images.

% Optimizer: Adam \cite{Adam} and RMSprop
% \begin{enumerate}
%     \item Benchmark accuracy: predict all to be the majority class
%     \item Unbalance class problem. Balance training data, plus hard negative mining, unbalanced validation and test
%     \item Logistic Regression featurize all the images first and then fit Logistic Reg
%     \item MLP (monitor gradient vanishing and information flow through layer to set number of layers and activation function, initialization): MLP with ReLu, with Leaky Relu, with Data Augmentation. 
%     \item Sensitive analysis on accuracy to build confidence interval of cross-validation errors
%     \item Show the dead neuron or some image flows through the layers + loss and accuracy curve




%     \item Show some images of misclassification and make some sense out of it
% \end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Case Study
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation and Result}\label{sec:case_study}
We train the neural network through the \textit{Keras} library with TensorFlow backend with a single NVIDIA K80 Tesla GPU with 64GB memory on a quad-core CPU machine. The network weights are initialized through Xavier initializer \cite{xavier}. The mini batch size for stochastic gradient descent optimizer is either 20 or 32.

After cleaning and manual filtering, we are left with 14,284 positive samples (\textit{Damaged}) and 7,209 negative samples (\textit{Undamaged}) of unique coordinates. 5,000 samples of each class are in the training set. 1,000 samples of each class are in the validation set. The rest of the data are reserved to form the test sets. 

Since it is computationally costly to train the CNN repeatedly, we do not tune all the hyper-parameters through a full grid search or full cross-validation. Only some reasonable combinations of the hyper-parameters are considered in a greedy manner. Among the parameters, the window size is truly a challenge. We do not try all the sizes with all hyper-parameters. We first implement a simple model with all the window sizes and find that 128x128 window yields an ideal result. 

We also implement a logistic regression (LR) on the featurized data to see how it compares to fully connected layers. LR under-performs in most cases but also achieves quite good accuracy. This illustrates that the image featurization through the network is very crucial to extract good features such that a simple algorithm can perform well enough on this data. 

For activation functions in CNN, a rectified linear unit (ReLU) is a common choice, thanks to its simplicity in gradient computation and prevention of vanishing gradient. As seen in  Figure ~\ref{fig:flow}, clamping the activation at $0$ could potentially cause a lot of filters to be dead. Therefore, we also consider using a leaky ReLU activation, with  $\alpha = 0.1$ in this case, based on the survey in \cite{leaky}. However, leaky ReLU does not improve the accuracy very much. 

To counter over-fitting, which is a recurrent problem of deep learning, we also adopt data augmentation in the training set through random rotation, horizontal flip, vertical and horizontal shift, shear, and zoom. This can effectively increase the number of training samples to ensure more generalization and achieve better validation and test accuracy (we do not perform data augmentation in the validation and test set). Furthermore, we also employ 50\% dropout and L2 regularization with $\lambda = 10^{-6}$ in the fully connected layer. These measures are shown to fight over-fitting effectively and significantly improve the validation accuracy in Figure~\ref{fig:dropout}.
% \begin{figure}[h]
% \hspace*{-0.4cm}{\centering
% \subfigure[Over-fitting happens after a few epochs]{\includegraphics[trim= 0 0 0 21, clip, width=4.08in]{images/4-acc-transfer.png}}
% \hspace*{0.3cm}\subfigure[Little sign of over-fitting]{\includegraphics[trim= 5 0 0 20, clip, width=3.444in]{images/2-acc-100epochs-aug-drop.png}}
% \\
% \caption{\small{Prevent over-fitting using data augmentation, drop-out, and regularization}}
% \label{fig:dropout}
% }
% \end{figure}

\begin{figure}[h]
\hspace*{-0.4cm}{\centering
\subfigure[Over-fitting happens after a few epochs]{\includegraphics[trim= 0 0 0 21, clip, width=3.37in]{4-acc-transfer.png}}
\hspace*{0.3cm}\subfigure[Little sign of over-fitting]{\includegraphics[trim= 5 0 0 20, clip, width=2.84in]{2-acc-100epochs-aug-drop.png}}
\\
\caption{\small{Prevent over-fitting using data augmentation, drop-out, and regularization}}
\label{fig:dropout}
}
\end{figure}

As mentioned in Section ~\ref{sec:featurization}, we consider using a pre-built architecture VGG-16 (transfer learning) and building a fresh network. In Figure~\ref{fig:Transfer_vs_Custom}, we see that using a deeper and larger network we can achieve a high level accuracy earlier but accuracy pretty much plateaus (over-fitting happens) after a few epochs. Our simpler network can facilitate learning gradually, where accuracy keeps increasing to achieve better value, and take much less time to train. 

% \begin{figure}[h]
% \hspace*{-1cm}{\centering
% \subfigure[Transfer learning using pre-built network]{\includegraphics[trim= 0 0 0 20.2, clip, width=4.25in]{images/8-everything-reulu-adam-acc.png}}
% \subfigure[Custom network]{\includegraphics[trim= 0 0 0 19, clip,width=3.296in]{images/2-acc-100epochs-aug-drop-adam.png}}
% \\
% \caption{\small{Comparison between using a pre-built network and our custom network}}
% \label{fig:Transfer_vs_Custom}
% }
% \end{figure}

\begin{figure}[h]
\hspace*{-1cm}{\centering
\subfigure[Transfer learning using pre-built network]{\includegraphics[trim= 0 0 0 20.2, clip, width=3.676in]{8-everything-reulu-adam-acc.png}}
\subfigure[Custom network]{\includegraphics[trim= 0 0 0 19, clip,width=2.85in]{2-acc-100epochs-aug-drop-adam.png}}
\\
\caption{\small{Comparison between using a pre-built network and our custom network}}
\label{fig:Transfer_vs_Custom}
}
\end{figure}

We use two adaptive, momentum based optimizers RMSprop and Adam \cite{Adam} with initial learning rate of $10^{-4}$. Adam generally leads to about 1\% higher validation accuracy and it can be seen that Adam leads to less noisy learning (Figure~\ref{fig:Adam_vs_RMSprop}). 

% \begin{figure}[h]
% {\centering
% \subfigure[RMSprop]{\includegraphics[trim= 4 0 0 20.5, clip,width=2.95in]{images/2-acc-100epochs-aug-drop.png}}
% \subfigure[Adam]{\includegraphics[trim= 0 0 0 19, clip,width=3.2in]{images/2-acc-100epochs-aug-drop-adam.png}}
% \\
% \caption{\small{Comparison between using RMSprop and Adam optimizers}}
% \label{fig:Adam_vs_RMSprop}
% }
% \end{figure}

\begin{figure}[h]{\centering
\subfigure[RMSprop]{\includegraphics[trim= 4 2 0 21, clip,width=2.9in]{2-acc-100epochs-aug-drop.png}}
\subfigure[Adam]{\includegraphics[trim= 0 0 3 19, clip,width=3.2in]{2-acc-100epochs-aug-drop-adam.png}}
\\
\caption{\small{Comparison between using RMSprop and Adam optimizers}}
\label{fig:Adam_vs_RMSprop}
}
\end{figure}

Table \ref{tab:accuracy} demonstrates the performance of various models. The best performing model is our customized model with data augmentation and dropout using Adam optimizer, which can achieve 97.08\% accuracy on the unbalanced test set. The AUC metric is also computed and shows a satisfying result of 99.8\% on the unbalanced test set. 

\begin{table*}[t]
\caption{Model performance}
\begin{center}
\begin{tabular}{lllll}
\multicolumn{1}{c}{\bf Model}  &\multicolumn{1}{c}{\bf Validation Accuracy}
  &\multicolumn{1}{c}{\bf \begin{tabular}{@{}c@{}}Test Accuracy \\ (Balanced) \end{tabular}}
   &\multicolumn{1}{c}{\bf \begin{tabular}{@{}c@{}}Test Accuracy \\ (Unbalanced) \end{tabular}}

\\ \hline %\\
CNN         &95.8\%      &94.69\%  &95.47\% \\
Leaky CNN   &96.1\%       &94.79\%  &95.27\%\\
CNN + DA + DO  &97.44\%      &96.44\%  &96.56\%\\
CNN + DA + DO (Adam) &\bf{98.06\%}     &\bf{97.29\%} &\bf{97.08\%}\\
Transfer + DO   &93.45\%      &92.8\%   &92.8\%\\
Transfer + DA + DO  &91.1\%       &88.49\%  &85.99\%\\
LR + L2 = 1    &93.55\%      &92.2\%   &91.45\%\\
Transfer + DA + FDO &96.5\%       &95.34\%  &95.73\%\\
Leaky+Transfer + DA + FDO +L2  &96.13\%        &95.59\% &95.68\%\\
Leaky+ Transfer + DA + FDO +L2 (Adam) &97.5\%  &96.19\% &96.21\%\\
\hline
\end{tabular}
\end{center}
\bigskip

Legend: CNN: Convolutional Neural Network; Leaky: Leaky ReLU activation function, else, default is ReLU; DA: Data Augmentation; LR: Logistic Regression; L2: L2 regularization; (Adam): Adam optimizer, else, default is RMSprop optimizer; DO: 50\% drop out only in the fully connected layer; FDO: Full drop out, i.e 25\% drop out after every max pooling layer and 50\% in the fully connected layer; Transfer: Transfer learning using VGG-16 architecture 
\label{tab:accuracy}
\end{table*}

\begin{figure}[h]{\centering
\subfigure[AUC of balanced test set]{\includegraphics[trim= 0 0 0 20, clip,width=3in]{AUC_balanced.png}}
\subfigure[AUC of unbalanced test set]{\includegraphics[trim= 0 0 0 19.8, clip,width=3in]{AUC_unbalanced.png}}
\\
\caption{\small{AUC for balanced and unbalanced test sets using our best performing model (CNN + DA + DO (Adam)) }}
\label{fig:AUC}
}
\end{figure}

Although the result is satisfactory, we also look at a few typical cases where the algorithm makes wrong classification to see if any intuition can be derived. Figure~\ref{fig:false_positive} shows some of the false positive cases. We hypothesize that the algorithm could predict the damage through flood water and/or debris detection. Under such hypothesis, the cars in the center of Figure~\ref{fig:false_positive}(a), the lake water in Figure~\ref{fig:false_positive}(b), the cloud covering the house in Figure~\ref{fig:false_positive}(c), and the trees covering the roof in Figure~\ref{fig:false_positive}(f) can potentially mislead the model. For the false negative cases in Figure~\ref{fig:false_negative}, it is harder to make sense out of the prediction. Even through visual inspection, we cannot see Figures~\ref{fig:false_negative}(a)(b) as being damaged. There could potentially be tagging mistakes by the volunteers. On the other hand, Figures~\ref{fig:false_negative}(e)(f) are clearly damaged but the algorithm misses them. 

\begin{figure}[h]{\centering
\subfigure[]{\includegraphics[trim= 0 0 0 20, clip,width=1.5in]{confused_by_cars.png}}
\subfigure[]{\includegraphics[trim= 0 0 0 20, clip,width=1.51in]{confused_by_roof.png}}
\subfigure[]
{\includegraphics[trim= 0 0 0 20, clip,width=1.52in]{confused_by_cloud.png}}
\subfigure[]
{\includegraphics[trim= 0 0 0 20, clip,width=1.55in]{confused_by_water.png}}
\subfigure[]
{\includegraphics[trim= 0 0 0 20, clip,width=1.55in]{confused_by_wate_roof.png}}
\subfigure[]
{\includegraphics[trim= 0 0 0 20, clip,width=1.5in]{confused_by_tree.png}}
\\
\caption{\small{False positive examples (label is Undamaged, prediction is Damaged)}}
\label{fig:false_positive}
}
\end{figure}

\begin{figure}[h]{\centering
\subfigure[]{\raisebox{0.5mm}{\includegraphics[trim= 0 0 0 20, clip,width=1.5in]{volunteer_mistake.png}}}
\subfigure[]{\includegraphics[trim= 0 0 0 20, clip,width =1.5in]{not_clear_damage_.png}}
\subfigure[]
{\includegraphics[trim= 0 0 0 20, clip,width=1.48in]{not_clear_damage_2.png}}
\subfigure[]
{\includegraphics[trim= 0 0 0 20, clip,width=1.48in]{not_clear_damage_3.png}}
\subfigure[]
{\includegraphics[trim= 0 0 0 20, clip,width=1.5in]{machine_mistake.png}}
\subfigure[]
{\includegraphics[trim= 0 0 0 21, clip,width=1.5in]{machine_mistakes.png}}
\\
\caption{\small{False negative examples (label is Damaged, prediction is Undamaged)}}
\label{fig:false_negative}
}
\end{figure}
%  \begin{table}[h]
% \centering
% \caption{Comparison between the metamodel-based SIS and the EMCE for the case study}
% \label{tab:case}
% \begin{tabular}{ccccc}
% \hline
% Response & Method    & Mean   & Standard Error & CMC Ratio \\ \hline
% Edgewise & Metamodel & 0.0486 & 0.0018         & 7.0\%       \\
%          & EMCE      & 0.0486 & 0.0015         & 4.9\%       \\
% Flapwise & Metamodel & 0.0514 & 0.0028         & 32\%        \\
%          & EMCE      & 0.0535 & 0.0030         & 37\%        \\ \hline
% \end{tabular}
% \end{table}
%M/N_T = 30 % for both methods

% In the metamodel-based SIS, the metamodel is carefully built by fitting a nonhomogeneous generalized extreme value distribution to the pilot data \cite{Choe2015}. As such,  we can see that the performance of EMCE is comparable to that of metamodel-based SIS with a high quality  metamodel.  However, as seen in Section~\ref{sec-ex-sis}, when the metamodel quality is not good enough, EMCE provides a better computational efficiency. Since EMCE is an automated method, it can be a promising method when building a metamodel is difficult.    %  when  and Such a comparable performance of EMCE is promising,  whereas the metamodel-based approach requires the careful metamodel building.


%{\bf 2 Figures} (scatter \& density plot together): edge \& flap unif pilot; edge \& flap Original input, SIS-meta, SIS-EMCE density plot



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Discussion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and Future Research}\label{sec:conclusion}
We demonstrated that through deep learning, automatic detection of damaged buildings can be done satisfactorily. Although our data can be specific to the geographical condition and building properties in the Greater Houston area during Hurricane Harvey, the model can be further improved and generalized to other future disaster events in other regions if we can collect more positives samples from other past events and negative samples from other areas. 

For faster disaster response, we need a model that can work with low quality images generated on a particular day, especially near the hurricane landfall, which can be covered by cloud or imperfectly orthorectified. We will further investigate the model to see if it can be robust against such noise and distortion to reduce the amount of manual processing.

% Since the positive samples are limited and valuable, we hope to further investigate how to save the samples that are partially blacked out through boundary mirror and enhance contrast to cloud covered samples. 

Through the inspection of false positive cases, we realized there could be a link to pixel-level classification to segment different damage types and levels. For instance, we can classify flooded buildings and wrecked buildings separately. More accurate classification may be achieved through more exact shapes of different types of damage. However, this requires a massive effort to label different damage shapes and types to train the model. 

We also wish to expand the current research to road damage annotation which could help plan effective transportation routes of food, medical equipment, or fuels to the disaster victims. 

% Handling of black boundary by mirror extension.
% Hard negative mining
% \vspace{-.3cm}

\section*{Acknowledgement}
This work was partially supported by the National Science Foundation (NSF grant CMMI-1824681). We would like to thank DigitalGlobe for data sharing through their Open Data Program. We also thank Amy Xu, Aryton Tediarjo, Daniel Colina, Dengxian Yang, Mary Barnes, Nick Monsees, Ty Good, Xiaoyan Peng, Xuejiao Li, Yu-Ting Chen, Zach McCauley, Zechariah Cheung, and Zhanlin Liu in the Disaster Data Science Lab at the University of Washington, Seattle for their help with data collection and processing. 
\appendices
\section{Dataset and code used in this paper}
The dataset and code used in this paper are available at the first author's Github repository: \url{https://github.com/qcao10/DamageDetection}. The dataset is also available at the IEEE DataPort (DOI: 10.21227/284c-p879).\\


% you can choose not to have a title for an appendix
% if you want by leaving the argument blank


% use section* for acknowledgment

% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{IEEEabrv,tomnod_BIB}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
% \begin{thebibliography}{1}

% \bibitem{IEEEhowto:kopka}
% H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%   0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

% \end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\begin{IEEEbiography}[{\includegraphics[width=1.1in,height=1.25in,clip]{ProfessionalPhoto.png}}]{Quoc Dung Cao}
is a Ph.D. student at the University of Washington, where he is researching on
computer vision application to disaster management. His works involve analyzing satellite imagery to quantify damage level after a hurricane event. He is also investigating the model robustness against noise and distortion, which is often unavoidable in geo-information data. He also wishes to expand the result and methodology to road damage annotation which could help plan effective transportation routes of food, medical equipment, or fuels to the disaster victims.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1.1in,height=1.25in,clip]{20171210_163053-IEEE-bio-Choe.jpg}}]{Youngjun Choe}
received the B.Sc. degrees in Physics and Management Science from KAIST, Korea in 2010, and both the M.A. degree in Statistics and the Ph.D. degree in Industrial \& Operations Engineering from the University of Michigan, Ann Arbor, MI, USA in 2016. 

He is currently an Assistant Professor of Industrial \& Systems Engineering at the University of Washington, Seattle, WA, USA. His research centers around developing statistical methods to infer on extreme events using empirical and simulated data. 
%is an Assistant Professor of Industrial \& Systems Engineering at the University of Washington, Seattle. His research centers around developing statistical methods to infer on extreme events (e.g., natural hazard-induced disasters) using empirical and simulated data. He received his Ph.D. in Industrial \& Operations Engineering and M.A. in Statistics from the University of Michigan, Ann Arbor. He holds bachelor's degrees in Physics and Management Science from KAIST in Korea. 
\end{IEEEbiography}

% if you will not have a photo at all:
% \begin{IEEEbiographynophoto}{Youngjun Choe}
% Biography text here.
% \end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage



% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}