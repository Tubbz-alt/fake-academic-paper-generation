%!TEX root = main.tex
\documentclass[../main.tex]{subfiles}
\begin{document}

\section{Experiments}

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{figures/datasets.pdf}
\caption{Here we see example images from our three datasets with their corresponding, automatically generated, visual explanations below.} 
\label{fig:datasets}
\vspace{-10pt}
\end{figure*}


\subsection{Datasets}
There are existing datasets with explanations in the form of annotated visual attributes that could be used to teach visual categories to human learners.
However, we found that in many cases, \eg, \cite{wah2011caltech}, the provided attributes were too coarse-grained (\eg `forehead color') to be useful for teaching highly similar categories.
Other alternatives were too noisy to be informative \eg \cite{deng2016}, but present an interesting avenue for future human-in-the-loop explanation generation.
Instead, we selected three diverse datasets to span a range of different teaching use cases.
Example images and explanations from each dataset can be seen in \figref{fig:datasets}. 

\noindent{\bf Butterflies}
%\paragraph{Butterflies}
Our first dataset represents the teaching of naturalists who wish to identify different species of plants and animals in the wild. 
It contains images of five different species of butterflies captured in a large variety of real-world situations with varying image quality from the iNaturalist dataset~\cite{van2017inaturalist}.
Two of the species, Monarch and Viceroy, are very similar in appearance and the third, Queen, looks the same from underneath.
The final two species, the Red Admiral and the Cabbage White are distinct in appearance. 
We extracted a bounding box around each butterfly and manually discarded images of caterpillars.
In total, the dataset contains 2,224 images, close to uniformly distributed across each of the species. 

\noindent{\bf OCT Eyes} 
%\paragraph{OCT Eyes}
Our second dataset mimics the teaching of a trainee ophthalmologist that is tasked with learning how to identify different retinal diseases in images. 
It consists of image slices of the retina obtained through optical coherence tomography (OCT). 
OCT is a non-invasive imaging technique that uses light waves to take cross-sectional images of the retina. 
It is commonly used in ophthalmology, where on the order of 30 million OCT scans are performed around the world annually \cite{swanson2011ophthalmic}. 
Here, the learner's goal is to classify each of the images into one of three classes: Normal, contains Macular Edema, or contains Subretinal Fluid.
Diabetic Macular Edema is one of the leading causes of vision loss for people with diabetes \cite{romero2011managing}. 
Many of the examples are challenging and require subtle inspection to correctly identify the condition.
The dataset contains 1,125 images with ground truth annotations provided by retina specialists.


\noindent{\bf Chinese Characters} 
%\paragraph{Chinese Characters}
Finally, we use a dataset of three different Chinese characters extracted from \cite{liu2011casia}, that was also used in \cite{johns2015}.
Here, we are exploring the scenario of a non-native Chinese speaker attempting to learn to identify new characters. 
Each of the 717 images is a different handwritten example of one of the following three characters: Grass, Mound, or Stem. 
The images vary in difficulty as there is a large variety in the handwriting quality and style for the different individuals that contributed to the dataset. 




%
%
%
\subsection{Experimental Setup}
We conducted experiments with real human participants on the crowdsourcing platform Mechanical Turk using the same methodology as \cite{singla2014near, johns2015}. 
Crowd workers, \ie learners, were randomly assigned to a dataset and baseline teaching strategy. 
On average we received $40$ participants per strategy and dataset combination. 
We assumed that the learners were motivated to perform well but acknowledge that this might not always be the case. 
First, the learners were shown a brief tutorial illustrating how our web-based teaching interface worked. 
When teaching began, they were presented with a sequence of images, shown one at a time, selected by the assigned teaching algorithm.
After viewing each image in turn, they were then asked to select from a list of options indicating the class they believed the image to contain.
Depending on the assigned teaching strategy, they were given feedback either in the form of the correct class label, or the correct class label and its corresponding visual explanation.
The visual explanations were displayed on top of the input images, alternating between the input image and the explanation every 0.5 seconds. 
Upon receiving feedback, learners had to wait for a minimum of 2 seconds before they could proceed to the next teaching image. 
For each dataset we showed $20$ teaching images, followed by $20$ randomly selected testing images where no feedback was provided during testing. 
This random sequence of test images was different for each learner. 
Longer teaching sequences would likely lead to better test performance but there is a trade off between the learner's attention span and their accuracy. 
We shuffled the order of the response buttons depicting the class labels for each learner before teaching began to ensure that there was no bias towards a particular button position.

We compared our full \EXPLAIN model to three baseline algorithms: 1) \RANDIM random selection of images, 2) \RANDEXP random selection with visual explanations, and 3) our multiclass version of \STRICT \cite{singla2014near}. 
Note, this last baseline did not include density weighting or explanations.
For both \EXPLAIN and \STRICT we generated a fixed teaching sequence once offline and used it for all learners with the same hypothesis space for both.
We set the noise level of the learners to $\alpha = 0.5$, and the explanation and representativeness parameters to $\beta = \gamma = 1.0$.



\begin{figure*}[t]
    \centering
    \subfigure[Butterflies]{  
        \centering
        \includegraphics[trim={0px 0px 0px 20px},clip, width=.22\textwidth]{figures/butterflies_test_new.pdf}
    }~
    \subfigure[OCT Eyes]{
        \centering
        \includegraphics[trim={0px 0px 0px 20px},clip, width=.22\textwidth]{figures/oct_test_new.pdf}
    }~
     \subfigure[Chinese Characters]{
       \centering
        \includegraphics[trim={0px 0px 0px 20px},clip, width=.32\textwidth]{figures/chinese_test_new.pdf}
    }~
    \subfigure[OCT Eyes Confusion]{
        \centering
        \includegraphics[trim={0px 0px 0px 0px},clip, width=.20\textwidth]{figures/oct_confusion_new.pdf}
    }
    \caption{Test time classification performance for human learners. A) - C) Show learners binned by test time accuracy. The horizontal axes represent average test scores, where larger numbers indicate higher accuracy. The vertical axes are the number of learners. D) Test time learner confusion matrices for the OCT Eyes dataset. We see that both explanation based strategies result in smaller off-diagonal entries.}
    \label{fig:results}
    \vspace{-10pt}
\end{figure*}


%
%
%
\subsection{Results}
In \tableref{tab:results_summary} we report the average test time accuracy for each learner for the different baseline teaching strategies and datasets.
\figref{fig:results} A) - C) displays the histograms of these accuracies illustrating that strategies that include explanations tend to do better overall.

\begin{table}[t]
\scriptsize
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline 
                   &\RANDIM&\RANDEXP&\STRICT&\EXPLAIN \\ \hline\hline
Butterflies        & 65.20  & 67.31          & 65.00  & {\bf 68.33} \\
OCT Eyes           & 51.05  & 62.58          & 64.63  & {\bf 72.38} \\
Chinese            & 47.05  & {\bf 58.90}    & 51.91  & 46.35       \\
Chinese - Crowd      &        &                & 53.06  & {\bf 65.44} \\\hline
\end{tabular}
\caption{Average test time accuracies for Mechanical Turk learners across the three different test datasets.
Compared to the standard image only baseline \RANDIM, the inclusion of our explanations in \RANDEXP results in better test time performance across all datasets.
Our \EXPLAIN model results in superior learners in two of the three datasets with CNN generated hypothesis spaces.}
\label{tab:results_summary}
\vspace{-10pt}
\end{table}

For the Butterflies dataset we see that a greater percentage of learners get high scores when taught with \EXPLAIN, see \figref{fig:results} A). 
This is a challenging dataset, and we observe that many learners remain confused between the three similar species by the end of teaching, perhaps necessitating longer teaching sequences. 

The OCT Eyes images are likely to be the most unfamiliar to our learners compared to the other datasets.
However, the images are well aligned and do not contain large out of plane view point changes or confusing background texture. 
This enables us to generate high-quality explanations that localize the characteristic morphologies for the different retinal diseases. 
For the \RANDIM baseline in \figref{fig:results} B) the difference in learner performance can be explained by the images selected and their intrinsic motivation.
In \figref{fig:results} D) we see the average test time confusion matrices across all learners for the OCT Eye dataset.
It is clear that \EXPLAIN results in less cross-category confusion \ie smaller values in the off-diagonal entries.
We see that learners tend to make fewer mistakes when identifying Subretinal Fluid as it is relatively distinct, while there is more confusion between Macular Edema and Normal. 

The Chinese Characters dataset represents an interesting failure case for \EXPLAIN when using the CNN generated hypothesis space, see \figref{fig:results} C). 
In \figref{fig:chinese_fail} we see the average performance during teaching for all learners and the first five teaching images selected by \EXPLAIN. 
\EXPLAIN selects a particularly difficult image for the fourth teaching example.
It happens to be the same class as the previous image, but is visually very different.
From inspecting the test time confusion matrix we see that this early difficult example potentially biases learners as they end up with lower performance for the \emph{Grass} class. 
The random based strategies have the advantage of generating different teaching sequences for each learner by uniformly sampling the input space thus introducing some additional robustness.
In \tableref{tab:results_summary} we see that \RANDEXP performs well on this dataset, indicating that the performance dip for \EXPLAIN may be a result of the automatically generated hypothesis space or the explanation interpretability scores, rather than the quality of the explanations themselves.
To test this hypothesis, we assigned manual interpretability scores and generated a separate embedding space more closely aligned with human notions of similarity by soliciting pairwise similarity estimates  on Mechanical Turk to construct a new embedding space (Chinese -  Crowd)\footnote{Full details are provided in the supplementary material.}.
Teaching with this embedding space (\EXPLAINCROWD) results in the best test time performance overall, see the last row in \tableref{tab:results_summary}.  


\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\columnwidth]{figures/chinese_fail.pdf}
    \caption{Average accuracy during teaching for the Chinese Characters dataset using the CNN generated hypothesis space, where random guessing is $33\%$. For all strategies, we observe a general improvement in learners' ability over time. Above we see the first five teaching images selected by \EXPLAIN. The fourth image is a difficult instance, resulting in the vast majority of learners guessing the wrong category.}
    \label{fig:chinese_fail}
    \vspace{-10pt}
\end{figure}


\end{document}