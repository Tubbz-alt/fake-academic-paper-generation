\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage{subcaption}
\usepackage{multirow}
\usepackage[labelfont=bf]{caption}
\graphicspath{{./figures/}}
\newcommand{\architecture}[1]{\emph{#1}}
\newcommand{\arch}[1]{\emph{#1}}

\newcommand{\mrcell}[1]{\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}} #1 \end{tabular}}}
\newcommand{\lmrcell}[1]{\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}} #1 \end{tabular}}}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\renewcommand{\sectionautorefname}{Section}
\renewcommand{\subsectionautorefname}{Section}
\renewcommand{\subsubsectionautorefname}{Section}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{2863} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Training Competitive Binary Neural Networks from Scratch}

\author{Joseph Bethge\footnotemark[1] , Marvin Bornstein\footnotemark[2] , Adrian Loy\footnotemark[2] , Haojin Yang\footnotemark[1] , Christoph Meinel\footnotemark[1]  \\
Hasso Plattner Institute, University of Potsdam, Germany\\
P.O. Box 900460, Potsdam D-14480\\
\footnotemark[1] {\tt\small firstname.surname@hpi.de}, \footnotemark[2] {\tt\small firstname.surname@student.hpi.de}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%%\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
% !TeX root = ../paper_for_review.tex

Convolutional neural networks have achieved astonishing results in different application areas.
Various methods that allow us to use these models on mobile and embedded devices have been proposed.
Especially binary neural networks are a promising approach for devices with low computational power.
However, training accurate binary models from scratch remains a challenge.
Previous work often uses prior knowledge from full-precision models and complex training strategies.
In our work, we focus on increasing the performance of binary neural networks without such prior knowledge and a much simpler training strategy.
In our experiments we show that we are able to achieve state-of-the-art results on standard benchmark datasets.
Further, to the best of our knowledge, we are the first to successfully adopt a network architecture with dense connections for binary networks, which lets us improve the state-of-the-art even further.
Our source code can be found online: \\
\url{https://github.com/hpi-xnor/BMXNet-v2}
\end{abstract}

%%%%%%%%% BODY TEXT
% !TeX root = ../paper_for_review.tex\section{Introduction}\label{sec:intro}

Nowadays, significant progress through research is made towards automating different tasks of our everyday lives.
From vacuum robots in our homes to entire production facilities run by robots, many tasks in our world are already highly automated.
Other advances, such as self-driving cars, are currently being developed and depend on strong machine learning solutions.
%Further, more and more ordinary devices are equipped with embedded chips.
The amount of apps on smartphones, which adopt deep learning techniques to solve a variety of tasks, is rising rapidly and will likely continue to do so in the future.
All these devices have limited computational power, often while trying to minimize energy consumption, but have many use cases for machine learning. %and might provide future applications for machine learning.

We will consider the example of a fully automated self-driving car. %, which requires guaranteed real-time image processing.%In inessential systems, a cloud platform could run the machine learning models and report back results.%However, neither stability nor low latency are guaranteed by a wireless network, therefore this approach can not be used in systems developed for autonomous driving.%Other application areas which do not need real-time processing, such as healthcare, have orthogonal challenges, for example, data privacy of sensitive data.%All of these problems can be avoided by hosting the machine learning models directly on the device itself, in our example, the car.%However, there are other challenges, such as limited computational resources and limited memory, in addition to a possible reliance on battery power.
It is crucial for such a system to achieve high accuracy coupled with guaranteed real-time image processing.
Furthermore, the image processing system needs to be hosted in the car itself, as a stable Internet connection with low latency cannot be guaranteed in this setting.
This requirement limits the available computational power and memory, but at the same time profits from a low energy consumption.
A promising technique that can deal well with these conditions are Binary Neural Networks (BNNs).
In a BNN the commonly used full-precision weights of a convolutional neural network are replaced with binary weights.
This results in a storage compression by a factor of 32$\times$ and allows for significantly more efficient inference on CPU-only architectures.

% These existing approaches have promising results, as they often have an accuracy comparable to full-precision networks, while having a lower memory footprint and less operations for computation.

We discuss existing approaches in \autoref{sec:related}.
Moreover, we identified three ways to increase the accuracy of a binary model and describe how we applied them to a binary network with dense shortcut connections:
removing bottleneck designs, increasing the number of shortcut connections throughout the network, and replacing certain layers with full-precision layers.
We describe these and other common techniques together with our implementation details in \autoref{sec:method}.
Afterwards, we discuss the results of our approach on the MNIST, CIFAR10 and ImageNet datasets in \autoref{sec:experiments}.
We evaluate the influence of the previously described techniques on existing approaches and with our proposed model based on dense shortcut connections.
The results show that we can reach state-of-the-art results for existing architectures and improve results even further with our proposed model.
% We discuss the results of a variety of experiments on the MNIST, CIFAR10 and ImageNet datasets (\autoref{sec:experiments}).% TODO: check about contents of conclusion and update the following sentence
Finally, we examine future ideas and conclude our work in \autoref{sec:conclusion}.

Summarized, our contributions in this paper are:
\begin{itemize}
\itemsep0em 
    \item We present a simple training strategy for binary models without using a pretrained full-precision model.
    \item We provide empirical evidence that this strategy does not benefit from other commonly used methods, \eg, scaling factors or usage of custom gradient calculation.
    % \item We show the importance of removing bottleneck architectures from binary networks and carefully selecting the gradient clipping threshold.
    % \item We theoretically reason why three common techniques which were used for  BNNs significantly.
    \item We show that increasing the number of shortcut connections improves the classification accuracy of BNNs significantly and show a novel way to create efficient binary models based on dense shortcut connections.
    % \item We demonstrate that replacing the downsampling layers of binary models with full-precision layers leads to significant accuracy gain.
    \item We reach state-of-the-art accuracy compared to other approaches for different model architectures and sizes.
    % \item We publish our code and developed models.
    % We share our code and developed models in this paper for research use.\footnote{URL will be included here in the final version}
\end{itemize}% \begin{table}[t]% \centering% \caption{% Comparison of available implementations for binary neural networks.% We use BMXNet since the other implementations are difficult to use for actual applications, because actual model saving and deployment is not possible.% }% \label{tab:comparison}% \begin{tabular}{|l|c|c|c|c|c|c|c|c|}% \hline% \textbf{Title}                                                                                 & \multicolumn{1}{l|}{\textbf{GPU}} & \multicolumn{1}{l|}{\textbf{CPU}} & \multicolumn{1}{l|}{\textbf{\begin{tabular}[c]{@{}l@{}}Python\\ API\end{tabular}}} & \multicolumn{1}{l|}{\textbf{\begin{tabular}[c]{@{}l@{}}C++\\ API\end{tabular}}} & \multicolumn{1}{l|}{\textbf{\begin{tabular}[c]{@{}l@{}}Save\\ Binary\\ Model\end{tabular}}} & \multicolumn{1}{l|}{\textbf{\begin{tabular}[c]{@{}l@{}}Deploy\\ on\\ Mobile\end{tabular}}} & \multicolumn{1}{l|}{\textbf{\begin{tabular}[c]{@{}l@{}}Open\\ Source\end{tabular}}} & \multicolumn{1}{l|}{\textbf{\begin{tabular}[c]{@{}l@{}}Cross\\ Platform\end{tabular}}} \\ \hline% \begin{tabular}[c]{@{}l@{}}BNNs \cite{Courbariaux2016}  \end{tabular} & \checkmark             &                                   & \checkmark                                                                         &                                                                                 &                                                                                             &                                                                                            & \checkmark                                                                          &                                                                                        \\ \hline% DoReFa-Net \cite{Zhou2016}                                                                     & \checkmark                        & \checkmark                        & \checkmark                                                                         &                                                                                 &                                                                                             &                                                                                            & \checkmark                                                                          & \checkmark                                                                             \\ \hline% XNOR-Net \cite{Rastegari2016}                                                                  & \checkmark                        &                                   &                                                                                    &                                                                                 &                                                                                             &                                                                                            & \checkmark                                                                          &                                                                                        \\ \hline% % \begin{tabular}[c]{@{}l@{}}Binary\\ MXNet\end{tabular}                                         & \checkmark                        & \checkmark                        & \checkmark                                                                         & \checkmark                                                                      & \checkmark                                                                                  & \checkmark                                                                                 & \checkmark                                                                          & \checkmark                                                                             \\ \hline% BMXNet \cite{HPI_xnor}                                                                         & \checkmark                        & \checkmark                        & \checkmark                                                                         & \checkmark                                                                      & \checkmark                                                                                  & \checkmark                                                                                 & \checkmark                                                                          & \checkmark                                                                             \\ \hline% \end{tabular}% \end{table}
% !TeX root = ../paper_for_review.tex\section{Related Work}\label{sec:related}% ResNet (https://arxiv.org/abs/1512.03385)% DenseNet (https://arxiv.org/abs/1608.06993)% Network Compression via compact design, e.g., %     squeezeNet (https://arxiv.org/abs/1602.07360)%     mobileNet (https://arxiv.org/abs/1704.04861)%     Shufflenet (https://arxiv.org/abs/1707.01083)% Binarized/quantized Network%     BNN (https://arxiv.org/abs/1602.02830)%     XNOR (https://arxiv.org/abs/1603.05279)%     DoReFa (https://arxiv.org/abs/1606.06160)%     ABC-Net (nips2017 https://arxiv.org/abs/1711.11294)% Should point out the shortcomings of existing work, could our work solve those shortcomings or complement the missing research part?% In this section we first present two network architectures, Residual Networks \cite{He2017} and Densely Connected Networks \cite{Huang2016}, which focus on increasing information flow through the network.% Afterwards we give an overview about networks and techniques which were designed to allow execution on mobile or embedded devices.%In this section we first present two network architectures, the Residual Network (ResNet) \cite{He2017} and the Densely Connected Network (DenseNet) \cite{Huang2016}.%Afterwards, we present related work for binarization and compression techniques.
In this section we present related work for binarization and compression techniques.
% These are often applied to the ResNet architecture and, in our case, to the DenseNet architecture.% These networks are often used as base networks, to which then binarization or compression techniques are applied. % Previous work in this field often uses a \architecture{ResNet} for binary neural networks \cite{Rastegari2016,HPI_xnor,lin2017towards,Liu_2018_ECCV}.% A \architecture{DenseNet} for binary neural networks.%\subsection{Compression and Binarization Techniques}
There are two main approaches which allow for execution on mobile devices by accelerating inference:
% Network Compression via compact design
On the one hand, information in a CNN can be compressed through compact network design.
These designs use full-precision floating point numbers as weights, but reduce the total number of parameters and operations through clever network design, while preventing loss of accuracy.
% Binarized/quantized Network
On the other hand, information can be compressed by avoiding the common usage of full-precision floating point weights, which use 32 bits of storage.
Instead, quantized floating-point numbers with lower precision (\eg 4 bit of storage) or even binary (1 bit of storage) weights are used in these approaches.

% Network Compression via compact design, e.g., %     squeezeNet (https://arxiv.org/abs/1602.07360)%     mobileNet (https://arxiv.org/abs/1704.04861)%     Shufflenet (https://arxiv.org/abs/1707.01083)

First, we present a selection of techniques which utilize the former method.
The first of these approaches, \emph{SqueezeNet}, was presented by Iandola \etal\cite{Iandola2016}.
The authors replace a large portion of 3$\times$3 filters with smaller 1$\times$1 filters in convolutional layers and reduce the number of input channels to the remaining 3$\times$3 filters for a reduced number of parameters.
Additionally, they facilitate late downsampling to maximize their accuracy and use \emph{deep compression}\cite{Han2015} for an overall model size of 0.5 MB.

A different approach, \emph{MobileNets}, was implemented by Howard \etal\cite{Howard2017}.
They use a depth-wise separable convolution where convolutions apply a single 3$\times$3 filter to each input channel.
Subsequently, a 1$\times$1 convolution is applied to combine their outputs.
Zhang \etal\cite{Zhang2017} use channel shuffling to achieve group convolutions in addition to depth-wise convolution.
Their \emph{ShuffleNet} achieves comparably lower error rate for the same number of operations needed for \emph{MobileNets}.
These approaches reduce memory requirements, but still require GPU hardware for efficient training and inference.
% All these methods lack a strategy to accelerate the computation on CPUs.
A strategy to accelerate the computation of all these methods for CPUs has yet to be developed.
%Specific acceleration strategies for CPUs still need to be developed for these methods.% TODO such architecture design will always need expert knowledge for different task, use cases? does it?% Binarized/quantized Network%     BNN (https://arxiv.org/abs/1602.02830)%     XNOR (https://arxiv.org/abs/1603.05279)%     DoReFa (https://arxiv.org/abs/1606.06160)%     ABC-Net (nips2017 https://arxiv.org/abs/1711.11294)%The origin of these approachs is in \emph{Binarized Neural Networks}, where weights and activations are restricted to either +1 or -1, as presented by Hubara \etal \cite{Courbariaux2016}.
In contrast to this, approaches which use binary weights instead of full-precision weights achieve compression and acceleration.
However, the drawback usually is a severe drop in accuracy.
These approaches are based on \emph{Binarized Neural Networks}, introduced by Hubara \etal\cite{Courbariaux2016}, where weights and activations are restricted to +1 and -1.
% They replace matrix multiplications with an equivalent and more efficient calculation using $\mathrm{xnor}$ and $\mathrm{popcount}$ operations.
They provide efficient calculation methods for the equivalent of a matrix multiplication by using $\mathrm{xnor}$ and $\mathrm{popcount}$ operations.
\emph{XNOR-Nets}, published by Rastegari \etal\cite{Rastegari2016}, improved the performance of binary neural networks by introducing changes to the network layout.
Furthermore, they include a channel-wise scaling factor to reduce the approximation error of full-precision weights.
Another approach, called \emph{DoReFa-Net}, was presented by Zhou \etal\cite{Zhou2016}.
They focus on quantizing the gradients together with different bit-widths (down to binary values) for weights and activations and replace the channel-wise scaling factor with one constant scalar for all filters.
A different attempt to strictly use nothing except binary weights is taken in \emph{ABC-Nets} by Lin \etal\cite{lin2017towards}.
They use 3 to 5 binary weight bases to approximate full-precision weights.
This approximation increases model complexity and size, but reduces the gap between the accuracy of full-precision and binary networks to 5\%.
% This approach achieves a drop in top1-accuracy of only about 5\% on the ImageNet dataset compared to a full-precision network using the \architecture{ResNet} architecture.
Wan \etal\cite{Wan_2018_ECCV} improved accuracy by using binary weights and ternary activations in their \emph{Ternary-Binary Network}. 
They train their model from scratch, but they have more operations compared to fully binary models (without an increase in memory consumption).
% They train their model from scratch but use more operations than a network, which is completely binary.
In \emph{Bi-Real Net}, Liu \etal\cite{Liu_2018_ECCV} modify the \arch{ResNet} architecture by adding additional shortcuts and reducing the size of the convolution layers.
They propose a change of gradient computation during backpropagation compared to other approaches.
\architecture{Bi-Real Nets} are trained using a complex training strategy to fine-tune a pretrained full-precision network to create a binary model with 56.4\% accuracy.
Our work differs from their approach, as we directly train a binary network from scratch.

% different gradient computation ok? Bezieht sich auf approx sign im backward and gradient scaling an den weights% Their next step is to apply binarization to weights and activations with a different gradient computation of the gradients during backpropagation.% This results in a binary model with 56.4\% accuracy.%to fine-tune a full-precision network to create a binary model with 56.4\% accuracy.% Our own experiments suggest, that some of their improvement over previous approaches comes from replacing certain binary transition layers with full-precision layers.%as we only need one simple training from scratch instead of depending on a previously trained full-precision model.% They reach 56.4\% accuracy with a ResNet-like model, but use a complex training strategy consisting of multiple steps.% Therefore finding a way to accurately train a binary neural network still remains an unsolved task.% in contract, binary network can achieve compression and acceleration easier, which is based on its nature... but the drawback is the strong accuracy drop. Although the ABC-Net achieved promising accuracy improvement via adding more bit bases (can be considered as a multiple 1-bit network),  but the drawback is the dramatical increasing of the model complexity and model size, which is similar to the multi-bits quantised network. Therefore find way to efficiently train accurate bnn is still a yet not solved task.
% !TeX root = ../paper_for_review.tex\section{Methodology}\label{sec:method}% If our goal is systematic evaluation of BNN, how do we design the workflow, what do we want to evaluate?%     ?Energy consumption?%         (We don’t have result on this eval, but we can cite this paper: https://arxiv.org/pdf/1602.02830.pdf, get inspiration from the Chapter 3 “Very Power Efficient in Forward Pass”, and add some discussion...   % Short introduction about BMXNet, (BMXNet should be anonymous in the submission)%     Elaborate some basic metrics for BNN, e.g STE (straight-through estimator), xnor-popcount operation (could look at the BMXNet paper)% Some good practice for training BNN%     Optimizer/Common Hyperparameters%     …

In this section we first provide the major implementation principles of the framework we use for implementing and training binary models.
Following this, we examine the usage of scaling factors.
Finally, we discuss design principles for binary network layouts and introduce a novel binary model architecture based on \arch{DenseNets}.

\subsection{Implementation of Binary Layers}\label{sec:implementation-details}

Our implementation is based on the BMXNet framework first presented by Yang \etal\cite{HPI_xnor}, which itself is based on the MXNet framework.
% However, we changed the underlying implementation to a newer version of MXNet and cleaned up some implementation problems in the original BMXNet.
We use the sign function for activation, thus transforming floating-point values into binary values:
\begin{equation}
    \mathrm{sign}(x) = \begin{cases} 
    +1 ~\text{if}~ x \geq 0, \\
    -1 ~\text{otherwise}.
    \end{cases}
\end{equation}
The implementation uses a Straight-Through Estimator (STE) \cite{hinton2012neural} with the addition, that it cancels the gradients, when the inputs get too large, as proposed by Hubara \etal\cite{Courbariaux2016}.
The gradient canceling helps the optimization process, since backpropagation no longer increases the absolute value of an input larger than the clipping threshold (which has no actual effect on the loss because $\mathrm{sign}$ does not depend on the absolute value).
Let $c$ denote the objective function, $r_i$ be a real number input, and $r_o\in\{-1,+1\}$ a binary output.
Furthermore, $t_\mathrm{clip}$ is the threshold for clipping gradients, which was set to $t_\mathrm{clip}=1$ in previous works~\cite{Courbariaux2016,Zhou2016}.
Then, the resulting STE is:
% \begin{align}%     \text{Forward:} ~r_o=\mathrm{sign}(r_i)~. ~ \text{Backward:} ~\frac{\partial c}{\partial r_i}=\frac{\partial c}{\partial r_o}1_{|r_i|\leq t_\mathrm{clip}}~.% \end{align}\begin{align}
    \text{Forward:}& ~r_o=\mathrm{sign}(r_i)~. \\
    \text{Backward:}& ~\frac{\partial c}{\partial r_i}=\frac{\partial c}{\partial r_o}1_{|r_i|\leq t_\mathrm{clip}}~.
\end{align}
Liu \etal\cite{Liu_2018_ECCV} claim that a tighter approximation, called $\mathrm{approxsign}$, can be made by replacing the backward pass with
\begin{align}
    ~\frac{\partial c}{\partial r_i}=\frac{\partial c}{\partial r_o}1_{|r_i|\leq t_\mathrm{clip}}\cdot\begin{cases} 
    2-2r_i ~\text{if}~ r_i \geq 0, \\
    2+2r_i ~\text{otherwise}.
    \end{cases}
\end{align}
Since this could also benefit when training a binary network from scratch, we evaluated this in our experiments.

A large amount of calculations in full-precision networks is usually spent on calculating dot products of matrices, as needed for fully connected and convolutional layers.
The computational cost of binary neural networks can be highly reduced by using the $\mathrm{xnor}$ and $\mathrm{popcount}$ CPU instructions, first presented by Rastegari \etal\cite{Rastegari2016}.
% Both operations combined approximate the calculation of dot products of matrices.% That is because element-wise multiplication and addition of a dot product can be replaced with the \texttt{xnor} instruction and then counting all bits, which are set to 1 (\texttt{popcount})~\cite{Rastegari2016}.% Let $x,w \in \{-1,+1\}^n$ denote the input and weights respectively ($n$ is the number of inputs).
They show that the matrix multiplication of a binary input $x$ and weight $w$ can be replaced as follows ($n$ is the number of weights):
\begin{equation}
\label{eqn:xnor-popcount}
    x \cdot w = 2 \odot \mathrm{bitcount}(\mathrm{xnor}(x',w')) - n~.
    % x \cdot w = 2 \cdot \mathrm{bitcount}(\mathrm{xnor}(x',w')) - n~.
\end{equation}
Note, that $x'$ and $w'$ are converted from $x$ and $w$ by replacing $\{-1,+1\}$ with $\{0,1\}$.
% Since preliminary experiments showed, that an implementation as custom CUDA kernels was slower than using the highly optimized cuDNN implementation.
This means normal training methods with GPU acceleration (\eg cuDNN implementation) can be used (the left side of \autoref{eqn:xnor-popcount}).
% The above simplification means, that we can still use normal training methods with GPU acceleration (e.g. cuDNN implementation).% We simply need to convert weights from $\{-1,+1\}$ to $\{0,1\}$ before deployment in a CPU architecture.
Afterwards, we can take advantage of the fast CPU implementation with $\mathrm{xnor}$ and $\mathrm{popcount}$ (the right side of \autoref{eqn:xnor-popcount}) without any accuracy loss (an example can be found in the supplementary material).
To further speedup the final CPU implementation the adjustment by the number of weights can be learned during training (derived from \autoref{eqn:xnor-popcount}):
\begin{equation}
\label{eqn:xnor-popcount-swapped}
    \frac{x \cdot w + n}{2} = \mathrm{bitcount}(\mathrm{xnor}(x',w'))~.
\end{equation}
Further, we decide to use no weight decay during training.
This was done in previous work before without much explanation \cite{Liu_2018_ECCV}, so we add our rationale here:
Since gradient canceling already prevents the network from optimizing to absolute values larger than the clipping threshold (i.e.~the values are already optimal for the current mini-batch), adding weight decay would move these weights away from their optimal values.

\subsection{Scaling Methods}\label{sec:scaling}%     Which related parameters should be evaluated?%         Scaling factor%         Weight update methods: bb, bf, ff%         Clipping threshold for gradient% the approximation error to the full precision counterpart% For both, weights and activations, binarization will introduce an approximation error.% This error can be viewed as noise that is added to each layer \cite{you2010audio, Zhou2017}.% In their analysis, Zhou \etal \cite{Zhou2017} show that the degradation of the prediction accuracy in a CNN linearly depends on the sum of this noise.% Thus, each approximation error should be kept minimal.\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{scaling}
    \caption{General computation graph for a scaled binary convolution. \emph{binconv} is a convolution based on the optimized operation in \autoref{eqn:xnor-popcount}. \emph{reduce} computes a scaling factor for activations or weights (can be chosen differently), and \emph{mul} is a multiplication operation.}
    \label{fig:scaling}
\end{figure}

In this section, we discuss the usage of a scaling factor during training.
Binarization will always introduce an approximation error compared to a full-precision signal.
In their analysis, Zhou \etal\cite{Zhou2017} show that this error linearly degrades the accuracy of a CNN.
One way to reduce the approximation error, is to use scaling factors \cite{Zhou2016,Rastegari2016,Liu_2018_ECCV}.
Generally, they follow the structure as in \autoref{fig:scaling}. 

% Hereby, any real-valued weight filter $\mathbf{W} \in \mathbb{R}^{c \times w \times h}$ is approximated with a binary weight filter $\bar{\mathbf{W}} \in \{-1, +1\}^{{c \times w \times h}}$ and a scaling scalar $\alpha \in \mathbb{R}$ such that $\mathbf{W} \approx \alpha\bar{\mathbf{W}}$.% Following \cite{Rastegari2016}, $\alpha$ can be derived for each output channel by computing the absolute mean of each weight filter $\alpha = \frac{1}{n}\sum|\mathbf{W}|$.% Further, efficient input (i.e. activation) scaling can be done by reducing input $\mathbf{X} \in \mathbb{R}^{c \times w_{\text{in}} \times h_{\text{in}}}$ to an activation-wise scaling matrix $\mathbf{K} \in \mathbb{R}^{w_{\text{out}} \times h_{\text{out}}}$ and a binary input tensor $\bar{\mathbf{X}} \in \{-1, +1\}^{c \times w_{\text{in}} \times h_{\text{in}}}$.

Rastegari \etal\cite{Rastegari2016} choose $\mathrm{reduce}(w) = f_{s_w}(w) = \frac{1}{n}||w||_{1,1}$ for each weight filter $w$.
% , but stopping the resulting gradient $\frac{\partial f_{s_w}}{\partial w} = 0$
They further propose an efficient method for scaling each feature (i.e.~$\mathrm{reduce}(x) = \mathbf{K}$ referring to their paper \cite{Rastegari2016}).

% \begin{align}%     \mathbf{x} \ast \mathbf{x} \approx \mathrm{binconv}(\mathrm{sign}(x), \mathrm{sign}(w)) \cdot f_{s_x}(x) \cdot f_{s_w}(w)% \end{align}
In contrast, Zhou \etal\cite{Zhou2016} reported that a filter-wise weight scaling does not yield improvements.
They use one scalar for all weight filters instead, allowing them to also use a binary convolution in the backward pass.
Liu \etal\cite{Liu_2018_ECCV} suggest to use the weight scaling $f_{s_w}$ only in the backward pass to achieve \emph{magnitude aware gradients}.

The scaling factors should help binary convolutions to increase the value range.
Producing results closer to those of full-precision convolutions and reducing the approximation error.
However, these different scaling values influence specific output channels of the convolution.
% Recent work by Liu \etal \cite{Liu_2018_ECCV} uses a scaling derived from the weights to get gradients closer to full-precision gradients, but only during training.% Theoretically a scaling allows results much closer to a full-precision matrix multiplication.
Therefore, a BatchNorm \cite{ioffe2015batch} layer directly after the convolution (which is used in \architecture{ResNet} and \architecture{DenseNet} architectures) theoretically minimizes the difference between a binary convolution with scaling and one without.
% This is because a BatchNorm scales the inputs channel-wise towards a fixed distribution similar to a normal distribution with zero mean and standard deviation of one.

Thus, we hypothesize that learning a useful scaling factor is made inherently difficult by BatchNorm layers.
We empirically evaluated this in our experiments (see \autoref{sec:results-scaling}), but want to note that this reasoning might not apply if a binary model is fine-tuned from a full-precision model. % instead of being trained from scratch, since it will directly reduce the approximation error.% a useful scaling factor could be derived from a full-precision model instead of being learned.% The result of a convolution operation is multiplied by this scaling factor. % , which is the mean of absolute values% We evaluated whether this scaling factor proves useful in all cases, because it adds additional complexity to the computation and the implementation .\subsection{Network Architectures}\label{sec:architectures}%\begin{figure}[t]
\captionsetup[subfigure]{justification=centering}
\begin{center}
\begin{subfigure}[t]{0.32\linewidth}
   \centering
   \includegraphics[width=0.915\linewidth]{netblocks/resnet-bottleneck}
   \caption{ResNet \\ (bottleneck)}
   \label{fig:netblocks-resnet-bottleneck}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
   \centering
   \includegraphics[width=0.915\linewidth]{netblocks/resnet}
   \caption{ResNet \\ (no bottleneck)}
   \label{fig:netblocks-resnet}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
   \centering
   \includegraphics[width=0.915\linewidth]{netblocks/birealnet}
   \caption{ResNetE \\ (added shortcut)}
   \label{fig:netblocks-resnete}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
   \centering
   \includegraphics[width=0.99\linewidth]{netblocks/densenet-bottleneck}
   \caption{DenseNet \\ (bottleneck)}
   \label{fig:netblocks-densenet-bottleneck}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
   \centering
   \includegraphics[width=0.99\linewidth]{netblocks/densenet}
   \caption{DenseNet \\ (no bottleneck)}
   \label{fig:netblocks-densenet}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
   \centering
   \includegraphics[width=0.99\linewidth]{netblocks/densenet-split}
   \caption{DenseNetE \\ (our suggestion)}
   \label{fig:netblocks-densenet-split}
\end{subfigure}
\end{center}
\caption{
    % A single building block of different network architectures (the length of bold black lines represents the number of filters).
    % (\subref{fig:netblocks-resnet-bottleneck}) The low number of filters in the original \architecture{ResNet} design reduces information capacity for binary neural networks.
    % (\subref{fig:netblocks-resnet}) The \architecture{ResNet} architecture without the bottleneck design (three convolutions are replaced by two with a higher number of filters).
    % (\subref{fig:netblocks-resnete}) The \architecture{ResNet} architecture with an additional shortcut, first introduced by Liu \etal \cite{Liu_2018_ECCV}.
    % (\subref{fig:netblocks-densenet-bottleneck}) The original \architecture{DenseNet} design with a bottleneck in the second convolution operation.
    % (\subref{fig:netblocks-densenet}) The bottleneck of a \architecture{DenseNet} can be replaced by a single $3\times3$ convolution.
    % (\subref{fig:netblocks-densenet-split}) Our suggested change to a \architecture{DenseNet} where a convolution with N filters is replaced by two layers with $\frac{N}{2}$ filters each
   A single building block of different network architectures (the length of bold black lines represents the number of filters).
    (\subref{fig:netblocks-resnet-bottleneck}) The original \architecture{ResNet} design features a bottleneck architecture.
    A low number of filters reduces information capacity for binary neural networks.
    (\subref{fig:netblocks-resnet}) A variation of the \architecture{ResNet} architecture without the bottleneck design.
    The number of filters is increased, but with only two convolutions instead of three.
    (\subref{fig:netblocks-resnete}) The \architecture{ResNet} architecture with an additional shortcut, first introduced by Liu \etal \cite{Liu_2018_ECCV}.
    (\subref{fig:netblocks-densenet-bottleneck}) The original \architecture{DenseNet} design with a bottleneck in the second convolution operation.
    (\subref{fig:netblocks-densenet}) The \architecture{DenseNet} design without a bottleneck.
    The two convolution operations are replaced by one $3\times3$ convolution.
    (\subref{fig:netblocks-densenet-split}) Our suggested change to a \architecture{DenseNet} where a convolution with N filters is replaced by two layers with $\frac{N}{2}$ filters each.
}
\label{fig:netblocks}
\end{figure}%%     Which DNN architectures are suitable for BNN, which are not…%         AlexNet%         Google Inception networks (1-3)%         ResNet%         DenseNet%         Questions: %             Bottleneck for a BNN?%             Width of Information Flow%             Will more connections help?

In this section we describe general concepts for binary deep neural network architectures first.
Afterwards, we show details about \architecture{ResNet}\cite{He2017} and our suggested binary \architecture{DenseNet} architecture \cite{Huang2016}.
% Further, we focus on the effect of reducing weights in favor of increasing the number of connections on the example of the \architecture{DenseNet} architecture.

Before thinking about model architectures, we must consider the main drawbacks of binary neural networks.
First of all, the information density is theoretically 32 times lower, compared to full-precision networks.
Research suggests, that the difference between 32 bits and 8 bits seems to be minimal and 8-bit networks can achieve almost identical accuracy as full-precision networks \cite{Han2015}.
However, when decreasing bit-width to four or even one bit (binary), the accuracy drops significantly \cite{Courbariaux2016,Zhou2016}.
Therefore, the precision loss needs to be alleviated through other techniques, for example by increasing information flow through the network.
We identified three main methods, which help to preserve information despite binarization of the model:
% One possible way to do this, is increasing information flow through the network.% This can be successfully done through shortcut connections, as is proposed in Residual Networks \cite{He2017} and Densely Connected Networks \cite{Huang2016} (see \autoref{fig:netblocks}\subref{fig:netblocks-resnet-bottleneck}, \subref{fig:netblocks-densenet-bottleneck}).% These shortcut connections allow layers later in the network to access information gained in earlier layers despite of information loss through binarization.

First, a binary model should use as many shortcut connections as possible in the network.
These connections allow layers later in the network to access information gained in earlier layers despite of information loss through binarization.
Such shortcut connections were proposed for full-precision model architectures in Residual Networks \cite{He2017} and Densely Connected Networks \cite{Huang2016}.
Furthermore, this means increasing the number of connections between layers should lead to better model performance, especially for binary networks.

Secondly, following the same idea, network architectures including bottlenecks are always a challenge to adopt.
A bottleneck architecture reduces the number of filters and values significantly between the layers, resulting in less information flow through binary neural networks.
% Therefore we hypothesize, that bottleneck parts need to be eliminated or at least reduced in severity for accurate binary neural networks to achieve best results (see \autoref{fig:netblocks}\subref{fig:netblocks-resnet}, \subref{fig:netblocks-densenet}).
Therefore we hypothesize, that either we need to eliminate the bottleneck parts or at least increase the number of filters in these bottleneck parts for binary neural networks to achieve best results.

The third way to preserve information (thus increasing model accuracy) comes from replacing certain crucial layers in a binary network with full precision layers.
The reasoning is as follows:
If layers are binarized, which do not have a shortcut connection, the information lost (due to binarization) can not be recovered in subsequent layers of the network.
This affects the first (convolutional) layer and the last layer (a fully connected layer which has a number of output neurons equal to the number of classes).
These layers generate the initial information for the network or consume the final information for the prediction, respectively.
Therefore, we use full-precision layers for the first and the final layer for all network architectures.
We follow authors of previous work on this decision \cite{Rastegari2016,Zhou2016}, who have empirically shown that binarizing these layers decreases accuracy by a large margin and that the saving of memory and operations is minimal.
Another crucial part of deep networks is the downsampling convolution which converts all previously collected information of the network to smaller feature maps with more channels (this convolution often has stride two and output channels equal to twice the number of input channels).
Any information lost in this downsampling process is effectively no longer available.
Therefore, it should always be considered whether these downsampling layers should be replaced with full-precision layers, even though it increases model size and number of operations.

In the following sections we show how all three methods are applied to a \arch{ResNet} (seen in previous work) and how we applied them to a \arch{DenseNet}. 

% In general, we only use full-precision weights for  for all involved deep networks.% All other layers use binary weights in general.% We follow previous work on this decision \cite{Rastegari2016,Zhou2016}, because binarizing these layers decreases accuracy by a lot, and the saving of memory and operations is minimal.% The intuition is, that these are crucial parts of the network regarding information flow, since these are the layers that either generate the initial information or consume it for the final prediction.% Therefore, any information lost due to binarization in these layers can not be recovered.% On the contrary most of the regular layers in both a \architecture{ResNet} and a \architecture{DenseNet} only add information on top of previous layers (see the blocks described in \autoref{sec:architectures}).% 1 - increase number of shortcuts/connections (E)% 2 - remove bottleneck architecture% 3 - increase number of filters or use full-precision for parts of the network where lost information can not be recovered by shortcuts (start, end, downsampling)% 2 - (see \autoref{fig:netblocks}\subref{fig:netblocks-resnet-bottleneck}, \subref{fig:netblocks-densenet-bottleneck})%  -> (see \autoref{fig:netblocks}\subref{fig:netblocks-resnet}, \subref{fig:netblocks-densenet})% 3 - (see \autoref{fig:netblocks-resnete}), (see \autoref{fig:netblocks-densenet-split})% \begin{figure*}[t]% \captionsetup[subfigure]{justification=centering}% \begin{center}% \begin{subfigure}[t]{0.32\linewidth}%    \centering%    \includegraphics[width=0.95\linewidth]{connections/128}%    \caption{}%    \label{fig:connections-128}% \end{subfigure}% \hfill% \begin{subfigure}[t]{0.32\linewidth}%    \centering%    \includegraphics[width=0.95\linewidth]{connections/64}%    \caption{}%    \label{fig:connections-64}% \end{subfigure}% \hfill% \begin{subfigure}[t]{0.32\linewidth}%    \centering%    \includegraphics[width=0.95\linewidth]{connections/32}%    \caption{}%    \label{fig:connections-32}% \end{subfigure}% \end{center}% \caption{%   Different ways to extract information with 3$\times$3 convolutions.%   (\subref{fig:connections-128}) A large block which generates a high amount of features through one convolution.%   (\subref{fig:connections-64}) Splitting one large block in two, which are half as large and generate half as many features respectively.%   This allows the features generated in the first block to be used by the second block.%   (\subref{fig:connections-32}) This process can be repeated until a minimal desirable block size is found (e.g. 32 for binary neural networks)%   % Binary networks profit from weights devisable by 32 for faster inference.% }% \label{fig:connections}% \end{figure*}% \subsection{Increasing Information Flow}% Old part about mixed DenseNet and ResNet follows here:% % Therefore, we devised a way to increase the number of connections in networks.% To increase the information flow, the blocks which add or derive new features to \architecture{ResNet} and \architecture{DenseNet} (see \autoref{fig:netblocks}) have to be modified.% In full-precision networks, the size of such a block ranges from 64 to 512 for \architecture{ResNet} \cite{He2017}.% An additional shortcut between the two convolutions can be added to a \architecture{ResNet} block (see \autoref{fig:netblocks-resnete}), this was first proposed by Liu \etal \cite{Liu_2018_ECCV}.% Since their architecture contains other changes as well, we generelize this specific change in block design and call it \architecture{ResNetE} (see \autoref{sec:downsample} on their additional changes).% In a \architecture{DenseNet} the size of such a block is called growth rate and the original authors set it to $k=32$ \cite{Huang2016}.% Our experiments showed, that reusing the full-precision \architecture{DenseNet} architecture for binary neural networks does not achieve satisfactory performance, even after replacing the bottleneck architecture.% We propose different possibilities to increase the information flow for a \architecture{DenseNet} architecture.% The growth rate can be increased (\eg $k=64, k=128$), we can use a larger number of blocks, or a combination of both.% Both individual approaches add roughly the same amount of parameters to the network.% To keep the number of parameters equal for a given \architecture{DenseNet} we can halve the growth rate and double the number of blocks at the same time (see \autoref{fig:netblocks-densenet-split}) or vice versa.% Our hypothesis of favoring an increased number of connections over simply adding more weights indicates, that in this case increasing the number of blocks should provide better results (or a reduction of the total number of parameters for equal model performance) compared to increasing the growth rate.% Furthermore, the original full-precision DenseNet reduces the total number of channels in the downsampling layers.% Our experiments showed, that without adjusting the architecture in these downsampling layers, a binary \architecture{DenseNet} achieves results of less than 40\% accuracy on ImageNet.% To preserve information flow in these parts of the network we found two options:% First, we can use no reduction at all, or at least use a lower reduction rate (therefore using a higher number of channels compared to a full-precision architecture).% Since the number of channels is initially low in the first downsampling layer (\eg 384 for $k=128$), we choose not to reduce the number of channels.% In the later parts of the network the filter number is higher (\eg 640 for $k=128$), so we use a slight reduction of 1.4 to keep the model size similar to a \architecture{ResNetE}.% Secondly, we can replace the binary layer in this downsampling layer with a full-precision one, which is discussed in the next section.% \subsection{Bitwidth of downsampling layers}% \label{sec:downsample}% In general, we only use full-precision weights for the first (convolutional) layer and the last layer (a fully connected layer which has a number of output neurons equal to the number of classes) for all involved deep networks.% All other layers use binary weights in general.% We follow previous work on this decision \cite{Rastegari2016,Zhou2016}, because binarizing these layers decreases accuracy by a lot, and the saving of memory and operations is minimal.% The intuition is, that these are crucial parts of the network regarding information flow, since these are the layers that either generate the initial information or consume it for the final prediction.% Therefore, any information lost due to binarization in these layers can not be recovered.% On the contrary most of the regular layers in both a \architecture{ResNet} and a \architecture{DenseNet} only add information on top of previous layers (see the blocks described in \autoref{sec:architectures}).% % Binarizing other layers does not lead to such use binary weights.% Another exeception was made in previous work \etal \cite{Rastegari2016,Liu_2018_ECCV}, where the binary convolution layers in the downsampling blocks of the \architecture{ResNet} architecture are replaced with full-precision ones (see Figure \textbf{TODO}).% Switching these layers to full-precision preserves more information and should lead to better results, but also increases model size.% Previous work has not provided their experimental results for this shortcut and simply claim it drops accuracy significantly.% % They instead attribute their accuracy gains only to their complex training strategy for fine-tuning from a full-precision model.% We hypothesize, that this should provide a significant accuracy gain but only comes at the cost of increased model size and number of operations.% We developed a way to make a similar adjustment to the \architecture{DenseNetE} architecture, by using full-precision convolution layers in the downsampling layers \cite{Huang2016} (see Figure \textbf{TODO}).% Since the full-precision features preserve information much better, we can use reduction rates equal (or even higher) to the reduction rate 2 of a full-precision DenseNet for all downsampling layers.% These higher reduction rates also reduce the number of full-precision (and binary) weights and operations through the whole network, thus allowing us to reach a similar model size and complexity.% However, the information flow should theoretically be better in these layers of the network where information lost due to binarization can not be recovered later in the network.%\begin{figure}[t]
\captionsetup[subfigure]{justification=centering}
\begin{center}
\begin{subfigure}[t]{0.57\linewidth}
   \centering
   \includegraphics[width=0.95\linewidth]{scaling_transition/resnet}
   \caption{ResNet}
   \label{fig:transition-resnet}
\end{subfigure}
\begin{subfigure}[t]{0.39\linewidth}
   \centering
   \includegraphics[width=0.95\linewidth]{scaling_transition/densenet}
   \caption{DenseNet}
   \label{fig:transition-densenet}
\end{subfigure}
\end{center}
\caption{
The downsampling layers of \arch{ResNet} and \arch{DenseNet}.
The bold black lines mark the downsampling layers which can be replaced with full-precision layers.
If we use this full-precision layer in a \arch{DenseNet}, we increase the reduction rate to reduce the number of channels (the dashed lines depict the number of channels without reduction).
}
\label{fig:downsampling}
\end{figure}%\subsubsection{ResNet Architecture}\label{sec:architectures-resnet}

The \architecture{ResNet} architecture, introduced by He \etal\cite{He2017}, was the first model architecture that allowed to train models with 18 or more (up to 152) layers.
\architecture{ResNet} models combine the information of all previous layers with shortcut connections.
This is done by adding the input of a block to its output with an identity connection. 
Consequently, these shortcut connections add no extra weights and very little computational cost, while leading to more meaningful gradients in deeper layers.
The bottleneck of a \arch{ResNet} can be removed by replacing the three convolution layers (kernel sizes 1, 3, 1) of a regular \arch{ResNet} block with two three by three convolution layers with a higher number of filters (see \autoref{fig:netblocks-resnet-bottleneck}, \subref{fig:netblocks-resnet}).

Increasing the number of connections can be done by reducing the block size from two convolutions per block to one convolution per block, as proposed by Liu \etal\cite{Liu_2018_ECCV}.
This leads to twice the amount of shortcuts, as there are as many shortcuts as blocks, if the amount of layers is kept the same (see \autoref{fig:netblocks-resnete}).
%modifying an additional shortcut between these two convolution layers (see \autoref{fig:netblocks-resnete}).%This additional shortcut was first proposed by Liu \etal \cite{Liu_2018_ECCV}.
However, their method also incorporates other changes to the \arch{ResNet} architecture. 
Therefore we call this specific change in the block design \architecture{ResNetE} (short for Extra shortcut).

Their second change was replacing the downsampling convolution layer (see \autoref{fig:transition-resnet}).
%This was first proposed by Rastegari \etal \cite{Rastegari2016}, but neither work separately evaluates the influence of this shortcut on model accuracy and size.
This was first proposed by Rastegari \etal\cite{Rastegari2016}, but neither work quantifies the exact accuracy gain nor the impact on the model size of this design choice.

\subsubsection{DenseNet Architecture}\label{sec:architectures-densenet}%In \architecture{DenseNets}, proposed by Huang \etal \cite{Huang2016}, the shortcut connections are instead built by concatenating the input of a block to its output. \architecture{DenseNets}, proposed by Huang \etal\cite{Huang2016}, use shortcut connections that, contrary to \arch{ResNets}, concatenate the input of a block to its output (see \autoref{fig:netblocks-densenet-bottleneck}, \subref{fig:netblocks-resnet}).
Therefore, new information gained in one layer can be reused throughout the entire depth of the network.
To reduce the total model size, the original full-precision architecture includes a bottleneck design for each block and additionally reduces the number of channels in transition layers.
This effectively keeps the network at a significantly smaller total size, even though the concatenation adds new information into the network every layer.
%A \architecture{DenseNet} has blocks which derive new features similar to those of a \arch{ResNet}.%The difference is, that these blocks concatenate new features to their inputs.
The number of newly appended features is called growth rate ($k$) and Huang \etal\cite{Huang2016} use $k=32$.
The bottleneck of the \arch{DenseNet} architecture can be modified by replacing the two convolution layers (kernel sizes 1 and 3) with one $3 \times 3$ convolution (see \autoref{fig:netblocks-densenet-bottleneck}, \subref{fig:netblocks-densenet}). 

However, our experiments showed that reusing the full-precision \architecture{DenseNet} architecture for binary neural networks does not achieve satisfactory performance, even after this change.
There are different possibilities to increase the capacity of a binary \architecture{DenseNet} architecture.
The growth rate can be increased (\eg$k=64, k=128$), we can use a larger number of blocks, or a combination of both.
Both individual approaches add roughly the same amount of parameters to the network.
To keep the number of parameters equal for a given \architecture{DenseNet} we can halve the growth rate and double the number of blocks at the same time (see \autoref{fig:netblocks-densenet-split}) or vice versa.
We assume that in this case increasing the number of blocks should provide better results compared to increasing the growth rate. 
This assumption is derived from our second hypothesis: favoring an increased number of connections over simply adding weights.
Similar to a \arch{ResNet} we refer to this adjustment as the \architecture{DenseNetE} architecture.
However, we note that the actual number of layers and growth rate can be chosen rather freely and evaluate different configurations.

Finally, another characteristic difference of a \arch{DenseNet} compared to a \arch{ResNet} is that the downsampling layer reduces the number of channels \cite{Huang2016}.
% Finally, the binary downsampling layer of a DenseNet can be replaced by a full-precision convolution instead, which should increase accuracy (see Figure \textbf{TODO}).% Furthermore, the original full-precision DenseNet reduces the total number of channels in the downsampling layers.
Our experiments showed, that without adjusting the architecture in these downsampling layers, a binary \architecture{DenseNet} achieves results of less than 40\% accuracy on ImageNet.
To preserve information flow in these parts of the network we found two options:
On the one hand, we can use no reduction at all, or at least use a lower reduction rate (using a higher number of channels compared to a full-precision architecture).
Since the number of channels is initially low in the first downsampling layer (\eg 384 for $k=128$), we do not need to reduce the number of channels in the first transition layer.
However, in the later parts of the network the filter number is higher (\eg 640 at the second transition for $k=128$), so we use a slight reduction of 1.4 to keep the model size similar to a binary \architecture{ResNetE} of equal complexity.
On the other hand, we can replace the binary layer in this downsampling layer with a full-precision one (see \autoref{fig:transition-densenet}).
Since the full-precision convolution preserves more information, we can use reduction rates equal to (or even higher than) the reduction rate 2 of a full-precision \arch{DenseNet} for all downsampling layers.
These higher reduction rates also reduce the number of full-precision (and binary) weights and operations through the whole network, thus allowing us to reach a similar (or even lower) model size compared to the first approach.

Because of the previous reasons, we coupled the decision whether to use a binary or a full-precision downsampling convolution with the choice of reduction rate.
The two variants we compare in our experiments (see \autoref{sec:experiments}) are thus called \emph{full-precision downsampling with high reduction} (halve the number of channels in all transition layers) and \emph{binary downsampling with low reduction} (no reduction in the first transition, divide number of channels by 1.4 in the second and third transition).

% Since it also slightly increases model size and number of full-precision operations.% The influence of this technique on the \architecture{ResNet} and the \architecture{DenseNet} architecture (TODO: ref).% Another point of discussion is how to train a neural network during the backward pass.% Usually a full-precision value is stored during training of a neural network (i.e. in forward and backward pass).% However, except for the first and last layer of the network, a binary neural network model can only use binary weights.% Therefore, for accurate training, we need to consider binary values during calculation of forward and backward pass.% These binary values can be calculated through the sign function during the forward pass.% In the backward pass, you could directly calculate the gradients on the binary weights and update the weights accordingly through back propagation (method \texttt{bb}).% This method needs a large gradient to cause the weights to flip to another value, because small gradients will not cause change in weights.% Alternatively, the full-precision value that is stored could only be converted to binary values as needed before computations.% In this case gradients and weight updates are calculated based on the full-precision value, and applied to it directly (method \texttt{ff}).% This way, small weight changes could accumulate, and once the sign of a weight flips, the binary value changes.% Alternatively, the gradients can be calculated on the computed binary value, but are then applied to the full-precision value (method \texttt{bf}).% and the threshold for clipping the gradients% Another parameter specific to binary neural networks, is the clipping threshold $t_\mathrm{clip}$.% The value of this parameter influences which gradients are canceled and which are not.% Therefore the parameter has a significant influence on the training result, and we evaluated different values for this parameter (also in \autoref{sec:experiments-acc}).% \subsection{Visualization of Trained Models}% %     How could we distinguish the difference between full-precision and binary neurons?% %         Visualization of trained models (What did binary conv layer learn?)% %         Visualization using visualBackProp and some typical examples% %         (more visualization techniques in the future work)% In this section, we show how we determine the differences of learned features between binary neural networks and full-precision networks with feature visualization.% We used an implementation of the \emph{deep dream} visualization \cite{mordvintsev2015} to visualize what the trained models had learned (see \autoref{fig:deepdream}).% The core idea is a normal forward pass followed by specifying an optimization objective, such as maximizing a certain neuron, filter, layer, or class during the backward pass.% % Based on this objective, we calculate the gradients with backpropagation, and update the image accordingly with a small step.% % For our visualization we choose one filter of the last convolutional layers in the network for each image.% % TODO% Another tool we used for visualization is \emph{VisualBackProp}~\cite{Bojarski2016} (see ...).% It uses the high certainty about relevancy of information of the later layers in the network together with the higher resolution of earlier layers to efficiently identify those parts in the image which contribute most to the prediction.
% !TeX root = ../paper_for_review.tex\section{Experiments and Discussion}\label{sec:experiments}% Accuracy related evaluation %     Dataset: MNIST, CIFAR10 and ImageNet%     Get the result on mnist using bf-bb-ff%     Get result on CIFAR10 using densenet-21-binary (we already have the resnet-18 result on it)%     ...% Evaluation of model sizes (compared to accuracy?)%     Compare different depths of resnet% Compare accuracies of different network architectures (resnet/densenet/...) with same size% discussion for transition blocks% First, we focus on classification accuracy as a measure to determine which parameter choices are better.% Afterwards, we examine the results of our feature visualization techniques.% Afterwards, we briefly examine the feasibility of our CPU implementation for inference by measuring execution time.% \subsection{Classification Accuracy}% \label{sec:experiments-acc}% subsection: architecture evaluation% bottleneck design comparison% alexnet/vgg standard design - training difficult% resnet simple shortcut - training from scratch possible% more connections? - densenet% subsection: evaluate type of shortcut connections% densenet - resnet% subsection: hyperparameters% weight update mode% gradient clipping thresholding% factor

Following the structure of the previous section, we provide our experimental results to analyze our method with respect to different parameters and techniques.
We apply classification accuracy as the general measurement to evaluate the different architectures, methods etc.
For brevity, the term \emph{accuracy} always refers to the Top-1 accuracy, unless otherwise noted.
Also, differences in accuracies will be noted as $x$\%, but refer to percent point differences.
We use the MNIST~\cite{lecun-mnisthandwrittendigit-2010}, CIFAR-10~\cite{cifar10} and ImageNet~\cite{imagenet_cvpr09} datasets in terms of different levels of task complexity.
The experiments were performed on a work station with an Intel(R) Core(TM) i9-7900X CPU, 64 GB RAM and 4$\times$GeForce GTX1080Ti GPUs.
All models are trained with the Adam optimizer \cite{kingma2014adam} with an initial learning rate (alpha) of $10^{-2}$ for CIFAR-10 and $10^{-3}$ for ImageNet.
We trained our ImageNet models for 40 or 50 epochs, and multiply the learning rate by 0.1 at epochs 34 and 37, or epochs 40 and 45 respectively.
We use a Gaussian distribution to initialize the weights in the network according to the method proposed by Glorot and Bengio~\cite{glorot2010understanding}.

% TODO: if we need space remove this table and the text belonging to it%% table of weight update mode\begin{table}[]
\caption{Evaluation of our binary model performance on the MNIST and CIFAR-10 data sets compared to the results of Yang \etal \cite{HPI_xnor}.}
% \begin{tabular}{| p{1.5cm} | p{2.08cm} | p{1.1cm} | p{1cm} | p{1cm} |}
\begin{tabular}{|l|l|l|l|l|}
    \hline
                     & Architecture & \mrcell{Model\\size} & \mrcell{Accu-\\ racy} & \mrcell{Acc. \\ (\cite{HPI_xnor})} \\
% \begin{tabular}{| p{1.8cm} | p{2.4cm} | p{1.5cm} | p{5.1cm} |}
%     \hline
%                      & Architecture & Accuracy & Model Size (Binary/Full Precision) \\
    \hline
MNIST & LeNet  & 202KB & \textbf{99.0\%} & 97\% \\
    \hline
CIFAR-10 & ResNetE-18  & 1.39MB & \textbf{87.6\%} & 86\% \\ \cline{2-3} 
    \hline
CIFAR-10 & DenseNetE-21  & 1.49MB & \textbf{90.3\%} & - \\ \cline{2-3}    
    \hline 
\end{tabular} 
\label{tab:mnist-cifar-overview}
\end{table}% We evaluated the influence of this full-precision convolution.% We did not apply a scaling factor as proposed by Rastegari et al. in~\cite{Rastegari2016} in our experiments.% Instead we examined a (similar) scaling factor method proposed by Zhou et al.~\cite{Zhou2016}.% However, as shown in our hyperparameter evaluation (page \pageref{sec:scale-factor}) we chose not to apply this scaling factor for our other experiments.
First, we show the results of a binary \emph{LeNet} for the MNIST dataset and a binary \architecture{ResNetE-18} and a \architecture{DenseNetE-21} in \autoref{tab:mnist-cifar-overview} compared to the approach of Yang \etal\cite{HPI_xnor}.
These results prove that our approach and implementation work on simple datasets, such as MNIST and CIFAR-10, and can reach favorable results compared to other work with the same approach and the same architecture.
Moreover, they reveal promising results with our proposed \architecture{DenseNetE} architecture, since the model size is increased by only 0.1MB for a 2.7\% increase in accuracy.
% Additionally our results are about 2 percent better, when using a similar model, but that we can further improve the results on CIFAR-10 .

In the following sections, we first evaluate and discuss the influence of using scaling factors and the $\mathrm{approxsign}$ function in the backward pass of the activations for the \architecture{ResNetE} network.
Following this, we evaluate the impact of the amount of blocks for our proposed \architecture{DenseNet} architecture.
Then, the design choice of using binary or full-precision downsampling layers for \architecture{DenseNet} and \architecture{ResNetE} models is empirically verified.
Furthermore, we show how the bit-width of downsampling layers changes model performance.
%based both, the \architecture{ResNetE} and the \architecture{DenseNetE} architecture.\begin{table}[]
\caption{
    % TODO mention which scaling is done here, binet or xnor style
    The influence of using scaling, a full-precision downsampling convolution, and the $\mathrm{approxsign}$ function on the CIFAR-10 dataset based on a \architecture{ResNetE-18}.
    % Scaling produces worse results for training a model from scratch, while using full-precision downsampling increases results slightly.
    Using $\mathrm{approxsign}$ instead of $\mathrm{sign}$ slightly boosts accuracy, but only if training a model with scaling factors. % TODO move to text not in figure?
}
\begin{tabular}{|l|l|l|l|}
\hline
\lmrcell{Use \\ scaling \\ of \cite{Rastegari2016}} & \mrcell{Downsampl. \\ convolution} & \mrcell{Use \\ $\mathrm{approxsign}$ \\ of \cite{Liu_2018_ECCV}} & \mrcell{Accuracy \\ Top1/Top5} \\ \hline
\multirow{4}{*}{no}  & \multirow{2}{*}{binary}         & yes         & 84.9\%/99.3\%                   \\ \cline{3-4} 
                     &                                 & no          & 87.2\%/\textbf{99.5\%}          \\ \cline{2-4} 
                     & \multirow{2}{*}{full-precision} & yes         & 86.1\%/99.4\%                   \\ \cline{3-4} 
                     &                                 & no          & \textbf{87.6\%}/\textbf{99.5\%} \\ \hline
\multirow{4}{*}{yes} & \multirow{2}{*}{binary}         & yes         & 84.2\%/99.2\%                   \\ \cline{3-4} 
                     &                                 & no          & 83.6\%/99.2\%                   \\ \cline{2-4} 
                     & \multirow{2}{*}{full-precision} & yes         & 84.4\%/99.3\%                   \\ \cline{3-4} 
                     &                                 & no          & 84.7\%/99.2\%                   \\ \hline
\end{tabular}
\label{tab:cifar-scaling-downsampling-approxsign}
\end{table}\begin{table}[]
\caption{
    The influence of using scaling, a full-precision downsampling convolution, and the $\mathrm{approxsign}$ function on the ImageNet dataset based on a \architecture{ResNetE-18}.
}
\begin{tabular}{|l|l|l|l|}
\hline
\lmrcell{Use \\ scaling \\ of \cite{Rastegari2016}} & \mrcell{Downsampl. \\ convolution} & \mrcell{Use \\ $\mathrm{approxsign}$ \\ of \cite{Liu_2018_ECCV}} & \mrcell{Accuracy \\ Top1/Top5} \\ \hline
\multirow{4}{*}{no}  & \multirow{2}{*}{binary}         & yes         & 54.3\%/77.6\%           \\ \cline{3-4}
                     &                                 & no          & 54.4\%/77.5\%           \\ \cline{2-4}
                     & \multirow{2}{*}{full-precision} & yes         & 56.6\%/\textbf{79.3\%}  \\ \cline{3-4}
                     &                                 & no          & \textbf{56.7\%}/79.2\%  \\ \hline
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\multirow{4}{*}{yes} & \multirow{2}{*}{binary}         & yes         & 53.3\%/76.4\%           \\ \cline{3-4} 
                     &                                 & no          & 52.7\%/76.1\%           \\ \cline{2-4} 
                     & \multirow{2}{*}{full-precision} & yes         & 55.3\%/78.3\%           \\ \cline{3-4} 
                     &                                 & no          & 55.6\%/78.4\%           \\ \hline
% TODO: if experiments do not finish in time, replace above 4 lines with the following one:
% \multirow{2}{*}{yes} &  binary                         & no          & 52.7\%/76.1\%           \\ \cline{2-4} 
%                      &  full-precision                 & no          & 55.6\%/78.4\%           \\ \hline
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{tabular}
\label{tab:imagenet-scaling-downsampling-approxsign}
\end{table}% Scaling produces worse results for training a model from scratch, while using full-precision downsampling increases results slightly.% Using $\mathrm{approxsign}$ instead of $\mathrm{sign}$ slightly boosts accuracy, but only if training a model with scaling factors.\subsection{Scaling Methods}\label{sec:results-scaling}

In this section, we discuss the influence of scaling factors (as proposed by Rastegari \etal\cite{Rastegari2016}) on the accuracy of our trained models based on the \architecture{ResNetE} architecture.
First, the results of our CIFAR-10 experiments verify our hypothesis, that applying scaling when training a model from scratch does not lead to better accuracy (see \autoref{tab:cifar-scaling-downsampling-approxsign}).
All models show a decrease of accuracy between 0.7\% and 3.6\% when applying scaling factors.
% Secondly, we evaluated the influence of scaling on our best model application for the ImageNet dataset (see \autoref{tab:imagenet-scaling-downsampling-approxsign}).% The result is similar, applying scaling reduces model accuracy by 1.2\%.% TODO: use above two lines if training is not finished by the deadline instead of bottom two lines% TODO: otherwise adapt percentage below%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Secondly, we evaluated the influence of scaling for the ImageNet dataset (see \autoref{tab:imagenet-scaling-downsampling-approxsign}).
The result is similar, applying scaling reduces model accuracy ranging from 1.0\% to 1.4\%.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We conclude that the scaling is ineffective and suspect two arguments for this:
the model can not learn a useful scaling factor when training from scratch or the BatchNorm layers following each convolution layer absorb the effect of the scaling factors.
If the first reason applies this is a limitation of our approach of training from scratch, and might not apply to trainings based on fine-tuning a full-precision model.
If the latter reason applies it should neither increase nor decrease accuracy, which is what we can see for CIFAR-10 (but not for ImageNet) and might still help approaches which are based on fine-tuning.

\subsection{Backward Pass of the Sign Function}\label{sec:results-approx}

In this section, we discuss the influence of the backward pass used for the $\mathrm{sign}$ function.
We compared the regular backward pass, called $\mathrm{sign}$, with the adapted backward pass, called $\mathrm{approxsign}$ (see \autoref{sec:implementation-details}).
First, the results of our CIFAR-10 experiments seem to depend on whether we use scaling or not.
If we use scaling, both functions perform similarly (see \autoref{tab:cifar-scaling-downsampling-approxsign}).
Without scaling the $\mathrm{approxsign}$ function leads to less accurate models on CIFAR-10.

In our experiments on ImageNet, the performance difference between the use of the functions is minimal (see \autoref{tab:imagenet-scaling-downsampling-approxsign}). 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Using one scaling method over the other gives no significant change in model accuracy.
Using one scaling method over the other gives no significant change in model accuracy with one exception: the usage of the sign function results in an accuracy increase of 0.6\% if we use scaling and no full-precision shortcut.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Therefore, we conclude that applying the $\mathrm{approxsign}$ function instead of the $\mathrm{sign}$ function seems to be specific to fine-tuning from full-precision models.

\begin{table}[]
\caption{
The accuracy of different binary \arch{DenseNet} models by successively splitting blocks evaluated on ImageNet.
As the number of connections increases, the model size (and number of binary operations) changes marginally, but the accuracy increases significantly.
}
\begin{tabular}{|l|l|l|l|}
\hline
\lmrcell{Blocks \\ (layers)} & \mrcell{Growth-\\ rate} & \mrcell{Model size \\ (binary)} & \mrcell{Accuracy \\ Top1/Top5}      \\ \hline
8 (13)   & 256         & 3.31 MB    & 50.2\%/73.7\% \\ \hline
16 (21)  & 128         & 3.39 MB    & 52.7\%/75.7\% \\ \hline
32 (37)  & 64          & 3.45 MB    &\textbf{54.3\%}/\textbf{77.3\%} \\ \hline
\end{tabular}
\label{tab:densenet-growth-rate-vs-layers}
\end{table}\subsection{Splitting Layers of DenseNet}\label{sec:results-split-densenet}

We tested our proposed architecture change by comparing \arch{DenseNet} models with varying growth rates and number of blocks (and thus layers).
The results show, that increasing the number of connections by adding more layers over simply increasing growth rate increases accuracy in an efficient way (see \autoref{tab:densenet-growth-rate-vs-layers}).
Doubling the number of blocks and halving the growth rate leads to an accuracy gain ranging from 1.4\% to 2.5\%.
However, it seems to have diminishing returns, and training of very deep binary \arch{DenseNet} becomes slow, since less of the calculations can be parallelized.
We note that during inference on low-powered devices this is less of a problem compared to training, since the total number of operations is similar between the models (and no additional memory is needed during inference for storing intermediate results, \eg the outputs of the sign function).
Therefore, we have not trained even more highly connected models, but highly suspect that this would increase accuracy even further.
The total model size slightly increases, since every second half of a split block has slightly more inputs compared to those of a double-sized normal block.
In conclusion, our technique of increasing number of connections is highly effective and size-efficient for a binary \arch{DenseNet}.

% ----------- TABLE 5\begin{table}[]
\caption{
The difference of performance for different binary \arch{DenseNet} models when using different downsampling methods (see \autoref{sec:architectures-densenet}) evaluated on ImageNet.
%A full-precision downsampling with higher reduction leads to more accurate results with smaller model size.
}
\begin{tabular}{|l|l|l|l|}
\hline
\lmrcell{Blocks \\ (layers), \\ growth-rate} & \mrcell{Model \\ size \\ (binary)} & \mrcell{Downsampl. \\ convolution, \\ reduction} & \mrcell{Accuracy \\ Top1/Top5}      \\ \hline
\multirow{2}{*}{16 (21), 128}           & 3.39 MB    & binary, low   & 52.7\%/75.7\%                   \\ \cline{2-4}
                                        & 3.03 MB    & FP, high  & \textbf{55.9\%}/\textbf{78.5\%} \\ \hline
\multirow{2}{*}{32 (37), 64}            & 3.45 MB    & binary, low   & 54.3\%/77.3\%                   \\ \cline{2-4}
                                        & 3.08 MB    & FP, high  & \textbf{57.1\%}/\textbf{80.0\%} \\ \hline
\end{tabular}
\label{tab:densenet-downsampling}
\end{table}%TODO remove percentage sign? Might improve readability% ----------- TABLE 6\begin{table}[]
\caption{
Comparison on the ImageNet dataset \cite{imagenet_cvpr09} of our proposed network with binary downsampling branches (see \autoref{sec:architectures}) to ABC-Net \cite{lin2017towards}, which uses this design choice as well.
}
% \begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
Model          & Our result (model size)                                  & ABC-Net \cite{lin2017towards} \\ \hline
ResNet-18      & 54.4\%/77.5\%                   (3.36 MB)                & 42.7\%/67.6\%                 \\ \hline
DenseNet       & 54.3\%/77.3\%                   (3.45 MB)                & -                             \\ \hline
ResNet-34      & 58.1\%/80.6\%                   (4.59 MB)                & -                             \\ \hline
\end{tabular}
% \end{center}
\label{tab:imagenet-binary-downs}
\end{table}% ---------- TABLE 7\begin{table*}[]
\caption{
Comparison of our methods to state-of-the-art binary models on the ImageNet dataset \cite{imagenet_cvpr09}. 
All these methods use full-precision weights in the convolution layers of the downsampling branches (see \autoref{sec:architectures}).
}
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
Model        & Our result                    (model size)             & BiReal-Net \cite{Liu_2018_ECCV} & TBN \cite{Wan_2018_ECCV}  & XNOR-Net \cite{Rastegari2016} & Full-precision \\ \hline
ResNet-18    & \textbf{56.9\%}/\textbf{79.7\%} (4.0 MB)                 & 56.4\%/79.5\%                   & 55.6\%/74.2\%             & 51.2\%/73.2\%                 & 69.3\%/89.2\%  \\ \hline
DenseNet     & \textbf{58.6\%}/\textbf{81.0\%} (3.99 MB)                & -                               & -                         & -                             & -              \\ \hline
ResNet-34    & 60.0\%/82.0\%                   (5.23 MB)                & \textbf{62.2\%}/\textbf{83.9\%} & 58.2\%/81.0\%       & -                             & 73.3\%/91.3\%  \\ \hline
\end{tabular}
\end{center}
\label{tab:imagenet-full-downs}
\end{table*}\subsection{Downsampling Layers}\label{sec:results-downsampling}

We evaluated the difference between using binary and full-precision downsampling layers for both \arch{ResNet} and \arch{DenseNet}.
First, we examine the results of \arch{ResNetE-18} on CIFAR-10.
Using full-precision downsampling over binary leads to an accuracy gain between 0.3\% and 2.3\% (see \autoref{tab:cifar-scaling-downsampling-approxsign}).
However, the model size also increases by 0.64 MB from 1.39 MB to 2.03 MB, which is is arguably too much for this minor increase of accuracy.
Our results on the \arch{ResNet} architecture show a significant difference on ImageNet (see \autoref{tab:imagenet-scaling-downsampling-approxsign}).
The accuracy increases by 2\% when using full-precision downsampling.
Similar to CIFAR-10, the model size increases by 0.64 MB, in this case from 3.36 MB to 4.0 MB.
The larger base model size makes the relative model size difference lower and provides a stronger argument for this trade-off.
We conclude that the increase in accuracy is significant, especially for ImageNet. % todo concluding that something is significant? maybe rephrase here
However, in our opinion, it does not seem to be large enough to just neglect to acknowledge the significant increase in model size.

In the following we present our results of a binary \arch{DenseNet} when using a full-precision downsampling with high reduction over a binary downsampling with low reduction.
The results of a binary \arch{DenseNet-21} with growth rate 128 for CIFAR-10 result show an accuracy increase of 2.7\% from 87.6\% to 90.3\%.
The model size increases from 673 KB to 1.49 MB.
This is an arguably sharp increase in model size, but the model is still smaller than a comparable \arch{ResNet-18} with a much higher accuracy.
The results of two \arch{DenseNet} architectures (16 and 32 blocks combined with 128 and 64 growth rate respectively) for ImageNet show an increase of accuracy ranging from 2.8\% to 3.2\% (see \autoref{tab:densenet-downsampling}).
Further, because of the higher reduction rate, the model size decreases by 0.36 MB at the same time.
This shows a higher effectiveness and efficiency of using a full-precision downsampling layer for a \arch{DenseNet} compared to a \arch{ResNet}.
% Therefore we conclude, that using a full-precision is completely justified.\subsection{Comparison to State-of-the-art Approaches}\label{sec:results-state-of-the-art}

We evaluated our overall approach of training from scratch for a \arch{ResNetE-18}, a \arch{ResNetE-34} and our new architecture \arch{DenseNetE}.
Following our results on the influence of the downsampling convolution, we split the comparison between architectures with a full-precision and a binary downsampling convolution.

First, we would like to present the results for models with a binary downsampling convolution (see \autoref{tab:imagenet-binary-downs}).
In this case, we use the best \arch{DenseNetE-37} model with the highest number of connections as shown in our previous experiments (see \autoref{sec:results-split-densenet}).
It has a size comparable to that of a \arch{ResNet-18}.
We recognize that our training strategy of training from scratch leads to excellent results compared to the \arch{ABC-Net}\cite{lin2017towards} approach for both \arch{ResNets} with 18 and 34 layers.
The accuracy of our \arch{DenseNetE-37} is close to that of a \arch{ResNet} (with a difference of 0.2\%), but it does not improve accuracy.
This shows that the techniques applied to a \arch{ResNetE} and our \arch{DenseNetE-37} already successfully increase accuracy by a large margin, even without using full-precision downsampling layers.

Secondly, we also examine the results of models with full-precision downsampling layers (see \autoref{tab:imagenet-full-downs}).
We chose a growth rate of 160 and a reduction rate of 2.2 for a \arch{DenseNetE-21} to match the model size and complexity of a \arch{ResNetE-18} as closely as possible (3.99 MB and 4 MB respectively).
Our results show, that we can achieve results similar to a \arch{BiReal-Net}\cite{Liu_2018_ECCV} for 18 layers.
The accuracy is even slightly (0.5\%) higher, even though \arch{BiReal-Net} is trained with a more complex training strategy.
But, when we compare a \arch{ResNet-E} trained from scratch to a \arch{BiReal-Net} with 34 layers, we see that accuracy of our approach is 2\% lower in comparison.
Inspecting our training loss had us suspect, that this gap could be reduced by adapting our choice of optimizer, learning rates, and training for more epochs, but did not want to change this choice to keep our own results comparable.
Moreover, our proposed \arch{DenseNetE-21} model reaches 58.6\% (an overall improvement of 2.2\% over \arch{BiReal-Net-18}) with the same model size.
We conclude that accurate binary models can be successfully trained from scratch and do not necessarily need to use a fine-tuning strategy based on pretrained full-precision models.
% Further, our suggested \arch{DenseNetE-21} can surpass previous binary model performance.% TODO: change model sizes to Mbit (better comparable to XNOR and Bireal Paper)?% % Table AlexNet, Inception, ResNet% \begin{table*}[t]% \caption{% Classification accuracy (Top-1 and Top-5) of several popular deep learning architectures using binary weights and activations in their convolution and fully connected layers.% Full-precision results are denoted with FP.% \architecture{ResNet-34-thin} applies a lower number of filters ($64, 64, 128, 256, 512$), whereas \architecture{ResNet-34-wide} and \architecture{ResNet-68-wide} use a higher number of filters ($64, 128, 256, 512, 1024$).}% \begin{center}% \begin{tabular}{ |c|c|c|c|c|c|c| }% \hline% Architecture & Top-1 & Top-5 & Epoch & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}c@{}}Model Size\\(Binary/Full Precision)\end{tabular}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}c@{}}Top-1\\FP\end{tabular}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}c@{}}Top-5\\FP\end{tabular}} \\% \hline% AlexNet & 30.2\% &  54.0\% & 70 & 22MB/233MB & 62.5\% & 83.0\% \\% \hline% InceptionBN & 24.8\% & 48.0\% & 80 & 8MB/44MB & - & 92.1\% \\% \hline% ResNet-18 & 42.0\% & 66.2\% & 37 & 3.4MB/45MB & - & - \\% \hline% ResNet-18 (from \cite{lin2017towards}) & 42.7\% & 67.6\% & - & - & - & - \\% \hline% % \multicolumn{1}{|l|}{\begin{tabular}[c]{@{}c@{}}ResNet-18 (reference\\result from \cite{lin2017towards})\end{tabular}} & 42.7\% & 67.6\% & - & - & - & - \\% % \hline% ResNet-26 bottleneck & 25.2\% & 47.1\% & 40 & - & - & - \\% \hline% ResNet-34-thin & 44.3\% & 69.1\% & 40 & 4.8MB/84MB & 78.2\% & 94.3\% \\% \hline% ResNet-34-wide & 54.0\% & 77.2\% & 37 & 15MB/329MB & - & - \\% \hline% ResNet-68-wide & 57.5\% & 80.3\% & 40 & 25MB/635MB & - & - \\% \hline% \end{tabular}% \end{center}% \label{tab_dn_eval1}% \end{table*}% % %% \subsection{Popular Deep Architectures}% %% Q1: which deep learning architectures are suitable for BNN?% %%% why vgg and inception network not good, --> we also did further experiment on bottelneck on resnet-18% In this experiment our intention is to evaluate a selection of popular deep learning architectures by using binary weights and activations.% We wanted to discover positive and negative design patterns with respect to training binary neural networks.% The first experiment is based on AlexNet~\cite{krizhevsky2012imagenet}, InceptionBN~\cite{ioffe2015batch} and ResNet~\cite{He2017} (see \autoref{tab_dn_eval1}).% % \autoref{tab_dn_eval1} depicts the results.   % Using the AlexNet architecture, we were not able to achieve similar results as presented by Rastegari et al.~\cite{Rastegari2016}.% This might be due to us disregarding their scaling factor approach.% % The result of InceptionBN is even worse than AlexNet, we were quite surprised at this finding.% Further, we were quite surprised that InceptionBN achieved even worse results than AlexNet.% Our assumption for the bad result is that the Inception series applies ``bottleneck'' blocks intended to reduce the number of parameters and computational costs, which may negatively impact information flow.% With this idea, we continued the experiments with several ResNet models, and the results seem to verify our conjecture.% If the \architecture{ResNet} architecture is used for full-precision networks, gradually increasing the width and depth of the network yields improvements in accuracy.% % As expected, the bottleneck blocks again the performance, we were not able to obtain accuracy gain by ResNet-26 bottleneck compared to ResNet-18.% On the contrary, when using binary neural networks, the bottleneck design seems to limit the performance as is expected.% We were not able to obtain higher accuracy with the \architecture{ResNet-26 bottleneck} architecture compared to \architecture{ResNet-18}.% Additionally, if we only increase the depth, without increasing the number of filters, we were not able to obtain a significant increase in accuracy (\architecture{ResNet-34-thin} compared to \architecture{ResNet-18}).% To test our theory, that the bottleneck design hinders information flow, we enlarged the number of filters throughout the network from $(64, 64, 128, 256, 512)$ to $(64, 128, 256, 512, 1024)$.% This achieves almost \textbf{10\%} top-1 accuracy gain in a ResNet architecture with 34 layers (\architecture{ResNet-34-wide}).% Further improvements can be obtained by using \architecture{ResNet-68-wide} with both increased depth and width.% This suggests, that network width and depth should be increased simultaneously for best results.% We also conducted experiments on further architectures such as \architecture{VGG-Net}~\cite{simonyan2014very}, \architecture{Inception-resnet}~\cite{szegedy2017inception} and \architecture{MobileNet}~\cite{Howard2017}.% Although we applied batch normalization, the \architecture{VGG}-style networks with more than 10 layers have to be trained accumulatively (layer by layer),% since the models did not achieve any result when we trained them from scratch.% Other networks such as \architecture{Inception-ResNet} and \architecture{MobileNet} are also not appropriate for the binary training due to their designed architecture (bottleneck design and models with a low number of filters).% We assume that the shortcut connections of the \architecture{ResNet} architecture can retain the information flow unobstructed during the training.% % which can also be considered as the compensation to the information flow.% This is why we could directly train a binary \architecture{ResNet} model from scratch without additional support.% According to the confidence of the results obtained in our experiment, we achieved the same level in terms of classification accuracy comparing to the latest result from \architecture{ABC-Net}~\cite{lin2017towards} (\architecture{ResNet-18} result with weight base 1 and activation base 1).% As we learned from the previous experiments, we consider the shortcut connections as a useful compensation for the reduced information flow.% But this raised the following question: could we improve the model performance further by simply increasing the number of shortcut connections?% % To answer this question, we conducted further experiments based on the \architecture{DenseNet}~\cite{Huang2016} architecture, which are described in the next section.% To answer this question, we conducted further experiments based on the \architecture{DenseNet}~\cite{Huang2016} architecture.% %% % Table denseNet, ResNet% \begin{table*}[t]% \caption{% Classification accuracy comparison by using binary DenseNet and ResNet models for the ImageNet dataset. % % The amounts of model parameters (complexity) have been conducted in similar level for both architectures, which is intended to verify the gained improvement by using dense connections.% The amount of parameters are kept on a similar level for both architectures to verify that improvements are based on the increased number of connections and not an increase of parameters.% }% \begin{center}% \begin{tabular}{ |c|c|c|c|c|c| }% \hline% Architecture & Top-1 & Top-5 & Epoch & Model Size (FP) & Number of Parameters\\% \hline% DenseNet-21 & 50.0\% & 73.3\% & 49 & 44MB & 11 498 086\\% \hline% ResNet-18 & 42.0\% & 66.2\% & 37 & 45MB & 11 691 950\\% \hline% DenseNet-45 & 57.9\% & 80.0\% & 52 & 250MB & 62 611 886\\% \hline% ResNet-34-wide & 54.0\% & 77.2\% & 37 & 329MB & 86 049 198\\% \hline% ResNet-68-wide & 57.5\% & 80.3\% & 35 & 635MB & 166 283 182\\% \hline% \end{tabular}% \end{center}% \label{tab_dense_res_comp}% \end{table*}% %% \subsection{Shortcut Connections Driven Accuracy Gain}% %% Q2: think about why resnet is much better than other architechture? --> shortcut connection can keep the information flow % %% could we further improve the accuracy when we add more connections? e.g., denseNet?% In our first experiment we created binary models using both \architecture{DenseNet} and \architecture{ResNet} architectures with similar complexities.% We keep the amount of parameters on a roughly equal level to verify that the improvements obtained by using the \architecture{DenseNet} architecture are coming from the increased number of connections and not a general increase of parameters.% Our evaluation results show that these dense connections can significantly compensate for the information loss from binarization (see \autoref{tab_dense_res_comp}).% The gained improvement by using \architecture{DenseNet-21} compared to \architecture{ResNet-18} is up to \textbf{8\%}\footnote{We note, that this is significantly more, than the improvement between two full-precision models with a similar number of parameters (\emph{DenseNet-264} and \architecture{ResNet-50}), which is less than \textbf{2\%} (22.15\% and 23.9\% top 1 error rate, reported by \cite{Huang2016}).}, whereas the number of utilized parameters is even lower.% Furthermore, when we compare binary \architecture{ResNet-68-wide} to \architecture{DenseNet-45}, the latter has less than half the number of parameters compared to the former, but can achieve a very similar result in terms of classification accuracy.% \begin{figure*}[t]% \includegraphics[width=\linewidth]{connections/results}% \caption{% Model performance of binary \architecture{DenseNet} models with different growth rates $k$ and number of blocks $b$.% Increasing $b$, while decreasing $k$ leads to smaller models, without a significant decrease in accuracy, since the reduction of weights is compensated by increasing the number of connections% }% \label{fig:connection-results}% \end{figure*}% In our second set of experiments, we wanted to confirm our hypothesis, that increasing the number of blocks is more efficient than just increasing block size on the example of a \architecture{DenseNet} architecture.% We distinguish the four architectures through the two main parameters relevant for this experiment: growth rate $k$ per block, number of blocks per unit $b$, and total number of layers $n$, where $n = 8 \cdot b + 5$.% The four architectures we are comparing are:% \architecture{DenseNet-13} ($k=256$, $b=1$),% \architecture{DenseNet-21} ($k=128$, $b=2$),% \architecture{DenseNet-37} ($k=64$, $b=4$), and% \architecture{DenseNet-69} ($k=32$, $b=8$).% Despite a 31\% reduction in model size between \architecture{DenseNet-13} and \architecture{DenseNet-69}, the accuracy loss is only 1\% (see \autoref{fig:connection-results}).% % Add reduction in number of weights% We further conclude, that this similarity is not randomness, since all architectures perform very similarly over the whole training process.% We note again, that for all models the first convolutional layer and the final fully-connected layer use full-precision weights.% Further, we set the size of the former layer depending on the growth rate $k$, with a number of filters equal to $2\cdot k$.% Therefore, a large portion of the model size reduction comes from reducing the size of first convolutional layer, which subsequently also reduces the size of the final fully connected layer.% However, a larger fully-connected layer could simply add additional duplicate or similar features, without affecting performance.% This would mean, that the reduction of model size in our experiments comes from a different independent variable.% To elimnate this possibility, we ran a post-hoc analysis to check whether we can reduce the size of the first layer without impacting performance.% We used \architecture{DenseNet-13} with a reduced first layer, which has the same size as for \architecture{DenseNet-69} (which uses $k=32$), so $2 \cdot k = 64$ filters.% Even though the performance of the model is similar for the first few epochs, the accuracy does not reach comparable levels:% % after 25 Epochs, its Top-1 accuracy is only 45.9\% (2.8\% lower) and its Top-5 accuracy is only 69.8\% (2.3\% lower).% % after 30 Epochs, its Top-1 accuracy is only 46.4\% (2.6\% lower) and its Top-5 accuracy is only 70.3\% (2.2\% lower).% after 31 Epochs, its Top-1 accuracy is only 47.1\% (2.1\% lower) and its Top-5 accuracy is only 70.7\% (2.1\% lower).% % TODO add final result% In addition to degrading the accuracy more than as if increasing connections, it only reduces the model size by 6\% (0.4 MB), since the transition layers are unchanged.% This confirms our hypothesis, that we can eliminate the usual reduction in accuracy of a binary neural network when reducing the number of weights by increasing the number of connections.% In summary, we have learned two important findings from the previous experiments for training an accurate binary network: % \begin{itemize}%   \item Increasing information flow through the network improves classification accuracy of a binary neural network.%   \item We found two ways to realize this: Increase the network width appropriately while increasing depth or increasing the number of shortcut connections.% \end{itemize}% % TODO ->SubSection, page 12% \subsection{Specific Hyperparameter Evaluation}% %% Q3: Some hyperparameters will also impact the accuracy:% %%% gradient clipping threshold% In this section we evaluated two specific hyperparameters for training a binary neural network: the gradient clipping threshold and usage of a scaling factor.% % % \begin{figure*}[!t]% \begin{center}% \begin{subfigure}[t]{0.475\linewidth}%    \centering%    \includegraphics[width=\linewidth]{gradient_clipping_threshold.pdf}%    \caption{}%    \label{fig_gradient_clipping_threshold}% \end{subfigure}% \begin{subfigure}[t]{0.475\linewidth}%    \centering%    \includegraphics[width=\linewidth]{scaling_scalar.pdf}%    \caption{}%    \label{fig_scaling_scalar}% \end{subfigure}% % \begin{subfigure}[t]{0.6\linewidth}% %    \centering% %    \includegraphics[width=\linewidth]{bb-bf-ff.png}% %    \caption{}% %    \label{fig_bb_bf_ff}% % \end{subfigure}% \end{center}% \caption[Efficiency analysis of xnor-dot engine]{% (\subref{fig_gradient_clipping_threshold}) Classification accuracy by varying gradient clipping threshold.% The applied validation model is trained on ImageNet with \architecture{ResNet-18} topology.% (\subref{fig_scaling_scalar}) Accuracy evaluation by using scaling factor on network weights in three different modes: (\emph{N}) no scaling, (\emph{B}) use scaling factor on weights only in backward computation, and (\emph{FB}) apply weight scaling in both forward and backward pass.% % (\subref{fig_bb_bf_ff}) Weight update mode evaluation results based on \architecture{ResNet-18} and \architecture{DenseNet-21} using ImageNet. % % \textbf{bb} mode shows surprisingly good performance compared to bf and ff.% }% \end{figure*}% Using a \textbf{gradient clipping threshold $t_{clip}$} was originally proposed by Hubara et al.~\cite{Courbariaux2016}, and reused in more recent work~\cite{Rastegari2016,lin2017towards} (see~\autoref{sec:hyperp}).% In short, when using STE we only let the gradients pass through if the input $r_i$ satisfies $|r_i| \leq t_{clip}$.% Setting $t_{clip} = 1$ is presented in the literature with only cursory explanation.% Thus, we evaluated it by exploring a proper value range (see \autoref{fig_gradient_clipping_threshold}).% We used classification accuracy as the evaluation metric and selected thresholds from the value range of $[0.1, 2.0]$ empirically. % The validation model is trained on the ImageNet dataset with the \architecture{ResNet-18} network architecture.% From the results, we can recognize that $t_{clip} = 1$ is not neccessarily optimal for all applications.% Therefore, we suggest that the scaling factor should be considered as a hyperparameter in future work.% In our case the optimum is between 0.5 and 0.75.% We thus applied $t_{clip} = 0.5$ to all the other experiments in this paper.% %%% scaling factor% \textbf{Scaling factors} have been proposed by Rastegari et al.~\cite{Rastegari2016}.% \label{sec:scale-factor}% In their work, the scaling factor is the mean of absolute values of each output channel of weights.% Subsequently, Zhou et al.~\cite{Zhou2016} proposed a scaling factor, which is intended to scale all filters instead of performing channel-wise scaling.% The intuition behind both methods is to increase the value range of weights with the intention of solving the information loss problem during training of a binary network.% We conducted an evaluation of accuracy according to three running modes according to the implementation of Zhou et al.~\cite{Zhou2016}: (N) no scaling, (B) use the scaling factor on weights only in backward computation, (FB) apply weight scaling in both forward and backward pass.% The result indicates that no accuracy gain can be obtained by using a scaling factor on the \architecture{ResNet-18} network architecture (see \autoref{fig_scaling_scalar}).% Therefore we did not apply a scaling factor in our other experiments.% % TODO, make it clear only the one scaling factor does not really work% %%% weight_update_mode% %% eval results for cifar10, mnist, imagenet, using resnet and densenet% We proposed \textbf{weight update modes} in \autoref{sec:hyperp}.% The corresponding evaluation results are presented in this section.% We used the MNIST, CIFAR-10 and ImageNet data sets in order to evaluate the update modes under different levels of difficulties.% The mode definition is described as follows:% \begin{itemize}%   \item  \textbf{bb}: Calculate gradients on binary weights, and update binary weights%   \item  \textbf{bf}: Calculate gradients on binary weights, update the full-precision weights%   \item  \textbf{ff}: Calculate gradients on full-precision weights, update full-precision weights% \end{itemize}% Our results (see \autoref{tab:mnist-cifar-overview}) show that using full precision weights for parameter updates achieved higher accuracy on MNIST (cf. \emph{bf} and \emph{ff}).% On CIFAR-10, we obtained a very similar result using \emph{bb} and \emph{bf}.% However, on the large scale data set ImageNet \emph{bb} always achieves better results (see \autoref{tab:mnist-cifar-overview}), followed by \emph{bf}.% The mode \emph{ff} shows much worse result for both \architecture{ResNet-18} and \architecture{DenseNet-21}.% Overall, the results suggest to use \emph{bb} or \emph{bf} mode for the gradient computation and parameter update.% By using \emph{bb} mode we are able to save runtime memory, because we avoid keeping both full precision and binary weights the backward pass.% %% % Speed evaluation of XNOR-popcount based on BMXNet kernel% %     (Basically, we have done)% \subsection{Efficiency Analysis}% %% \begin{figure*}[!t]% \begin{center}% \begin{subfigure}[]{0.49\linewidth}%    \centering%    \includegraphics[width=\linewidth]{proctime_gemm.pdf}%    \caption{}%    \label{fig_xnor_1conv_layer}% \end{subfigure}% \begin{subfigure}[]{0.49\linewidth}%    \centering%    \includegraphics[width=\linewidth]{xnor-gemm-resnet18.pdf}%    \caption{}%    \label{fig_xnor_resnet18}% \end{subfigure}% \end{center}% \caption[Efficiency analysis of xnor-dot engine]{% (\subref{fig_xnor_1conv_layer}) Processing time comparison of different gemm methods based on one convolutional layer. \emph{xnor\_64\_omp} achieved about 50$\times$ and 125$\times$ acceleration in comparison with Cblas(Atlas) and naive gemm kernel, respectively. By accumulating \emph{xnor\_64\_omp} and input binarization we can achieve about 13$\times$ acceleration compared to Cblas method.% (\subref{fig_xnor_resnet18}) We measured the total processing time of convolutional layers of a \architecture{ResNet-18} model. The total processing time of \emph{xnor\_gemm} and input binarization is about 10$\times$ faster than Cblas, and also way more efficient than the \emph{unpack\_patch2col} method.}% \label{fig_xnor_processing_time}% \end{figure*}% %% Generally, convolutional and fully connected layers are the most commonly used components for deep neural networks.% % Therefore, GEMM (GEneral Matrix Multiplication) is still the operation, which is applied most often for implementing such layers in the mainstream deep learning frameworks.% Therefore, GEMM (GEneral Matrix Multiplication) is still the operation, which is applied most often for implementing such layers in deep learning frameworks.% As described in \autoref{eqn:xnor-popcount} (p. \pageref{sec:method}) the \texttt{xnor} and \texttt{popcount} operations can be used to approximate the multiplication and addition of two matrices.% Note, that there is a difference depending on processor architecture, since the instructions of $\times$64 CPU can handle two times 64 matrices, whereas $\times$86 and ARMv7 processors can only process two times 32 matrices.%  % times 64 matrix elements in very concise processor instructions on $\times$64 CPUs or two times 32 elements on $\times$86 and ARMv7 processors. % The hardware support of CPUs for those operations allow us to translate both operations directly into a single assembly command. % The population count instruction is available on $\times$86 and $\times$64 CPUs supporting SSE4.2, while on ARM architecture it is included in the NEON instruction set.% %% \begin{figure*}[!t]% \begin{center}% \begin{subfigure}[t]{0.47\linewidth}%    \centering%    \includegraphics[width=\linewidth]{speedup_varying_filter_number.pdf}%    \caption{}%    \label{fig_speedup_filter_number}% \end{subfigure}% \begin{subfigure}[t]{0.47\linewidth}%    \centering%    \includegraphics[width=\linewidth]{speedup_varying_kernel.pdf}%    \caption{}%    \label{fig_speedup_kernel_size}% \end{subfigure}% \end{center}% \caption{% (\subref{fig_speedup_filter_number})% Speedup comparison by varying filter number of the convolution layer.% The naive gemm method is utilized as baseline, the y-axis denotes the speedup rate according to it.% The input channel size is fixed to 256 while the kernel size and batch size are set to 5$\times$5 and 200 respectively.% (\subref{fig_speedup_kernel_size})% Speedup comparison by varying kernel size of the convolution layer.% The input channel size, batch size and filter number have been set to 256, 200 and 64 respectively.% The naive gemm method is used as baseline, while the y-axis denotes the corresponding speedup rate }% \end{figure*}% %% In order to verify their efficiency, we conducted experiments using different gemm methods.% All the experiments in this section have been performed on a Ubuntu 16.04/64-bit platform with Intel(R) Core(TM) i7-4710HQ 2.50GHz$\times4$ CPU with popcnt instruction (SSE4.2) and 8G RAM.% We first performed the evaluation by using one convolution layer and used the following parameters: % filter number=64, kernel size=5$\times$5, batch size=200, and the matrix size M, N, K for gemm are 64, 12800, and kernel$_{w}$ $\times$ kernel$_{h}$ $\times$ input channel size, respectively.% % \autoref{fig_xnor_1conv_layer} shows the experimental results. % % The colored columns denote the processing time in milliseconds across varying input channel size;% We compared the processing time in milliseconds, where \emph{xnor\_32} and \emph{xnor\_64} denote the \emph{xnor\_gemm} operator in 32 bit and 64 bit, respectively.% The values of \emph{xnor\_64\_omp} denote the 64 bit \emph{xnor\_gemm} accelerated by using the OpenMP\footnote{\url{http://www.openmp.org/}} parallel programming library.% We also accumulated the processing time of input data binarization and the matrix multiplication in \emph{binarize input and xnor\_64\_omp} .% We observed from our results (see \autoref{fig_xnor_1conv_layer}) that \emph{xnor\_64\_omp} achieved about 50$\times$ and 125$\times$ acceleration in comparison with Cblas (Atlas\footnote{\url{http://math-atlas.sourceforge.net/}}) and a naive gemm kernel, respectively.% Considering the accumulated input data binarization we still achieve around 13$\times$ acceleration compared to the Cblas method.% We also examined the total processing time of all convolutional layers of a \architecture{ResNet-18} model using Cblas and the \emph{xnor dot} engine, respectively (see \autoref{fig_xnor_resnet18}).% In general, \emph{xnor\_gemm} with input binarization is around 10$\times$ faster than Cblas gemm.% We also measure the \emph{unpack\_patch2col} (also called ``im2col'' in the literature) method of the convolutional layer, % and the results show that it uses more than 75\% of the total processing time.% It indicates that further speedup of this part could significantly improve the overall performance of a convolutional layer.% \autoref{fig_speedup_filter_number} and \autoref{fig_speedup_kernel_size} illustrate the speedup rate achieved by varying filter number and kernel size. % We implemented a naive gemm baseline method for comparison purpose. % Further implementation details of \emph{xnor\_gemm}, \emph{xnor\_gemm\_omp}, and naive gemm kernel can be found in our supplementary material.% % By increasing filter number it can cause a logarithmic curve like growth in speedup rate of input binarization, whilst the other methods roughly keep stable.% The speedup rate appears to be logarithmically growing by the number of filters, because time used for input binarization becomes negligible.% Moreover, \autoref{fig_speedup_kernel_size} depicts that a too small or too large kernel size can worsen the speedup.%%% add the implementation trick: convertion from +1,-1 to 0, 1% \subsection{Visualization Results}% \begin{figure*}[t]% \begin{center}% \begin{subfigure}[]{0.45\linewidth}%     \begin{minipage}[b]{\linewidth}%     \centering%         % \includegraphics[width=0.45\linewidth]{deepdream/densenet-full-precision/1_1}%         % \includegraphics[width=0.45\linewidth]{deepdream/densenet-full-precision/1_2}%         \includegraphics[width=0.45\linewidth]{deepdream/densenet-full-precision/11_9}%         % \\%         % \includegraphics[width=0.45\linewidth]{deepdream/densenet-full-precision/1_3}%         % \includegraphics[width=0.45\linewidth]{deepdream/densenet-full-precision/1_10}%         \includegraphics[width=0.45\linewidth]{deepdream/densenet-full-precision/6_3}%         \caption{DenseNet (FP)}%         \label{fig:deepdream-densenet-fp}%     \end{minipage}% \end{subfigure}% \begin{subfigure}[]{0.45\linewidth}%     \begin{minipage}[b]{\linewidth}%     \centering%         % \includegraphics[width=0.45\linewidth]{deepdream/densenet-21-128/0_2}%         % \includegraphics[width=0.45\linewidth]{deepdream/densenet-21-128/1_7}%         \includegraphics[width=0.45\linewidth]{deepdream/densenet-21-128/1_7}%         % \\%         % \includegraphics[width=0.45\linewidth]{deepdream/densenet-21-128/1_8}%         % \includegraphics[width=0.45\linewidth]{deepdream/densenet-21-128/1_9}%         \includegraphics[width=0.45\linewidth]{deepdream/densenet-21-128/1_9}%         \caption{DenseNet-21}%         \label{fig:deepdream-densenet-21}%     \end{minipage}% \end{subfigure}% % \begin{subfigure}[]{0.45\linewidth}% %     \begin{minipage}[b]{\linewidth}% %     \centering% %         % \includegraphics[width=0.45\linewidth]{deepdream/densenet-45-160/0_11}% %         % \includegraphics[width=0.45\linewidth]{deepdream/densenet-45-160/0_12}% %         \includegraphics[width=0.45\linewidth]{deepdream/densenet-45-160/0_12}% %         % \\% %         % \includegraphics[width=0.45\linewidth]{deepdream/densenet-45-160/0_13}% %         % \includegraphics[width=0.45\linewidth]{deepdream/densenet-45-160/2_7}% %         \includegraphics[width=0.45\linewidth]{deepdream/densenet-45-160/2_7}% %         \caption{DenseNet (45 layers)}% %         \label{fig:deepdream-densenet-45}% %     \end{minipage}% % \end{subfigure}% % \begin{subfigure}[]{0.31\linewidth}% %     \begin{minipage}[b]{\linewidth}% %         \includegraphics[width=0.45\linewidth]{deepdream/resnet-18/0_1}% %         \includegraphics[width=0.45\linewidth]{deepdream/resnet-18/0_1}% %         \\% %         \includegraphics[width=0.45\linewidth]{deepdream/resnet-18/0_1}% %         \includegraphics[width=0.45\linewidth]{deepdream/resnet-18/0_1}% %         \caption{Other}% %         \label{fig:deepdream-other}% %     \end{minipage}% % \end{subfigure}% \begin{subfigure}[]{0.45\linewidth}%     \begin{minipage}[b]{\linewidth}%     \centering%         \includegraphics[width=0.45\linewidth]{deepdream/resnet-18/0_1}%         % \includegraphics[width=0.45\linewidth]{deepdream/resnet-18/0_2}%         % \\%         \includegraphics[width=0.45\linewidth]{deepdream/resnet-18/0_8}%         % \includegraphics[width=0.45\linewidth]{deepdream/resnet-18/0_20}%         \caption{ResNet-18}%         \label{fig:deepdream-resnet-18}%     \end{minipage}% \end{subfigure}% \begin{subfigure}[]{0.45\linewidth}%     \begin{minipage}[b]{\linewidth}%     \centering%         % \includegraphics[width=0.45\linewidth]{deepdream/resnext101/0_10}%         % \includegraphics[width=0.45\linewidth]{deepdream/resnext101/0_11}%         % \\%         \includegraphics[width=0.45\linewidth]{deepdream/resnext101/0_12}%         \includegraphics[width=0.45\linewidth]{deepdream/resnext101/2_2}%         \caption{ResNet-68}%         \label{fig:deepdream-resnext-101}%     \end{minipage}% \end{subfigure}% \end{center}% \caption{%   The \emph{deep dream} \cite{mordvintsev2015} visualization of binary models with different complexity and size (best viewed digitally with zoom).%   The \emph{DenseNet} full precision model (\subref{fig:deepdream-densenet-fp}) is the only one, which produces visualizations of animal faces and objects.%   Additional models and samples can be seen in the supplementary material% }% \label{fig:deepdream}% \end{figure*}% % TODO which layer% \begin{figure*}[t]% \begin{center}% \includegraphics[width=0.49\linewidth]{visual_backprop/fp_resnet/c_44}% \includegraphics[width=0.49\linewidth]{visual_backprop/fp_resnet/c_68}% \includegraphics[width=0.49\linewidth]{visual_backprop/binary_resnet/c_44}% \includegraphics[width=0.49\linewidth]{visual_backprop/binary_resnet/c_68}% \includegraphics[width=0.49\linewidth]{visual_backprop/binary_densenet/c_44}% \includegraphics[width=0.49\linewidth]{visual_backprop/binary_densenet/c_68}% % \includegraphics[width=0.49\linewidth]{visual_backprop/binary_densenet_large/c_44}% % \includegraphics[width=0.49\linewidth]{visual_backprop/binary_densenet_large/c_68}% \end{center}% \caption{%   Two samples of the \emph{ImageNet} dataset visualized with \emph{VisualBackProp} of binary neural network architectures (from top to bottom): full-precision \architecture{ResNet-18}, binary \architecture{ResNet-18}, binary \architecture{DenseNet-21}.%   Each depiction shows (from left to right): original image, activation map, composite of both (best viewed digitally with zoom).%   Additional samples can be seen in the supplementary material% }% \label{fig:visbackprop}% \end{figure*}% To better understand the differences between binary and full-precision networks, and the various binary architectures, we created several visualizations.% The results show, that the full-precision version of the \emph{DenseNet} captures overall concepts, since rough objects, such as animal faces, can be recognized in the \emph{DeepDream} visualization (see \autoref{fig:deepdream-densenet-fp}).% The binary networks perform much worse.% Especially the \architecture{ResNet} architecture (see \autoref{fig:deepdream-resnet-18}) with 18 layers seems to learn much more noisy and less coherent shapes.% Further, we can see small and large areas of gray, which hints at the missing information flow in certain parts of the network.% This most likely comes from the loss of information through binarization which stops neurons from activating.% This issue is less visible for a larger architecture, but even there, small areas of gray appear (see \autoref{fig:deepdream-resnext-101}).% However the \emph{DenseNet} architecture (see \autoref{fig:deepdream-densenet-21}) with 21 layers, which has a comparable number of parameters, produces more object-like pictures with less noise.% The areas without any activations seem to not exist, indicating that the information can be passed through the network more efficiently in a binary neural network.% % The respective difference between larger models with more parameters and higher accuracy are not as prominent (see \autoref{fig:deepdream}\subref{fig:deepdream-densenet-45}, \subref{fig:deepdream-resnext-101}).% The visualization with \emph{VisualBackprop} shows a similar difference in quality of the learned features (see \autoref{fig:visbackprop}).% It reflects the parts of the image, which contributed to the final prediction of the model.% The visualization of a full-precision \architecture{ResNet-18} clearly highlights the remarkable features of the classes to be detected (e.g. the outline of lighthouse, or the head of a dog).% In contrast, the visualization of a binary \architecture{ResNet-18} only highlights small relevant parts of the image, and considers other less relevant elements in the image (e.g. a horizon behind a lighthouse).% The binary \emph{DenseNet-21} model also achieves less clarity than the full-precision model, but highlights more of the relevant features (e.g. parts of the outline of a dog).
% !TeX root = ../paper_for_review.tex\section{Conclusion}\label{sec:conclusion}% Summary

In this paper, we presented our strategy to train binary neural networks from scratch.
We clearly separated the existing techniques to increase the number of connections of a binary \arch{ResNet} model and applied them to derive an accurate binary model based on a \arch{DenseNet} architecture.
Moreover, we showed the influence of these different techniques through comprehensive experiments and compared our approach to other state-of-the-art approaches.
We concluded that accurate binary models can be successfully trained from scratch and our proposed binary architecture even surpasses the state-of-the-art accuracy.
%However, complex fine-tuning strategies on pretrained full-precision models can still help for optimizing larger models.
However, larger models can still benefit from complex fine-tuning strategies on pretrained full-precision models.

As future work, we would like to examine whether it is possible to better quantify the degree of importance of a layer in the network regarding the preservation of information.
Algorithmic approaches for this problem have already been proposed \cite{zhou2018adaptive}.
However, the theoretical knowledge would provide the basis to develop new architectures which benefit from this kind of model quantization.
Such a kind of novel architectures could help to reduce the accuracy gap between binary and full-precision layers.


{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
