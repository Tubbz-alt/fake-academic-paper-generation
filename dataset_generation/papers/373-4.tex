\documentclass[../main.tex]{subfiles}
\begin{document}

\section{Conclusion}
\label{conclusion}
We introduced \EXPLAIN, an algorithm for teaching multiple visual categories to human learners with interpretable feedback.
Interpretable feedback can be applied to any existing visual teaching algorithm.
It provides cues to the learner during teaching in the form of visual explanations and explicitly models how they incorporate this information when updating their belief. 
This enables us to generate more informative teaching sequences resulting in improved test time performance \ie, better generalization from the learners on unseen images. 
Experiments featuring real human participants show that our approach is superior to existing methods that only provide weaker class label feedback.
In future, we plan to investigate extending our approach to the interactive teaching setting, where the model updates online based on the learner's responses \eg, \cite{singla2013actively,du2011active,johns2015}. 
%Finally, low quality explanations may hinder the learners ability to learn. 
%One possibility to improve the quality of the explanations is to collect small amounts of expert annotations and propagate them to the remaining unlabeled images \eg~\cite{thewlis2017unsupervised}.

\end{document}


