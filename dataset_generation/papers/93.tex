
% This is "sig-alternate.tex" V2.1 April 2013
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate-05-2015}

\usepackage{comment}
\usepackage{amsmath,bm}
\usepackage{amsfonts}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{flushend}
\usepackage{booktabs}
%\renewcommand{\arraystretch}{1.1}

\begin{document}

% Copyright
\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

\newcommand\Tstrut{\rule{0pt}{2.7ex}}         % = `top' strut
\newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}}   % = `bottom' strut

\newcommand{\xavi}[1]{\textcolor{red}{\textbf{X:} #1}}
\newcommand{\glo}[1]{\textcolor{blue}{\textbf{G:} #1}}
\newcommand{\eli}[1]{\textcolor{magenta}{\textbf{E:} #1}}

% DOI
\doi{10.475/123_4}

% ISBN
\isbn{123-4567-24-567/08/06}

%Conference
%\conferenceinfo{PLDI '13}{June 16--19, 2013, Seattle, WA, USA}

\acmPrice{\$15.00}

%
% --- Author Metadata here ---
%\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{How to Make an Image More Memorable? \\A Deep Style Transfer Approach}
%\title{Retrieving Memorabilizing Images}
%\subtitle{Leave your title suggestions here}

%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.


\numberofauthors{1}
\author{
Aliaksandr Siarohin$^1$, Gloria Zen$^1$, Cveta Majtanovic$^1$,\\ Xavier Alameda-Pineda$^2$, Elisa Ricci$^{3}$ and Nicu Sebe$^1$\\
\affaddr{$^1$University of Trento, \texttt{name.lastname@unitn.it}}\\
\affaddr{$^2$INRIA Grenoble, \texttt{xavier.alameda-pineda@inria.fr}}\\
\affaddr{$^{3}$Fondazione Bruno Kessler and University of Perugia, \texttt{eliricci@fbk.eu}}
}

\begin{comment}
\numberofauthors{8} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Ben Trovato\titlenote{Dr.~Trovato insisted his name be first.}\\
       \affaddr{Institute for Clarity in Documentation}\\
       \affaddr{1932 Wallamaloo Lane}\\
       \affaddr{Wallamaloo, New Zealand}\\
       \email{trovato@corporation.com}
% 2nd. author
\alignauthor
G.K.M. Tobin\titlenote{The secretary disavows
any knowledge of this author's actions.}\\
       \affaddr{Institute for Clarity in Documentation}\\
       \affaddr{P.O. Box 1212}\\
       \affaddr{Dublin, Ohio 43017-6221}\\
       \email{webmaster@marysville-ohio.com}
% 3rd. author
\alignauthor Lars Th{\o}rv{\"a}ld\titlenote{This author is the
one who did all the really hard work.}\\
       \affaddr{The Th{\o}rv{\"a}ld Group}\\
       \affaddr{1 Th{\o}rv{\"a}ld Circle}\\
       \affaddr{Hekla, Iceland}\\
       \email{larst@affiliation.org}
\and  % use '\and' if you need 'another row' of author names
% 4th. author
\alignauthor Lawrence P. Leipuner\\
       \affaddr{Brookhaven Laboratories}\\
       \affaddr{Brookhaven National Lab}\\
       \affaddr{P.O. Box 5000}\\
       \email{lleipuner@researchlabs.org}
% 5th. author
\alignauthor Sean Fogarty\\
       \affaddr{NASA Ames Research Center}\\
       \affaddr{Moffett Field}\\
       \affaddr{California 94035}\\
       \email{fogartys@amesres.org}
% 6th. author
\alignauthor Charles Palmer\\
       \affaddr{Palmer Research Laboratories}\\
       \affaddr{8600 Datapoint Drive}\\
       \affaddr{San Antonio, Texas 78229}\\
       \email{cpalmer@prl.com}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
%\additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
%email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
%(The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
\date{30 July 1999}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.
\end{comment}

\maketitle
\begin{abstract}
Recent works have shown that it is possible to automatically predict intrinsic image properties like memorability. In this paper, we take a step forward addressing the question: ``\emph{Can we make an image more memorable?}''.
Methods for automatically increasing image memorability would have an impact in many application fields like education, gaming or advertising.
%For example, students would like to remember interesting things with less effort and companies would like to increase the chances of their logo or ads to be remembered.  
Our work is inspired by the popular \textit{editing-by-applying-filters} paradigm adopted in photo editing applications, like Instagram and Prisma.
%These applications provide a fast and simple approach to enhance user photos. 
%They usually act on low level cues while preserving 
In this context, the problem of increasing image memorability maps to that of retrieving ``memorabilizing'' filters or style ``seeds''.
Still, users generally have to go through most of the available filters before finding the desired solution, thus turning the editing process into a resource and time consuming task. 
%This is especially crucial in the case of style transfer-based applications like Prisma, which has been experiencing issues like capacity overload or prolonged waiting time\footnote{http://thetechhacker.com/2016/09/21/prisma-alternative-apps-fast-conversion}. %either when they rely on the computational power of remote servers to perform a neural-network based style transfer or when they run offline\footnote{http://money.cnn.com/2016/08/23/technology/prisma-offline}.
%
In this work, we show that it is possible to automatically retrieve the \emph{best} style seeds for a given image, thus % eliminaing the annoying process of fingi
remarkably reducing the number of human attempts needed to find a good match. % and, consequently, reducing the time and resource cost of the photo enhancement process.
Our approach leverages from recent advances in the field of image synthesis and adopts a deep architecture for generating a memorable picture from a given input image and a style seed.  Importantly, to automatically select the best style a novel learning-based solution, also relying on deep models, is proposed.
%We also provide a lightweight solution which can efficiently learn from a limited set of matches, allowing the seed retrieval from a relatively large set of candidate seeds, thus increasing the potential user creativity space, while dramatically reducing the number of training matches required.
%Focusing on memorability transfer based on low level cues, %thus discarding the effect of semantic, we use abstract art paintings from the DeviantArt dataset as our seed images. 
Our experimental evaluation, conducted on publicly available benchmarks, demonstrates the effectiveness of the proposed approach for generating memorable images through automatic style seed selection. 
\end{abstract}


%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
%CCS →  Computing methodologies →  Artificial intelligence →  Computer vision →  Computer vision tasks →  Visual content-based indexing and retrieval
 \begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010178.10010224</concept_id>
<concept_desc>Computing methodologies~Computer vision</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010371.10010382.10010236</concept_id>
<concept_desc>Computing methodologies~Computational photography</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Computer vision}
\ccsdesc[300]{Computing methodologies~Computational photography}



%
% End generated code
%

%
%  Use this command to print the description
%
\printccsdesc

% We no longer use \terms command
%\terms{Theory}

\keywords{Memorability; photo enhancement; deep style transfer; retrieval.}


\begin{figure}[t]
\includegraphics[width=\linewidth]{img/sample/teaser4.png}
\vspace{-0.7cm}
\caption{Sample results illustrating our idea (best viewed in colors). Given a generic image (left), automatically find the best style filters (center) that augment the memorability of the image (right). Memorability scores in the range [0,1] are reported in the small boxes.}
%\caption{Sample results illustrating our idea (best viewed in colors). Given a generic image captured by a user, can we make it more memorable? Inspired by the common photo editing paradigm based on applying pre-defined filters, we propose a novel method which automatically retrieve the best filters, or style seeds, to ``memorabilize'' the input image. Memorability scores in a range [0,1], automatically assessed, are reported in the small boxes. }
\label{fig:teaser}
\vspace{-0.3cm}
\end{figure}

\vspace{1.0cm}
\section{Introduction}
%Throughout human history, people have adopted the fact that a complex idea can be conveyed with just a glance at a picture. The outcome of a deeper interpretation and better understanding of this state has been used in many fields such as psychology and cognitive studies, education, photography, art as well as in journalism and marketing. 

Today's expansion of infographics is certainly related to one of the everyday life idiom ``A picture is worth a thousand words'' (or more precisely 84.1 words~\cite{blackwell1997correction}) and to the need of providing the fastest possible knowledge transfer in the current ``information overload'' age. A recent study~\cite{hilbert2012much} showed that everyone is bombarded by the equivalent of 174 newspapers of data every day. In this context, we ask ourselves: \emph{Is it possible to transform a user-chosen image so that it has more chances to be remembered}?

%Is it possible to transform an image so that it excels over this huge jungle of information, that is to automatically increase the memorability of a user-chosen image?
%But, which one among all these informations actually remain in memory and what kind of criteria human brain uses not to forget them? 
%Measuring the cognitive aspects of human perception has been the focus of psychology and neuroscience in order to understand how the brain works, what are the algorithmic principles the brain uses and how can we approach the modeling of those principles.
%NICU: {Is it possible to automatically increasethe memorability of a user-chosen image so that it facilitates highlighting the salient part of the information consumed by the user?}

For this question to be properly stated, it requires the existence of a measure of memorability, and recent studies proved that memorability is intrinsic to the visual content and is measurable~\cite{isola2011what,isola2014what}. Indeed, these studies use the \textit{memory pairs game} to provide an objective evaluation of image memorability, which has surprisingly low variance across trials.
%Although one can assume that there are individual differences in human perception, recently developed predicting models have shown that memorability is indeed an intrinsic and stable property of an image and a high consistency is found between memorization performances exhibited by different viewers~\cite{isola2011what,isola2014what}. 
Recent studies have also provided tools to detect the visual features responsible for both memorable and easily forgettable images. For instance, images that tend to be forgotten lack distinctiveness, like natural landscapes, whereas pictures with people, specific actions and events or central objects are way more memorable~\cite{brady2008visual}. 
Previous papers have also analyzed the relationship between emotions and memorability~\cite{cahill1995novel}. 
%Neuroscience has been measuring brain activity associated with the fact of remembering emotional and neutral photos~\cite{sharot2004emotion} in order to find the answer on how emotions affect memory.
%feeling of remembering. A recent work has shown that emotions evoked by visual content can be predicted and transferred between images~\cite{peng2015mixed}.
In a similar line of though, researchers wondered how to accurately predict which images will be remembered and which will be not. Recent experiments showed near-human performance in estimating, measuring and predicting visual memorability~\cite{khosla2015understanding}, where MemNet, a model trained on the largest annotated image memorability dataset, LaMem, has been proposed.
%Not long ago, an open question was how far we can go in predicting with high accuracy which images people will remember or not. Estimating, measuring and predicting visual memorability at near-human performance has been demonstrated in~\cite{khosla2015understanding}, where MemNet, a model trained on a large variety of images from LaMem, the largest annotated image memorability dataset, has been proposed. 

While previous studies on automatic prediction of memorability from images paved the way towards the automatic recognition of image memorability, many questions are still open. For instance: \textit{is it possible to increase the memorability of an image, while keeping its high-level content}? Imagine an advertising campaign concerning the design for a new product targeting a specific market sector. Once the very expensive designing phase is over, the client receives a set of images advertising the new product. Such images tell a story: in the attempt of increasing the image's memorability, the high-level content, that is \textit{the meaning}, should remain intact. We therefore focus on how to automatically modify the style of the image, that is how to \textit{filter} the image, so as to make it more memorable.
%For instance, what if we have an image that we consider not memorable enough? Is there anything we could do to increase its memorability? It could have happened that a company has already invested quite a big budget in an advertising campaign but the associated visual contents turned out not to be as memorable as it was originally assumed. Is there anything the company can do to "fix" the advertising images and still use them in a planned campaign?

Some popular commercial products are based on this image filtering philosophy, for other purposes than memorability though. For instance, Instagram\footnote{https://www.instagram.com}, a photo and video sharing Internet platform launched in 2010, allows the users to filter the visual content with several pre-defined filters before sharing. %Instagram has presumably been an essential inspiration for most of the photo editing based application creators. 
Similarly, Prisma\footnote{http://prisma-ai.com} %hit the market, known as the application that
turns user memories into art by using artificial intelligence. %Prisma is based on the findings by
In parallel to the development of these commercial products, several recent research studies in computer vision and multimedia have focused on creating artistic images of high perceptual quality with artificial intelligence models. For instance, Gatys \textit{et al.}~\cite{gatys2016image} have proposed an approach where a deep network is used to manipulate the content of a natural image adapting it to the style of a given artwork. Subsequently, more efficient deep architectures for implementing a style transfer have been introduced~\cite{ulyanov2016texture}. Importantly, none of these commercial products and related research studies incorporate the notion of image memorability. 
%In commercial products different user-friendly image manipulation tools are commonly adopted, \textit{e.g.} among Instagram\footnote{https://www.instagram.com} consumers, ever since this photo and video sharing platform has been launched in 2010. Instagram has presumably been an essential inspiration for most of the photo editing based application creators. For instance, in 2016  Prisma\footnote{http://prisma-ai.com} hit the market, known as the application that turns memories into art by using artificial intelligence. In parallel to the developments of these products, several recent research studies in computer vision and multimedia focused on creating artistic images of high perceptual quality with artificial intelligence models. For instance, Gatys et al.~\cite{gatys2016image} proposed an approach where a deep network is used to manipulate the content of a natural image adapting it to the style of a given artwork. Subsequently, more efficient deep architectures for implementing a style transfer have been proposed~\cite{ulyanov2016texture}. 
%However, none of these previous works focused on manipulating images to increase their memorability.
 %State-of-the-art computer vision techniques have even been used to automatically predict emotional valence of images~\cite{sartori2015affective}. 
 
%Up to the authors' knowledge, there are only two research studies devoted to modifying image memorability: \cite{khosla2013modifying} addresses this problem only for face images and~\cite{khosla2015understanding} modifies images focusing on specific high-level content. Furthermore, these two approaches do not exploit the potential of deep neural architectures. In all, a comprehensive study on how to increase the memorability of natural images is currently lacking.

%Indeed, with the exception of few works~\cite{khosla2013modifying,khosla2015understanding}, a comprehensive study of increasing general image memorability is currently lacking.
%Existing works either focus on limited image categories, \textit{e.g.} faces~\cite{khosla2013modifying}, or aim to modify images focusing on specific high level contents \cite{}. Furthermore, these studies do not employ powerful deep architectures. 

In this work, we propose a novel approach for increasing the memorability of images which is inspired by the \textit{editing-by-filtering} framework (Fig.~\ref{fig:teaser}). Our method relies on three deep networks. A first deep architecture, the \textit{Synthesizer} network, is used to synthesize a memorable image from the input picture and a style picture. A second network acts as a style \textit{Selector} and it is used to retrieve the ``best'' style seed to provide to the Synthesizer, (\textit{i.e.} the one that will produce the highest increase in terms of memorability) given the input picture. %(Figure~\ref{fig:method}). 
To train the Selector, pairs of images and vectors of memorability gap scores (indicating the increase/decrease in memorability when applying each seed to the image) are used. A third network, the \textit{Scorer}, which predicts the memorability score from a given input image, is used to compute the memorability gaps necessary to train the Selector.
%The second network is trained on pairs of images and associated style pictures, and the corresponding memorability increase scores are used for learning.
Our approach is extensively evaluated on the publicly available LaMem dataset \cite{khosla2015understanding} and we show that it can be successfully used to automatically increase the memorability of natural images.

%Our motivation is not only contained in investigating what makes an image memorable. Instead, we would like to turn the focus towards analysis of potential correlation with their affective quality, along with developing a computational model capable of predicting general memorability improvement while maintaining all the image properties as natural as possible. We are interested in discovering and defining a particular methodology of proper image seeds choice, enabling us to recreate images with the increased general memorability score.   

The main contributions of our work are the following:
\vspace{-0.1cm}
\begin{itemize}
\setlength\itemsep{0.0cm}
    \item We tackle the challenging task of increasing image memorability while keeping the high-level content intact (thus modifying only the style of the image).%\eli{this can be dangerous because people may think we work on old fashion CV}
    \item We cast this into a style-based image synthesis problem using deep architectures 
    and propose an automatic method to retrieve the style seeds that are expected to lead to the largest increase of memorability for the input image.
    \item We propose a lightweight solution for training the Selector network implementing the style seed selection process, allowing us to efficiently learn our model with a reduced number of training data while considering  relatively large variations of style pictures.
\end{itemize}


% Motivations. Implications of the work in fields like advertising (e.g. increasing logo memorability) and educations (e.g. interestingness vs memorability )

% Previous works have shown that memorability is an intrinsic property of images. A few works have shown that images can be modified to vary their memorability.


\begin{figure*}[t]
\includegraphics[width=\linewidth]{img/method/method4.png}
\vspace{-0.6cm}
\caption{Overview of our method. At training time, the Synthesizer $\mathbb{S}$ and the Scorer $\mathbb{M}$ serve to generate the training data (highlighted with a red dotted frame) for the seed Selector $\mathbb{R}$. 
%\glo{Note that our lightweight approach can deal with partial training data}. 
At test time, the seed Selector provides for each new image a sorted list of style seeds, based on the predicted memorability increase $\mathbb{R}_{s}(\mathbf{I}_v)$.  }
\label{fig:method}
\vspace{-0.3cm}
\end{figure*}

\section{Related Works}
The concept of memorability and its relation with other aspects of the human mind has been long studied from a psychological perspective~\cite{maren1999long,anderson2006emotion,brady2008visual,hunt2006distinctiveness,phelps2004human,bradley1992remembering}. 
Works in psychology and neuroscience mostly focused on visual memory, studying for instance the human capacity of remembering object details~\cite{brady2008visual}, the effect of emotions on memory~\cite{anderson2006emotion,phelps2004human,bradley1992remembering} or the brain's learning mechanisms, \textit{e.g.} the role of the amygdala in memory~\cite{maren1999long,phelps2004human}.
%Recent works from the computer vision perspective have mostly focused on memorability property of visual content.
For a few years now, more automated studies on memorability have arisen: from the collection of image datasets specifically designed to study memorability, to user-friendly techniques to annotate these data with memorability scores. The community is now paying attention to understand the causes of visual memorability and its prominent links with, for instance, image content, low- and mid-level visual features and evoked emotions.
%\subsection{Image Memorability}
%A significant line of works has focused on image memorability, either in psychology
%~\cite{maren1999long,anderson2006emotion,bainbridge2013intrinsic,brady2008visual,hunt2006%distinctiveness,phelps2004human,bradley1992remembering} %~\cite{anderson2006emotion,brady2008visual,hunt2006distinctiveness,phelps2004human,bradley1992remembering} and, in recent years, from a computer vision perspective~\cite{isola2011what,isola2011understanding,khosla2012memorability,khosla2012image,gygli2013interestingness,borkin2013makes,khosla2013modifying,isola2014what,khosla2015understanding,bylinskii2015intrinsic}.
%\cite{amengual2015review}

Isola \textit{et al.}~\cite{isola2011what} showed that visual memorability is an intrinsic property of images, and that it can be explained by considering only image visual features. Besides the expected inter-subject variability, \cite{isola2011what} reported a large consistency among viewers when measuring the memorability of several images. Typically, such measures are obtained by playing a memory game. Other studies proved that memorability can also be automatically predicted. Recently, Khosla \textit{et al.}~\cite{khosla2015understanding} used CNN-based features from MemNet to achieve a prediction accuracy very close to human performance, \textit{i.e.} up to the limit of the inter-subject variability, thus outperforming previous works using hand-crafted features such as objects or visual attributes~\cite{isola2014what}. %\xavi{I am not sure this sentence goes here ``In our work, we rely on MemNet for training our memorability predictors.''}

In parallel, large research efforts have been invested in understanding what makes an image memorable and, in a complementary manner, which is the relation between image memorability and other subjective properties of visual data, such as interestingness, aesthetics or evoked emotions. Gygli \textit{et al.}~\cite{gygli2013interestingness} observed that memorability negatively correlates with visual interestingness. Curiously, they also showed that human beings perform quite bad at judging the memorability of an image, thus further justifying the use of memory games for annotation. In the same study, it was shown that aesthetic, visual interestingness and human judgements of memorability are highly correlated. Similar results were reported later on in~\cite{isola2014what}, confirming these findings. A possible mundane interpretation of these findings is that people wish to remember what they like or find interesting, though this is not always the case.

Khosla \textit{et al.}~\cite{khosla2015understanding} showed that, with the exception of \emph{amusement}, images that evoke negative emotions like \emph{disgust}, \emph{anger} and \emph{fear} are more likely to be remembered. Conversely, images that evoke emotions like \emph{awe} and \emph{contentment} tend to be less memorable. Similarly, the authors of~\cite{isola2011understanding} showed that attributes like \emph{peaceful} are negatively correlated with memorability. Other works showed that \emph{arousal} has a strong effect on human memory~\cite{anderson2006emotion,cahill1995novel,bradley1992remembering,mcgaugh2006make} at two different stages: either during the encoding of visual information (\textit{e.g.}, increased attention and/or processing) or post-encoding (\textit{e.g.}, enhanced consolidation when recalling the stored visual information). Memorability was also investigated with respect to distinctiveness and low-level cues such as colors in~\cite{borkin2013makes} and with respect to eye fixation in~\cite{khosla2015understanding,bylinskii2015intrinsic}. In more detail, \cite{borkin2013makes} discussed how images that stand out of the context (\textit{i.e.}, they are unexpected or unique) are more easily remembered and that memorability significantly depends upon the number of distinct colors in the image. %Khosla et al.~\cite{khosla2015understanding} show that images with a more consistent human fixation tend to be more memorable, suggesting that images with a specific point on which to focus are more easily remembered.
%Several works show that some image categories are intrinsically more memorable or forgettable~\cite{isola2014what}. For example, images  containing close-up of  humans, faces and objects are more likely to be remembered, while scenes of open natural scenes, landscape, peaceful scenarios and texture surfaces tend to be forgotten. 
%In our work, we aim to increase memorability of a given image regardless of its category. 
These findings support our intuition that it is possible to {manipulate an image to increase its memorability}.
%increase image memorability by altering \eli{only the visual low level cues}. 
Indeed, this can happen for example by indirectly modifying image distinctiveness or the evoked arousal. Along the same line of though, Peng \textit{et al.} \cite{peng2015mixed} attempted to modify the emotions evoked by an image adjusting its color tone and its texture-related features.

Recent works analyzed how images can be modified to increase or decrease their memorability~\cite{khosla2013modifying,khosla2015understanding}. These are based on other contemporary studies that focused on generating memorability maps of images~\cite{khosla2012image,khosla2012memorability,khosla2014what}. In particular, Khosla \textit{et al.}~\cite{khosla2015understanding} showed that by removing visual details from an image through a cartonization process the memorability score can be modified. However, they did not provide a methodology to systematically increase the memorability of pictures. The same group~\cite{khosla2013modifying} also demonstrated that it is possible to increase the memorability of faces, while maintaining the identity of the person and properties like age, attractiveness and emotional magnitude. Up to our knowledge, this is the first attempt to automatically increase the memorability of generic images (not only faces).
%Furthermore, previous studies~\cite{khosla2013modifying,khosla2015understanding} did not exploit deep neural architectures.
%Indeed, while the method in~\cite{khosla2013modifying} is specifically designed for face images and it is not clear how to adapt it to general images, the system in~\cite{khosla2015understanding} is based on eliminating image parts that are judge unnecessary or unmemorable, introducing artifacts and afftecting the high level content of images.} % \eli{In all, from a user perspective this can be quite limiting or have undesired effects.}

%There is a very recent line of work on designing methods that automatically modify the memorability of the images~\cite{khosla2013modifying,khosla2015understanding}. These are based on other contemporary studies that focus on generating memorability maps of images~\cite{khosla2012image,khosla2012memorability,khosla2014what}. Khosla \textit{i.e.}~\cite{khosla2013modifying} showed that it is possible to increase the memorability of faces, while maintaining the identity of the person and properties like age, attractiveness and emotional magnitude. \eli{In a later study~\cite{khosla2015understanding}, the same research group modified the image memorability by removing part of visual details through an image cartonization process~\cite{decarlo2002stylization}. Specifically, they showed that deleting the less memorable regions, thus highlighting the more memorable areas of the image, allows in general to produce more memorable pictures with respect to when the more memorable regions are covered. Still, they also observe that removing details from images in general produce a decrease in memorability. This finding can probably be explained by the fact than even less memorable details somehow contribute to the overall image memorability. These recent studies on modifying the memorability of the images suffer from two main drawbacks. Indeed, while the method in~\cite{khosla2013modifying} is specifically designed for face images and it is not clear how to adapt it to general images, the system in~\cite{khosla2015understanding} is based on eliminating image parts that are judge unnecessary or unmemorable, introducing artifacts and afftecting the high level content of images.} % \eli{In all, from a user perspective this can be quite limiting or have undesired effects.}

%All these findings motivate us to pursue this line of research and to develop a methodological framework that is able to automatically increase the memorability of an input set of images. Our understanding is that the method should be able to do that (i) in an efficient manner, (ii) without significant modifications of the high-level image content and (iii) at the same time as selecting the subset of images that is going to have the highest memorability once they are modified. Such tool may have direct impact in fields like education, elderly care or user-generated data analysis. Indeed, rapidly selecting the most memorable images among all possible high-level content preserving modifications of the initial image set could help editing better educational supports, designing more effective brain training games for elderly people or producing better summaries from lifelog camera image streams or leisure picture albums.

%Generally speaking, we would like to investigate a system able to (i) automatically increase the memorability of an image without significant modifications in the high-level content of the image, (ii) does this in an efficient manner, (iii) is able to select the most memorable images (after modification) within an image set. Applications of this are...

%\xavi{Again, perhaps we should summarize this at the end of the section and limit ourselves to describe the SoTA here. This finding further motivate our work in increasing visual memorability, where such an automatic tool may have a direct impact in fields like education, elderly care or user generated data analysis (e.g. automatic summary from lifelog camera image streams).}


%Building upon recent advances in literature~\cite{isola2011what,khosla2015understanding}, few recent works have focused on the challenging task of increasing image memorability~\cite{khosla2013modifying,khosla2015understanding,khosla2012image}. Khosla et al.~\cite{khosla2013modifying} show that it is possible to increase the memorability of faces, while maintaining the identity of the person and properties like age, attractiveness and emotional magnitude. Other works focus on generating a memorability map, which can be the basis for further manipulating the image~\cite{khosla2012image,khosla2012memorability,khosla2015understanding}. Khosla et al.~\cite{khosla2015understanding} show that it is possible to modify the image memorability by removing part of visual details through an image cartonization process~\cite{decarlo2002stylization}. Specifically, they show that deleting the less memorable regions, thus highlighting the more memorable areas of the image, allows in general to produce more memorable pictures with respect to when the more memorable regions are covered. Still, they also observe that removing details from images in general produce a decrease in memorability. This finding can probably be explained by the fact than even less memorable details somehow contribute to the overall image memorability. These last two works~\cite{khosla2013modifying,khosla2015understanding} are probably the closest to ours. Still, our work differences from them respectively since we aim to increase the memorability of generic images and we aim to preserve the high level content of a given image, while we act by modifying low level cues like colors or style. To the best of our knowledge, no other works have focused on automatically increasing the memorability of a generic image.


\section{Method}

In this section we introduce the proposed framework to automatically increase the memorability of an input image. Our method is designed in a way such that the process of ``memorabilizing'' images is performed in an efficient manner while preserving most of the high-level image content. %at the same time as selecting the subset of images that is going to have the highest memorability once they are modified. 

\subsection{Overview}
The proposed approach co-articulates three main components, namely: the seed Selector, the Scorer and the Synthesizer, and so we refer to it as $S^3$ or \emph{S-cube}. In order to give a general idea of the overall methodological framework, we illustrate the pipeline associated to S-cube in Figure~\ref{fig:method}. The {Selector} is the core of our approach: for a generic input image $\textbf{I}$ and given a set of style image seeds ${\cal S}$, the Selector retrieves the subset of ${\cal S}$ that will be able to produce the largest increase of memorability. In details, the seed Selector predicts the expected increase/decrease of memorability that each seed $\mathbf{S}\in{\cal S}$ will produce in the input image $\mathbf{I}$, and consequently it ranks the seeds according to the expected increase of memorability. At training time, the Synthesizer and the Scorer are used to generate images from many input image-seed pairs and to score these pairs, respectively. Each input image is then associated to the relative increase/decrease of memorability obtained with each of the seeds. With this information, we can learn to predict the increase/decrease of memorability for a new image, and therefore rank the seeds according to the expected increase. Indeed, at query time, the Selector is able to retrieve the most memorabilizing seeds and give them to the Synthesizer. In the following, we first formalize the S-cube framework and then describe each of the three components in detail.

%considering a predefined set of style seeds $\textbf{S}_s \in \mathcal{S}$, it predicts the increase (or decrease) in memorability scores obtained when synthesizing the input image $\textbf{I}_g$ with each style seed $\textbf{S}_s$, and rank the seeds according to these values. The {Scorer} and the {Synthesizer} serve to generate the training data for to the Selector, which consists of a relatively large set of image-seed matches and the corresponding memorability score gaps. At query time, the Synthesizer and the Scorer can be used to generate the final results, such as the enhanced images and the corresponding memorability increases for the evaluation. In the next sections, we present our method in details.

\subsection{The S-cube approach}
\label{sec:scube}
%Our approach is formalized as follows. 
Let us denote the Scorer, the Synthesizer and the seed Selector models by $\mathbb{M}$,  $\mathbb{S}$ and $\mathbb{R}$, respectively. During the \textbf{training phase} the three models are learned.
The Scoring model $\mathbb{M}$ returns the memorability value of a generic image $\mathbf{I}$, $\mathbb{M}(\mathbf{I})$, and it is learned by means of a training set of images annotated with memorability: ${\cal M}=\{\mathbf{I}_i^{\cal M},m_i\}_{i=1}^I$. In addition to this training set, we also consider a generating set of natural images ${\cal G}=\{\mathbf{I}_g^{\cal G}\}_{g=1}^G$ and a set of style seed images ${\cal S}=\{\mathbf{S}_s\}_{s=1}^S$. The Synthesizer produces an image from an image-seed pair, 
%$\mathbf{I}_{gs} = \mathbb{S}(\mathbf{I}_g,\mathbf{S}_s)$.
%In order to obtain this scoring model, let us assume the existence of an image dataset annotated with memorability scores ${\cal M}=\{\mathbf{I}_i,m_i\}_{i=1}^M$, which is used to train the model $M$. In addition to the scoring model, we assume the existence of a training set of generic images ${\cal G}=\{\mathbf{I}_g\}_{g=1}^G$ and a set of seed images ${\cal S}=\{\mathbf{S}_s\}_{s=1}^S$, that will be used in combination to create new images. In this respect, we assume the existence of a synthesizing procedure $S$ able to output a new image from an image-seed pair:
\begin{equation}
    \mathbf{I}_{gs} = \mathbb{S}\left(\mathbf{I}_g^{\cal G},\mathbf{S}_s\right).
\end{equation}\\
%We then use the scoring model $\mathbb{M}$ to compute the memorability score gap between the synthesized and original images:
%image $\mathbf{I}_{gs}$ and the original image $\mathbf{I}_g$:
%\begin{equation}
%m_{gs} = \mathbb{M} (\mathbf{I}_{gs}) - \mathbb{M} (\mathbf{I}_g^{\cal G}).
%\end{equation}
%The seed-wise concatenation of these scores, denoted by $\mathbf{m}_g = (m_{gs})_{s=1}^S$, are used to learn the Seed Selector.
%Thus, the generated information for a relatively large set of image-seed pairs is used to train our seed selector.
The scoring model $\mathbb{M}$ and the Synthesizer $\mathbb{S}$ are the required steps to train the seed Selector $\mathbb{R}$. Indeed, for each  image $\mathbf{I}_g^{\cal G}\in{\cal G}$ and for each style seed $\mathbf{S}_s\in{\cal S}$, the synthesis procedure generates $\mathbf{I}_{gs}$.
The Scoring model is used to compute the memorability score gap between the synthesized and the original images:
%image $\mathbf{I}_{gs}$ and the original image $\mathbf{I}_g$:
\begin{equation}
m_{gs}^\mathbb{M} = \mathbb{M} (\mathbf{I}_{gs}) - \mathbb{M} (\mathbf{I}_g^{\cal G}).
\end{equation}
The seed-wise concatenation of these scores, denoted by $\mathbf{m}_g^\mathbb{M} = (m_{gs}^\mathbb{M})_{s=1}^S$, is used to learn the seed Selector. Specifically, a training set of natural images labeled with the seed-wise concatenation of memorability gaps ${\cal R}=\{\mathbf{I}_g^{\cal G},\mathbf{m}_g^\mathbb{M}\}_{g=1}^{G}$ is constructed. The process of seed selection is casted as a regression problem and the mapping $\mathbb{R}$ between an image and the associated vector of memorability gap scores is learned. This indirectly produces a ranking of the seeds in terms of their the ability to memorabilize images (\textit{i.e.} the best seed corresponds to the largest memorability increase).

During the \textbf{test phase} and given a novel image $\mathbf{I}_v$, the seed Selector is applied to predict the vector of memorability gap scores associated to all style seeds, \textit{i.e.} $\mathbf{m}_v=\mathbb{R}(\mathbf{I}_v)$. A ranking of seeds is then derived from the vector $\mathbf{m}_v$. Based on this ranking the Synthesizer is applied to the test image $\textbf{I}_v$ considering only the top $Q$ style seeds $\textbf{S}_s$ and produces a set of stylized images $\{\mathbf{I}_{qs}\}_{q=1}^Q$. % = \mathbb{S}(\textbf{I}_g,\mathbb{S}_s)$.

In the following we describe the three main building blocks of our approach, providing details of our implementation.


%which is trained on an additional image set annotated with memorability scores ${\cal N}=\{\mathbf{I}_i,m_i\}_{i=1}^N$, where ${\cal M} \cap {\cal N} = \empty$.Indeed, we train the Selector with the information provided by the synthesizer $S$ and the score model $M$, we cannot use the same model to evaluate the newly generated images at query time. Subsequently, we will use two independent sets of the evaluation subset will be used to train an \textit{external scoring model}, as opposed to the internal scoring model that will be trained with the training set. The external model is independent of all the information (images) used in the synthesizing procedure and thus can automatically assess the memorability of the generated images.\\

%Our approach is formalized as follows. Let us assume the existence of an image dataset annotated with memorability scores ${\cal D}=\{\mathbf{I}_i,m_i\}_{i=1}^I$. This dataset will be split in three subsets, a scoring set ${\cal S}=\{\mathbf{I}_i,m_i\}_{i\in{\cal I}_S}$, a training set ${\cal T}=\{\mathbf{I}_i,m_i\}_{i\in{\cal I}_T}$ and an evaluation set ${\cal E}=\{\mathbf{I}_i,m_i\}_{i\in{\cal I}_E}$. The role of each of this subsets is discussed below. The sets of indices of the scoring, training and evaluation sets are respectively denoted by ${\cal I}_S$, ${\cal I}_T$ and ${\cal I}_E$, and they form a partition of $\{1,\ldots,I\}$. In short, each of the images belongs only to one of the scoring, training or evaluation subsets. In addition to the images annotated with memorability, we assume the existence of a set of seed images ${\cal S}=\{\mathbf{S}_s\}_{s=1}^S$, that will be used in combination with the synthesizing set, to create new images. In this respect, we assume the existence of a synthesizing procedure $G$ able to output a new image from an image-seed pair:
%\begin{equation}
%    \mathbf{S}_{gs} = S\left(\mathbf{I}_g,\mathbf{S}_s\right).
%\end{equation}\\

%We require the existence of three data sets so as to properly evaluate the seed selection procedure. Indeed, the information in the training and synthesizing sets will be used to generate new images, and therefore we cannot use the same information to evaluate this newly generated images. Subsequently, the evaluation subset will be used to train an \textit{external scoring model}, as opposed to the internal scoring model that will be trained with the training set. The external model is independent of all the information (images) used in the synthesizing procedure and thus can automatically assess the memorability of the generated images.\\

%Roughly speaking, the method works as follows. The training and evaluation sets are used to learn the internal and external memorability scoring models, respectively. More formally, we learn two mappings $M^{\cal T}$ and $M^{\cal E}$ from the subsets ${\cal T}$ and ${\cal E}$, that can be used to obtain to independent assessments of the memorability of any image $\mathbf{I}$. In the following, $M^{\cal T}$ will be used in the automatic seed selection procedure, while $M^{\cal E}$ will play the role of the \textit{external scoring model}. The former will be used to evaluate the quality of the overall strategy, which follows a two-step philosophy: (i) select the best seed(s) for increasing memorability and (ii) generate the (hopefully) more memorable image using $G$. These steps are discussed in Sections~\ref{sec:synthesis} and~\ref{sec:seed_select} respectively. In Section~\ref{sec:scoring} we describe the scoring model.

\subsection{The Scorer}
\label{sec:scoring}

\begin{sloppypar}
The scoring model $\mathbb{M}$ returns an estimate of the memorability associated to an input image $\textbf{I}$. In our work, we use the memorability predictor based on LaMem in~\cite{khosla2015understanding}, which is the state of the art to automatically compute image memorability. In details, following~\cite{khosla2015understanding} we consider a hybrid CNN model~\cite{hybrydcnn}. The network is pre-trained first for the object classification task (\textit{i.e.} on ImageNet database) and then for the scene classification task (\textit{i.e.} on Places dataset). Then, we randomly split the LaMem {training set}%\footnote{in order to compare with~\cite{khosla2015understanding} we used the first split of LaMem for defining the training and test set for the task of memorability prediction}
into 
two disjoint subsets (of 22,500 images each), ${\cal M}$ and ${\cal E}$. We use the pretrained model and the two subsets to learn two independent scoring models $\mathbb{M}$ and $\mathbb{E}$. While, as discussed above, $\mathbb{M}$ is used during the training phase of our approach, the model $\mathbb{E}$ is adopted for evaluation (see Section \ref{sec:dataset}). For training, we run $70$k iterations of stochastic gradient descent with momentum 0.9, learning rate $10^{-3}$ and batch size 256. 
\end{sloppypar}
%Learning method:
%SGD : Learning rates higer then 1e-3 result in predicting nan very
%fast, so mostly I tried 1e-3. (I also tried 1e-4 and 5e-4 but the converge very slowly)
%Nesterov: Same results for learning rates. (This is method which I use mostly)
%Adam: After 10k iterations start predicting constant.
%-------------------------------------------
%The best result so far gives sgd with 1e-3 learning rate and 70k iterations. --> 0.645.
%-----------------------------------------
%I train 2 networks on 22500 from train set. Here is results on test set 10k images:

%SGD FIRST PART: lr:1e-3 iter:70k momentum:0.9
%Squared value error: 0.0098770984373405
%Absolute value error: 0.080373565751314
%Variance of absolute error: 0.003417530123895
%Correlation coefficient: 0.63354819047048
%SGD SECOND PART: lr:1e-3 iter:70k momentum:0.9
%Squared value error: 0.0094164225051136
%Absolute value error: 0.07762550458014
%Variance of absolute error: 0.0033910426461539
%Correlation coefficient: 0.6278636404441
%-----------------------------------------



%Furthermore, in order to obtain the internal $\mathbb{M}$ and external $\mathbb{E}$ model predictors, we randomly split the LaMem dataset into two disjoint subsets, ${\cal M}$ and ${\cal E}$. These two datasets are equally large (22,500 images) and are used to train $\mathbb{M}$ and $\mathbb{E}$ respectively. \xavi{We obtain a final correlation rank of 0.64 for both $\mathbb{M}$ and $\mathbb{E}$.} We highlight that $\mathbb{M}$ and $\mathbb{E}$ can be used as two independent memorability scoring functions, since (i) models trained on LaMem are able to reach close-to-human performance and (ii) ${\cal M}$ and ${\cal E}$ are disjoint.

%The scoring model returns an estimate of the memorability associated to an input image $\textbf{I}$. In our work, we use the memorability predictor based on LaMem~\cite{khosla2015understanding} as our Scorer. 
%Indeed, previous works have shown that LaMem predictor achieves a correlation rank near to human performance. Based on this finding, we use LaMem scores for approximating actual memorability scores. 
%\subsection{Training the scoring models} As memorability depends on both scenes and objects, we initialize the training using the pre-trained Hybrid-CNN \cite{hybrydcnn}, as described in \cite{khosla2015understanding}. We replace the last layer with one node, which should predict memorability. The loss was average square error. In order to compare with~\cite{khosla2015understanding}, we considered the same split of LaMem dataset and tried several parameter setups to achieve the same performance as in~\cite{khosla2015understanding}. Specifically, we obtain a correlation rank of 0.65 on the first split of LaMem using the following hyper-parameters: we use nesterov momentum with momentum 0.9 as learning method and we set the learning rate to 1e-3, the batch size to 256 and the number of iterations to 70K. 

%Furthermore, in order to obtain the two internal $M^{\cal T}$ and external $M^{\cal E}$ model predictors, we generate two independent sets of images with a random split of the original LaMem training set. We train both models using the same amount of training data, 22,500 images, and the optimal parameter set identified during the previous step. We obtain a final correlation rank of 0.64 for both $M^{\cal T}$ and $M^{\cal E}$. 

%It is worth noting that the predictive model can be substituted with a novel annotation collection. This is especially important in the other application scenario different from memorability, where the automatic prediction does not achieve an accuracy close to human performance. 

\begin{figure}[t]
\centering
\begin{tabular}{ccccc}
\hspace{-0.3cm}
\frame{\includegraphics[height=1.5cm, width=1.5cm]{img/sample/00000168.jpg}} &
\hspace{-0.3cm}
\frame{\includegraphics[height=1.5cm, width=1.5cm]{img/seeds/olas_y_algas_by_bluestwaves-d32eflc.jpg}} &
\hspace{-0.1cm}
\frame{\includegraphics[height=1.5cm, width=1.5cm]{img/sample/olas_0_5_00000168.jpg}} &
\hspace{-0.3cm}
\frame{\includegraphics[height=1.5cm, width=1.5cm]{img/sample/olas_2_00000168.jpg}} &
\hspace{-0.3cm}
\frame{\includegraphics[height=1.5cm, width=1.5cm]{img/sample/olas_10_00000168.jpg}} \\
%----------------------------------------------------
\hspace{-0.3cm}
\frame{\includegraphics[height=1.5cm, width=1.5cm]{img/sample/00000158.jpg}} &
\hspace{-0.3cm}
\frame{\includegraphics[height=1.5cm, width=1.5cm]{img/seeds/magokoro_by_inthemorning.jpg}} &
\hspace{-0.1cm}
\frame{\includegraphics[height=1.5cm, width=1.5cm]{img/sample/magokoro_0_5_00000158.jpg}} &
\hspace{-0.3cm}
\frame{\includegraphics[height=1.5cm, width=1.5cm]{img/sample/magokoro_2_00000158.jpg}} &
\hspace{-0.3cm}
\frame{\includegraphics[height=1.5cm, width=1.5cm]{img/sample/magokoro_10_00000158.jpg}} \\
%-------------------------------------------------------
%\hspace{-0.3cm}
%\frame{\includegraphics[height=1.5cm, width=1.5cm]{img/sample/00000153.jpg}} &
%\hspace{-0.3cm}
%\frame{\includegraphics[height=1.5cm, width=1.5cm]{img/seeds/painting_i_by_timi_o-d547ky0.jpg}} &
%\hspace{-0.1cm}
%\frame{\includegraphics[height=1.5cm, width=1.5cm]{img/sample/painting_0_5_00000153.jpg}} &
%\hspace{-0.3cm}
%\frame{\includegraphics[height=1.5cm, width=1.5cm]{img/sample/painting_2_00000153.jpg}} &
%\hspace{-0.3cm}
%\frame{\includegraphics[height=1.5cm, width=1.5cm]{img/sample/painting_10_00000153.jpg}} \\
%-------------------------------------------------------
\textbf{I}$_v$ & \textbf{S}$_s$ & $0.5$ & $2$ & $10$ 
\end{tabular}
%\vspace{-0.4cm}
\caption{Sample results. (Left) Original images and applied style seeds. (Right) Synthesized images at varying parameter $\alpha$, which regulates the trade-off between preserving the original content of the given image $\mathbf{I}_v$ and transferring the style $\mathbf{S}_s$. % during the image synthesis process. 
} % In our work, we set $\alpha=2$.}
\label{fig:style}
%\vspace{-0.5cm}
\end{figure}


\subsection{The Synthesizer}
\label{sec:synthesis}
The {Synthesizer} takes as an input a generic image $\textbf{I}_g$ and a style seed image $\textbf{S}_s$ and produces an stylized image $\mathbf{I}_{gs} = \mathbb{S}(\textbf{I}_g,\mathbb{S}_s)$. We use the strategy proposed in~\cite{ulyanov2016texture}, which consists on training a different feed-forward network for every seed. As seeds, we use 100 abstract paintings from the DeviantArt database~\cite{sartori2015affective}, and therefore we train $S=100$ networks for $10$k iterations with learning rate $10^{-2}$. The most important hyper-parameter is the coefficient $\alpha$, which regulates the trade-off between preserving the original image content and producing something closer to the style seed (see Figure \ref{fig:style}). In our experiments we evaluated the effect of $\alpha$ (see Section \ref{sec:exp}). % within a set of discrete values in the range from $0.5$ to $10$. For the sake of conciseness, we report the results obtained at $\alpha=2$ and $\alpha=10$, since these correspond to the extreme performance cases. 
It is worth noticing that the methodology proposed in this article is independent of the synthesis procedure. Indeed, we also tried other methods, namely Gatys \textit{et\ al.}~\cite{gatys2015neural} and Li \textit{et\ al.}~\cite{li2016precomputed}, but we selected~\cite{ulyanov2016texture} since it showed very good performance while keeping low computational complexity. This is especially important in our framework since the Synthesizer is also used to generate the training set for learning $\mathbb{R}$. % a priori for each image in ${\cal G}$ we need to synthesize $S$ images.

% following hyper-parameters: 
% number of iterations equal to 10K and learning rate equal to 1e-2. 
% Also, we consider discrete values in the range \{0.5, 10\} of the parameter $\alpha$, which regulates the trade off between preserving the image 
% content and the seed style.
% Sample results of synthesized images generated at varying $\alpha$ are reported in Figure~\ref{fig:style}. 
% We consider the results obtained with $\alpha$ set to 2 as those showing the best compromise between content and style. Thus we consider this value in 
% the following phases of our work. 
% Then for every image from ${\cal I}_g$ we generate up to 100 images ${\cal G}_{gs}$. 
% It is worth noting that our method works independently to the synthesis procedure chosen. For example, we also tried other methods like from Gatys et 
% al.~\cite{gatys2015neural} and Li et al.~\cite{li2016precomputed}. Still, these procedure resulted more expensive in terms of time cost, which is 
% especially crucial in the cases like ours that have to generate a large set of image-seed matches.


\subsection{The Seed Selector}
\label{sec:seed_select}
%As depicted above, at training time, the scoring model $\mathbb{M}$ and the synthesis procedure $\mathbb{S}$ are the required steps to train the seed selector. Indeed, for each generic image $\mathbf{I}_g^{\cal G}\in{\cal G}$ and for each style seed $\mathbf{S}_s\in{\cal S}$, the synthesis procedure generates $\mathbf{I}_{gs} = \mathbb{S}(\mathbf{I}_g^{\cal G},\mathbf{S}_s)$. The internal memorability scoring model is used to obtain an estimate of the memorability gap score $m_{gs} = \mathbb{M}(\mathbf{I}_{gs})-\mathbb{M}(\mathbf{I}_g^{\cal G})$ with respect to the original image. 
The core part of our approach is the Selector.
Given a training set of natural images labeled with the vector of memorability gaps: ${\cal R}=\{\mathbf{I}_g^{\cal G},\mathbf{m}_g^\mathbb{M}\}_{g=1}^{G}$, the seed Selector $\mathbb{R}$ is trained minimizing the following objective:
\begin{equation}{\cal L}_\mathbb{R} = \sum_{g=1}^G  \mathcal{L}\left(\mathbb{R}(\mathbf{I}_g^{\cal G}),\mathbf{m}_g^\mathbb{M}\right).
 \label{loss}
\end{equation}
where $\mathcal{L}$ is a loss function which measures the discrepancy between the learned vector $\mathbb{R}(\mathbf{I}_g^{\cal G})$ and the memorability gap scores $\mathbf{m}_g^\mathbb{M}$.
%\begin{equation}
% {\cal L}_\mathbb{R} = \sum_{g=1}^G \sum_{s=1}^S \ell\left(\mathbb{R}_s(\mathbf{I}_g^{\cal G}),m_{gs}\right),
%\end{equation}
%where $\mathbb{R}_s$ is the $s$-th component of $\mathbb{R}$.
%The main idea of the proposed strategy is to use memorability gap scores for learning $\mathbb{R}$. Indeed, b
By training the seed Selector with memorability gaps, we are learning \textit{by how much each of the seeds increases or decreases the memorability of a given image}. This has several advantages. First, we can very easily rank the seeds by the expected increase in memorability they will produce if used together with the input image and the synthesis procedure. Second, if several seeds have similar expected memorability increase, they can be proposed to the user for further selection. Third, if all seeds are expected to decrease the memorability, the optimal choice of not modifying the image can easily be made. Fourth, once $\mathbb{R}$ is trained, all this information comes at the price of evaluating $\mathbb{R}$ for a new image, which is cheaper than running $\mathbb{S}$ and $\mathbb{M}$ $S$ times.

Even if this strategy has many advantages at testing time, the most prominent drawback is that, to create the training set $\mathcal{R}$, one should ideally call the synthesis procedure for all possible image-seed pairs. This clearly reduces the scalability and the flexibility of the proposed approach. The scalability because training the model on a large image dataset means generating a much larger dataset (\textit{i.e.}, $S$ times larger). The flexibility because if one wishes to add a new seed to the set ${\cal S}$, then all image-seed pairs for the new seed need to be synthesized and this takes time. Therefore, it would be desirable to find a way to overcome these limitations while keeping the advantages described in the previous paragraph.

The solution to these issues comes with a model able to learn from a partially synthesized set, in which not all image-seed pairs are generated and scored. This means that the memorability gap vector $\mathbf{m}^{\mathbb M}_g$ has  missing entries. In this way we only require to generate \textit{enough} image-seed pairs. To this aim, we propose to use a decomposable loss function $\mathcal{L}$. Formally, we define a binary variable $\omega_{gs}$ set to $1$ if the 
$gs$-th image-seed pair is available and to $0$ otherwise and rewrite the objective function in (\ref{loss}) as:
\begin{equation}{\cal L}_\mathbb{R} = \sum_{g=1}^G \sum_{s=1}^S \omega_{gs}\ell\left(\mathbb{R}_s(\mathbf{I}_g^{\cal G}),m_{gs}^\mathbb{M}\right).
\end{equation}
where $\mathbb{R}_s$ is the $s$-th component of $\mathbb{R}$ and $\ell$ is the square loss.
We implemented this model using an AlexNet architecture, where the prediction errors for the missing entries of $\mathbf{m}^{\mathbb{M}}_g$ are not back-propagated. 
Specifically, we considered the pre-trained Hybryd-CNN and fine-tune only the layers fc6, fc7, conv5, conv4 using learning rate equal to $10^{-3}$, momentum equal to 0.9 and batch size 64. The choice of  Hybryd-CNN is considered more appropriate when dealing with generic images since the network is pre-trained both on images of places and objects.


%\subsection{The Synthesizer}
%\subsection{The synthesis procedure}
%\label{sec:synthesis}
%The {Synthesizer} takes as an input a generic image $\textbf{I}_g$, and a style seed image $\textbf{I}_s$ and produces as an output the stylized generated image. For image synthesizing we use the model proposed in \cite{ulyanov2016texture}. The method consists in training a feed-forward network for every seed that we want to use. Since we use 100 seeds we train 100 networks with following hyper-parameters: number of iterations equal to 10K and learning rate equal to 1e-2. Also, we consider discrete values in the range \{0.5, 10\} of the parameter $\alpha$, which regulates the trade off between preserving the image content and the seed style. Sample results of synthesized images generated at varying $\alpha$ are reported in Figure~\ref{fig:style}. We consider the results obtained with $\alpha$ set to 2 as those showing the best compromise between content and style. Thus we consider this value in the following phases of our work. Then for every image from ${\cal I}_g$ we generate up to 100 images ${\cal G}_{gs}$. It is worth noting that our method works independently to the synthesis procedure chosen. For example, we also tried other methods like from Gatys et al.~\cite{gatys2015neural} and Li et al.~\cite{li2016precomputed}. Still, these procedure resulted more expensive in terms of time cost, which is especially crucial in the cases like ours that have to generate a large set of image-seed matches.


%\subsection{Automatic seed selection}
%\label{sec:seed_select}
%Once the internal scoring model $M^{\cal T}$ and the Synthesizers are trained, we can adopt the following naive strategy to automatically select the seed. For each image in the synthesizing set $\mathbf{I}_g$, $g\in{\cal I}_G$ and for each seed $\mathbf{S}_s$ we generate the image $\mathbf{G}_{gs}$. We evaluate this image with the internal scoring model, and compute the difference with respect to the original score obtaining:
%\begin{equation}
%    m^{\cal T}_{gs} = M^{\cal T}(\mathbf{G}_{gs}) - M^{\cal T}(\mathbf{I}_g).
%\end{equation}

%For the $g$-th image, we collect all these scores in a vector $m^{\cal T}_{g}=(m^{\cal T}_{gs})_{s=1}^S$. This vector implicitly ranks the increase/decrease of memorability when applying the generating procedure to image $\mathbf{I}_g$ with all seeds.

%We can now define a ranking training set ${\cal R}=\{\mathbf{I}_g,m^{\cal T}_{g}\}_{g\in{\cal I}_G}$, which can be further used to train a ranking model $R$, by minimizing the following loss:
%\begin{equation}
%\sum_{g\in{\cal I}_G}\sum_{s=1}^S \ell \left(m^{\cal T}_{gs}, R_s(\mathbf{I}_g) \right),
%\end{equation}
%where $R_s$ denotes the $s$-th component of $R$.

%$R$ will be able to estimate which of the seeds will increase more the memorability of a given image $\mathbf{I}$ and by how much. \textcolor{red}{Guys this is great from the user perspective point of view, because we can even select the images that will be more memorable at the end, in an absolute manner, within a set. For instance your holidays pictures: not only which seed but also which pictures you want to modify.} In this way, we are solving the problem of retrieving the best seed to increase (or even decrease if desired) the memorability of a given image.

%The drawback of this naive strategy is that we require the method to apply the generating procedure to all image-seed pairs. This strategy is obviously not scalable for large-scale datasets. Indeed, one could easily imagine a fixed seed set, but with the proven power of the data-hungry deep learning models, it is unreasonable nowadays to devise methods that do not easily scale. In order to overcome this limitation we propose the following partial learning strategy.

%Let us assume that we cannot afford to generate every image-seed pair, but only a (random) subset of seed per each image, in such a way that we do not have complete generating information for each image, but that all seeds are used several times to generate new images. Formally, the vector $m^{\cal T}_{g}$ will have some missing entries. We can thus assume the existence of a binary variable $\omega_{gs}$ set to $1$ if the image-seed pair $gs$ has been generated and $0$ otherwise. We can know minimize the following loss:
%\begin{equation}
%\sum_{g\in{\cal I}_G}\sum_{s=1}^S \omega_{gs} \ell \left(m^{\cal T}_{gs}, R_s(\mathbf{I}_g) \right).
%\end{equation}


%Maximum number of epochs (pass through all dataset): 150. (we don't have effective number of epoch)
%We used a validation set from LaMem for early stopping.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}[t]
\begin{center}
\scalebox{0.95}{
\begin{tabular*}{0.9\linewidth}{cc@{\extracolsep{\fill}}c cc cc c cc cc}
\toprule
& & & \multicolumn{2}{c}{$A^{\mathbb M}$} &  \multicolumn{2}{c}{$A^{\mathbb E}$} && \multicolumn{2}{c}{MSE$^{\mathbb M}$} &  
\multicolumn{2}{c}{MSE$^{\mathbb E}$} \Tstrut\Bstrut \\ \midrule
 $\alpha$ & $\bar{\omega}$ && $\mathcal{B}$ & S-cube & $\mathcal{B}$ & S-cube  && $\mathcal{B}$ & S-cube & $\mathcal{B}$ & S-cube  
\Tstrut\Bstrut  \\  \midrule
\multirow{4}{*}{2} & 0.01 && \textbf{63.21} &	57.12 & \textbf{60.96}	& 56.01  && \textbf{0.0113} & 0.0138 & \textbf{0.0119} & 0.0137 \Tstrut  \\
 & 0.1  && 64.49 & \textbf{64.70} & 61.07	& \textbf{62.22} && \textbf{0.0112} & 0.0114 & \textbf{0.0117} & 0.0119  \\
 & 0.5  && 64.41 & \textbf{67.18} & 61.06	& \textbf{64.38} && 0.0112 &	\textbf{0.0102} & 0.0117 & \textbf{0.0106} \\
 & 1    && 64.41 & \textbf{67.80} & 61.06	& \textbf{64.71} && 0.0112 &	\textbf{0.0102} & 0.0117 & \textbf{0.0108} \\
\midrule
\multirow{4}{*}{10} & 0.01 && \textbf{67.91}  & 64.74 & \textbf{68.31} & 64.74 && \textbf{0.0126} & 0.0151 & \textbf{0.0134} & 0.0163 \Tstrut \\
 & 0.1  && 68.04  & \textbf{72.25} & 68.36 & \textbf{70.96} && 0.0125 & \textbf{0.0116} & 0.0132 & \textbf{0.0121} \\
 & 0.5  && 67.99  & \textbf{73.26} & 68.31 & \textbf{71.72} && 0.0125 & \textbf{0.0109} & 0.0132 & \textbf{0.0112} \\
 & 1    && 68.04  & \textbf{73.26} & 68.31 & \textbf{71.75} && 0.0125 & \textbf{0.0108} & 0.0132 & \textbf{0.0111} \\\bottomrule
\end{tabular*}
}
\vspace{-0.2cm}
\end{center}
\caption{Performance of our method S-cube compared to baseline $\mathcal{B}$ at varying percentage of training data $\bar{\omega}$ and style 
coefficient $\alpha$, measured in terms of (left) accuracy $A$ and (right) mean squared error (MSE). Performances have been evaluated using both the 
internal $\mathbb{M}$ and external $\mathbb{E}$ predictor.}
\label{tab:perf}
\end{table*}


\section{Experimental Validation}
\label{sec:exp}
\begin{sloppypar}
We assess the performance of our approach in successfully retrieving the most memorabilizing seeds to increase the memorability of arbitrary images (Sec.~\ref{sec:res-memorability}).
The datasets and experimental protocol used in our study are described in Sec.~\ref{sec:dataset}.

\subsection{Datasets and Experimental Protocol}
\label{sec:dataset}
In our experiments we consider two publicly available datasets, LaMem\footnote{http://memorability.csail.mit.edu} and DeviantArt\footnote{http://disi.unitn.it/$\sim$sartori/datasets/deviantart-dataset}.

\textbf{LaMem.} The LaMem dataset \cite{khosla2015understanding} is the largest dataset used to study memorability. It is a collection of 58,741 images gathered from a number of previously existing datasets, including the affective images dataset~\cite{machajdik2010affective}, which consists of Art and Abstract paintings. The memorability scores were collected for all the dataset pictures using an optimized protocol of the memorability game. The corpus was released to overcome the limitations of previous works on memorability which considered small datasets and very specific image domains. The large appearance variations of the images makes LaMem particularly suitable for our purpose.  

\textbf{DeviantArt.} This dataset~\cite{sartori2015affective} consists of a set of 500 abstract art paintings collected from deviantArt (dA), an online social network site devoted to user-generated art. Since the scope of our study requires avoiding substantial modifications of the high-level content of the image, we selected the style seeds from abstract paintings. Indeed, abstract art relies in textures and color combinations, thus an excellent candidate when attempting the automatic modification of the low-level image content.

\textbf{Protocol.} In our experiments using the LaMem dataset we consider the same training (45,000 images), test (10,000 images) and validation (3,741 images) data adopted in~\cite{khosla2015understanding}. We split the LaMem training set into two subsets of 22,500 images each (see also Section \ref{sec:scoring}), ${\cal M}$ and ${\cal E}$, which are used to train two predictors $\mathbb{M}$ and 
$\mathbb{E}$, respectively. The model $\mathbb{M}$ is the Scorer employed in our framework, while $\mathbb{E}$ (which we will denote in the following as the external predictor) is used to evaluate the performance of our approach, as a proxy for human assessment. We highlight that $\mathbb{M}$ and $\mathbb{E}$ can be used as two independent memorability scoring functions, since ${\cal M}$ and ${\cal E}$ are disjoint. The validation set is used to implement the early stopping.
To evaluate the performance of our scorer models ${\cal M}$ and ${\cal E}$, following \cite{khosla2015understanding}, we compute the rank correlation between predicted and actual memorability on LaMem test set. We obtain a rank correlation of 0.63 with both models, while ~\cite{khosla2015understanding} achieves a rank correlation of 0.64 training on the whole LaMem training set. As reported in ~\cite{khosla2015understanding}, this is close to human performance (0.68).

The test set of LaMem ($10$k images) is then used (i) to learn the proposed seed Selector and (ii) to evaluate the overall framework (and the Selector in particular). In detail, we split LaMem test set into train, validation and test for our Selector with proportion 8:1:1, meaning 8,000 for training and 1,000 for validation and test. The training set for the Selector was already introduced as ${\cal G}$. We denote the test set as ${\cal V}$. The validation set is used to perform early stopping, if required. 

Regarding the seeds, we estimated the memorability of all paintings of DeviantArt using $\mathbb{M}$ and selected the 50 most and he 50 least memorable images as seeds for our study (${\cal S}$). The memorability scores of the deviantArt images range from 0.556 to 0.938. 
\end{sloppypar}

%\subsection{Experimental Protocol}
%\eli{In order to properly evaluate the seed selection procedure, we require the existence of a test set of generic images ${\cal N}=\{\mathbf{I}_n^{\cal N}\}_{n=1}^N$, where ${\cal N} \cap {\cal G} = \emptyset$, and of an external scoring model $\mathbb{E}$. While the test set is required so that the performance of the Seed Selector is evaluated on a set of images never seen during its training, the external scoring model acts as an automatic proxy to asses the performance of the proposed approach. Needless to say, the external scoring model $\mathbb{E}$ is trained with a set of images ${\cal E}$ that has no intersection whatsoever with any of the previous image sets. In practice, in order to make sure that no image is used for two different trainings or at testing and training time simultaneously, all the image sets are disjoint.} \xavi{We obtain a final correlation rank of 0.64 for both $\mathbb{M}$ and $\mathbb{E}$.} We highlight that $\mathbb{M}$ and $\mathbb{E}$ can be used as two independent memorability scoring functions, since (i) models trained on LaMem are able to reach close-to-human performance and (ii) ${\cal M}$ and ${\cal E}$ are disjoint.

\textbf{Baseline.} To the best of our knowledge this is the first work showing that it is possible to automatically increase the memorability of a generic image. For this reason, a direct and quantitative comparison with previous studies is not possible. Indeed, the recent work~\cite{khosla2015understanding} showed that it is possible to compute accurate memorability maps from images, which can be used as bases for further image manipulations. They also observed that using a memorability map for removing image details, such as through a cartoonization process, typically lead to a memorability decrease. Oppositely, we aim to effectively increase image memorability without modifying the high level content of the images. Therefore, the approach by~\cite{khosla2015understanding} does not directly compare with ours. The only potential competitor to our approach would be~\cite{khosla2013modifying}, except that the method is specifically designed for face photographs. Indeed, the proposed approach aims to modify the memorability while keeping other attributes (age, gender, expression) as well as the identify untouched. Therefore, the principle of~\cite{khosla2013modifying} cannot be straightforwardly transferred to generic images. Consequently, we define an \textit{average} baseline ${\cal B}$ that consists on ranking the style seeds according to the average memorability increase, formulated as:
\begin{equation}
\bar{m}_s^\mathbb{M} = \frac{1}{G} \sum_{g=1}^{G} m_{gs}^\mathbb{M}.
\end{equation}

%\subsection{Quantitative evaluation}
\subsection{Increasing image memorability}
\label{sec:res-memorability}

%\textbf{Style seed selection}.
We first evaluate the performance of our method at predicting the memorability increase of an image-seed match, where the seed is taken from the 
set of style seeds ${\cal S}$, and the generic image $\mathbf{I}_v^{\cal V}$ is taken from a set of (yet) unseen images ${\cal 
V}=\{\mathbf{I}_v^{\cal V}\}_{v=1}^V$. We use two different performance measures: the mean squared error (MSE) and the accuracy $A$, which are 
defined as follows:
\begin{equation}
\textrm{MSE}^\mathbb{X} = \frac{1}{SV} \sum_{s=1}^{S} \sum_{v=1}^{V} \left( m^\mathbb{X}_{vs} - \mathbb{R}_s(\mathbf{I}_v^{\cal V})\right)^2
\end{equation}
and
\begin{equation}
A^\mathbb{X} = \frac{1}{SV} \sum_{s=1}^{S} \sum_{v=1}^{V} ( 1-  | H ( m_{vs}^\mathbb{X})  -  H ( \mathbb{R}_s(\bm{I}_v^{\cal V}) ) |)
\end{equation}
where $\mathbb{X}$ indicates the internal or external predictor, respectively $\mathbb{X} = \{\mathbb{M},\mathbb{E}\}$, and $H$ is the Heaviside step function.



Table~\ref{tab:perf} reports the performance of both the proposed approach (S-cube) and the baseline (${\cal B}$) under different experimental setups. Indeed, we report the accuracy (left) and the MSE (right) evaluated using the scoring model $\mathbb{M}$ and the external scoring model 
$\mathbb{E}$ (left two and right two columns of each block), for different values of $\alpha$ and the average amount of image-seed matches $\bar{\omega}$. More precisely, $\bar{\omega}=1$ means that all image-seed pairs are used, $\bar{\omega}=0.1$ means that only 10\% is used, and 
so on.

Generally speaking our method outperforms the baseline if enough image-seed pairs are available. We argue that, as it is well known, deep architectures require a \textit{sufficient} amount of data to be effective. Indeed, when $\bar{\omega}=0.01$, the network optimization procedure attemps to learn a regression from the raw image to a 100-dimensional space with, in average, only one of this dimensions propagating the 
error back to the network. Although this dimension is different for each image, we may be facing a situation in which not enough information is 
propagated back to the parameters so as to effectively learn a robust regressor. This situation is coherent when the scoring method changes from 
$\mathbb{M}$ to $\mathbb{E}$. We can clearly observe a decrease in the performance measures when using $\mathbb{E}$, as expected. Indeed, since the 
seed selector has been trained to learn the memorability gap of $\mathbb{M}$, the performance is higher when using $\mathbb{M}$ than $\mathbb{E}$.

% The performances of our method are reported in Table~\ref{tab:perf}. It can be noticed that our method which automatically select the best seeds 
% based for each input image usually outperforms the baseline method $\cal B$ which uses the same fixed ranking for every image. Specifically, S-cube 
% performances tend to degrade when the percentage of training data $\bar{\omega}_{gs}$ is reduced over 10\%. We explain this with the fact that  80,000 
% image-seed matches may not be enough to train our model. A similar trend can be observed when comparing the performances measured with the internal 
% $\mathbb{M}$ and external $\mathbb{E}$ model, both in terms of accuracy $A$ and MSE. and different values of $\alpha$,  equal to 2 
% (Fig.~\ref{tab:perf}(left) and to 10 (Fig.~\ref{tab:perf}(right)). 

Furthermore, we report the performance of our method using two different values of the style coefficient $\alpha=\{2,10\}$. It can be noticed that 
our method performs better in terms of MSE when $\alpha=2$, while accuracy is usually higher for $\alpha=10$. What a priori could be 
seen as a divergent behavior, can be explained by the fact that imposing a higher weight to the style produces higher memorability gaps $m_{gs}$, 
thus it may generate a higher error in the estimation. %Also, the synthesized images tend to look more similar to the input style (see Figure~\ref{fig:style}) thus it may be more predictable the verse of the memorability increase, if positive rather than negative. 
We interpret these  results as an indication that MSE and $A$ can be good criteria for finding the best setup in terms percentage of training data, but not necessarily to 
set other parameters.% like $\alpha$. 
%that were experimentally chosen as discussed before.

\begin{table}[t]
\begin{center}

\scalebox{0.95}{
\begin{tabular*}{\linewidth}{c@{\extracolsep{\fill}} cc cc}
\toprule
 & \multicolumn{2}{c}{$A^{\mathbb{E}}$} &  \multicolumn{2}{c}{MSE$^{\mathbb{E}}$} \Tstrut\Bstrut \\  \midrule
$\bar{\omega}$ & VGG16& AlexNet  & VGG16 & AlexNet   \Tstrut\Bstrut \\  \midrule
0.01 & \textbf{61.56} &  56.01 & \textbf{0.0121} & 0.0137     \Tstrut\\
0.1  & \textbf{64.76} &  62.22 & \textbf{0.0109} & 0.0119  \\
0.5  & 63.49 & \textbf{64.38} & 0.0111 & \textbf{0.0106}  \\
1    & 63.44 & \textbf{64.71} & 0.0111 & \textbf{0.0108}  \\
\bottomrule
\end{tabular*}}
\vspace{-0.2cm}
%\hspace{1.2cm}
\caption{Performances of our method S-cube based on AlexNet (fine-tuning Hybrid-CNN~\cite{hybrydcnn}) and VGG16 (pre-trained on ImageNet), measured in terms of MSE$^{\mathbb{E}}$ and $A^\mathbb{E}$, at varying percentage of training data $\bar{\omega}$.}
\label{tab:vgg16}
\end{center}
\vspace{-0.3cm}
\end{table}

\begin{table}[t]
\begin{center}
\scalebox{0.95}{
\begin{tabular*}{\linewidth}{c@{\extracolsep{\fill}} cc cc}
\toprule
 & \multicolumn{2}{c}{$A^{\mathbb{E}}$} &  \multicolumn{2}{c}{MSE$^{\mathbb{E}}$} \Tstrut\Bstrut \\  \midrule
$S$ & $\cal B$ & S-cube & $\cal B$ & S-cube  \Tstrut\Bstrut \\  \midrule
20 &  60.66 & \textbf{63.15} & 0.0114 & \textbf{0.0111}    \Tstrut\\
50  & 61.09  & \textbf{63.51} & 0.0116 & \textbf{0.0109}    \\
100  & 61.06 & \textbf{64.38} & 0.0117 &  \textbf{0.0106}  \\
\bottomrule
\end{tabular*}}
\vspace{-0.2cm}
%\hspace{1.2cm}
\caption{Performance of our method in terms of MSE$^\mathbb{E}$ and $A^\mathbb{E}$ ($\alpha=2$ and $\bar{\omega}=0.5$) at varying the cardinality $S$ of the style seed set.}
\label{tab:size_of_S}
\end{center}
\vspace{-0.3cm}
\end{table}

We also investigated the impact of the network depth and trained a seed Selector using VGG16 instead of AlexNet. 
We fine-tuned the layers fc6, fc7, and all conv5, using Nesterov momentum with momentum 0.9, learning rate $10^{-3}$ and batch size 64.
%\glo{VGG parameter setup}
%The parameter setup for vgg16:
%Learning rate: 1e-3.
%Method: nesterov_momentum.
%Momentum: 0.9.
%Number of epochs (pass through all dataset): 300 (For 1% and 10%) and
%500 (For 50% and 100%).
%Batch size: 64.
%I fine-tune only: fc6,fc7,conv5_* layers. (I tried to fine-tune all
%and layers and fc6,fc7,conv5_*, conv4_* but it perform worsen on 100%
%of the data)
Importantly, while AlexNet was trained as a hybrid-CNN~\cite{khosla2015understanding}, the pre-trained model for VGG16 was trained on ImageNet. We found very interesting results and report them in Table~\ref{tab:vgg16}, for $\alpha=2$. The behavior of AlexNet was already discussed in the previous paragraphs. Interestingly we observe similar trends in VGG. Indeed, when not enough training pairs are available the results are pretty unsatisfying. However, in relative terms, the results for small $\bar{\omega}$ are far better for VGG16 than for AlexNet. We attribute this to the fact that VGG16 is much larger, and therefore the amount of knowledge encoded in the pre-trained model has a stronger regularization effect in our problem than when using AlexNet. The main drawback is that, when enough data are available and since the amount of parameters in VGG16 is much larger than in AlexNet, the latest exhibits higher performance than the former. We recall that the seed Selector is trained with 8k images, and hypothesize that fine-tuning with larger datasets (something not possible if we want to use the splits provided in LaMem) will raise the performance of the VGG16-based seed Selector.

% Additionally, we implement an additional version of our method using VGG16 instead of AlexNet. While the former method - pre-trained on ImageNET - 
% has shown advances with respect to the latter in terms of performances\cite{...}, in the case of memorability we follow~\cite{khosla2015understanding} 
% and use Hybrid-CNN as a starting point fro AlexNet. Thus, while not directly comparable, we report in Figure~\ref{fig:vgg16} the results obtained with 
% the two versions of our method for the case of $\alpha=2$. Interestingly, we observe that AlexNet outperforms VGG16 when using over half of the 
% training data, while VGG16 performs better with reduced number of training data. A similar trend was observed also in the case of $\alpha=10$ and 
% internal predictor $M^{\cal T}$, results are not reported due to lack of space. While in the case of memorability using AlexNet turns out to be a 
% more convenient choice, this outcome can be interesting in the case the method wants to be applied to other abstract concepts or in case of limited 
% training data, e.g. in the case of data collection needed. Thus VGG16 model mey be preferred in these cases.

Furthermore, we studied the behavior of the framework when varying the size $S$ of the seed set. Results are shown in Table~\ref{tab:size_of_S}. 
Specifically, we select two sets of 50 and 20 seeds out of the initial 100, randomly sampling these seeds half from the 50 most and half from the 50 least memorable ones.   
In terms of accuracy, the performance of both the proposed method and the baseline remain pretty stable when decreasing the number of seeds. This behavior was also observed in Table~\ref{tab:perf}, especially for the baseline method. However, a different trend is observed for the MSE. Indeed, while the MSE of the proposed method increases when reducing the number of seeds (as expected), the opposite trend is found for the baseline method. We argue that, even if the baseline method is robust in terms of selecting the bests seeds to a decrease of the number of seeds, it does not do a good job at predicting the actual memorability increase. Instead, the proposed method is able to select the bests seeds and better measure their impact, especially when more seeds are available. This is important if the method wants to be deployed with larger seed sets. Application-wise this is quite a desirable feature since the seeds are automatically selected and hence the amount of seeds used is transparent to the user.


\begin{figure}[t]
%\hspace{0.2cm} 
\hspace{-0.15cm}
\includegraphics[width=0.48\linewidth]{img/imgs_sw10/plot_topN_newN.png}
\hspace{-0.2cm}
\includegraphics[width=0.48\linewidth]{img/imgs_sw10/plot_varS_new.png}
\vspace{-0.3cm}
%\hspace{0.2cm} \includegraphics[width=0.45\textwidth]{img/imgs_sw10/delta_mem_steps_S.png}
\caption{Sorted average memorability gaps $\bar{m}_v$ obtained with our method S-cube (left) averaging over varying number of top N seeds and (right) at varying the cardinality $S$ of the seed set, with $N=10$.}
\label{fig:delta_mem_stat1}
\vspace{-0.4cm}
\end{figure}


%\textbf{Increasing image memorability}.
%\label{sec:res-memorability}
Finally, we assess the validity of our method as a tool for effectively increasing the memorability of a generic input image $\mathbf{I}_v$. 
In Figure~\ref{fig:delta_mem_stat1} (left) we report the average memorability gaps $\bar{m}_v$ obtained over the test set $\cal V$, when averaging over the top $N$ seeds retrieved, with $N=3,10,20$ and all the images. It can be noted that $\bar{m}_v$ achieve higher values when smaller sets of top N seeds are considered, as an indication that our method effectively retrieve the most memoralizable seeds. In Figure~\ref{fig:delta_mem_stat1} (right) we report the average memorability gaps $\bar{m}_g$ obtained over the test set $\cal V$ with our mehtod S-cube, considering $N=10$ and a varying number of style seeds $S$. It can be noted that a larger number of seeds allows to achieve higher increase.
Figure~\ref{fig:qualitative} illustrates some ``image memoralization'' sample results obtained with our method. %\xavi{Comment figure 5}

%\subsection{Discussion}

%We plan to investigate Argument why we did not use GANs(?)
%Finally, we would like to remark that other approaches for image synthesis based on style transfer can be embedded in our method. We use the one from~\cite{ulyanov2016texture} rather than \cite{gatys2015neural} and \cite{li2016precomputed}, as the stylization process~\cite{ulyanov2016texture} is the fastest to generate the training samples for learning the seed Selector. 

%The choice of architecture is to keep the user in the loop and allow him/her a range of creativity/freedom on the style selection.

Summarizing, we presented an exhaustive experimental evaluation showing several interesting results. First, the proposed S-cube approach effectively learns the seeds that are expected to produce the largest increase in memorability. This increase is consistently validated when measuring it with the external scorer $\mathbb{E}$. We also investigated the effect of the choice of architecture for the seed Selector and the effect of the amount of seeds in the overall performance. Finally, we have shown the per-image memorability increase when using the top few seeds, and varying the size of the seed set. In all, the manuscript provides experimental evidence that the proposed method is able to automatically increase the memorability of generic images.

\section{Conclusions}
This paper presented a novel approach to increase image memorability based on the \emph{editing-by-filtering} philosophy. Methodologically speaking, we propose to use three deep architecures as the Scorer, the Synthesizer and the Selector. The novelty of our approach relies on the fact that the Selector is able to rank the seeds according to the expected increase of memorability and select the best ones so as to feed the Synthesizer. The effectiveness of our approach both in increasing memorability and in selecting the top memoralizable style seeds has been evaluated on a public benchmark. % And our experimental validation setup is fully reproducible.

%Our approach has several advantages. At query time, it allows to automatically select the best styles while saving the cost of running the style transfer. At training time, the cost of running style transfer is significantly reduced with a light-weight solution which allows the network to learn from incomplete data, thus not requiring the image-seed synthesis for all the pairs of images and seeds considered. %This allows to achieve high performance of the Selector at query time (high number of training image) and to maintain a user freedom in selecting the style (using a relatively high number of possible seeds). %For example, in our work we consider 100 seed styles, while the user can effectively achieve memorability increase while browsing the first 20.

%Another interesting extension of the method may be to perform an analysis based the image category such as portrait, landscape, streetscape, etc. For example, the method may automatically discover the best styles for different categories.

We believe that the problem of increasing image memorability can have a direct impact in many fields like education, elderly care or user-generated data analysis. Indeed,   memorabilizing images could help editing educational supports, designing more effective brain training games for elderly people, producing better summaries from lifelog camera image streams or leisure picture albums.

While in this work we focused on memorability, the architecture of our approach of highly versatile %generic?
and can potentially be applied to other concepts such as aesthetic judgement or emotions. A necessary condition to this is a sufficient precision of the Scorer, which should be as closer to human performance as possible. When this condition does not occur, the automatic prediction can be replaced introducing a data annotation campaign. % In our work, the burden of the data collection campaign can be significantly reduced thanks to the proposed lightweight version of our method.
The philosophy followed in this study could be extended to take into account other image properties such as aesthetics or evoked emotions \textit{simultaneously}. This is highly interesting and not straightforward, and we consider it as one of the main future work guidelines.

While literature on predicting image abstract concepts like memorability is quite huge, the literature in image synthesis with deep networks is still in its early infancy.
A promising line of work is represented by Generative Adversarial Networks (GANs) \cite{goodfellow2014generative}. %, which have been successfully used for applications like face aging~\cite{wang2016recurrent} or ...~\cite{}. 
However, it is not straightforward to apply GANs and still retaining the \emph{editing-by-filters} philosophy. Indeed, one prominent feature of our methodology is that we keep the user in the loop of the image manipulation process, %selecting the style and expressing their own creativity, also 
by allowing them to participate to the style selection, once the most promising seeds are automatically provided. 
Future research works will also investigate an alternative holistic approach based on GANs.
%a wide range of style options. 

%For the training, we provide a lightweight solution which can efficiently learn from a limited set of matches, allowing the seed retrieval from a relatively large set of candidate seeds, thus increasing the potential user creativity space, while dramatically reducing the number of training matches required.

%limitation:
%1) we cannot add a new seed without a long retraining process


%Our method is generic and potentially may be applied for other concepts like aesthetic judgement or emotions. 
%Methods for automatically increasing image memorability would have an impact in many application fields like education, gaming or advertising.
%For example, students would like to remember interesting things with less effort and companies would like to increase the chances of their logo or ads to be remembered.  

%Also, in a photo editing application scenario, the ideal situation would be that a user may be able to fine tune for example the desired level of memorability rather than aesthetic.
%A multi-concepts style seed selector may be an interesting use case study for the future.



%All these findings motivate us to pursue this line of research and to develop a methodological framework that is able to automatically increase the memorability of an input set of images. Our understanding is that the method should be able to do that (i) in an efficient manner, (ii) without significant modifications of the high-level image content and (iii) at the same time as selecting the subset of images that is going to have the highest memorability once they are modified. Such tool may have direct impact in fields like education, elderly care or user-generated data analysis. Indeed, rapidly selecting the most memorable images among all possible high-level content preserving modifications of the initial image set could help editing better educational supports, designing more effective brain training games for elderly people or producing better summaries from lifelog camera image streams or leisure picture albums.



\begin{figure*}[t]
\begin{tabular}{ccc} 
\includegraphics[height=2.4cm, width=2.4cm]{img/imgs_sw10/00010743.jpg} & 
\includegraphics[height=2.4cm, width=2.4cm]{img/seeds/0902235_Holland_Bridge_1_by_jfkpaint.jpg} & 
\includegraphics[height=2.4cm, width=2.4cm]{img/imgs_sw10/00010743_Holland.jpg} 
\\
$\mathbb{E}$: 0.72 &  & ($\mathbb{R}$: 0.76 ) $\mathbb{E}$: 0.78  \\
\includegraphics[height=2.4cm, width=2.4cm]{img/imgs_sw10/00052860.jpg} & 
\includegraphics[height=2.4cm, width=2.4cm]{img/seeds/0868667_Paper_Cuts_by_lien.jpg} & 
\includegraphics[height=2.4cm, width=2.4cm]{img/imgs_sw10/00052860_Paper.jpg} 
\\
$\mathbb{E}$: 0.78 &  & ($\mathbb{R}$: 0.78) $\mathbb{E}$: 0.85 \\
\includegraphics[height=2.4cm, width=2.4cm]{img/imgs_sw10/00037619.jpg} &
\includegraphics[height=2.4cm, width=2.4cm]{img/seeds/0863760_Color_based_by.jpg} & 
\includegraphics[height=2.4cm, width=2.4cm]{img/imgs_sw10/00037619_Color_b.jpg} 
\\
$\mathbb{E}$: 0.54 &  & ($\mathbb{R}$:0.63) $\mathbb{E}$: 0.78 
\end{tabular}
\hspace{0.6cm}
%---------------------------------------------------------------
\begin{tabular}{ccc} 
\includegraphics[height=2.4cm, width=2.4cm]{img/imgs_sw10/00035682.jpg} & 
\includegraphics[height=2.4cm, width=2.4cm]{img/seeds/0863706_abstract_by_candychua-d4hjyys.jpg} & 
\includegraphics[height=2.4cm, width=2.4cm]{img/imgs_sw10/00035682_abstract_by.jpg} 
\\
$\mathbb{E}$: 0.60 &  & ($\mathbb{R}$: 0.78) $\mathbb{E}$: 0.81   \\
\includegraphics[height=2.4cm, width=2.4cm]{img/imgs_sw10/00041754.jpg} & 
\includegraphics[height=2.4cm, width=2.4cm]{img/seeds/0860778_Prism_by_klbailey.jpg} & 
\includegraphics[height=2.4cm, width=2.4cm]{img/imgs_sw10/00041754_Prism_by.jpg} 
\\
$\mathbb{E}$: 0.67 &  & ($\mathbb{R}$: 0.80) $\mathbb{E}$: 0.82   \\
\includegraphics[height=2.4cm, width=2.4cm]{img/imgs_sw10/00018753.jpg} & 
\includegraphics[height=2.4cm, width=2.4cm]{img/seeds/__Lost___by_AznMexTofu.jpg} & 
\includegraphics[height=2.4cm, width=2.4cm]{img/imgs_sw10/00018753_AznMexTofu.jpg} 
\\
$\mathbb{E}$: 0.72 &  & ($\mathbb{R}$: 0.69) $\mathbb{E}$: 0.89
\end{tabular}
\caption{Sample results: (left) original input image, (center) retrieved style seed and (right) corresponding synthesized image. The memorability score measured with the external model $\mathbb{E}$ is reported below each image. The memorability score predicted by the Selector $\mathbb{R}$ based on the image-seed match is reported below the resulting synthesized image.}
\label{fig:qualitative}
\end{figure*}


%\end{document}  % This is where a 'short' article might terminate

%ACKNOWLEDGMENTS are optional
%\section{Acknowledgments}

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns

\end{document}
