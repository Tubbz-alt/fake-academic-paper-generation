\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage{subcaption}
\usepackage{multirow}
\usepackage[labelfont=bf]{caption}
\graphicspath{{./figures/}}
\newcommand{\architecture}[1]{\emph{#1}}
\newcommand{\arch}[1]{\emph{#1}}

\newcommand{\mrcell}[1]{\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}} #1 \end{tabular}}}
\newcommand{\lmrcell}[1]{\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}} #1 \end{tabular}}}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\renewcommand{\sectionautorefname}{Section}
\renewcommand{\subsectionautorefname}{Section}
\renewcommand{\subsubsectionautorefname}{Section}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{2863} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Supplementary Material for:\\ Training Competitive Binary Neural Networks from Scratch}

\author{
Joseph Bethge\footnotemark[1] , Marvin Bornstein\footnotemark[2] , Adrian Loy\footnotemark[2] , Haojin Yang\footnotemark[1] , Christoph Meinel\footnotemark[1]  \\
Hasso Plattner Institute, University of Potsdam, Germany\\
P.O. Box 900460, Potsdam D-14480\\
\footnotemark[1] {\tt\small firstname.surname@hpi.de}, \footnotemark[2] {\tt\small firstname.surname@student.hpi.de}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT

As mentioned in the submitted paper, we provide a concrete example to show how the equations (5) and (6) can be used to calculate the result of a matrix multiplication.
Furthermore, we provide insight on our intuition about the low impact of using scaling factors.
To make this example easily comprehensible, we created a sheet with the formulas for use with either Microsoft Excel (see \texttt{example.xlsx}) or OpenOffice/LibreOffice (see \texttt{example.ods}).
The content of both sheets is identical.
% We refer to the cell areas in the same way the programs do (\eg \texttt{A1:J6}), which means you can copy these descriptions, paste them into the area selection box (usually found to the left of the equation field), and press Enter to highlight these areas.
We refer to the cell areas in the same way the programs do (\eg \texttt{A1:J6}).
That means, that these area descriptions can be copied and pasted into the area selection box (usually found to the left of the equation field) to highlight the corresponding areas.
In our example, we consider a simple fully-connected layer for simplicity, but the same idea works for convolutional layers.
The example layer has four inputs and three outputs.
For reference we show equations (5) and (6) again:

\begin{equation} \tag{5}
\label{eqn:xnor-popcount}
    x \cdot w = 2 \odot \mathrm{bitcount}(\mathrm{xnor}(x',w')) - n~.
    % x \cdot w = 2 \cdot \mathrm{bitcount}(\mathrm{xnor}(x',w')) - n~.
\end{equation}
\begin{equation} \tag{6}
\label{eqn:xnor-popcount-swapped}
    \frac{x \cdot w + n}{2} = \mathrm{bitcount}(\mathrm{xnor}(x',w'))~.
\end{equation}

All cells highlighted in blue represent equation (5), cells in red represent equation (6).

\section{Example for xnor/popcount}
\label{sec:related}

First of all, the full-precision matrix multiplication can be found in area \texttt{A1:J6}.
Inputs and weights are highlighted in gray and can be changed.
The corresponding result for the three outputs in \texttt{H6:J6}.
Located below (\texttt{A8:J15}), is the binary matrix multiplication (which is the left side of equation (5)) based on the sign function of the full-precision inputs and weights.
The cells \texttt{H14:J14} show the direct result of this multiplication.
However, as described in equation (6), we can additionally learn the mimic operation (the multiplication with 2 and subtraction of $n$) during training, which would lead to results as in \texttt{H15:J15}.

Secondly, we use a different sign function during inference, which outputs 0 or 1 instead of -1 or +1 respectively.
This is equivalent to the right sides of equations (5) and (6) (see area \texttt{A21:J27}).
Note, that if we learn the mimic operation during training we can directly use the result (\texttt{H26:J26} according to equation (6)).
However, otherwise it needs to be calculated during inference (\texttt{H27:J27} according to equation (5)).

Independent of when we apply the mimic operation, the xnor/popcount replacement produces the exact same results (see \texttt{A29:J30}).

\section{Intution of scaling}
\label{sec:scaling}

In addition, we provide an example for our explanation, that scaling might be ineffective when learning from scratch (corresponding to the last two paragraphs of Section 3.2 in our paper).

Consider the result of a scaled binary matrix multiplication (see \texttt{D17:J19}).
We can see, that using scaling methods in general decrease the error between a full-precision convolution and a binary convolution (see the absolute error without scaling in \texttt{N14:N19}).
However, as we mentioned in the paper, a normalization (such as done in a BatchNormalization layer) almost negates this difference.
For this simple example, we first normalize the results of all outputs by subtracting the mean and dividing by the mean of the absolute values (see \texttt{T5:Y19}).
Afterwards, we can see that the difference between using scaling and not using scaling is only minimal (see \texttt{R14:R19}).
The input scaling is completely absorbed by the normalization, hence the result is always equal to the result of the unscaled binary convolution.
% The input scaling is completely absorbed by the normalization, and is always equal to the result of the binary convolution.
Only the weight scaling can slightly improve the error.
That being said, the weight scaling can also increase the error slightly for certain inputs (\eg in the original sheet in \texttt{X6:X19}).

% {\small
% \bibliographystyle{ieee}
% \bibliography{egbib}
% }

\end{document}
