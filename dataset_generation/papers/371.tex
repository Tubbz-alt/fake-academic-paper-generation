
%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
\renewcommand\baselinestretch{0.9896}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.



\usepackage{bbding}


% *** CITATION PACKAGES ***
%
\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.



\usepackage{multirow}


% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
   \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at:
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.
%\usepackage{url}



% *** SUBFIGURE PACKAGES ***
\ifCLASSOPTIONcompsoc
  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\else
  \usepackage[caption=false,font=footnotesize]{subfig}
\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and
% Axel Sommerfeldt. This package may be useful when used in conjunction with
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}

%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Road Extraction by Deep Residual U-Net}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Zhengxin Zhang$^{\dagger}$, %~\IEEEmembership{Student Member,~IEEE,}
	Qingjie Liu$^{\dagger *}$,~\IEEEmembership{Member,~IEEE}
	and Yunhong Wang,~\IEEEmembership{Senior Member,~IEEE}
	%
\thanks{This work was supported by the Natural Science Foundation of China (NSFC) under Grant 61601011.}
\thanks{Authors are with the State Key Laboratory of Virtual Reality Technology and Systems under the School of Computer Science and Engineering, Beihang University, Beijing 100191, China.}
\thanks{$^{\dagger}$ Authors contribute equally to this letter. }
\thanks{$^{*}$ Corresponding author: Qingjie Liu (qingjie.liu@buaa.edu.cn).}
\thanks{Manuscript received **, 2017; revised **, 2017.}
}
%\thanks{M. Shell was with the Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, 30332 USA e-mail: (see http://www.michaelshell.org/contact.html).}% <-this % stops a space
%\thanks{J. Doe and J. Doe are with Anonymous University.}% <-this % stops a space
%\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}

% note the % following the last \IEEEmembership and also \thanks -
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
%
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{IEEE GEOSCIENCE AND REMOTE SENSING LETTERS}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
%
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
Road extraction from aerial images has been a hot research topic in the field of remote sensing image analysis. In this letter, a semantic segmentation neural network which combines the strengths of residual learning and U-Net is proposed for road area extraction. The network is built with residual units and has similar architecture to that of U-Net. The benefits of this model is two-fold: first, residual units ease  training of deep networks. Second, the rich skip connections within the network could facilitate information propagation, allowing us to design networks with fewer parameters however better performance. We test our network on a public road dataset and compare it with U-Net and other two state of the art deep learning based road extraction methods. The proposed approach outperforms all the comparing methods, which demonstrates its superiority over recently developed state of the arts. 
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Road extraction, Convolutional Neural Network, Deep Residual U-Net.
\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
%\IEEEpeerreviewmaketitle



\section{Introduction}
\IEEEPARstart{R}{oad} extraction is one of the fundamental tasks in the field of remote sensing. It has a wide range of applications such as automatic road navigation, unmanned vehicles, urban planning, and geographic information update, etc. Although it has been received considerable attentions in the last decade, road extraction from high resolution remote sensing images is still a challenging task because of the noise, occlusions and complexity of the background in raw remote sensing imagery.
%For example, trees on the both sides of the road have severe impact on the extraction of the road.

A variety of methods have been proposed to extract roads from remote sensing images in recent years. Most of these methods can be divided into two categories: road area extraction and road centerline extraction. Road area  extraction~\cite{Xin2009Road,mnih2010learning,Unsalan2012Road,Cheng2015Urban,Saito2016Multiple,Alshehhi2017Hierarchical} can generate pixel-level labeling of roads, while road centerline extraction~\cite{Liu2015Main,Sujatha2015Connected} aims at detecting skeletons of a road. There are also methods extract both road areas and centerline, simultaneously~\cite{Cheng2017Automatic}. Since road centerline can be easily obtained from road areas using algorithms such as morphological thinning~\cite{Cheng2016Road}, this letter focuses on road area extraction from high resolution remote sensing images.

Road area extraction can be considered as a segmentation or pixel-level classification problem. For instance, Song and Civco~\cite{Song2004Road} proposed a method utilizing shape index feature and support vector machine (SVM) to detect road areas. Das et al.~\cite{Das2011Use} exploited two salient features of roads and designed a multistage framework to extract roads from high resolution multi-spectral images using probabilistic SVM. Alshehhi and Marpu~\cite{Alshehhi2017Hierarchical} proposed an unsupervised road extraction method based on hierarchical graph-based image segmentation.

Recent years have witnessed great progress in deep learning. Methods based on deep neural networks have achieved state-of-the-art performance on a variety of computer vision tasks, such as scene recognition~\cite{Zhou2014Learning} and object detection~\cite{Ren2017Faster}. Researchers in remote sensing community also seek to leverage the power of deep neural networks to solve the problems of interpretation and understanding of remote sensing data~\cite{mnih2010learning,mnih2012learning,zhang2016cnn,Saito2016Multiple,Zhang2016Deep,zhang2016functional}. These methods provide better results than traditional ones, showing great potential of applying deep learning techniques to analyze remote sensing tasks.

In the field of road extraction, one of the first attempts of applying deep learning techniques was made by Mnih and Hinton~\cite{mnih2010learning}. They proposed a method employing restricted Boltzmann machines (RBMs) to detect road areas from high resolution aerial images. To achieve better results, a pre-processing step before the detection and a post-processing step after the detection were applied. The pre-processing was deployed to reduce the dimensionality of the input data. The post-processing was employed to remove disconnected blotches and fill in the holes in the roads. Different from Mnih and Hinton's method~\cite{mnih2010learning} that use RBMs as basic blocks to built deep neural networks, Saito et al.~\cite{Saito2016Multiple} employed Convolutional Neural Network (CNNs) to extract buildings and roads directly from raw remote sensing imagery. This method achieves better results than Mnih and Hinton's method~\cite{mnih2010learning} on the Massachusetts roads dataset.

Recently, lots of works have suggested that a deeper network would have better performance~\cite{szegedy2015going,simonyan2014very}. However, it is very difficult to train a very deep architecture due to problems such as vanishing gradients. To overcome this problem, He et al.~\cite{resnet2015deep} proposed the deep residual learning framework that utilize an identity mapping~\cite{resnet2016} to facilitate  training. Instead of using skip connection in Fully Convolutional Networks (FCNs)~\cite{FCN2015fully}, Ronneberger et al.~\cite{U-NET2015} proposed the U-Net that concatenate  feature maps from different levels to improve segmentation accuracy. U-Net combines low level detail information and high level semantic information, thus achieves promising performance on biomedical image segmentation~\cite{U-NET2015}. 

Inspired by the deep residual learning~\cite{resnet2015deep} and U-Net~\cite{U-NET2015}, in this letter we propose the deep residual U-Net, an architecture that take advantage of strengths from both deep residual learning and U-Net architecture. The proposed deep residual U-Net (ResUnet) is built based on the architecture of U-Net. The differences between our deep ResUnet and U-Net are in two-fold. First, we use residual units instead of plain neural units as basic blocks to build the deep ResUnet. Second, the cropping operation is unnecessary thus removed from our network, leading to a much more elegant architecture and better performance. 

%The rest of this letter is organized as follows. In Section \ref{sec:methodology} we elaborate details of the proposed architecture. Section \ref{sec:experiment} gives experiments and comparisons on road extraction. Finally, this letter is concluded in Section \ref{sec:conslusion}.

\vspace{-0.3cm}
\section{Methodology}
\label{sec:methodology}
%\vspace{-0.3cm}

\subsection{Deep ResUnet}
\subsubsection{U-Net}
In semantic segmentation, to get a finer result, it is very important to use low level details while retaining high level semantic information~\cite{FCN2015fully,U-NET2015}. However, training such a deep neural network is very hard especially when only limited training samples are available. One way to solve this problem is employing a pre-trained network then fine-tuning it on the target dataset, as done in \cite{FCN2015fully}. Another way is employing extensive data augmentation, as done in U-Net~\cite{U-NET2015}. In addition to data augmentation, we believe the architecture of  U-Net also contributes to relieving the training problem. The intuition behind this is that copying low level features to the corresponding high levels actually creates a path for information propagation allowing signals propagate between low and high levels in a much easier way, which not only facilitating backward propagation during training, but also compensating low level finer details to high level semantic features. This somehow shares similar idea to that of residual neural network~\cite{resnet2015deep}. In this letter, we show that the performance of U-Net can be further improved by substituting the plain unit with a residual unit. 
\begin{figure}[t!]
	\begin{center}
		\includegraphics[width=0.87\columnwidth]{res-u-net-blocks.pdf}
		\caption{Building blocks of neural networks. (a) Plain neural unit used in U-Net and (b) residual unit with identity mapping used in the proposed ResUnet.}
		\label{Fig:ResNet Block}
	\end{center}
	\vspace{-0.6cm}
\end{figure}
\subsubsection{Residual unit}
Going deeper would improve the performance of a multi-layer neural network, however could hamper the training, and a degradation problem maybe occur~\cite{resnet2015deep}. To overcome these problems, He et al.~\cite{resnet2015deep} proposed the residual neural network to facilitate  training and address the degradation problem. The residual neural network consists of a series of stacked residual units. Each residual unit can be illustrated as a general form:
\begin{equation}\label{Equ:Residual Uint}
\begin{split}
\mathbf{y}_{l}\ \ \ & = h(\mathbf{x}_{l})+\mathcal{F}(\mathbf{x}_{l}, \mathcal{W}_{l}), \\
\mathbf{x}_{l+1} & = f(\mathbf{y}_{l}),
\end{split}
\end{equation}
where $\mathbf{x}_{l}$ and $\mathbf{x}_{l+1}$ are the input and output of the $l$-th residual unit, $\mathcal{F}(\cdot)$ is the residual function, $f(\mathbf{y}_l)$ is activation function and $h(\mathbf{x}_{l})$ is a identity mapping function, a typical one is  $h(\mathbf{x}_{l}) = \mathbf{x}_{l}$. Fig.~\ref{Fig:ResNet Block} shows the difference between a plain and residual unit. There are multiple combinations of batch normalization (BN), ReLU activation and convolutional layers in a residual unit. He et al. presented a detailed discussion on impacts of different combinations in \cite{resnet2016} and suggested a full pre-activation design as shown in  Fig.~\ref{Fig:ResNet Block}(b). In this work, we also employ full pre-activation residual unit to build our deep residual U-Net. 
\begin{figure}[tbp!]
	\begin{center}
		\includegraphics[width=0.87\columnwidth]{Deep-Res-U-Net.pdf}
		\caption{The architecture of the proposed deep ResUnet.}
		\label{Fig:Deep Res-U-Net}
	\end{center}
	\vspace{-0.7cm}
\end{figure}

\subsubsection{Deep ResUnet}
Here we propose the deep ResUnet, a semantic segmentation neural network which combines strengths of both U-Net and residual neural network. This combination bring us two benefits: 1) the residual unit will ease training of the network; 2) the skip connections within a residual unit and between low levels and high levels of the network will facilitate information propagation without degradation, making it possible to design a neural network with much fewer parameters however could achieve comparable ever better performance on semantic segmentation. 

In this work, we utilize a 7-level architecture of deep ResUnet for road area extraction, as shown in Fig.~\ref{Fig:Deep Res-U-Net}. The network comprises of three parts: encoding, bridge and decoding.\footnote{U-Net used  ``contracting" and ``expansive" paths to denote the feature extraction and up-convolution stages of the network. In this letter, we prefer the terms encoding and decoding because we think it is more meaningful and easer to understand.} The first part encodes the input image into compact representations. The last part recovers the representations to a pixel-wise categorization, i.e. semantic segmentation. The middle part serves like a bridge connecting the encoding and decoding paths. All of the three parts are built with residual units which consist of two $3\times3$ convolution blocks and an identity mapping. Each convolution block includes a BN layer, a ReLU activation layer and a convolutional layer. The identity mapping connects input and output of the unit. 

Encoding path has three residual units. In each unit, instead of using pooling operation to downsample the feature map size, a stride of 2 is applied to the first convolution block to reduce the feature map by half. Correspondingly, decoding path composes of three residual units, too. Before each unit, there is an up-sampling of feature maps from lower level and a concatenation with the feature maps from the corresponding encoding path. After the last level of decoding path, a $1\times1$ convolution and a sigmod activation layer is used to project the multi-channel feature maps into the desired segmentation. In total we have 15 convolutional layers comparing with 23 layers of U-Net. It is worth noting that the indispensable cropping in U-Net is unnecessary in our network. The parameters and output size of each step are presented in Table~\ref{Table:Feature Size}. 

\begin{table}[!htb]
	\tiny
	\centering
	\caption{The network structure of ResUnet.}	
	\label{Table:Feature Size}	
	\begin{tabular}{ccllcl}
		\hline
		\hline
		& Unit level  & Conv layer & Filter  & Stride &  Output size\\
		\hline
		Input                      &        &     &   &    &  $224\times 224 \times 3$ \\
		\hline	
		\multirow{6}{*}{Encoding}
		& \multirow{2}{*}{Level 1}& Conv 1  & $3\times 3/64$ & 1 &  $224\times 224 \times 64$ \\
		&        & Conv 2  &  $3\times 3/64$ & 1 &  $224\times 224 \times 64$ \\
		\cline{2-6}			
		& \multirow{2}{*}{Level 2}& Conv 3  &  $3\times 3/128$ & 2 &  $112\times 112 \times 128$ \\
		&        & Conv 4  &  $3\times 3/128$ & 1 &  $112\times 112 \times 128$ \\
		\cline{2-6}
		& \multirow{2}{*}{Level 3}& Conv 5  &  $3\times 3/256$ & 2 &  $56\times 56 \times 256$ \\
		&        & Conv 6  &  $3\times 3/256$ & 1 &  $56\times 56 \times 256$ \\
		\cline{2-6}						
		\hline
		\multirow{2}{*}{Bridge}
		&\multirow{2}{*}{Level 4}         &Conv 7  &  $3\times 3/512$ & 2 &  $28\times 28 \times 512$ \\
		&	      &Conv 8 &  $3\times 3/512$ & 1 &  $28\times 28 \times 512$\\
		\hline
		\multirow{6}{*}{Decoding}	
		& \multirow{2}{*}{Level 5} &Conv 9 &  $3\times 3/256$ & 1 &  $56\times 56 \times 256$ \\
		&        &Conv 10 &  $3\times 3/256$ & 1 & $56\times 56 \times 256$ \\
		\cline{2-6}
		& \multirow{2}{*}{Level 6} &Conv 11 &  $3\times 3/128$ & 1 &  $112\times 112 \times 128$ \\
		&        &Conv 12 &  $3\times 3/128$ & 1 &  $112\times 112 \times 128$ \\
		\cline{2-6}			
		& \multirow{2}{*}{Level 7} &Conv 13 &  $3\times 3/64$ & 1 &   $224\times 224 \times 64$ \\
		&        &Conv 14 &  $3\times 3/64$ & 1 &  $224\times 224 \times 64$ \\
		\hline
		Output                      &        &Conv 15 &  $1\times 1$ & 1 &  $224\times 224 \times 1$ \\		
		\hline
		\hline
	\end{tabular}
	\vspace{-0.6cm}
\end{table}



\subsection{Loss function}
Given a set of training images and the corresponding ground truth segmentations $\{I_i,s_i\}$, our goal is to estimate parameters $W$ of the network, such that it produce accurate and robust road areas. This is achieved through minimizing the loss between the  segmentations generated by $Net(I_i;W)$ and the ground truth $s_i$. In this work, we use Mean Squared Error (MSE) as the loss function:
\begin{equation}\label{Equ:mse}
\mathcal{L}(W) = \frac{1}{N}\sum\limits^{N}_{i=1}||Net(I_i;W) - s_i||^2,
\end{equation}
where $N$ is the number of the training samples. We use the stochastic gradient descent (SGD) to train our network. One should know that other loss functions that are derivable can also be used to train the network. For instance, U-Net adopted pixel-wise cross entropy as loss function to optimize the model. 
%We also tried it on our task however obtained a worse result than MSE, thus in this letter we stick to the latter loss function. 

\subsection{Result refinement}

%\begin{figure}[t!]
%\begin{center}
%    \includegraphics[width=1.0\columnwidth]{figures/test_sites/flowchart_1.jpg}
%	\caption{(i) The dashed line marks image patch for input and the solid line marks the output after removing offset; (ii) One patch's classification result is the average of all image patches' classification outputs which cover it.}
%	\label{Fig:Offset and Model Averaging}
%\end{center}
%\end{figure}
The input and output of our semantic segmentation network have the same size in width and height, both are $224\times224$. The pixels near  boundaries of the output have lower accuracy than center ones due to zero padding in the convolutional layer. To get a better result, we use an overlap strategy to produce the segmentation results of a large image. The input sub-images are cropped from the original image with an overlap of $o$ ($o=14$ in our experiments). The final results are obtained by stitching all sub-segmentations together. The values in the overlap regions are averaged. 

\vspace{-0.2cm}
\section{Experiments}
\label{sec:experiment}

To demonstrate the accuracy and efficiency of the proposed deep ResUnet, we test it on Massachusetts roads dataset\footnote{https://www.cs.toronto.edu/\~{}vmnih/data/} and compare it with three state of the art methods, including Mnih's~\cite{mnih2010learning} method, Saito's method~\cite{Saito2016Multiple} and U-Net~\cite{U-NET2015}. 

\vspace{-0.2cm}
\subsection{Dataset}
The Massachusetts roads dataset was built by Mihn et al.~\cite{mnih2010learning}. The dataset consists of 1171 images in total, including 1108 images for training, 14 images for validation and 49 images for testing. The size of all the images in this dataset is $1500\times1500$ pixels with a resolution of 1.2 meter per pixel. This dataset roughly covers 500 km$^2$ space crossing from urban, sub-urban to rural areas and a wide range of ground objects including roads, rivers, sea, various buildings, vegetations, schools, bridges, ports, vehicles, etc. In this work, we train our network on the training set of this dataset and report results on its test set.


\subsection{Implementation details}
The proposed model was implemented using Keras~\cite{chollet2015keras} framework and optimized by minimizing Eqn.~\ref{Equ:mse} through SGD algorithm. There are 1108 training images sized $1500\times1500$ available for training. Theoretically, our network can take arbitrary size image as input, however it will need amount of GPU memory to store the feature maps. In this letter, we utilize fixed-sized training images ($224\times224$ as described in Table~\ref{Table:Feature Size}) to train the model. These training images are randomly sampled from the original images. At last, 30,000 samples are generated and fed into the network to learn the parameters. It should be noted that, no data augmentation is used during training. We start training the model with a mini-batch size of 8 on a NVIDIA Titan 1080 GPU. The learning rate was initially set to 0.001 and reduced by a factor of 0.1 in every 20 epochs. The network will converge in 50 epochs. 
\subsection{Evaluation metrics}
The most common metrics for evaluating a binary classification method are precision and recall. In remote sensing, these metrics are also called correctness and completeness. The precision is the fraction of predicted road pixels which are labeled as roads and the recall is the fraction of all the labeled road pixels that are correctly predicted.

Because of the difficulty in correctly labeling all the road pixels, Mnih et al.~\cite{mnih2010learning} introduced the relaxed precision and recall scores~\cite{Ehrig2005Relaxed} into road extraction. The relaxed precision is defined as the fraction of number of pixels predicted as road within a range of $\rho$ pixels from pixels labeled as road. The relaxed recall is the fraction of number of pixels labeled as road that are within a range of $\rho$ pixels from pixels predicted as road. In this experiment, the slack parameter $\rho$ is set to 3, which is consistent with previous studies\cite{mnih2010learning,Saito2016Multiple}. We also report break-even points of different methods. The break-even point is defined as the point on the relaxed precision-recall curve where its precision value equals its recall value. In other words, break-even point is the intersection of precision-recall curve and line $y=x$. 
%In Fig. ~\ref{Fig:PR}, we can see all the break-even point are on the straight line $y=x$.

\subsection{Comparisons}
Comparisons with three state of the art deep learning based road extraction methods are conducted on the test set of Massachusetts roads dataset. The break-even points of the proposed and comparing methods are reported in Table~\ref{Table:Value at Breakeven Point}. Fig.~\ref{Fig:PR} presents the relaxed precision-recall curves of U-Net and our network and their break-even points, along with break-even points of comparing methods. It can be seen that our method performs better than all other three approaches in terms of relaxed precision and recall. Although the parameters of our network is only $1/4$ of U-Net (7.8M versus 30.6M), promising improvement are achieved on the road extraction task. 

\begin{table}[!hbp]
\vspace{-0.2cm}
\begin{center}
\caption{Comparisons of the proposed and other three deep learning based road extraction method on Massachusetts roads dataset in terms of breakeven point. A higher breakeven point indicates a better performance in precision and recall. }
\label{Table:Value at Breakeven Point}
\begin{tabular}{l|c}
\hline
Model & Breakeven point\\
\hline
Mnih-CNN\cite{mnih2010learning} & 0.8873  \\
\hline
Mnih-CNN+CRF\cite{mnih2010learning} & 0.8904  \\
\hline
Mnih-CNN+Post-Processing\cite{mnih2010learning} & 0.9006  \\
\hline
Saito-CNN\cite{Saito2016Multiple} & 0.9047 \\
\hline
U-Net\cite{U-NET2015} & 0.9053 \\
\hline
ResUnet & \textbf{0.9187} \\
\hline
\end{tabular}
\end{center}
\vspace{-0.3cm}
\end{table}


\begin{figure}[!t]
\begin{center}
		\includegraphics[width=1\columnwidth]{pr1.pdf}
	    \caption{The relaxed precision-recall curves of U-Net and the proposed method on Massachusetts roads dataset. The marks `$\star$' and `$\times$' are break-even points of different methods.}
	    \label{Fig:PR}
\end{center}
\vspace{-0.5cm}
\end{figure}

\begin{figure*}[ht]
\begin{center}
		\includegraphics[width=1\textwidth]{imag_compare_2.pdf}
	    \caption{Example results on the test set of Massachusetts roads dataset. (a) Input image; (b) Ground truth; (c) Saito et al.~\cite{Saito2016Multiple}; (d) U-Net~\cite{U-NET2015}; (e) The proposed ResUnet. Zoom in to see more details.}
	    \label{Fig:Result Comparison_0}
\end{center}
\vspace{-0.6cm}
\end{figure*}

Fig.~\ref{Fig:Result Comparison_0} illustrates four example results of Saito et al.~\cite{Saito2016Multiple}, U-Net~\cite{U-NET2015} and the proposed ResUnet. It can be seen, our method shows cleaner results with less noise than the other two methods. Especially when there are two-lane roads, our method can segmentation each lane with high confidence, generating clean and sharp two-lane roads, while other methods may confuse lanes with each other, as demonstrate in the third row of Fig.~\ref{Fig:Result Comparison_0}. Similarly, in the intersection regions, our method also produces better results.

Context information is very important when analyzing objects with complex structures. Our network considers context information of roads, thus can distinguish roads from similar objects such as building roofs, airfield runways. From the first row of Fig.~\ref{Fig:Result Comparison_0} we can see that, even the runway has very similar features to a highway, our method can successfully segmentation side road from the runway. In addition to this, the context information also make it robust to occlusions. For example, parts of the roads on the rectangle of the second row are covered by trees. Saito's method and U-Net cannot detect road under the trees, however our method labeled them successfully. A failure case is shown in the yellow rectangle of the last row. Our method missed the roads in the parking lot. This is mainly because most of roads in parking lots are not labeled. Therefore, although these roads share the same features to the normal ones, considering the context information our network regard them as backgrounds. 


\section{Conclusion}
\label{sec:conslusion}
In this letter, we have proposed the ResUnet for road extraction from high resolution remote sensing images. The proposed network combines the strengths of residual learning and U-Net. The skip connections within the residual units and between the encoding and decoding paths of the network will facilitate information propagations both in forward and backward computations. This property not only ease training but also allows us to design simple yet powerful neural networks. The proposed network outperforms U-Net with only 1/4 of its parameters, as well as other two state of the art deep learning based road extraction methods.





\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,bibi}

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)


% biography section
%
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

%\begin{IEEEbiography}{Michael Shell}

%\end{IEEEbiography}

% if you will not have a photo at all:
%\begin{IEEEbiographynophoto}{John Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

%\begin{IEEEbiographynophoto}{Jane Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}




