\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{longtable}
\usepackage{adjustbox}

% Include other packages here, before hyperref.
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\graphicspath{ {figures/} }
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{20} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\newcommand*{\affmark}[1][*]{\textsuperscript{#1}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
%\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{Semi-supervised Learning: Fusion of Self-supervised, Supervised Learning, and Multimodal Cues for Tactical Driver Behavior Detection}

\author{Athma Narayanan\\
Honda Research Institute, USA\\
{\tt\small anarayanan@honda-ri.com}
\and
Yi-Ting Chen\\
Honda Research Institute, USA\\
{\tt\small ychen@honda-ri.com}\\
\and
Srikanth Malla\\
Honda Research Institute, USA\\
{\tt\small smalla@honda-ri.com}\\
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   In this paper, we presented a preliminary study for tactical driver behavior detection from untrimmed naturalistic driving recordings. While supervised learning based detection is a common approach, it suffers when labeled data is scarce. Manual annotation is both time-consuming and expensive. To emphasize this problem, we experimented on a 104-hour real-world naturalistic driving dataset with a set of predefined driving behaviors annotated. There are three challenges in the dataset. First, predefined driving behaviors are sparse in a naturalistic driving setting. Second, the distribution of driving behaviors is long-tail. Third, a huge intra-class variation is observed. To address these issues, recent self-supervised and supervised learning and fusion of multimodal cues are leveraged into our architecture design. Preliminary experiments and discussions are reported.          
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Intelligent transportation systems require interdisciplinary efforts including computer vision, machine learning, robotics, psychology, and control theory. It is challenging to drive in real world because decisions need to be made with incomplete information and diverse situations. Moreover, modeling uncertain behaviors of road users is still unsolved.

Towards this goal, we collected a naturalistic driving dataset, which will appear in CVPR'18 main conference~\cite{RamanishkaCVPR2018}%\footnote{To follow the double-blind policy, the current reference removes authors and titles.}. 
The total size of the dataset is 104 video hours with the predefined driving behaviors annotated. %Note that we experimented the proposed approach on a 60-hour subset and compared the performance to the baseline algorithm reported in~\cite{RamanishkaCVPR2018} on the same set. %In the supplementary material, we will report the same training/testing split as in~\cite{RamanishkaCVPR2018}. %The data was collected by an instrumented vehicle with different sensors including cameras, LiDAR, GPS/IMU, and CAN BUS. The sensor data are synchronized and timestamped using ROS\footnote{http://www.ros.org/}. %For the sensor calibration, we follow KITTI dataset~\cite{Geiger2012CVPR} for camera, LiDAR and GPS/IMU calibration. 
We defined a 4-layer scheme to annotate driver behaviors including tactical driver behaviors and interactive behaviors between the drivers and traffic participants. More details of the annotation and definition of driver behaviors will be provided in the supplementary material. Note that driving behaviors are a combination of driver behaviors, the interactive behaviors between driver and traffic participants, and traffic participants' behaviors. In this paper, we focus on detecting tactical driver behaviors as in~\cite{RamanishkaCVPR2018}. 

Manual annotation of driving behaviors is time-consuming and expensive. To minimize human efforts, automatic detection mechanism is necessary. While supervised learning is a common approach to address the problem, it suffers from when labeled data is scarce. This issue is presented in the collected dataset. Specifically, the dataset has the following three challenges. First, predefined driving behaviors are sparse. Only 15\% of data is labeled. Most of the time, drivers are doing "going straight," "stopping for red light," and "parking." Second, the distribution of driving behaviors is long-tail. For example, we observe more "turning" than "U-turn." Third, a huge intra-class variation is observed. For instance, a "turning right" is different from a "turning right while yielding a group of pedestrians."

We leverage recent advances in self-supervised learning for structure from motion~\cite{ZhouCVPR2017}, supervised learning for semantic segmentation~\cite{LinCVPR2017,ChenPAMI2018}, imbalance class distribution handling~\cite{Lin2017b}, and multimodal fusion~\cite{hazirbas2016fusenet} to address aforementioned issues. The proposed algorithm is presented.    

% {\color{red}In~\cite{RamanishkaCVPR2018}, we presented a baseline algorithm for tactical driver behavior retrieval. The model employed the image feature from the \textit{Conv2d\_7b\_1x1} layer of InceptionResnet-V2~\cite{SzegedyarXiv2016} pretrained on ImageNet. The image features are concatenated with raw CAN BUS signal as the input to the LSTM for tactical driver behavior recognition.} The baseline model  

% \begin{figure}
% \begin{center}
% \includegraphics[scale=0.28, trim={0cm 0cm 0cm 0cm},clip]{baseline1}
%    \caption{Baseline architecture combining Image and CAN sensor data.}
% \end{center}   
% \label{fig:Architecture}
% \end{figure}



% % \begin{figure}
% % \begin{center}
% % \includegraphics[scale=0.28, trim={0cm 0cm 0cm 0cm},clip]{fusion1}
% %    \caption{A architecture combining self-supervised learning, semi-supervised learning and multimodal fusion is presented for tactical driver behavior recognition.}
% % \end{center}   
% % \label{fig:Architecture}
% % \end{figure}

\setlength{\belowcaptionskip}{-10.5pt}

\begin{figure*}
\label{baseline2}
\begin{center}
\includegraphics[scale=0.21, trim={0cm 0cm 0cm 0cm},clip]{final}
   \caption{Figure (a) The baseline architecture combining InceptionResNet-V2 Image features and CAN sensor data~\cite{RamanishkaCVPR2018}. 
Figure (b) A baseline architecture using features obtained by~\cite{santana2016learning}. Figure (c) The proposed architecture combining self-supervised learning, semi-supervised learning and multimodal cues is presented for tactical driver behavior recognition.}
\end{center}   
\end{figure*}



\section{Methodology and Experiments}
 
% Explain that we have 5/6 different experiments. Explain that all have common LSTM architecture.Give training details.Then Add subheading for each experiment and explain the architecture before LSTM.
% Write a para on explaining results and cite the tabular column.
% Add tabular column.

We hypothesize that semantic context, 3D scene structure and vehicle motion are crucial tactical driver behavior detection. We intended to leverage features extracted from these cues than using features trained by supervision. This section gives the details of the architecture design and the preliminary results. 

Given a synchronized images and Controller
 Area Network (CAN bus) sensors data, the baseline model~\cite{RamanishkaCVPR2018} sampled input frames from video streams and values from CAN bus sensors at 3 Hz. The frame representation is extracted from the \textit{Conv2d\_7b\_1x1} layer of InceptionResnet-V2~\cite{SzegedyIV16} pretrained on ImageNet~\cite{Deng09imagenet}. The features are convolved with a $1 \times 1$ convolution to reduce
dimensionality from $8 \times 8 \times 1536$ to
$8 \times 8 \times 20$. Raw sensor values are passed through a fully-connected layer to obtain a one dimensional feature vector which is further concatenated with image features.
The concatenated features are fed into a Long-Short Term Memory (LSTM)~\cite{Hochreiter:1997:LSM:1246443.1246450} to encode the necessary history of past measurements. %The hidden state size is set to $2000$ in all experiments. 
During training, we formed batches of sequence segments by sequentially iterating over driving sessions. The last LSTM hidden state from the previous batch is used to initialize the LSTM hidden state on the next step. The training is performed using truncated backpropagation through time. Addtionally, the data imbalance between foreground and background frames are handled using the recently proposed technique for modifying cross-entropy loss to deal with the class imbalance~\cite{Lin2017b}. Note that the details of the training protocol will be provided in the supplementary material.

%The LSTM uses batches of size 40 with each sequence length set to 90 samples. 
%
%The dropout \textit{keep} probability on the input and output of the LSTM to 0.9.
%


Five different experiments were conducted as shown in Table \ref{table:results}. Note that we adopted the same architecture as in~\cite{RamanishkaCVPR2018} to detect tactical driver behaviors, but with different image features. First, we presented the baseline as in \cite{RamanishkaCVPR2018}. Second, we trained an auto-encoder with adversarial loss to generate image features as in \cite{santana2016learning}. We expect the reconstruction of images can learn the scene composition of the dataset. To reduce the reliance on direct supervision, we leveraged image reconstruction features to serve as a substitute. We took the intermediate encoder feature representation to the LSTM. Third, as 3D scene structure is crucial, we leveraged the unsupervised learning based structure from motion~\cite{ZhouCVPR2017}. 
%
%In this unsupervised approach, the target frame and nearby camera view frames are used to simultaneously train depth and camera pose using the task of view synthesis as supervision signal. 
%
Fourth, for semantic context, we modify Deeplab~\cite{ChenPAMI2018} to incorporate Feature Pyramid Network~\cite{LinCVPR2017} to enrich features at higher resolution features. Finally, features from 3D scene structure and semantic context are fused with CAN bus features by concatenation and batch normalization, similar to the work done in~\cite{hazirbas2016fusenet}. The aforementioned architectures are shown in Figure 1.

% \begin{figure}
% \begin{center}
% \includegraphics[scale=0.28, trim={0cm 0cm 0cm 0cm},clip]{proposed}
%    \caption{Proposed architecture combining self-supervised learning, semi-supervised learning and multimodal fusion is presented for tactical driver behavior recognition.}
% \end{center}   
% \label{fig:arch}
% \end{figure}
\begin{table}
\small
\caption{Experimental results on a set of 104-hour data. All number are in \%.} % title of Table
\centering % used for centering table
  \begin{adjustbox}{max width=8cm}
\begin{tabular}{c c c c c c} % centered columns (4 columns)
\hline\hline %inserts double horizontal lines
Driver behavior class  &\cite{RamanishkaCVPR2018} & \cite{santana2016learning} & Depth+CAN & Seg+CAN & Our  \\ [0.5ex] % inserts table
%heading
\hline % inserts single horizontal line
left lane change & 35.72 & 14.06& \textbf{38.96} &37.13&34.45  \\ 
right lane change & 25.48 & 6.42&25.32 &23.25&\bf{28.06} \\
railroad passing & \textbf{7.27} & 0.14& 0.82 &3.06&5.40  \\
left lane branch & 20.00 & 4.09 & 26.68 &35.94&\bf{43.05} \\
right lane branch & 0.74 & 0.59 &{1.58} &\bf{2.97}& {2.10} \\
left turn & 73.52 & 66.00&{74.21} &\bf{77.78}& 75.07 \\
right turn & 73.95 & 73.52 &\bf{76.20} &75.63&75.82 \\
U-turn & 15.78 & 26.77 &\bf{32.54} &\bf{27.77}&{26.40} \\
intersection passing & 74.12 & 29.45 &69.98 &\bf{79.69} &{77.70} \\
crosswalk passing & 4.04 & 0.63 &6.65 &\bf{14.02}&{13.14} \\
merge  & 6.35 & 0.40 &9.17 &14.83&\bf{16.42} \\[1ex] % [1ex] adds vertical space
\hline
\textbf{mean} & 30.63 & 20.19 &32.92 &35.64&\textbf{36.15} \\[0.5ex]

\hline %inserts single line
\end{tabular}
\end{adjustbox}
\label{table:results} % is used to refer this table in the text
\end{table}

\section{Discussion}
Our experiments indicate that robust fusion of image features from auxiliary tasks such as 3D scene structure and semantic context help the driver behavior detection tasks as demonstrated in Table \ref{table:results}. With  semantic context and 3D scene structure, we see improvements in classes such as intersection passing, cross-walk passing, U-turn and merge class. The proposed architecture improves the performance of~\cite{santana2016learning} by 16 \%. This demonstrates the effectiveness of the proposed features over features obtained by reconstruction. 

However, we expected a significant performance boost in those behaviors with strong correlations to semantic context (e.g., lane change due to the existence of lane markers in semantic context). The fusion of semantic context with CAN (i.e., $4^{th}$ column results) does not reflect this hypothesis. Note that \textbf{railroad} is not trained in the current semantic context algorithm. A better architecture design in the multimodal fusion, imbalanced distribution, and temporal modeling is necessary for further improvement.


%We choose a semi supervised approach leveraging supervision from only semantic segmentation and perform unsupervised depth estimation to infer 3D structure. 

%Moreover, we perform extensive experiments on various sensor input combinations adding one sensor modality at a time. The proposed architecture,training details and experiments are explained below.


%{ \bf Proposed Architecture.} Given a synchronized Image and CAN sequence we perform semantic segmentation and unsupervised depth estimation in parallel. 

%Following the state-of-the-art approaches in  semantic segmentation, we apply Resnet-101~\cite{HeCVPR2016} as the backbone for feature extraction and initialize weights from a pretrained ImageNet model~\cite{imagenet_cvpr09}.Additionally, we modified Resnet-101 to adopt the Feature Pyramid Network (FPN) structure as in~\cite{lin2016fpn}, i.e., by adding top-down pathway and skip-connections, to enrich features at higher resolution feature maps.

%3D structure features are got using structure from motion~\cite{ZhouCVPR2017}. In this unsupervised approach, the target frame and nearby camera view frames are used to simultaneously train depth and camera pose using the task of view synthesis as supervision signal.



%\section{Discussion and Ongoing Works}
%Due to the lack of large scale annotations we exploit as much information as possible from the data. Manually balancing data, during training, improves accuracy. Yet, massive amounts of unlabeled data remains deserted. Weakly supervised learning utilizes this data and alleviates the large scale explicitly encoded constraint. This motivates directions beyond supervised learning. 

% Beside labeled data scarcity, driving behavior recognition faces foreground-background and intra-foreground imbalance problems. Foreground classes, like turning left or right, are overwhelmed by moving straightforward-- background class. Manually balancing data, during training, improves accuracy. Yet, massive amounts of unlabeled data remains deserted. Weakly supervised learning utilizes this data and alleviates the large scale explicitly encoded constraint. This motivates directions beyond supervised learning.

%Recent self-supervised learning frameworks leverage sequential verification task for learning action recognition. Misra et al.~\cite{misra2016shuffle} reason about the frames order in a binary classification problem settings. To boost performance, Fernando et al.~\cite{fernando2017self} extend the binary classification into a multi-classification problem -- a more difficult setting. In such setting, the network reasons through multiple stack of frames and classify the odd stack out. To make it more challenging, Lee et al.~\cite{lee2017unsupervised} predict the correct temporal order of frames.



% {\color{blue}  
% Big data is a key pillar for the recent deep neural network advancement~\cite{krizhevsky2012imagenet}. Yet even with data availability, annotation costs hinder supervised learning for applications like autonomous car driving. Annotation can be costly for various reasons; it can be time consuming as in action detection. It can be expensive for requiring expertise as in medical field. In contrast, recording unannotated videos is becoming cheaper. Thus, despite supervised learning success, some domains lag for lacking labeled data. In this paper, we investigate semi-supervised and self-supervised approaches for driving behavior recognition.}






\clearpage{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
