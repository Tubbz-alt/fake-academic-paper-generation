\documentclass[conference]{IEEEtran}
\usepackage{bm}
\usepackage{color,soul}
\usepackage{algorithm} 
\usepackage{algorithmic}
\usepackage{xcolor}




% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)

% *** MISC UTILITY PACKAGES ***
%
\usepackage{ifpdf}

% *** CITATION PACKAGES ***
\usepackage{cite}


% *** GRAPHICS RELATED PACKAGES ***
\ifCLASSINFOpdf
   \usepackage[pdftex]{graphicx}
\else
  \usepackage[dvips]{graphicx}
\fi



% *** MATH PACKAGES ***
\usepackage{amsmath}


% *** SPECIALIZED LIST PACKAGES ***

\usepackage{algorithmic}
\usepackage{array}

\ifCLASSOPTIONcompsoc
  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\else
  \usepackage[caption=false,font=footnotesize]{subfig}
\fi

\usepackage{fixltx2e}
\usepackage{url}

\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{import}
\usepackage{tikz}
\usetikzlibrary{positioning}

\begin{document}


\title{Robust Processing: A Robust Defense Method Against Adversary Attack}

% \author{
% \IEEEauthorblockA{Zhezhi He, Shaahin Angizi, Deliang Fan\\Department of Electrical and Computer Engineering\\
% University of Central Florida, Orlando, Florida  32816-2362\\
% Email: Elliot.he@knights.ucf.edu, Angizi@knights.ucf.edu, Dfan@ucf.edu}
% }

\maketitle

\begin{abstract}
Deep learning algorithms and networks are vulnerable to perturbed inputs which is known as adversarial attack. Many defense methodologies have been investigated to defend such adversarial attack. In this work, we propose a novel methodology to defend the existing powerful attack model. Such attack models has achieved record success against MNIST dataset to force it to miss-classify all of its inputs. Whereas Our proposed defense method robust processing achieves the best accuracy among the current state of the art defenses. It consist of tanh function, smoothing and batch normalization to process the input data which will make it more robust over adversarial attack. Robust processing improves the white box attack accuracy of MNIST from 94.3\% to 98.7\%. Even with increasing defense when others defenses completely fails, robust processing remains one of the strongest ever reported. Another strength of our defense is that, it eliminates the need for adversarial training as it can significantly increase the MNIST accuracy without adversarial training as well. This makes it a more generalized defense method with almost half training overhead and much improved accuracy. Robust processing can also increase the inference accuracy in the face of powerful attack on Cifar-10 and SVHN data set as well without much sacrificing clean data accuracy.
\end{abstract}


\section{Introduction}
Deep Neural Network (DNN) has achieved great success in performing classification \cite{andor2016globally}, image detection \cite{he2015delving} and speech recognition \cite{xiong2016achieving}. All these successes lead to another interesting topic about how well DNN will perform practically depends heavily on how well we address the issues concerning the security of these complex models. It was observed by  Szegedy et al. (2014) for computer vision \cite{goodfellow2014explaining} and Biggio et al. (2013) for malware detection \cite{biggio2013evasion} that DNNs are vulnerable to adversarial examples which can be generated just by slightly changing inputs or by introducing random noise to the inputs. Not only image classification but also other popular fields of DNN are facing vulnerability to adversarial examples \cite{kos2017adversarial,kos2017delving}. Similarly, The behaviour of popular Convolutional Neural Network (CNN) models is no exception as it shows no resistance to adversarial examples. 

\begin{figure}[h]
\centering
\scalebox{1}{
\includegraphics[width=8cm,height=3cm]{images/mnist.png}
}
\caption{Adversarial attack causing miss classification in MNIST data set}
\end{figure}%Till date we found very few methods that could stand out in defending adversarial examples just on even simple datasets such as MNIST.
There are now various works on generating adversarial attacks and developing corresponding defense methods. Attacking CNN can be classified into two major categories. In one case the attacker has full access to the network model and parameter which is known as white box attack. On the other hand, for black box attack the attacker sees the model as a black box and can only play with the data. Fast Gradient sign method is one of the popular white box attack method which uses the sign of the gradient of loss function \cite{goodfellow2014explaining}. Different authors have proposed different attack models to adapt the attacking techniques beating the improving defenses \cite{szegedy2013intriguing,kurakin2016adversarial,kos2017delving,papernot2016limitations,moosavi2016deepfool}. Some of the recent attack methods achieved even better success causing CNN to classify whole MNIST data set to wrong classes. For example, projected gradient descent is one of the most powerful white box attack till date \cite{mkadry2017towards}. 

Many defense method have been proposed to beat adversarial attacks. Some of them achieved good successes mainly against weak attacks, such a Fast Gradient Sign Method(FGSM) and Jacobian Slaiency Map attack (JSMA) \cite{gu2014towards,papernot2016distillation,papernot2016towards,xu2017feature}. However, None of them could recover current state of the art accuracy against powerful attacks. One major improvement to the those defenses came when defending model started using adversarial examples while training \cite{tramer2017ensemble,mkadry2017towards}. But this defense has to two major drawbacks. First, as a defender we do not know the attack model. So it is difficult to choose which adversarial attack method should be used to generate adversary for training. Second, it would double training cost. Still most recent defenses use this technique to strengthen their defense. In this work we look to eliminate the need for using adversarial training by processing the input with the objective to make the neural network more robust. Moreover, we observed all these defenses mentioned here loses its strength when the attack strength is increased. So by implementing this proposed defense we look to make the defense more robust against a wide variety of attacks.

Inspired by recent One hot coding and thermometer encoding of input data \cite{anonymous2018thermometer}, in this work, we propose robust processing (RP) of input data. We process the input data using Tanh function, smoothing and batch normalization. These techniques work in combination with discrete data to defend CNN against adversarial attacks. Our proposed method makes the CNN more robust to defend the strongest attack, Logit Space Gradient Ascent(LS-PGA) attack. It is a modified version of PGD attack on discrete data. The main contribution of this work is that we could achieve very high accuracy on MNIST without adversary training for the first time. If we include adversary training, our defense methodology provides the best accuracy against MNIST, Cifar 10 and SVHN till date. Not only that with increasing attack strength when all the defenses completely fails, robust processing still defends the CNN causing minimal accuracy loss.  Contributions of this work are summarized as follows:

\begin{itemize}
    \item We derived from the previous analysis of adversarial examples that introducing non linearity would increase the defense to any adversarial attack. As a result, We introduced a series of functions performing non linear transformation of input data to build a reobust defense.
    \item Tanh, maximum smoothing and batch normalization are the key component of robust processing. We choose a different combination of functions for different training set up and data sets.
    \item This proposed defense resulted in increasing the accuracy for MNIST data set from 94.02\% to ~98.66\% with adversarial training and from 0\% to 98.29\% without adversarial training. This is the best so far achieved on LS-PGA attack without adversarial examples. Even with adversarial training, previous attempts could only reach as far as ~95\%. 
    \item We showed that with increasing attack strength our defense remains one of the strongest. Our analysis provides a more detailed understanding of why robust processing stands out in the advent of adversary attack while others fail.
    \item Additionally by using Adversary training we could achieve 95.06\% accuracy on SVHN data set and 86.66\% accuracy on Cifar 10 data set.
    
\end{itemize}

In the next two sections we summarize the attack models and traditional techniques available to defend those attack models. In section IV our proposed defense technique is described and the following two sections represents the results and analysis of the proposed RP method.

\section{Attacking Methods}
In this work, we assume that the adversary has complete access to the network which is popularly known as the white box attack. It was observed from previous works that if a defense model performs well for white box attack it should naturally perform better against black box attack \cite{mkadry2017towards}. Since black box attack becomes really weak due to the unavailability of network parameters. That is why in general any defense that wants to prove its effectiveness must perform well against white box attack. In order to formulate the attack method for this work, we first investigate some of the popular white box attack models:
\begin{enumerate}
\item L-BFGS: Szegedy et al. \cite{szegedy2013intriguing} first introduced this algorithm in adversarial generation. It is a simple attack model that changes pixel values in test image to intentionally cause the classifier to classify it to a wrong class by minimizing a loss function.
\begin{gather*}
minimize ||x-x'||^2 \\ such\ that\ C(x')= l
\end{gather*}
Loss function was minimized to force the classifier to classify to targeted class by changing the input from x to x'. However, Some of the early defenses successfully defends against this attack model.

   
\item Fast Gradient Sign Method (FGSM): FGSM computes the sign of the cost functions gradient with respect to target input. Then it just shifts the values of the input pixel by a small amount in the direction of the sign \cite{goodfellow2014explaining}. 
\begin{gather*}{x'=x - \epsilon * \nabla loss_\textup{t}(x)}
\end{gather*}
where t is the target class. An improved version of this method was introduced by dividing the step into multiple $\alpha$ step.
\begin{gather*}{x'=x - clip_\textup{$\epsilon$} (\alpha * \nabla loss_\textup{t}(x))}
\end{gather*}
This iterative generation method achieves higher success than original FGSM method \cite{kurakin2016adversarial}.

\item Jacobian Saliency Map Attack (JSMA): In this method, the gradients are computed to create a saliency map. This map basically shows how changing each pixel would impact the resulted miss classification.  A larger value in this map indicates that changing that pixel is likely to cause the targeted false classification. So from the saliency map the algorithm picks the combination of pixels whose alteration is likely cause the targeted classification. This process of changing of input pixel with highest saliency score repeated until it succeeds or reaches a threshold \cite{papernot2016limitations}. It is a greedy algorithm and quite successful one against weak defenses.

\item Carlini and Wagner Attack: Carlini and Wagner recently developed a strong attacking algorithm by changing the cost function. This method is the first attacking algorithm to beat defensive distillation \cite{carlini2016defensive}. They modified the FGSM attack model while keeping the two principle fixed. First it needs an objective function whose loss should be minimized to generate the adversary. Secondly, at the same time keeping the distance between the adversary and original image small. These two principles have also been followed in other models such as JSMA. However, they modified the optimization function in a way so that it becomes easier to solve. The first change is using the logits-based objective function instead of the soft max-cross-entropy loss function. The second change is to convert the target to a argtanh space in order to take advantage of modern optimization algorithm such as Adam. This two modifications result in a very strong attack model which achieved a fair amount of success against defensive distillation.


\item Projected Gradient Descent(PGD):Madry et al. introduced Projected gradient descent which achieved 100\% success in fooling CNN to miss classify MNIST dataset  \cite{mkadry2017towards}. In their work they show that PGD will generate universal adversary among the first order approaches. They suggested that training network with this kind of adversary would make the neural network more robust. Taking their work forward this algorithm was modified for discrete input \cite{anonymous2018thermometer} known as discrete gradient ascent and logit spcae projected gradient ascent.
\end{enumerate}\begin{algorithm}
   \caption{LS-PGA Attack}
    \begin{algorithmic}
        \State Input: Image(x),Label(l), attack steps(n), $\epsilon,\delta$,k(quantized level),Loss function L(w,f(x),l)
        \State mask = 0
        \State $low =max[0,x-\epsilon]$
        \state $high=min[0,x+\epsilon]$
        \For{$i = 1$ to ${k+1}$}
            \State $mask=mask+f_{sdi}(\alpha*low+(1-\alpha*high)$
        \EndFor \newline
        \state $u^0\ Initialization\ using\ mask$ \newline
        \state $T=1$ \newline
        \state $z^0=F(\sigma(u^0/T))$ \newline
        \For{$j = 1$ to ${n}$}
        \newline
             $grad=\nabla_{z^{j-1}}.L(w,z^{j-1},y)$ \newline
            \state $u^j=u^{j-1}+\epsilon.grad$
            \newline
            $z^{j}=\sigma(z^{j-1}/T)$ \newline
            $T=T.\delta$
        \EndFor
        \newline
        Output:\z \after\n\iteration 
    \end{algorithmic}
\end{algorithm}

Since this method is more  suitable for discrete inputs we choose to evaluate our model against this attack. Additionally the success of this white box attack tempted us to evaluate our defense model against it. Finally, based on the success of these attacks against varying defenses we summarize their strength in table 1.
\begin{table}[]
\centering
\caption{Classification of the strength of the attacks}
\label{my-label}
\begin{tabular}{|l|l|l|l|l|}
\hline
Attack                                                                      & FGSM & JSMA & C \& W                                                                           & PGD                                                                                                                                    \\ \hline
\begin{tabular}[c]{@{}l@{}}List of \\ defense\\ it can \\ beat\end{tabular} & x    & x    & \begin{tabular}[c]{@{}l@{}}1.Distillation\\ 2. Feature \\ Squeezing\end{tabular} & \begin{tabular}[c]{@{}l@{}}1.Distillation\\ 2. Feature \\ squeezing\\ 3. PGD \\ training\\ 4.Thermometer \\ (Without adv)\end{tabular} \\ \hline
Strength                                                                    & Weak & Weak & Medium                                                                           & Strong                                                                                                                                 \\ \hline
\end{tabular}
\end{table}

The vast majority of machine learning problem is solved with first order  methods like gradient descent or other closely modified versions. That is why those attacks that rely on only first order information could be labeled as universal attack. Moreover, LS-PGA/PGD attack model depends only on the first order information as well. As a result defending against LS-PGA attack would certainly make the model robust against a wide variety of attack models. This attack takes place by placing each pixel in a random bucket within $\epsilon$.
At each step of the attack it will look at that bucket to find values within $\epsilon$ of the true value and select value that will do the most harm. The outcome of this attack will vary depending on the initialization. So the attack needs to be run several times to get the desired result. In case of Logit space projected gradient ascent first discrete encoding was relaxed to continuous space to perform projected gradient ascent. By changing the values of $\epsilon$  and attack step we can certainly change the strength of the attack. We report the performance of our defense against varying attack strength in section VI.

\section{Defense Techniques}
In this section, we will discuss state-of-the-art defense methodologies and their performance.
\begin{enumerate}
\item Defensive Distillation: Gradient masking is a naive defensive method intentionally forcing the gradient to become zero in response to adversary \cite{gu2014towards}. This technique however sacrifices accuracy on clean data set. Even though this strategy got a breakthrough when papernot et. al. introduced defensive distillation \cite{papernot2016distillation}. Defensive distillation uses soft labels in place of hard labels and tries to hide gradient from adversary. After training the model, it replaces hard labels in the soft max function as usual in place of soft labels. This defense got great success against JSMA and FGSM method. But Carlini and Wagner \cite{carlini2016defensive} in their attack actually breaks this defense and concludes hiding the gradient would not be an ideal solution to this problem. 
\item Feature squeezing: Input transformation is a technique that have been investigated previously. PCA (principle component analysis) was conducted on the input data \cite{bhagoji2017dimensionality} to reduce the dimensionality. However this resulted in decreasing the sensitivity of the network causing low inference accuracy on clean data. In feature squeezing they used a median filter and a low bit depth for the input feature to reduce the space for adversary generation. They successfully defended Carlini Wagner attack and also achieved good accuracy on PGD attack. This algorithm trains the network on 3 different model. One original input, one with low bit width and one with smoothing filter.If the resulted output of these three network differ more than a threshold value than the network detects it as an adversary and refuse to classify it \cite{xu2017feature}.
\item Adversarial Training: Adversarial examples could be defended by mixing adversarial examples with the clean data \cite{szegedy2013intriguing}. It basically regularizes the network and makes it more robust to adversarial inputs. However, the method with which the adversarial will be generated during the training has to match the attack model. Later madry et al. developed an universal adversary among first order approaches \cite{mkadry2017towards}. Thus training a model with PGD generated adversary would regularize a model far more than any other attack model. It achieved great success to defend MNIST on clean data recovering close to 95\% accuracy level. Another problem of training with mixed data set is that the training overhead would be doubled.
\item Thermometer Encoding: Thermometer encoding is a novel method to defend adversary \cite{anonymous2018thermometer}. It uses LS-PGA generated adversarial training to make their defense even more robust. They quantized the input bits to 15 discrete levels. These bits are represented as one hot coded value. Finally the data is passed through a thermometer encoder to make a very strong defense which could take the MNIST accuracy close to 95 \% if supplemented by adversarial training. However, without adversarial training this defense fails completely for MNIST data set. But this is the first method that successfully defends against PGD attack. They modified the attack to their own discrete version as LS-PGA/DGA.
\end{enumerate}

Above description clearly depicts that all of these defenses require adversarial training to defend against strong attack models such as LS-PGA/PGD. As a result, we propose robust processing of input data which may potentially eliminate the need of adversarial training. In this work, we will evaluate our defense method's performance compared with two of the most successfull defenses (i.e. PGD training and thermometer encoding) against LS-PGA attack. All these defenses actually failed to bring the accuracy of MNIST back to 99\%. One of the main objective of robust processing is to bring back the state of the art accuracy of MNIST against LS-PGA attack that too without adversary training. Additionally, we analyze our model's robustness by varying attack strength. We show that our defense remains more robust than the others against increasing attack strength.

\section{Proposed Defense Method}
In the analysis of existence of adversary in deep neural network, goodfellow et al. \cite{goodfellow2014explaining} concluded that deep neural networks exhibit vulnerability to adversarial examples due to their extreme linearity. Linearity of these models is the reason why they can not resist adversary. For example MNIST data set has an input precision of 8 bit. As a result, a common notion would be why a network would respond differently when the input is x+ $\epsilon$ instead of x where $\epsilon$ is really small. However, as practical experiments show that they do behave differently to these two inputs. To understand this we have to look into the linearity theory of deep models. Output of neural network y could be represented as y= $\sigma(W^TX')$.

\begin{gather*}{W^TX'=W^TX+W^T\epsilon}
\end{gather*}

Even if $\epsilon$ is really small, if a weight matrix has n dimension and an average value of m, then this perturbation would result in $\epsilon$*m*n increase of activation. Thus with increased dimension, this noise keeps increasing linearly. It suggests that, with sufficient large input dimension, a network will always be vulnerable to adversary. It would then provide two possible directions to solve this issue. One is to introduce non-linearity to the network and the other is to eliminate unnecessary information from input data. First one is a bit tricky as introducing non-linearity inside the neural network creates problem for calculating gradient. Previously, Different non-linear activation functions were investigated but none of them worked due to the difficulty of computing gradients and poor accuracy on real test data set. So in this work, we choose the second direction by adding a pre-processing layer in front of neural network model which will not have any gradient issue. Moreover, adding pre-processing layer in front of the CNN model does not hamper the accuracy on clean data. 

Our proposed robust processing (RP) has tanh, smoothing and Batch Normalization (BN) to process the input data in an attempt to make the CNN more robust. As shown in fig. 2 The input data goes through a non linear transformation first which is tanh function. A maximum smoothing filter with a with a window of 3 by 3 performs the smoothing after non-linear transformation. A batch normalization layer is also added which is similar to the BN layer in CNN but it does not have any learnable parameters. After robust processing input data were quantized to 15 different levels. They were presented in vectorized form of one hot coded value. Finally, they were encoded using thermometer encoding which basically inverts all the digits after first one appears. We observe that adding robust processing before qunatization made the model more robust. So during all our experiment we did quantize the input data after robust processing. In our experiment it was found that adversarial training was no longer necessary for MNIST. However, for Cifar 10 and SVHN this model needed a bit more modification and also adversarial training.

\begin{figure}[h]
\centering
\scalebox{1}{
\includegraphics[width=8cm]{images/block.pdf}
}
\caption{Block diagram of robust processing with CNN model}
\end{figure}

Our proposed robust processing layer consists of tanh, maximum smoothing and batch normalization layer. We put the defense method sequentially in figure 2. Where the data goes through a three layer of robust processing. Inputs are quantized before feeding into the original CNN model for training. The description of the functionality of those layers are presented here:
\begin{enumerate}
    \item Tanh and Sigmoid Function: The most popular activation functions are Sigmoid and Tanh functions. Both functions are used in typical deep neural networks. Mathematically they can be expressed as:
    \begin{equation}
    F_{tanh}(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}} 
    \end{equation}
    \begin{equation} F_{sigmoid}(x)=\frac{1}{1+e^{-x}} 
    \end{equation}

Again during experiments it was observed that tanh outperforms sigmoid function. So we choose tanh to do the non linear transformation in the first layer.
    
    \item Smoothing: It is one of the popular tools used in image processing for different applications. In this work, we use a 3 by 3 filter. It slides across the input image with a 3 by 3 window with stride of 3 and replaces each pixel by the maximum of those 9 values in that particular window. We also tested with other type of filter that will use the average of the 9 values and replace each pixels with the corresponding average value. We found maximum smoothing performing better than the average smoothing for our work. So all the experiments were performed using maximum smoothing only.
    \item Batch Normalization (BN) function: Our normalization layer is similar to the batch normalization layer in a typical Convolutional Neural Network. However, it does not have any learnable parameters. Rather, this is just a pre-processing layer in front of the original neural network model.The function can be written as this: 
    \begin{equation}
\small Y=\frac{X-\mu}{{\sigma}}
\end{equation}
where $Y$ and $X$ denote the input and output tensor respectively. $\sigma$, $\mu$ represents the standard deviation and mean of the input data for that particular batch. This is a linear layer which basically makes the mean of the input data to be zero and standard deviation to one.
\end{enumerate}\begin{figure}[h]
\centering
\scalebox{1}{
\includegraphics[width=8cm]{images/cal1.pdf}
}
\caption{Effect of pr-eprocessing layer shown on a 2 by 4 input image}
\end{figure}

The effects of three pre-processing layers are illustrated in fig. 3 using a 2 by 4 image example. We used tanh in place of sigmoid and also maximum smoothing in place of average smoothing. As these set of combinations were found to perform better than the other. Each input image is passed through a tanh layer which works as a filter. Followed by a 3 by 3 filter of Maximum smoothing. Then the data goes through a Batch Normalization layer followed by a 15 level quantization. Finally encoded values were feed into the model for training.

\section{Experimental Results}\subsection{Performance in MNIST Data set}

Our proposed defense method was first tested on a simple LeNet architecture for MNIST data set.MNISt is a set of hand written digit with 55000 training data and 10000 testing data. We used LS-PGA attack model to evaluate our model as described in section 2. In LS-PGA, we set $\epsilon$ =0.3, $\delta=1.2$, xi=1 and step-of-attack = 7 for the following experiments. Here, $\epsilon$ indicates how much the input pixels can be changes and $\delta$ is the attack step as described in section II.  In the later section, we also varied such parameters to see the effects on the performance of our proposed defense method.

Our defense model uses various processing layers to defend against adversary. We here investigate the efficacy of individual and combination of different pre-processing layers. Tanh function was chosen to be the first layer of  robust processing. It was found that the performance is better if we put tanh function before batch normalization layer. 
\begin{table}[h]
\centering
\caption{Result of Sigmoid and Tanh Functions}
\label{coding}
\begin{tabular}{@{}cccc@{}}
\toprule
\textbf{Function}  & \textbf{Clean training} & \textbf{ADV training} \\ \midrule
\textbf{Sigmoid} & \textbf{0} & \textbf{8} \\ \midrule
\textbf{tanh} & \textbf{69.55} & \textbf{94.42} \\ 
\bottomrule
\textbf{Thermometer Encoding (TE)} & \textbf{0 } & \textbf{94.02} \\ 
\bottomrule
\end{tabular}
\end{table}

Table 2 indicates that the performance of tanh was far better than sigmoid as a filtering function. As a result, for our experiment we choose tanh instead of sigmoid. Meanwhile, thermometer encoding fails completely when adversarial training was removed. Whereas tanh function improved the accuracy even without adversary training. After tanh, we add a smoothing layer to investigate the effect of this layer. Based on the experiment, we observe maximum smoothing performs better than average smoothing. 

\begin{figure}[h]
\centering
\scalebox{0.8}{
\begin{tikzpicture}
\begin{axis}[
    title={},
    xlabel={Batch Size },
    ylabel={accuracy},
    xmin=0, xmax=200,
    ymin=97, ymax=100,
    xtick={0,20,40,60,80,100,120,140,160,180,200},
    ytick={97,97.5,98,98.5,99,99.5,100},
    legend pos=north west,
    ymajorgrids=true,
    grid style=dashed,
]
 
\addplot[
    color=blue,
    mark=square,
    ]
    coordinates {
    (10,97.95)(50,98.19)(100,98.11)(150,98.05)(200,98.07)
    };
\addplot[
    color=red,
    mark=square,
    ]
    coordinates {
    (10,98.35)(50,98.51)(100,98.54)(150,98.64)(200,98.60)
    };
    \legend{Without Adversary Training,With Adversary Training}
\end{axis}
\end{tikzpicture}
}
\caption{Effect of changing batch size on accuracy both with and without adversary training}
\end{figure}

Finally we do the Normalization of the data after preforming the smoothing. One of the key function here is the batch normalization layer. As a result, the batch size becomes an important tuning parameter for this defense to optimize. We also investigated the defense efficacy using varying batch size on robust processing. Increasing the batch size would increase the accuracy of the overall data set. Because if the batch size is one, batch normalization is the same as normalizing the whole image. In experiments, we found that there is an optimal batch size to achieve highest accuracy without adversary training as shown in figure 4. When adversarial training is included, in general, the accuracy increases when increasing batch size. For the case of no adversary training, we found that an optimum batch size of 50, which leads to the best accuracy. However, looking at the clean training we decided to choose an optimum batch size of 100 for our experiments. Because in figure 4 the red line goes up a little bit while blue line goes down from 50 to 100 batch size. It means we could optimize the accuracy between with adversary training and without adversary training using a batch size of 100 in place of 50.

\begin{table}[]
\centering
\caption{Comparison of different combination of robust processing and other works}
\label{my-label}
\begin{tabular}{|l|l|l|l|l|}
\hline
 & \multicolumn{2}{l|}{\begin{tabular}[c]{@{}l@{}}Clean\\ Training\end{tabular}} & \multicolumn{2}{l|}{\begin{tabular}[c]{@{}l@{}}Adv.\\ Training\end{tabular}} \\ \hline
\textit{\textbf{Defense}} & \multicolumn{1}{r|}{\textit{\textbf{\begin{tabular}[c]{@{}r@{}}clean\\ data\end{tabular}}}} & \multicolumn{1}{r|}{\textit{\textbf{\begin{tabular}[c]{@{}r@{}}attack\\ data\end{tabular}}}} & \multicolumn{1}{r|}{\textit{\textbf{\begin{tabular}[c]{@{}r@{}}clean\\ data\end{tabular}}}} & \multicolumn{1}{r|}{\textit{\textbf{\begin{tabular}[c]{@{}r@{}}attack\\ data\end{tabular}}}} \\ \hline
tanh+BN & 99.2 & 98.3 & 99.43 & 98.65 \\ \hline
tanh+smooth & 99.34 & 0 & 98.16 & 83.54 \\ \hline
smooth+BN & 99.33 & 98.11 & 99.5 & 98.63 \\ \hline
All Three & 99.24 & 98.36 & 99.47 & 98.61 \\ \hline
Thermometer \cite{anonymous2018thermometer} & 99.2 & 0 & 99.03 & 94.02 \\ \hline
Madry et al.\cite{mkadry2017towards} & 99.4 & 0 & 99.2 & 95.7 \\ \hline
\end{tabular}
\end{table}

Table 2 tabulates the accuracy in MNIST with different combination of robust processing layers as discussed above.It gives more insight about their performance against the attack model. The experiment were performed using two different types of training. Clean training  is performed on original training data of MNIST. While adversarial training includes adversarial examples inside the training data. Both training result was evaluated using clean data and attack data. Clean data test basically tells how well the network performs the classification task when there is no attack. One of the key point here is to find which combination works as a better defense method, while causing clean data accuracy to go down marginally. However, by including adversarial training this loss of accuracy could be recovered. This is quite obvious as adversarial training increases the overall robustness of the network. Batch normalization turns out to be the most powerful tool in this defense when we do not include adversarial training. As a result tanh and smoothing filter fails without the support of batch normalization. The best result without adversary training could be obtained is 98.36 \% which is one of the major contribution of this defense. Previously, both thermometer encoding and Madry et al. found that, under PGD attack, CNN accuracy drops to zero in MNIST data set without adverarial training. Base on the experimental results, it could be concluded that adverarial training is not necessary using our proposed defense method for MNIST dataset, which reduces training overhead and keeps the accuracy on the clean data to 99.24 \%.

Additionally if adversarial training is included in our proposed method, the accuracy could further be improved. In this case, the combination of tanh function and batch normalization provides the best performance up to 98.65 \% result. Smoothing only helps to defend the attack when adversary training is not included. Next, we will conduct similar analysis in another two famous dataset, i.e. Cifar 10 and SVHN. 

\subsection{Performance in SVHN dataset}
The Street View House Numbers (SVHN) Dataset is a real world image for testing machine learning algorithms. For SVHN data set, we used Resnet-18 architecture \cite{he2016deep} in experiment and achieved state-of-the-art accuracy. Additionally we changed the defense structure for SVHN slightly. We removed the batch normalization layer from the defense model as we found out that adding that normalization layer for larger data set causes the accuracy to degrade on clean data. Instead we only used tanh function for robust processing on SVHN dataset. For this data set we used $\epsilon$ =0.047, $\delta=1.2$ xi=1 and step of attack = 10 as the the attack model parameter. We choose different attack model parameter for different data sets as reported by thermometer encoding defense. \cite{anonymous2018thermometer} just to make the comparison fair.

\begin{table}[h]
\centering
\caption{Result in SVHN dataset}
\label{coding}
\begin{tabular}{@{}cccc@{}}
\toprule
\textbf{Defense method}  & \textbf{Without ADV. training} & \textbf{ADV training} \\ \midrule
\textbf{TE \cite{anonymous2018thermometer}} & \textbf{48.02} & \textbf{95.02} \\ \midrule
\textbf{Proposed (RP)} & \textbf{49.39} & \textbf{95.06} \\ 
\bottomrule
\end{tabular}
\end{table}

From table-4, it can be seen that our proposed defense method combined with adversarial training could achieve 95.06 \% accuracy. But adversarial training had to be used since we removed BN for SVHN data to encounter for the loss of accuracy on clean data. Batch normalization layer worked as the key component in the defense of MNIST without adversary training. However, after including the adversary training and robust processing we could recover state of the art accuracy on SVHN data set as well.

\subsection{Performance in Cifar 10 dataset}

For Cifar 10 data set, we used Resnet-50 architecture with a little bit of modification on the attack strength. Even though thermometer encoding reported that using wider network helps in increasing the accuracy against adversarial examples and wide resnet is found to perform really well in defending adversary. However, in this paper we wanted to show the performance of functional filter in improving the accuracy from the previous defense model and we leave the analysis on the choice of architecture for future investigation. We choose the attack parameter for cifar 10 with $\epsilon$ equal to 0.031 and number of step of attack equal to 7.

\begin{table}[h]
\centering
\caption{Result on CIFAR 10 datasets using Resnet 50}
\label{coding}
\begin{tabular}{@{}cccc@{}}
\toprule
\textbf{Defense method}  & \textbf{Without ADV training} & \textbf{ADV training}  \\ \midrule
\textbf{TE \cite{anonymous2018thermometer}} & \textbf{43.50} & \textbf{83.60} \\ \midrule
\textbf{Proposed (RP)} & \textbf{46.14} & \textbf{86.61} \\ 
\bottomrule
\end{tabular}
\end{table}

Similar to SVHN we found good result using tanh as a functional filter for cifar 10 data. Additionally, it is observed that PGD training and thermometer encoding produced a lower accuracy even without adversarial training. We could improve the accuracy without adversary training by 3\% while maintaining close to 91\% accuracy on clean data. After using adversarial training we could get close to 86.66\% accuracy on CIFAR 10. So regardless of data set, our proposed defense method helps in improving accuracy without adversary training. However, to achieve better accuracy, we need to include adversarial training. Many other recent developments such as using a wide resnet architecture could also be used in future to make the defense even more powerful.

\section{Analysis}

Based on the experimental results discussed in previous section, it shows that Robust Processing can defend only MNIST data set without adversarial training. Function filtering can also defend against SVHN and CIFAR10 with adversarial training. We thus claim that the need for adversarial training for MNIST data set was eliminated. In this section we present some more depth analysis on MNIST data set to establish our claim.


In order to demonstrate the strength of our defense we need to see how far our defense can go in terms of defending in the advent of very strong attacks \cite{mkadry2017towards,anonymous2018thermometer}. By changing the value of $\epsilon$ the strength of the attack can be modified. A larger value of $\epsilon$ indicates stronger attack. It was observed that the performance of previous defenses decreases exponentially with increasing attack strength as shown in table 5. However, FF defense strength decreases only a small amount with increasing attack strength is another proof of the success of FF as a universal robust defense.

\begin{table}[h]
\centering
\caption{Result of defense after increasing the attack strength-$\epsilon$ on MNIST}

\begin{tabular}{ |c|c|c|c| } 
 \hline
 Value of $\epsilon$ & FF WithoutAdv & FF WithAdv & Thermo WithAdv \\ 
 0.1 & 98.99 & 99.07 & 98.52 \\
 0.2 & 98.68 & 98.88 & 96.85 \\ 
 0.3 & 98.36 & 98.61 & 94.02 \\
 0.4 & 97.78 & 98.22 & 82.64 \\ 
 0.5 & 95.52 & 97.88 & 38.08 \\
 \hline
\end{tabular}
\end{table}

As described earlier changing the value of $\epsilon$ would change the accuracy a lot since it decides how much the input image would be changed to cause the miss classifications. Our defense can withstand this power full attack even if the value of $\epsilon$ is increased to 0.5. No previous defense method, even under weaker attacks like FGSM, can withstand such large level of change against MNIST data set. What makes it more interesting is that this result can be achieved even without adversarial training. If this defense model combined with adversarial training then this would become the strongest defense ever reported against MNIST data set. On the other hand the accuracy thermometer encoding drops down to 38.08\% when $\epsilon$=0.5, even using adversarial training. 

\begin{table}[h]
\centering
\caption{Value of $\alpha$ on Different combination}
\begin{tabular}{ |c|c|c| } 
 \hline
 Defense & $\alpha$ & accuracy \\ 
 Tanh+BN & 49.8 & 98.65 \\ 
 Tanh+ Filter & 45.98 & 83.54 \\
 Filter+BN & 49.78 & 98.63 \\
 All three & 49.78 & 98.61 \\
 \hline
\end{tabular}
\end{table}

We also found that overall strength of the defense depends on the accuracy of clean data as well. In order to analyze it, we first define an important parameter called attacked data accuracy ratio \textit{($\alpha$)}. If we assume the accuracy on clean data is x\% and on attacked data is y\% then,
\begin{equation}
    \alpha= \frac{y}{(x+y)}*100\%
\end{equation}

We observed that defense combination that performed well in table 3 and 4 do have a higher value of $\alpha$. Lets look back at the same combination of table 3 for using adversarial training:


The higher the accuracy, the higher the value of $\alpha$ in table 7 . It is also the same for svhn and cifar 10. The significance of increasing $\alpha$ is that, with increasing performance of the defense, the clean data accuracy actually goes down a little bit. Better defense performance usually with a little sacrifice of clean data accuracy. Since BN performed the best for MNIST it also maintained a greater value of $\alpha$ indicating it will hamper the clean data accuracy most. As MNIST is a small dataset the effect was negligible. However, for SVHN and CIFAR 10, this effect on accuracy degradation on clean data was severe. That's why we choose to drop BN from the model. As we found tanh optimizes between clean data accuracy and attacked data accuracy best for those two.

\begin{figure}[h]
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{images/n1.pdf}
    \caption{Without robust processing}
    \label{fig:1}
  \end{subfigure}
  %
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{images/n3.pdf}
    \caption{With robust processing}
    \label{fig:2}
  \end{subfigure}
  \caption{Image Pixel Distribution showing the frequency of pixel form 0-255 in MNIST datset}
\end{figure}

In order to understand the robustness of our defense more intuitively it is necessary to understand the frequency distribution of dataset pixel. As shown in fig. 5, the whole dataset's pixel distribution completely changes after applying robust processing. We understand that this redistribution in the image pixels plays a role in the robustness of the defense. In the first figure fig. 5(a) a typical MNIST dataset frequency distribution is displayed with pixel values on the x axis and their appearing frequency in the y axis. This distribution has a lot of randomness and spikes which means some pixels do appear many more times than the others. Whereas in fig. 5(b) the pixel distribution is more evenly distributed across all pixel values. Another key point is before applying robust processing the pixel values are clustered around a certain frequency range. After processing their frequency range becomes more scattered. Not only that standard deviation between the pixels before and after processing are 78 and 252 respectively. Our robust processing does increase the pixel frequency range of the pixel and also increases the standard deviation of the data. As a result the pixel difference between the edges of the digit or object becomes larger. Thus fooling the network becomes harder as introducing noises will have less impact on the images due to larger transition near the edges.

\section{Summary}
We presented a robust defense method that uses a combination of pre-processing layers to defend against adversarial attacks. We choose tanh,smoothing and batch normalization as our robust processing layer. We could defend MNIST data set for the first time without using adversarial training. That reduces training time and complexity in the defense model. A combination of tanh, max smoothing and batch normalization worked in improving MNIST accuracy to close to 98\% from zero without adversarial examples. Moreoever, we showed that increasing attack defense where other defenses fail, our defense remains one of the strongest ever reported. Additionally the proposed RP model worked better than recent defenses against SVHN and CIFAR 10 as well. But, it required adversarial training to recover the accuracy upto 95\% for SVHN and 86\% for Cifar 10. We also showed under varying attack strength our defense would outperform its recent counter part. However, analysis of model architecture and black box attack on the performance on RP remains something for future investigation.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,./reference}



\end{document}


