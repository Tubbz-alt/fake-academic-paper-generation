\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{rotating}
\usepackage{multirow}
\newcommand{\ignore}[1]{}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

% \def\cvprPaperID{5631} % *** Enter the CVPR Paper ID here
% \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Towards Hiding Adversarial Examples from Network Interpretation}

\author{
     Akshayvarun Subramanya\footnotemark[1] \qquad
     Vipin Pillai\thanks{Equal contribution} \qquad Hamed Pirsiavash\\
    University of Maryland, Baltimore County (UMBC)\\
    \tt\small{\{akshayv1, vp7,hpirsiav\}@umbc.edu} \\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
}


\maketitle

\begin{abstract}
% Deep networks have been shown to be fooled rather easily using adversarial attack algorithms. However, there have been works which show that interpretation algorithms for deep networks can still highlight relevant locations of the image even when corrupted. We show that  network interpretation tools like Grad-CAM can highlight the adversarial patch as the cause of the change in prediction. To avoid this, we develop adversarial patch algorithms that not only change the prediction, but also hide themselves from network interpretation tools. We believe our algorithms can facilitate developing more robust network interpretation tools that truly explain the network's underlying decision making process as well as designing more robust deep learning tools that reveal the correct cause of their predictions.

Deep networks have been shown to be fooled rather easily using adversarial attack algorithms.
% However, there have been works which show that interpretation algorithms for deep networks can still highlight relevant locations of the image even when corrupted.
Practical methods such as adversarial patches have been shown to be extremely effective in causing misclassification. However, these patches can be highlighted using standard network interpretation algorithms, thus revealing the identity of the adversary.
We show that it is possible to create adversarial patches which not only fool the prediction, but also change what we interpret regarding the cause of prediction. We show that our algorithms can empower adversarial patches, by hiding them from network interpretation tools. We believe our algorithms can facilitate developing more robust network interpretation tools that truly explain the network's underlying decision making process.

% We know that adversarial attacks can fool the deep networks to produce incorrect predictions. We take this to another level and introduce adversarial attack algorithms that not only fool the network's prediction, but also fool our interpretation of the cause of the network's decision.  %Aks

%
\end{abstract}

\section{Introduction}
Deep learning has achieved great results in many domains including computer vision. However, it is still far from being deployed in many real-world applications due to reasons including:

{\bf (1) Explainable AI (XAI):} Explaining the prediction of deep networks is a challenging task simply because they are complex models with large number of parameters. Recently, XAI has become a trending research area in which the goal is to develop reliable interpretation algorithms that can explain the underlying decision making process. Designing such algorithms is a challenging task and considerable research \cite{simonyan2013deep,zhou2016learning,selvaraju2016grad} has been done to describe \textit{local explanations} - explaining the model's output for a given input \cite{baehrens2010explain}. We will be focusing on such methods in our work. Most of the these algorithms rely on studying the gradient of the output of a machine learning model with respect to its input.


{\bf (2) Adversarial examples:} Many works have shown that deep networks are vulnerable to adversarial examples, which are carefully constructed samples created by adding imperceptible perturbations to the original input to change the final decision of the network. This is important for two reasons: (a) Such vulnerabilities could be used by adversaries to fool AI algorithms when they are deployed in real-world applications such as Internet of Things (IoT) \cite{mosenia2017comprehensive} or self-driving cars \cite{sitawarin2018darts}. (b) Studying these attacks can lead to better understanding of how deep networks work and possibly improve generalization on new environments.

\begin{figure*}[!t]
  \begin{center}
\includegraphics[scale = 0.5]{teaser2.pdf}
  \caption{We show that Grad-CAM highlights the patch location in the image perturbed by regular targeted adversarial patches \cite{brown2017adversarial} (top row). Our modified attack algorithm goes beyond fooling the final prediction by hiding the patch in the Grad-CAM visualization, making it difficult to investigate the cause of the mistake. Note that Grad-CAM visualizes the cause of target category.}
%\vspace{-0.1in}
  \end{center}
\label{teaser}
\end{figure*}

We specifically focus on adversarial patches rather than regular adversarial examples since patches are a more practical form of attack, and also the cause of the misclassification is strictly limited to the patch area. Hence, it is not trivial for the attacker to mislead the interpretation to highlight non-patch regions without perturbing them.
% We specifically focus on adversarial patches because it is a more practical form of attack and the interpretation methods are easier to understand since the only region being corrupted is the patch area. In the case of regular adversarial examples, interpretation can be misled easily if there is sufficient perturbation in the background which highlights the deficiencies of the network itself rather than the interpretation method.

%Even though the prediction of the network might change, ideally we do not want the network's understanding of semantic categories to change. In \cite{selvaraju2016grad}, it was shown that an adversarial example created using standard methods, which misleads the classifier still retains the interpretation for the original category. However, we show that it is possible to change the network's interpretation of the original category for an adversarial input. An example of this can be found in Fig \ref{figPGD_1}.
% Adversarial examples created using well-known methods have been shown to have no effect on interpretation algorithms (Please refer section 6.2 in \cite{selvaraju2016grad}).%Aks


Consider an example of adversarial attack using adversarial patches as seen in \cite{brown2017adversarial}, we show that an interpretation algorithm like Grad-CAM \cite{selvaraju2016grad} usually highlights the location of such an adversarial patch making it clear that the image patch was responsible for misclassification. We are interested in designing stronger attack algorithms that not only change the prediction but also mislead the interpretation of the model to hide the attack from investigation. As an example, assume an adversary (one in a crowd of pedestrians) is wearing a t-shirt with a printed adversarial patch on the back that fools a self-driving car leading to an accident. Now, a simple investigation with standard network interpretation methods may reveal which pedestrian in the scene was the cause of the wrong decision, and thereby identifying the adversary. However, we show that it is possible for the adversary to learn a patch without revealing their identity (patch location) and thus escape scrutiny.
% Note that the noise within the patch is the sole reason for misclassification.

% We introduce a set of novel adversarial algorithms that instead of fooling the network's prediction, fool the network's interpretation achieved by state-of-the-art interpretation algorithms.
We do this by encouraging the optimization to change the interpretation of the network when constructing the corresponding adversarial example. Our work highlights that using a well-designed attack algorithm, an adversary can construct sophisticated adversarial examples which not only change the prediction but also remove any trace of corruption when inspected by network interpretation algorithms.
% which means that the adversary can not only fool the network, but also not leave a trace when the mispredictions are analyzed using network expla
% This is important since it takes adversarial attacks to another level that not only an adversary can fool the network, but also the adversary can hide itself from network explanation algorithms. For example, we show that our algorithms can modify the interpretation without modifying the final prediction. %Aks
% This is an interesting discovery since it shows that commonly used interpretation algorithms are not robust. An adversary can cause misclassification by perturbing a small patch location without the patch being highlighted in the interpretation. Note that the noise within the patch is the sole reason for misclassification.



\section{Related work}{\bf Adversarial examples:}
Adversarial examples were discovered by Szegedy et al. \cite{intriguing-arxiv-2013} who showed that state-of-the-art machine learning classifiers can be fooled comprehensively by simple backpropagation algorithms. Goodfellow et al. \cite{explainingharnessing-arxiv-2014} improved this by Fast Gradient Sign Method (FGSM) that needs only one iteration of optimization. The possibility of extending these examples to the real world was shown in \cite{world-arxiv-2016,sharif2016accessorize} and recently, \cite{athalye2017synthesizing} showed that adversarial examples could be robust to affine transformations. Madry et al. \cite{madry2017towards} proposed Projected Gradient Descent (PGD) which has been shown to be the best first-order adversary for fooling classifiers.
% Although many researchers have attempted to defend against these algorithms, few have been successful in defending against different adversaries.
% The most popular defense method is Adversarial Training \cite{atscale-arxiv-2016} in which we include adversarial examples using a specific adversary as part of our training set which improves robustness, but it has been shown that it is sensitive to the adversary chosen during training.
Kindermans et al. \cite{kindermans2017reliability} showed how saliency methods are unreliable by adding constant shift to input data and checking against different saliency methods. In our work, we show that it is not only possible to fool the classifier using an adversary, but also hide it from standard network interpretation algorithms.
%tremendous scope for improvement in this regard and knowledge of different kinds of adversaries can increase our chances of overcoming this deficiency in state-of-the-art training algorithms.

\begin{figure*}[!t]
  \begin{center}
  \begin{tabular}{| c c c c c|}
\hline
Original & Regular adv. patch & Regular adv. patch & Ours & Ours\\
 & & GCAM &  &GCAM\\
 \hline
\vspace{-.08in}
&&&&\\
\begin{sideways} Target: Bridegroom \end{sideways}
\includegraphics[width=.17\textwidth]{img_orig_ILSVRC2012_val_00008855.JPEG}&
\includegraphics[width=.17\textwidth]{img_adv_patch_ILSVRC2012_val_00008855_orig_693_target_982_pred_982_prob_99.png}&
\includegraphics[width=.17\textwidth]{mask_adv_patch_ILSVRC2012_val_00008855_orig_693_target_982_pred_982.JPEG}&
\includegraphics[width=.17\textwidth]{img_our_patch_ILSVRC2012_val_00008855_orig_693_target_982__pred_982_prob_09.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_ILSVRC2012_val_00008855_orig_693_target_982_pred_982.png}\\
Paddle & Bridegroom & Bridegroom & Bridegroom & Bridegroom \\
\begin{sideways} Target: Pomegranate \end{sideways}
\includegraphics[width=.17\textwidth]{img_orig_ILSVRC2012_val_00000088.JPEG}&
\includegraphics[width=.17\textwidth]{img_adv_patch_ILSVRC2012_val_00000088_orig_058_target_957_pred_957_prob_99.png}&
\includegraphics[width=.17\textwidth]{mask_adv_patch_ILSVRC2012_val_00000088_orig_058_target_957_pred_957.JPEG}&
\includegraphics[width=.17\textwidth]{img_our_patch__ILSVRC2012_val_00000088_orig_058_target_957__pred_957_prob_05.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_ILSVRC2012_val_00000088_orig_058_target_957_pred_957.png}\\
Water snake & Pomegranate & Pomegranate & Pomegranate & Pomegranate \\
\begin{sideways} \quad \quad Target: Fig \end{sideways}
\includegraphics[width=.17\textwidth]{img_orig_ILSVRC2012_val_00003784.JPEG}&
\includegraphics[width=.17\textwidth]{img_reg_patch_ILSVRC2012_val_00003784_orig_902_target_952_pred_952_prob_99.png}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_ILSVRC2012_val_00003784_orig_902_target_952_pred_952.JPEG}&
\includegraphics[width=.17\textwidth]{img_our_patch_ILSVRC2012_val_00003784_orig_902_target_952__pred_952_prob_04.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_ILSVRC2012_val_00003784_orig_902_target_952_pred_952.png}\\
Whistle & Fig & Fig & Fig & Fig \\
\begin{sideways} \quad \quad Target: Tray \end{sideways}
\includegraphics[width=.17\textwidth]{img_orig_ILSVRC2012_val_00006196.JPEG}&
\includegraphics[width=.17\textwidth]{img_reg_patch_ILSVRC2012_val_00006196_orig_474_target_868_pred_868_prob_99.png}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_ILSVRC2012_val_00006196_orig_474_target_868_pred_868.JPEG}&
\includegraphics[width=.17\textwidth]{img_our_patch_ILSVRC2012_val_00006196_orig_474_target_868__pred_868_prob_03.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_ILSVRC2012_val_00006196_orig_474_target_868_pred_868.png}\\
Cardigan  & Tray & Tray & Tray & Tray \\
\hline
\end{tabular}
\vspace{.05in}
  \caption{Comparison of Grad-CAM visualization results for targeted patch attacks  using our method (`Ours') vs regular adversarial patch (`AP'). The predicted label is written under each image, the attack was successful for all images, and Grad-CAM is always computed for the target category. Note that the patch is not highlighted in the right column.}
\label{fig_patch_target1}
  \end{center}
\end{figure*}
{\bf Adversarial patches:}
% .
Adversarial Patches \cite{brown2017adversarial,karmon2018lavan} were introduced recently as a more practical version of adversarial attacks where we restrict the spatial dimensions of the perturbation, but remove the imperceptibility constraint. These patches can be printed and `pasted' on top of an image to mislead classification networks. We improve this by hiding the patch location from network interpretation tools.

{\bf Interpretation of deep networks:}
As neural networks are getting closer towards deployment in real world applications, it is important that their results are interpretable.
% \cite{doshi2017accountability} discusses the legal and societal implications of explainable AI and suggests that although explainable systems might possibly be sub-optimal, it is a necessity that needs to be considered under design.
Researchers have proposed various algorithms in this direction. One of the earliest attempt was done in \cite{simonyan2013deep} where they calculate the derivative of the network's outputs w.r.t the input to compute class specific saliency maps. Zhou et al. \cite{zhou2014object} calculates the change in the network output when a small portion of the image (say $11\times11$ pixels) is covered by a random occluder. We call this \textbf{Occluding Patch}. CAM \cite{zhou2016learning} used weighted average map for each image based on their activations. The most popular one that we consider in this paper is called \textbf{Grad-CAM} \cite{selvaraju2016grad}, a gradient based method which provides visual explanations for any neural network architecture. Kunpeng \etal \cite{kunpeng2018gain} recently improved upon Grad-CAM using Guided attention mechanism with state-of-the-art results on segmentation tasks. Although the above methods have shown great improvement in explaining the network's decision, our work highlights that it is important to ensure that they are robust enough to adversaries as well.

% \begin{figure*}[t!]
%   \begin{center}
% \includegraphics[scale = 0.4]{teaser.pdf}
%   \caption{Our algorithm can fool the network to produce a wrong interpretation with Grad-CAM without changing the final prediction by adding a small perturbation to the input image. We optimize our noise image for the Grad-CAM visualization, but show that it can fool other visualization tools like CAM and occluding patch as well.
% %   We believe this happens since most visualization methods use the gradient of the output with respect to the input.
%   Note that ``English Springer'' is a ``dog'' category in ImageNet.}
%   \end{center}
% \label{teaser}
% \end{figure*}

% \begin{figure*}[!t]
%   \begin{center}
%   \begin{tabular}{| c c c c c|}
%   \hline
%   Original & AP & GCAM - AP & Ours & GCAM - Ours\\
% \hline
% \vspace{-.08in}
% &&&&\\
% \includegraphics[width=.17\textwidth]{figures/target_patch/img_orig_ILSVRC2012_val_00005126.JPEG}&
% \includegraphics[width=.17\textwidth]{figures/target_patch/img_reg_patch_ILSVRC2012_val_00005126_orig_245_target_805_pred_805_prob_99.png}&
% \includegraphics[width=.17\textwidth]{figures/target_patch/mask_reg_patch_ILSVRC2012_val_00005126_orig_245_target_805_pred_805.JPEG}&
% \includegraphics[width=.17\textwidth]{figures/target_patch/img_our_patch_ILSVRC2012_val_00005126_orig_245_target_805__pred_805_prob_24.png}&
% \includegraphics[width=.17\textwidth]{figures/target_patch/mask_our_patch_ILSVRC2012_val_00005126_orig_245_target_805_pred_805.png}\\
% French bulldog & Soccer ball & Soccer ball & Soccer ball & Soccer ball \\
% \includegraphics[width=.17\textwidth]{figures/target_patch/img_orig_ILSVRC2012_val_00002809.JPEG}&
% \includegraphics[width=.17\textwidth]{figures/target_patch/img_reg_patch_ILSVRC2012_val_00002809_orig_417_target_205_pred_205_prob_99.png}&
% \includegraphics[width=.17\textwidth]{figures/target_patch/mask_reg_patch_ILSVRC2012_val_00002809_orig_417_target_205_pred_205.JPEG}&
% \includegraphics[width=.17\textwidth]{figures/target_patch/img_our_patch_ILSVRC2012_val_00002809_orig_417_target_205__pred_205_prob_02.png}&
% \includegraphics[width=.17\textwidth]{figures/target_patch/mask_our_patch_ILSVRC2012_val_00002809_orig_417_target_205_pred_205.png}\\
% Balloon & Flat-coated  &Flat-coated  & Flat-coated  & Flat-coated \\
%  & Retriever & Retriever &  Retriever &  Retriever
% \\
% \includegraphics[width=.17\textwidth]{figures/target_patch/img_orig_ILSVRC2012_val_00004840.JPEG}&
% \includegraphics[width=.17\textwidth]{figures/target_patch/img_reg_patch_ILSVRC2012_val_00004840_orig_348_target_078_pred_078_prob_99.png}&
% \includegraphics[width=.17\textwidth]{figures/target_patch/mask_reg_patch_ILSVRC2012_val_00004840_orig_348_target_078_pred_078.JPEG}&
% \includegraphics[width=.17\textwidth]{figures/target_patch/img_our_patch_ILSVRC2012_val_00004840_orig_348_target_078__pred_078_prob_03.png}&
% \includegraphics[width=.17\textwidth]{figures/target_patch/mask_our_patch_ILSVRC2012_val_00004840_orig_348_target_078_pred_078.png}\\
% Ram & Tick & Tick & Tick & Tick \\
% \includegraphics[width=.17\textwidth]{figures/target_patch/img_orig_ILSVRC2012_val_00000791.JPEG}&
% \includegraphics[width=.17\textwidth]{figures/target_patch/img_reg_patch_ILSVRC2012_val_00000791_orig_125_target_411_pred_411_prob_99.png}&
% \includegraphics[width=.17\textwidth]{figures/target_patch/mask_reg_patch_ILSVRC2012_val_00000791_orig_125_target_411_pred_411.JPEG}&
% \includegraphics[width=.17\textwidth]{figures/target_patch/img_our_patch_ILSVRC2012_val_00000791_orig_125_target_411__pred_411_prob_04.png}&
% \includegraphics[width=.17\textwidth]{figures/target_patch/mask_our_patch_ILSVRC2012_val_00000791_orig_125_target_411_pred_411.png}\\
% Hermit crab & Apron & Apron & Apron & Apron \\
% \hline
% \end{tabular}
%   \caption{Additional results similar to Figure \ref{fig_patch_target1}.}
% \label{fig_patch_target2}
%   \end{center}

% \end{figure*}

\begin{figure*}[h]
  \begin{center}
  \begin{tabular}{| c c c c c|}
\hline  Original & Regular adv. patch & Regular adv. patch & Ours & Ours\\
 & & GCAM &  &GCAMs\\
\hline
\vspace{-.08in}
&&&&\\
\begin{sideways} \quad \quad Target: Sofa \end{sideways}
% \rotatebox{90}{\quad \quad Target: Sofa}
\includegraphics[width=.17\textwidth]{img_orig_000038.jpg}&
\includegraphics[width=.17\textwidth]{img_reg_patch_000038_orig_014_target_017_pred_017_prob_100.png}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_000038_orig_014_target_017_pred_017.JPEG}&
\includegraphics[width=.17\textwidth]{img_our_patch_000038_orig_014_target_017__pred_017_prob_86.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_000038_orig_014_target_017_pred_017.png}\\
Person & Sofa & Sofa & Sofa & Sofa \\
\begin{sideways} \quad \quad Target: Chair \end{sideways}
\includegraphics[width=.17\textwidth]{img_orig_000145.jpg}&
\includegraphics[width=.17\textwidth]{img_reg_patch_000145_orig_018_target_008_pred_008_prob_100.png}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_000145_orig_018_target_008_pred_008.JPEG}&
\includegraphics[width=.17\textwidth]{img_our_patch_000145_orig_018_target_008__pred_008_prob_97.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_000145_orig_018_target_008_pred_008.png}\\
Train & Chair & Chair & Chair & Chair \\
\begin{sideways} \quad \quad Target: Boat \end{sideways}
\includegraphics[width=.17\textwidth]{img_orig_000168.jpg}&
\includegraphics[width=.17\textwidth]{img_reg_patch_000168_orig_012_target_003_pred_003_prob_100.png}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_000168_orig_012_target_003_pred_003.JPEG}&
\includegraphics[width=.17\textwidth]{img_our_patch_000168_orig_012_target_003__pred_003_prob_91.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_000168_orig_012_target_003_pred_003.png}\\
Horse & Boat & Boat & Boat & Boat \\
\begin{sideways} \quad Target: Sofa \end{sideways}
\includegraphics[width=.17\textwidth]{img_orig_002007.jpg}&
\includegraphics[width=.17\textwidth]{img_reg_patch_002007_orig_000_target_017_pred_017_prob_100.png}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_002007_orig_000_target_017_pred_017.JPEG}&
\includegraphics[width=.17\textwidth]{img_our_patch_002007_orig_000_target_017__pred_017_prob_99.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_002007_orig_000_target_017_pred_017.png}\\
Aeroplane & Sofa & Sofa & Sofa & Sofa \\
\hline
\end{tabular}
\vspace{.05in}
  \caption{Comparison of Grad-CAM visualization results for targeted patch attacks for the least likely target category using our method (`Ours') vs regular adversarial patch (`Adv Patch') on the GAIN{$_{ext}$} \cite{kunpeng2018gain} network for VOC dataset. The predicted label is written under each image, the attack was successful for all images, and Grad-CAM is always computed for the target category. GAIN{$_{ext}$} is particularly designed to produce better Grad-CAM visualizations using direct supervision of the Grad-CAM output.}
\label{fig_patch_target1_GAIN}
  \end{center}
\end{figure*}


\begin{figure*}[h]
  \begin{center}
  \begin{tabular}{| c c c c c|}
\hline  Original & GCAM & Occluding Patch & GCAM & Occluding Patch \\
& Regular adv. Patch & Regular adv. Patch & Ours & Ours \\
\hline
\vspace{-.08in}
&&&&\\
\begin{sideways} Target: Dining Table \end{sideways}
% \rotatebox{90}{\quad \quad Target: Sofa}
\includegraphics[width=.17\textwidth]{img_orig_002081.jpg}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_002081_orig_019_target_010_pred_010.JPEG}&
\includegraphics[width=.17\textwidth]{occ_reg_patch_adv_img_002081_10.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_002081_orig_019_target_010_pred_010.png}&
\includegraphics[width=.17\textwidth]{occ_our_patch_adv_img_002081_10.png}\\
TV / Monitor & Dining Table & Dining Table & Dining Table & Dining Table \\
\begin{sideways} Target: TV / Monitor \end{sideways}
\includegraphics[width=.17\textwidth]{img_orig_002245.jpg}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_002245_orig_005_target_019_pred_019.JPEG}&
\includegraphics[width=.17\textwidth]{occ_reg_patch_adv_img_002245_19.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_002245_orig_005_target_019_pred_019.png}&
\includegraphics[width=.17\textwidth]{occ_our_patch_adv_img_002245_19.png}\\
Bus & TV / Monitor & TV / Monitor & TV / Monitor & TV / Monitor \\
\begin{sideways} \quad  Target: Bicycle \end{sideways}
\includegraphics[width=.17\textwidth]{img_orig_003019.jpg}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_003019_orig_007_target_001_pred_001.JPEG}&
\includegraphics[width=.17\textwidth]{occ_reg_patch_adv_img_003019_1.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_003019_orig_007_target_001_pred_001.png}&
\includegraphics[width=.17\textwidth]{occ_our_patch_adv_img_003019_1.png}\\
Cat & Bicycle & Bicycle & Bicycle & Bicycle \\
\begin{sideways}  Target: Potted Plant \end{sideways}
\includegraphics[width=.17\textwidth]{img_orig_003069.jpg}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_003069_orig_016_target_015_pred_015.JPEG}&
\includegraphics[width=.17\textwidth]{occ_reg_patch_adv_img_003069_15.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_003069_orig_016_target_015_pred_015.png}&
\includegraphics[width=.17\textwidth]{occ_our_patch_adv_img_003069_15.png}\\
Sheep & Potted Plant & Potted Plant & Potted Plant & Potted Plant \\
\hline
\end{tabular}
\vspace{.05in}
  \caption{Transfer of Grad-CAM visualization attack to Occluding Patch visualization. Here, we use targeted patch attacks for the least likely target category using our method (`Ours') vs regular adversarial patch (`Adv Patch') on the GAIN{$_{ext}$} \cite{kunpeng2018gain} network for VOC dataset. The predicted label is written under each image, the attack was successful for all images, and Grad-CAM and Occluding Patch visualizations are always computed for the target category. Note that the patch is hidden in both visualizations in columns 4 and 5.}
\label{fig_transfer}
  \end{center}

\end{figure*}

\section{Method}
%\vspace{-.1in}
We propose algorithms to learn adversarial patches that when pasted on the input image, can change the interpretation of the model's final decision. We will focus on Grad-CAM \cite{selvaraju2016grad}, which is one of the popular network interpretation methods in designing our algorithms and then, will show that our results generalize to other interpretation algorithms as well.

{\bf Background on Grad-CAM visualization} %\label{attacking_gcam_viz}

Consider a deep network for image classification task, e.g., VGG, and an image $x_0$. We feed the image to the network and get the final output $y$  where $y^c$ is the logit or class-score for the $c$'th  class. To interpret the network's decision for category $c$, we want to generate heatmap $G^c$ for a convolutional layer, e.g, {\em conv5}, which when up-sampled to the size of input image, highlights the regions of the image that have significant effect in producing higher values in $y^c$. We denote $A^k_{ij}$ as the activations of the $k$'th neuron at location $(i,j)$ of the chosen layer. Then, as in \cite{selvaraju2016grad}, we define:
$$\alpha^c_k = \frac{1}{Z}\sum_{i}\sum_{j}{\frac{\partial y^c}{\partial A^k_{ij}}}$$

\noindent where $Z$ is a normalizer and then calculate the heatmap as follows :
$$G_{ij}^c = max(0, \sum_k\alpha^c_kA^k_{ij})$$

Finally, we normalize it to get: $$\hat{G}^c := \frac{G^c}{|G^c|_1}$$.

\begin{figure*}[h]
  \begin{center}
  \begin{tabular}{| c c c c c|}
    \hline
Original & Regular adv. patch & Regular adv. patch & Ours & Ours\\
 & & GCAM &  &GCAM\\
  \hline
\vspace{-.08in}
&&&&\\
\includegraphics[width=.17\textwidth]{img_orig_ILSVRC2012_val_00000075.JPEG}&
\includegraphics[width=.17\textwidth]{img_reg_patch_ILSVRC2012_val_00000075_orig_080_pred_086_prob_74.png}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_ILSVRC2012_val_00000075_orig_080_pred_086.JPEG}&
\includegraphics[width=.17\textwidth]{img_our_patch_ILSVRC2012_val_00000075_orig_080__pred_429_prob_02.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_ILSVRC2012_val_00000075_orig_080_pred_429.png}\\
Black grouse & Partridge & Partridge & Baseball & Baseball \\
\includegraphics[width=.17\textwidth]{img_orig_ILSVRC2012_val_00000681.JPEG}&
\includegraphics[width=.17\textwidth]{img_reg_patch_ILSVRC2012_val_00000681_orig_699_pred_999_prob_63.png}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_ILSVRC2012_val_00000681_orig_699_pred_999.JPEG}&
\includegraphics[width=.17\textwidth]{img_our_patch_ILSVRC2012_val_00000681_orig_699__pred_093_prob_12.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_ILSVRC2012_val_00000681_orig_699_pred_093.png}\\
Panpipe & Bath tissue & Bath tissue & Hornbill & Hornbill \\
\includegraphics[width=.17\textwidth]{img_orig_ILSVRC2012_val_00000903.JPEG}&
\includegraphics[width=.17\textwidth]{img_reg_patch_ILSVRC2012_val_00000903_orig_919_pred_619_prob_66.png}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_ILSVRC2012_val_00000903_orig_919_pred_619.JPEG}&
\includegraphics[width=.17\textwidth]{img_our_patch_ILSVRC2012_val_00000903_orig_919__pred_706_prob_01.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_ILSVRC2012_val_00000903_orig_919_pred_706.png}\\
Street sign & Lampshade & Lampshade & Patio & Patio \\
\includegraphics[width=.17\textwidth]{img_orig_ILSVRC2012_val_00001288.JPEG}&
\includegraphics[width=.17\textwidth]{img_reg_patch_ILSVRC2012_val_00001288_orig_089_pred_326_prob_97.png}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_ILSVRC2012_val_00001288_orig_089_pred_326.JPEG}&
\includegraphics[width=.17\textwidth]{img_our_patch_ILSVRC2012_val_00001288_orig_089__pred_994_prob_02.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_ILSVRC2012_val_00001288_orig_089_pred_994.png}\\
Cockatoo  & Lycaenid & Lycaenid & Stinkhorn & Stinkhorn \\
  \hline

\end{tabular}
\vspace{.05in}
  \caption{Comparison of Grad-CAM visualization results for non-targeted patch attacks using our method (`Ours') vs regular adversarial patch (`AP'). The predicted label is written under each image, the non-targeted attack was successful for all images, and Grad-CAM is always computed for the predicted category.}
\label{fig_patch_nontarget1}
  \end{center}

\end{figure*}
\begin{figure*}[h]
  \begin{center}
  \begin{tabular}{| c c c c c|}
    \hline
Original & Regular adv. patch & Regular adv. patch & Ours & Ours\\
 & & GCAM &  &GCAM\\
  \hline
\vspace{-.08in}
&&&&\\
\includegraphics[width=.17\textwidth]{o_697.jpg}&
\includegraphics[width=.17\textwidth]{re.png}&
\includegraphics[width=.17\textwidth]{rm_000697_orig_014_pred_015.png}&
\includegraphics[width=.17\textwidth]{adv_000697_orig_014_pred_015_prob_99.png}&
\includegraphics[width=.17\textwidth]{am_000697_orig_014_pred_015.png}\\
Person & Pottedplant & Pottedplant & Pottedplant & Pottedplant \\
\includegraphics[width=.17\textwidth]{o_96.jpg}&
\includegraphics[width=.17\textwidth]{reg_000096_orig_014_pred_015_prob_100.png}&
\includegraphics[width=.17\textwidth]{rm_000096_orig_014_pred_015.png}&
\includegraphics[width=.17\textwidth]{adv_000096_orig_014_pred_015_prob_100.png}&
\includegraphics[width=.17\textwidth]{am_000096_orig_014_pred_015.png}\\
Person & Pottedplant & Pottedplant & Pottedplant & Pottedplant \\
\includegraphics[width=.17\textwidth]{o_2406.jpg}&
\includegraphics[width=.17\textwidth]{reg_002406_orig_006_pred_015_prob_100.png}&
\includegraphics[width=.17\textwidth]{rm_002406_orig_006_pred_015.png}&
\includegraphics[width=.17\textwidth]{adv_002406_orig_006_pred_015_prob_83.png}&
\includegraphics[width=.17\textwidth]{am_002406_orig_006_pred_015.png}\\
Car & Pottedplant &Pottedplant & Pottedplant & Pottedplant \\
\includegraphics[width=.17\textwidth]{o_5816.jpg}&
\includegraphics[width=.17\textwidth]{reg_005816_orig_013_pred_015_prob_100.png}&
\includegraphics[width=.17\textwidth]{rm_005816_orig_013_pred_015.png}&
\includegraphics[width=.17\textwidth]{adv_005816_orig_013_pred_015_prob_100.png}&
\includegraphics[width=.17\textwidth]{am_005816_orig_013_pred_015.png}\\
Motorbike  & Pottedplant & Pottedplant & Pottedplant & Pottedplant\\
\hline

\end{tabular}
\vspace{.05in}
  \caption{Comparison of Grad-CAM visualization results for universal targeted patch attacks using our method (`Ours') vs regular adversarial patch (`AP'). The top-1 predicted label is written under each image, the universal attack was successful for all VOC images, and Grad-CAM is always computed for the predicted category. The target category chosen was ``Pottedplant''.}
\label{fig_patch_universal}
  \end{center}

\end{figure*}
{\bf Background on adversarial patches:}

Consider an input image $x_0$ and a predefined constant binary mask $m$ that is $1$ on the location of the patch and $0$ everywhere else. We want to find an adversarial patch $z$ that changes the output of the network to catgegory $t$ when pasted on the image, so we solve:
$$ z = \argmin_{z} \ell_{ce}(x \odot (1-m)+z \odot m; t)$$

\noindent where $\ell_{ce}(.; t)$ is the cross entropy loss for the target category $t$
%and $c=\argmax(y)$ is the original network prediction
and $\odot$ is the element-wise product. This results in adversarial patches similar to \cite{brown2017adversarial}.


% Grad-CAM \cite{selvaraju2016grad} showed that when $G^c$ is resized to the size of input image, it highlights the regions of the input image that are important in classifying it as class $c$.

% To simplify the notation, we define the normalized $G$ as:
% $$\hat{G}^c(x_0+\delta x) := \frac{G^c(x_0+\delta x)}{|G^c(x_0+\delta x) |_1}$$ %Aks

%\subsection{Misleading interpretation of adversarial patches} \label{improv_adv_patches}
% Given an image of a ``dog'' in Fig \ref{figPGD}.a, Grad-CAM of the output corresponding to ``dog'' (Fig. \ref{figPGD}.d) visualizes what regions of the image contributed to the dog category. We can apply a standard adversarial attack algorithm, say PGD \cite{madry2017towards}, on this image to reduce the probability of ``dog'' category to almost zero (resulting example shown in Fig. \ref{figPGD}.b). Interestingly, the Grad-CAM of the output corresponding to ``dog'' category on the adversarial example is still very similar to the original Grad-CAM (shown in Fig. \ref{figPGD}.e). This suggests that even though the attack fools the network to change its prediction, it does not change the interpretation of the original prediction (also shown in \cite{selvaraju2016grad}). In this section, we are interested in developing a novel attack that not only fools the prediction, but also fools the interpretation. Fig \ref{figPGD}.c shows our result for which the Grad-CAM in Fig \ref{figPGD}.f is very different from the original one in Fig. \ref{figPGD}.d.  %Aks
\subsection{Misleading interpretation in targeted mode:}
\label{sec:target}
As shown in Figure \ref{fig_patch_target1}, when Grad-CAM of the target category is used to investigate the cause of misclassification  , it highlights the patch very strongly revealing the cause of the attack. This is expected as the adversary is restricted to perturbing the patch area and the patch is the cause of the final misclassifcation towards target category. %We are interested in advancing adversarial patches to hide the patch in the interpretation of the prediction.
To fool the network's interpretation so that the adversarial patch  is not highlighted at the interpretation of the final prediction, we add an additional term to our loss function in learning the patch to suppress the total activation of the visualization at the patch location $m$. Hence, assuming the perturbed image $\tilde x = x_0 \odot (1-m)+z \odot m$, we optimize:
\begin{equation}
\begin{split}
     \argmin_{z}\Big[\ell_{ce}(\tilde x; t) + \lambda \sum_{ij} \big(\hat{G}^t(\tilde x) \odot m\big)\Big ]
\end{split}
%\begin{split}
%     z & =  \argmin_{z}\Bigg[  \Big (\sum_{ij} \big(\hat{G}^c(\tilde x) \odot m\big)\Big )  - [p^c>t] \lambda \ell_{ce}(\tilde x; c)\Bigg] \\
%     & \textrm{where} \quad \tilde x = x_0 \odot (1-m)+z \odot m
%\end{split}
\end{equation}

\noindent where $t$ is the target category and $\lambda$ is the hyper-parameter to trade-off the effect of two loss terms. We choose the target label randomly across all classes excluding the original prediction similar to  ``step rnd" method in \cite{atscale-arxiv-2016}.
%  \vspace{-0.12in}
\subsection{Misleading interpretation in non-targeted mode}
% \vspace{-0.2in}

A similar approach can be used to develop a non-targeted attack by maximizing the cross entropy loss of the correct category. This can be considered a weaker form of attack since we have no control over the final category which is predicted after adding the patch. In this case, our optimization problem becomes:

% \begin{equation}
% \begin{split}
%      \argmin_{z}\Big[-[p(c)>p_0]\ell_{ce}(\tilde x; c) \\
%       + \lambda \sum_{ij} \big(\hat{G}^a(\tilde x) \odot m\big)\Big ]
% \end{split}
% \end{equation}

\begin{equation}
\begin{split}
     \argmin_{z}\Big[\max(0, M -\ell_{ce}(\tilde x; c)) + \lambda \sum_{ij} \big(\hat{G}^a(\tilde x) \odot m\big)\Big ] \\ \text{where} \quad a=\argmax_k{p(k)}. \nonumber
\end{split}
\end{equation}

% \noindent where $a=\argmax_k{p(k)}$, and $c$ is the correct category, $p(k)$ is the predicted probability of the category $k$, and $p_0$ is the chance probability. Since cross entropy loss is not upper-bounded, it can dominate the optimization, so we ignore it when the probability of the correct category is less than the chance level ($p_0 = 0.001$ for Imagenet) by multiplying $[p(c)>p_0]$, which is an indicator function. Note that the second term is still using the visualization of the target category.

\noindent where $c$ is the predicted category for the original image, $p(k)$ is the probability of category $k$, and $a$ is the top prediction at every iteration. Since cross entropy loss is not upper-bounded, it can dominate the optimization, so we use contrastive loss \cite{hadsell2006dimensionality} to ignore cross entropy  when the probability of $c$ is less than the chance level, thus $M=-log(p_0)$ where $p_0$ is the chance probability (e.g., 0.001 for ImageNet). Note that the second term is using the visualization of the current top category $a$.

To optimize above loss functions, we use an iterative approach similar to  projected gradient decent (PGD) algorithm \cite{madry2017towards}. We initialize $z^0$ randomly and then iteratively update it by: $\displaystyle z^{n+1} = z^n - \eta Sign({\frac{\partial \ell}{\partial z}})$ with learning rate $\eta$. At each iteration, we project $z$ to the feasible region by clipping it to the dynamic range of the image values.
% $\displaystyle z^{n+1} = z^n -  \eta Sign({\frac{\partial \ell}{\partial z}})$
% \vspace{-0.1in}
\begin{table*}[!t]
\centering
 \begin{tabular}{||c || c | c | c | c | c | c ||}
 \hline
 \multirow{2}{*}{Method} & Top-1 Acc(\%) & \multicolumn{2}{|c|}{Non-Targeted} & \multicolumn{3}{|c|}{Targeted} \\ [0.5ex]
 \cline{3-7}
 & & Acc (\%) & Energy Ratio (\%) & Acc (\%) & Target Acc (\%) & Energy Ratio(\%)\\
%  Energy within Adversarial patch (\%)
%  Energy within Our Patch(\%)
 \hline\hline
 Adversarial Patch \cite{brown2017adversarial} & 74.24 & 0.06 & 38.95 & 0.02 & 99.98 & 58.39   \\
 \hline
 Our Patch & 74.24 & 0.05 & \textbf{2.00} & 2.95 &77.88 & \textbf{5.21} \\
%  \hline
%  Original Image
%  \hline
%  Universal & &  \\ [1ex]
 \hline
\end{tabular}
\newline
    \caption{Comparison of heatmap energy within the 8\% patch area for the adversarial patch \cite{brown2017adversarial} and our patch. Accuracy denotes the fraction of images that had the same final predicted label as the original image. Target Accuracy denotes the fraction of images where the final predicted label has changed to the randomly chosen target label. %Note that the Target accuracy is not defined for the Non-Targeted attack.
    }
    \label{fig:comparison_patch_heatmap}
\end{table*}
% \begin{table*}[!t]
% \centering
%  \begin{tabular}{||c || c | c | c | c | c ||}
%  \hline
%  \multirow{2}{*}{Image} & Top-1 Acc(\%) & \multicolumn{2}{|c|}{Targeted} & \multicolumn{3}{|c|}{Universal} \\ [0.5ex]
%  \cline{3-7}
%  & & Acc (\%) & Energy (\%) & Acc (\%) & Target Acc (\%) & Energy (\%)\\
% %  Energy within Adversarial patch (\%)
% %  Energy within Our Patch(\%)
%  \hline\hline
%  Adversarial Patch \cite{brown2017adversarial} & 74.24 & 0.06 & 38.95 & 0.02 & 99.98 & 58.39   \\
%  \hline
%  Our Patch & 74.24 & 0.05 & \textbf{2.00} & 2.95 &77.88 & \textbf{5.21} \\
% %  \hline
% %  Original Image
% %  \hline
% %  Universal & &  \\ [1ex]
%  \hline
% \end{tabular}
% \newline
%     \caption{Comparison of heatmap energy within the 8\% patch area for the adversarial patch \cite{brown2017adversarial} and our patch trained on the GAIN{$_{ext}$} \cite{kunpeng2018gain} for VOC dataset.
%     }
%     \label{fig:comparison_patch_heatmap}
% \end{table*}
\begin{table}[!t]
\centering
 \begin{tabular}{||c || c | c ||}
 \hline
 \multirow{2}{*}{Method} & \multicolumn{2}{|c|}{Targeted} \\ [0.5ex]
 \cline{2-3}
 & \footnotesize{Target Acc (\%)} & \footnotesize{Energy Ratio (\%)}\\
%  Energy within Adversarial patch (\%)
%  Energy within Our Patch(\%)
 \hline\hline
 Adversarial Patch \cite{brown2017adversarial} & 94.34 & 29.02   \\
 \hline
 Our Patch & 94.70 & \textbf{2.45} \\
%  \hline
%  Original Image
%  \hline
%  Universal & &  \\ [1ex]
 \hline
\end{tabular}
\newline
    \caption{Comparison of Grad-CAM heatmap energy within the 8\% patch area for the adversarial patch \cite{brown2017adversarial} and our patch trained on the GAIN{$_{ext}$} \cite{kunpeng2018gain} for VOC dataset. The heatmap has far less energy in the patch area that the adversary can change.
    }
    \label{fig:comparison_patch_heatmap_GAIN}
\end{table}
\begin{table}[!t]
\centering
 \begin{tabular}{||c || c | c ||}
 \hline
 \multirow{2}{*}{Method} & \multicolumn{2}{|c|}{Targeted} \\ [0.5ex]
 \cline{2-3}
 & \footnotesize{Target Acc (\%)} & \footnotesize{Energy Ratio (\%)}\\
%  Energy within Adversarial patch (\%)
%  Energy within Our Patch(\%)
 \hline\hline
 Adversarial Patch \cite{brown2017adversarial} &  99.97& 61.91  \\
 \hline
 Our Patch & 92.28 & \textbf{0.56} \\
%  \hline
%  Original Image
%  \hline
%  Universal & &  \\ [1ex]
 \hline
\end{tabular}
\newline
    \caption{Comparison of heatmap energy for the universal attack case.
    }
    \label{fig:comparison_patch_univ_heatmap_GAIN}
\end{table}
\begin{table}[!t]
\centering
 \begin{tabular}{||c || c | c ||}
 \hline
 Method & \small{Targeted Attack Energy Ratio (\%)}\\[0.5ex]
%  Energy within Adversarial patch (\%)
%  Energy within Our Patch(\%)
 \hline\hline
 Adversarial Patch \cite{brown2017adversarial} & 61.59  \\
 \hline
 Our Patch & \textbf{24.19}  \\
%  \hline
%  Original Image
%  \hline
%  Universal & &  \\ [1ex]
 \hline
\end{tabular}
\newline
    \caption{Comparison of Occluding Patch \cite{zhou2014object} heatmap energy within the 8\% patch area for the adversarial patch \cite{brown2017adversarial} and our patch trained on the GAIN{$_{ext}$} \cite{kunpeng2018gain} for VOC dataset. Note that we still use Grad-CAM in training and evaluate on Occluding Patch. This shows our attack generalizes from Grad-CAM to occluding patch.
    }
    \label{table:comparison_patch_transfer_heatmap_GAIN}
\end{table}
\subsection{Misleading interpretation for guided attention models}
\label{improv_adv_exmaples_attention_models}
Recently, there have been works \cite{singh2017hide}, \cite{wei2017object} which focus on improving the attention maps of the predicted objects when training a classifier. Kunpeng \etal \cite{kunpeng2018gain} further improve upon this by providing supervision on the network's attention in an end-to-end way. This is done by designing loss functions that guide the network's focus on the entire area critical to the task of interest. We perform the targeted attack as described in section \ref{sec:target} on the GAIN{$_{ext}$} model from \cite{kunpeng2018gain}. Since the GAIN{$_{ext}$} model was fine-tuned for the PASCAL VOC dataset, we perform a targeted attack to change the prediction to the least likely category from the predictions and also ensure that the Grad-CAM heatmap does not overlap with the patch. We also observe that patches created our method transfer to Occluding Patch \cite{zhou2016learning} as seen in Figure \ref{fig_transfer}.  This shows thateven if the patch is optimized to fool only one visualization, it also results in fooling other visualizations as well, which is to the advantage of the adversary.
%\vspace{-0.1in}
\subsection{Universal Patches}
\label{universal_attention}
Universal attack is a much stronger form of attack wherein we train a patch that generalizes across images in fooling towards a particular category. Such an attack shows that it is possible to fool an unknown test image using a patch learnt using the training data. Similar to \ref{improv_adv_exmaples_attention_models}, we consider the GAIN$_{ext}$ model which is fine-tuned on PASCAL VOC dataset. We fix the target category as ``Pottedplant'' and learn the patch as a form of targeted attack as explained in \ref{sec:target} which ensures mis-classification towards the target category along with the heatmap on the patch area being minimal. This is the most practical form of attack, since the adversary needs to train the patch just once, which would be strong enough to fool multiple test scenarios.\%\subsection{Misleading interpretation of regular adversarial examples}
%\label{improv_adv_exmaples}
%We generalize our method to mislead the visualization of regular adversarial examples where the perturbation is bounded by the value but not spatial extent. Figure \ref{figPGD_1} shows that in attacking a "microphone" image to change the prediction to "joystick", the visualization for the correct category "microphone" is enact. Similar observation is also shown in Grad-CAM paper \cite{selvaraju2016grad}. We show that our algorithm could be adapted to not only change the prediction but also mislead the interpretation of the correct category. To do so, we design our loss function based on the histogram intersection similarity of the Grad-CAM visualizations. Hence, we run the following optimization:
%\vspace{-.1in}

%\begin{align}
%    \argmin_{\delta x} \Big[- [p(c)>t] \ell_{ce}(x_0+\delta x; c) + \\ \nonumber
%    \lambda \sum_{ij} \min \big( \hat{G}^c(x_0+\delta x),\hat{G}^c(x_0)\big) \Big]
%\end{align}
%\vspace{-.1in}

%\noindent where $c$ is the correct category and $min$ is the element-wise minimum operator. We use projected gradient decent (PGD) algorithm \cite{madry2017towards}, and bound $\delta x$ in the range $(-\epsilon, \epsilon)$ and also $x_0+\delta x$ in the dynamic range of the image values.

%\vspace{-.1in}




% Adversarial patches \cite{brown2017adversarial,karmon2018lavan} are designed to fool the network with a small image independent patch without the ${L_p}$ perturbation restriction. This patch can then be pasted anywhere on the image to mislead the classifier.
% Unlike standard adversarial examples, the values of the adversarial patch do not need to be small and should only be in the range of the dynamic values of the input image.
% Adversarial patches \cite{brown2017adversarial,karmon2018lavan} are a family of adversarial examples that instead of fooling the network with small ${L_p}$ perturbations of limited range of values $(-\epsilon, \epsilon)$, they fool the network with a small image independent patch (covering about 5-10\% of the image area) without the ${L_p}$ perturbation restriction. This patch can then be pasted anywhere on the image. Unlike standard adversarial examples, the values of the adversarial patch do not need to be small and should only be in the range of the dynamic values of the input image.
% This is a more practical attack compared to the standard adversarial attacks since for instance, one can simply print such a patch on paper and show it to a self-driving car to make it blind to pedestrians. %Aks

% Our experiments show that when an adversarial patch is fooling the network to incorrectly classify a ``dog'' image as ``chair'', the patch location is strongly highlighted in the result of Grad-CAM visualization for ``chair'' category. This limits the adversaries in such an attack since for instance in the self-driving car example, one can find the cause of the accident using network interpretation algorithms like Grad-CAM. In this section, we show that using our loss function $l_g$ that simply measures the difference in Grad-CAM visualization, we can hide the cause of adversarial patch attack.




%of the ''chair'' category is
%algorithm can easily highlight the
%adversarial examples

%es are not limited in the range of values as long as they are in the  in the

%So far, we introduced our loss loss function $l_g$ that simply measures the difference in Grad-CAM visualization.

%In Fig. \ref{}, Assume image of a dog shown in Fig \ref{},

%{\bf Attacking interpretation without affecting the output}

%{\bf Adversarial patches and interpretation}

% }


%\vspace{-.45in}

\section{Experiments}
%In this section, we present the results of our algorithms.
 We use pre-trained VGG-19 \cite{simonyan2014very} with batch normalization implemented in PyTorch.\\
% VGG-19 with batch normalization 	25.76 	8.15

% {\bf Dataset:} We run our main experiments on the validation set of ImageNet \cite{deng2009imagenet} ILSVRC2012 which contains 50,000 images.
% For ablative analysis experiments, we reduce the size of dataset by randomly sampling 5,000 images (5 per category) from the ImageNet validation set.


{\bf 4.1. Misleading interpretation of adversarial patches:}
\label{exp_misleading_adv_patches}
For the adversarial patch experiments described in the method section, we use a patch of size 64x64 on the top-left corner for 50,000 images of size 224x224 ($\sim$ 8\% of the image area) in the validation set of ImageNet \cite{deng2009imagenet} ILSVRC2012. We do 750 iterations with $\eta = 0.001$. To evaluate how much the patch is highlighted in the visualization, we construct the visualization heatmap $\hat{G}^c$ for the mistaken category, and calculate the ratio of the total energy at the patch location to that of the whole image. We call this metric ``energy ratio". It will be $0$ if the patch is not highlighted at all and $1$ if the heatmap is completely concentrated inside the patch. The quantitative results are shown in Table \ref{fig:comparison_patch_heatmap}.
% \vspace{-.1in}
% Our aim is to learn an adversarial patch that does not get highlighted by visualization algorithms like Grad-CAM. .
% Using Grad-CAM, the original adversarial patch \cite{brown2017adversarial} contains \textbf{15.5}\% of the total energy of the heatmap and ours (minimizing $l_p$) gets \textbf{8.7}\%. Using occluding patch, the original adversarial patch contains \textbf{39.3}\% of the total energy of the heatmap and ours gets \textbf{25.1}\%.

%We realized that usually minimizing $l_p$ pushes the Grad-CAM on the patch to zero which still makes it identifiable since other areas are non-zero. Hence, we may want to reduce the heatmap to the average value. This can be done by simply minimizing $l_p-a$ where $a$ is the ratio between the area of mask and the image.
% We also calculate the energy of the heatmap at the location of patch and compare it between the original adversarial patches \cite{brown2017adversarial} and ours. The original get 36.9\% and ours (minimizing $l_p$) gets 16.8\%. This means the heatmap is not concentrated at the patch location which conceals the patch making it less identifiable.


%2, (1) with forcing patch to be zero
%Fig: orig img, patch-added-orig img, gcam, our patch added img, gcam

%3. targeted
%Fig: orig img, patch-added-orig img, gcam, our patch added img, gcam

%4. absorbing heatmap: no change in label, but gcam fires on patch
%Fig: orig img, patch-added-orig img, gcam, our patch added img, gcam


% \begin{table}[h]
% \centering
%  \begin{tabular}{||c || c | c ||}
%  \hline
%  \multirow{2}{*}{Image} & \multicolumn{2}{|c|}{Targeted} \\ [0.5ex]
%  \cline{2-3}
%  & Target Acc (\%) & Energy (\%)\\
% %  Energy within Adversarial patch (\%)
% %  Energy within Our Patch(\%)

%  \hline\hline
%  Adversarial Patch \cite{brown2017adversarial} & 94.34 & 29.02   \\
%  \hline
%  Our Patch & 94.70 & \textbf{2.45} \\
% %  \hline
% %  Original Image
% %  \hline
% %  Universal & &  \\ [1ex]
%  \hline
% \end{tabular}
% \newline
%     \caption{Comparison of heatmap energy within the 8\% patch area for the adversarial patch \cite{brown2017adversarial} and our patch trained on the GAIN{$_{ext}$} \cite{kunpeng2018gain} for VOC dataset.
%     }
%     \label{fig:comparison_patch_heatmap_GAIN}
% \end{table}
{\bf 4.2. Misleading interpretation for guided attention models:}
We use the GAIN{${_{ext}}$} model fine-tuned on the VOC dataset from \cite{kunpeng2018gain} and VOC test set for these experiments. We use a patch of size 64x64 on the top-left corner for 4952 images of size 224x224 ($\sim$ 8\% of the image area) in the test set of the PASCAL VOC dataset. We do 750 iterations with $\eta = 0.1$. Since each image in the VOC dataset can contain more than one category, we use the least likely predicted category to perform the targeted patch attack. We evaluate using the same method as described in Section
{\textcolor{red}{4.1}}. %\ref{exp_misleading_adv_patches}.
The results of the evaluation are described in Table \ref{fig:comparison_patch_heatmap_GAIN}.

{\bf 4.3. Generalization beyond Grad-CAM:} We also show that our patches learned using Grad-CAM are hidden in the visualizations generated by Occluding Patch \cite{zhou2014object} method. In occluding patch method, we visualize the change in the final score of the model by sliding a small black box on the image. Larger decrease in the score means more important regions and hence they contribute more to the heatmap. The results are shown in Table \ref{table:comparison_patch_transfer_heatmap_GAIN} and Figure \ref{fig_transfer}.

%the transfer of our patches visualized using Occluded Patch \cite{zhou2014object} in Figure \ref{fig_transfer} and quantitative results are found in Table \ref{table:comparison_patch_transfer_heatmap_GAIN}. This shows that adversarial patches created using our method transfer across visualization techniques. We train using Grad-CAM and evaluate using occluded patch visualization method \cite{zhou2014object}.

{\bf 4.4. Universal Patches:}
For these experiments, we learn a patch of size 64x64 on the top-left but use the training set from PASCAL VOC dataset. The optimization performed is similar to the above section. We use 50 iterations per image with $\eta=0.05$ and $\lambda = 0.09$. As described in \ref{universal_attention}, we choose ``Pottedplant'' as the target category and the evaluation was done on test set. The results for these can be found in Figure \ref{fig_patch_universal} and Table \ref{fig:comparison_patch_univ_heatmap_GAIN} . We observe high fooling rates for both the methods, but our method has considerably low energy focused inside the patch area. Note that only the region of the patch is perturbed and everything else is untouched.

\ignore{

\subsection{Misleading interpretation of regular adversarial examples}
We use 5,000 uniformly random images from the validation set of ImageNet dataset. We do 500 iterations with $\eta = 0.05$. We use two metrics for quantitative evaluation of our method described in Section \ref{improv_adv_exmaples}:
{\bf (1) Localization error:} We use the Top-1 error of object localization metric similar to \cite{selvaraju2016grad} from the ImageNet competition. A higher number means a good attack where objects cannot be detected in the visualization.
{\bf (2) Histogram intersection:} We calculate the Grad-CAM heatmap of the original image and the adversarial image, normalize them to sum to one, and calculate the histogram intersection between them. A lower number means a good attack where there is not much of overlap between visualizations before and after the attack. Examples are shown in Figure \ref{figCAM_1}, Figure \ref{figPGD_1} and Table \ref{table:PGD}.

Moreover, Figure \ref{figCAM_1} also shows transferability of our attack across different visualization tools. It shows that our adversarial image which is tuned for Grad-CAM visualization, fools CAM \cite{zhou2016learning} and occluding patch \cite{zhou2014object} visualizations as well. In the occluding patch method, we visualize the change in the final score of the model by sliding a small black box on the image. Larger decrease in the score means more important regions and hence they contribute more to the heatmap. We use SqueezeNet for these experiments since CAM is not supported in networks without Global Average Pooling layer (GAP) \cite{zhou2016learning}.

%\vspace{-0.1in}
\begin{table*}[h]
\centering
\begin{tabular}{||c ||c | c |c| c|}
 \hline
 Image & Accuracy (\%) & Loc. Error(\%) & Histogram Intersection\\ [1ex]
 \hline\hline
 Original & - & 66.68 & 1.0 \\
 \hline
 PGD Adversarial & 0.26 & 67.74 & 0.78 $\pm$ 0.076 \\
 \hline
%  Our Adversarial &  &  & 0.68 $\pm$ 0.09 \\
Our Adversarial & 0.06 & 78.64 & 0.62 $\pm$ 0.088 \\
 \hline
\end{tabular}
\newline
\caption{Results for evaluating our method on ImageNet validation images using two metrics. Note that higher localization error and lower histogram intersection is better.}
\label{table:PGD}
\end{table*}
}

%\vspace{-.2in}





%\section{Discussion}
%1616: chair images: 2540, 1616
%{\bf Multiple instances:} In Fig. \ref{}, there are multiple chair instances that might have lead to ''chair'' classification decision. Grad-CAM highlights only one instance and our algorithms can easily satisfy the adversarial loss function by highlighting another instance. It is not clear what should an interpretation algorithm show in such cases where there are multiple possible interpretations. Defining optimization similar to ours may lead to inferring diversified modes of the network interpretation.

\section{Conclusion}
We presented novel adversarial attack algorithms that go beyond fooling the prediction by hiding the cause of the mistake from our common interpretation tools to result in a stronger attack. We show that our attack tuned for Grad-CAM can transfer to other visualization algorithms and we also show that we can create universal patches that can generalize fooling across images. Our work shows that there is a need for more robust deep learning tools that reveal the correct cause of network's predictions.\\ \\

{\bf Acknowledgement:} This work was performed under the following financial assistance award: 60NANB18D279 from U.S. Department of Commerce, National Institute of Standards and Technology, and also funding from SAP SE.


%This is an interesting observation that is in contradiction with \cite{} that shows we do not have an explanation for. We do not know the explanation for this effect, but believe studying it further may result in better understanding of the effect of BN layers in feature learning.

%Fig1:
%orig, gcam, adv, gcam
%dog probability

%Fig2:
%orig, gcam, PGD, gcam, ours, gcam
%dog probability

%Fig3:
%orig, non-targetted adv_patch, gcam, our adv, gcam
%dog probability

% }

% \begin{figure*}[h]
%   \begin{center}
%   \begin{tabular}{ c c c c c}
%   Original & AP & GCAM - AP & Ours & GCAM - Ours\\
% \includegraphics[width=.17\textwidth]{figures/target_patch/img_orig_ILSVRC2012_val_00001058.JPEG}&
% \includegraphics[width=.17\textwidth]{figures/target_patch/img_adv_patch_ILSVRC2012_val_00001058_orig_988_target_086_pred_086_prob_99.png}&
% \includegraphics[width=.17\textwidth]{figures/target_patch/mask_reg_patch_ILSVRC2012_val_00001058_orig_988_target_086_pred_086.JPEG}&
% \includegraphics[width=.17\textwidth]{figures/target_patch/img_our_patch_ILSVRC2012_val_00001058_orig_988_target_086__pred_086_prob_08.png}&
% \includegraphics[width=.17\textwidth]{figures/target_patch/mask_our_patch_ILSVRC2012_val_00001058_orig_988_target_086_pred_086.png}\\
% Acorn & Partridge & Partridge & Partridge & Partridge \\
% \includegraphics[width=.17\textwidth]{figures/target_patch/img_orig_ILSVRC2012_val_00001576.JPEG}&
% \includegraphics[width=.17\textwidth]{figures/target_patch/img_adv_patch_ILSVRC2012_val_00001576_orig_176_target_111_pred_111_prob_99.png}&
% \includegraphics[width=.17\textwidth]{figures/target_patch/mask_reg_patch_ILSVRC2012_val_00001576_orig_176_target_111_pred_111.JPEG}&
% \includegraphics[width=.17\textwidth]{figures/target_patch/img_our_patch_ILSVRC2012_val_00001576_orig_176_target_111__pred_111_prob_05.png}&
% \includegraphics[width=.17\textwidth]{figures/target_patch/mask_our_patch_ILSVRC2012_val_00001576_orig_176_target_111_pred_111.png}\\
% Gazelle hound & Roundworm & Roundworm & Roundworm & Roundworm \\
% \includegraphics[width=.17\textwidth]{figures/non_targeted_patch/img_orig_ILSVRC2012_val_00000086.JPEG}&
% \includegraphics[width=.17\textwidth]{figures/non_targeted_patch/img_adv_patch_ILSVRC2012_val_00000086_orig_498_pred_915_prob_98.png}&
% \includegraphics[width=.17\textwidth]{figures/non_targeted_patch/mask_adv_patch_ILSVRC2012_val_00000086_orig_498_pred_915.JPEG}&
% \includegraphics[width=.17\textwidth]{figures/non_targeted_patch/img_our_patch_ILSVRC2012_val_00000086_orig_498__pred_698_prob_68.png}&
% \includegraphics[width=.17\textwidth]{figures/non_targeted_patch/mask_our_patch_ILSVRC2012_val_00000086_orig_498_pred_698.png}\\
% Cinema & Yurt & Yurt & Palace & Palace \\
% \includegraphics[width=.17\textwidth]{figures/non_targeted_patch/img_orig_ILSVRC2012_val_00000151.JPEG}&
% \includegraphics[width=.17\textwidth]{figures/non_targeted_patch/img_adv_patch_ILSVRC2012_val_00000151_orig_681_pred_742_prob_90.png}&
% \includegraphics[width=.17\textwidth]{figures/non_targeted_patch/mask_reg_patch_ILSVRC2012_val_00000151_orig_681_pred_742.JPEG}&
% \includegraphics[width=.17\textwidth]{figures/non_targeted_patch/img_our_patch_ILSVRC2012_val_00000151_orig_681__pred_922_prob_16.png}&
% \includegraphics[width=.17\textwidth]{figures/non_targeted_patch/mask_our_patch_ILSVRC2012_val_00000151_orig_681_pred_922.png}\\
% Notebook (PC) & Printer & Printer & Menu & Menu \\
% \end{tabular}
%   \caption{Comparison of Grad-CAM visualization results for targeted patch attacks (first 2 rows) and non-targeted patch attacks (rows 3 \& 4) using our method (`Ours') vs regular adversarial patch (`AP').}
% \label{fig_patch_comparison}
%   \end{center}

% \end{figure*}

\ignore{
\begin{figure*}[!h]
\centering
Visualization for category: Microphone\\
  \begin{tabular}{c c c}
    \includegraphics[width=.20\textwidth, height=.20\textwidth]{figures/pgd/microphone/orig_ILSVRC2012_val_00003465.JPEG}&
    \includegraphics[width=.20\textwidth, height=.20\textwidth]{figures/pgd/microphone/pgd_ILSVRC2012_val_00003465.JPEG}&
    \includegraphics[width=.20\textwidth, height=.20\textwidth]{figures/pgd/microphone/adv_ILSVRC2012_val_00003465.JPEG}\\
    (a) Orig Image & (b) PGD Adv & (c) Grad-CAM Adv \\
    Pred: Microphone & Pred: Screw & Pred: Joystick\\
    \includegraphics[width=.20\textwidth, height=.20\textwidth]{figures/pgd/microphone/orig_mask_ILSVRC2012_val_00003465_0650.JPEG}&
    \includegraphics[width=.20\textwidth, height=.20\textwidth]{figures/pgd/microphone/pgd_mask_ILSVRC2012_val_00003465_mask.JPEG}&
    \includegraphics[width=.20\textwidth, height=.20\textwidth]{figures/pgd/microphone/adv_mask_ILSVRC2012_val_00003465_1_0083_0650.JPEG}\\
    (d) Grad-CAM Orig  & (e) Grad-CAM PGD  & (f) Grad-CAM Ours
  \end{tabular}
  \caption{The adversarial images generated by our method is able to fool the Grad-CAM visualization result (f) compared to the Grad-CAM visualization of the adversarial image generated by PGD (e).}
\label{figPGD_1}

\end{figure*}
}





% \newpage
\bibliographystyle{unsrt}
\bibliography{main}

% \appendix


% \begin{figure*}[h!]
% \begin{center}
%   \begin{tabular}{|c | c c c c c c|}
%   \hline
%   \begin{sideways} \scriptsize Original Image \end{sideways}
%   &
%   \begin{sideways}  \end{sideways}
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/reg_patch_ILSVRC2012_val_00018342_915_yurt_vs_trailer_truck.JPEG}&
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/reg_patch_ILSVRC2012_val_00018394_437_lighthouse_vs_warplane.JPEG}&
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/reg_patch_ILSVRC2012_val_00020130_619_lampshade_vs_warplane.JPEG}&
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/reg_patch_ILSVRC2012_val_00020228_673_computer_mouse_vs_sunglasses.JPEG}&
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/reg_patch_ILSVRC2012_val_00043304_947_mushroom_vs_rosehip.JPEG}&
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/reg_patch_ILSVRC2012_val_00040634_618_ladle_vs_maraca.JPEG} \\
%     % \includegraphics[width=.10\textwidth, height=.1\textwidth]{figures/patch_imgs/reg_patch_ILSVRC2012_val_00004067_682_obelisk_vs_solar_collector.JPEG}\\
%     & \scriptsize Trailer Truck & \scriptsize Warplane & \scriptsize Warplane & \scriptsize Sunglasses & \scriptsize Rose hip & \scriptsize Maraca\\
%     \hline
%     \hline

% %   \begin{sideways} \scriptsize Reg Patch \end{sideways}
% \multirow{2}{*}{\begin{sideways} \scriptsize Reg Patch \end{sideways}}
%   &
%   \begin{sideways} \scriptsize Grad-CAM \end{sideways}
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/reg_patch_ILSVRC2012_val_00018342_915_yurt_vs_trailer_truck.JPEG}&
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/reg_patch_ILSVRC2012_val_00018394_437_lighthouse_vs_warplane.JPEG}&
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/reg_patch_ILSVRC2012_val_00020130_619_lampshade_vs_warplane.JPEG}&
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/reg_patch_ILSVRC2012_val_00020228_673_computer_mouse_vs_sunglasses.JPEG}&
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/reg_patch_ILSVRC2012_val_00043304_947_mushroom_vs_rosehip.JPEG}&
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/reg_patch_ILSVRC2012_val_00040634_618_ladle_vs_maraca.JPEG} \\
%     &
%     \begin{sideways} \scriptsize Occluded Patch \end{sideways}
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/blackpatch_gcam_adv_img_00018342_355_llama_vs_trailer_truck.png}&
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/blackpatch_gcam_adv_img_00018394_872_tripod_vs_warplane.png}&
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/blackpatch_gcam_adv_img_00020130_845_syringe_vs_warplane.png}&
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/blackpatch_gcam_adv_img_00020228_606_iron_vs_sunglasses.png}&
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/blackpatch_gcam_adv_img_00043304_94_hummingbird_vs_rosehip.png}&
%      \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/blackpatch_gcam_adv_img_00040634_769_ruler_vs_maraca.png}\\
%     % \includegraphics[width=.10\textwidth, height=.1\textwidth]{figures/patch_imgs/reg_patch_ILSVRC2012_val_00004067_682_obelisk_vs_solar_collector.JPEG}\\
%     & \scriptsize Yurt & \scriptsize Lighthouse & \scriptsize Lampshade & \scriptsize Mouse & \scriptsize Mushroom  & \scriptsize Ladle \\
%     % & \scriptsize Orig: Trailer Truck & \scriptsize Orig: Warplane & \scriptsize Orig: Warplane & \scriptsize Orig: Sunglasses & \scriptsize Orig: Rose hip & \scriptsize Orig:Maraca\\
%     \hline
%     \hline
%     \multirow{2}{*}{\begin{sideways} \scriptsize Our Patch \end{sideways}} &
%     \begin{sideways} \scriptsize Grad-CAM \end{sideways}
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/our_patch_zero_case_ILSVRC2012_val_00018342_355_llama_vs_trailer_truck.png}&
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/our_patch_zero_case_ILSVRC2012_val_00018394_872_tripod_vs_warplane.png}&
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/our_patch_zero_case_ILSVRC2012_val_00020130_845_syringe_vs_warplane.png}&
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/our_patch_zero_case_ILSVRC2012_val_00020228_606_iron_vs_sunglasses.png}&
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/our_patch_zero_case_ILSVRC2012_val_00043304_094_hummingbird_vs_rosehip.png}&
%      \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/our_patch_zero_case_ILSVRC2012_val_00040634_769_ruler_vs_maraca.png}\\
%     &
%     \begin{sideways} \scriptsize Occluded Patch \end{sideways}
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/blackpatch_gcam_adv_img_00018342_355_llama_vs_trailer_truck.png}&
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/blackpatch_gcam_adv_img_00018394_872_tripod_vs_warplane.png}&
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/blackpatch_gcam_adv_img_00020130_845_syringe_vs_warplane.png}&
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/blackpatch_gcam_adv_img_00020228_606_iron_vs_sunglasses.png}&
%     \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/blackpatch_gcam_adv_img_00043304_94_hummingbird_vs_rosehip.png}&
%      \includegraphics[width=.12\textwidth, height=.12\textwidth]{figures/patch_imgs/blackpatch_gcam_adv_img_00040634_769_ruler_vs_maraca.png}\\
%     %   \includegraphics[width=.10\textwidth, height=.10\textwidth]{figures/patch_imgs/blackpatch_gcam_adv_img_00004067_727_planetarium_vs_solar_collector.png}\\
%     & \scriptsize Llama  & \scriptsize Tripod & \scriptsize Syringe  & \scriptsize Iron & \tiny Humming Bird  & \scriptsize Ruler\\
%     \hline
%   \end{tabular}
%   \caption{All visualizations are done for the predicted category. The first 2 rows use Grad-CAM visualization. The patch is clearly highlighted in the standard adversarial patches but not in our method. This is significant since the adversary cannot perturb pixels outside the patch area.}
%   \label{fig_patch}
% \end{center}
% \end{figure*}





% \subsection{Effect of Batch Normalization}
%Batch normalization (BN) layer is a great tool that accelerates the learning process. However, some recent works have reported that it hurts the performance in GANs \cite{} and self-supervised learning \cite{}.

% We tried our algorithms with different variations of VGG architecture both with and without Batch Normalization (BN). The results are shown in Table \ref{table:1}. Interestingly, in all cases, the models with BN are more vulnerable to the attack on interpretations. This is in contradiction with \cite{narodytska2016simple} that shows models with BN are more robust to standard adversarial attacks like FGSM \cite{explainingharnessing-arxiv-2014}. We do not have an explanation for this observation, but believe studying it may reveal more insights about the difference between our attacks and the standard attacks along with the effect of BN in interpretation and robustness of deep networks.



% \subsubsection{Second derivative:} Note that minimizing $\ell_G$ involves taking the second derivative of the output with respect to input $\displaystyle \frac{\partial^2 y}{\partial x^2}$, which can be done using double-backpropagation algorithm introduced in \cite{drucker1992improving}. While in Grad-CAM algorithm, any layer in the deep into the network can be used as $y$, the Grad-CAM uses the logits before {\em softmax}. Interestingly, logits are locally linear with respect to input in most popular networks since the only non-linear component (ReLU) is locally linear. Hence, if we use logits for $y$, the double-backpropagation part will not contribute to the optimization since its second derivative with respect to input is zero. However, the double-backpropagation will be necessary if we use the output of {\em softmax} for $y$ or we use a networks with nonlinear layers like ``Local Response Normalization'' as in the AlexNet or ``sigmoid''.


%\subsection{Transferring beyond Grad-CAM}
%In Table. \ref{table:trans}, we show that our adversarial images generated based on fooling Grad-CAM do fool other visualization algorithms like CAM and Occluding Patch \cite{zhou2014object}. In Occluding Patch, we slide a small patch on the image and build the heatmap by calculating the effect of that random patch in the output value. Fig. \ref{figCAM} shows an example for this experiment where on the second row, all visualization algorithms are fooled by our adversarial attack.



\ignore{
\newpage
% \section{Appendix}

\begin{figure*}[h!]
  \begin{center}
% Visualization for category: ``Rock Beauty''
  \begin{tabular}{| c c c c|}
  \hline

  & Grad-CAM & CAM & Occlusion\\
  \begin{sideways} Original Image \end{sideways}
\includegraphics[width=.17\textwidth]{figures/squeezenet/img_orig_ILSVRC2012_val_00005576.JPEG}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/mask_orig_ILSVRC2012_val_00005576_orig_0020_pred_0360.JPEG}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/img_orig_ILSVRC2012_val_00005576_CAM.png}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/img_orig_ILSVRC2012_val_00005576_OP.png}\\
 Pred: Water Ouzel &&&\\
 \begin{sideways} Our Adv. Image \end{sideways}
\includegraphics[width=.17\textwidth]{figures/squeezenet/img_ours_ILSVRC2012_val_00005576.JPEG}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/mask_ours_ILSVRC2012_val_00005576_orig_0020_pred_0360.JPEG}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/ILSVRC2012_val_00005576_adv_image_CAM.png}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/ILSVRC2012_val_00005576_all_masks_adv_OP.png}\\
 Pred: Otter &  &  &   \\
\hline
\hline
&&&\\
\begin{sideways} Original Image \end{sideways}
\includegraphics[width=.17\textwidth]{figures/squeezenet/img_orig_ILSVRC2012_val_00006174.JPEG}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/mask_orig_ILSVRC2012_val_00006174_orig_0270_pred_0265.JPEG}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/img_orig_ILSVRC2012_val_00006174_CAM.png}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/img_orig_ILSVRC2012_val_00006174_OP.png}\\
 Pred: White Wolf &&&\\
 \begin{sideways} Our Adv. Image \end{sideways}
\includegraphics[width=.17\textwidth]{figures/squeezenet/img_ours_ILSVRC2012_val_00006174.JPEG}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/mask_ours_ILSVRC2012_val_00006174_orig_0270_pred_0265.JPEG}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/ILSVRC2012_val_00006174_our_adv_image_CAM.png}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/ILSVRC2012_val_00006174_all_masks_adv_OP.png}\\
 Pred: Toy Poodle&&&  \\
\hline
\hline
&&&\\
\begin{sideways} Original Image \end{sideways}
\includegraphics[width=.17\textwidth]{figures/squeezenet/img_orig_ILSVRC2012_val_00009705_clean.JPEG}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/mask_orig_ILSVRC2012_val_00009705_orig_0497_pred_0873.JPEG}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/img_orig_ILSVRC2012_val_00009705_CAM.png}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/img_orig_ILSVRC2012_val_00009705_OP.png}\\
 Pred: Church &&&\\
 \begin{sideways} Our Adv. Image \end{sideways}
\includegraphics[width=.17\textwidth]{figures/squeezenet/ILSVRC2012_val_00009705.JPEG}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/mask_ours_ILSVRC2012_val_00009705_orig_0497_pred_0873.JPEG}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/ILSVRC2012_val_00009705_our_adv_image_CAM.png}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/ILSVRC2012_val_00009705_all_masks_adv_OP.png}\\
 Pred: Triumphal arch  &&& \\
\hline

\end{tabular}
  \caption{We use Grad-CAM, CAM \cite{zhou2016learning}, and occluding patch \cite{zhou2014object} methods to asses the transferability of our attack across different methods. All visualizations are for the predicted category of the original image on the top-left of each panel. We use SqueezeNet for these experiments since CAM is not supported in networks without Global Average Pooling layer (GAP) \cite{zhou2016learning}. On the second row, our adversarial attack can fool all three visualization algorithms.}
  \end{center}
  %\vspace{-.2in}
\label{figCAM_1}
\end{figure*}
}



% \begin{figure*}[h!]
% \begin{center}
% Visualization for category: Lighthouse
%   \begin{tabular}{ c c c c}
% \includegraphics[width=.22\textwidth]{figures/vgg19_bn/orig_ILSVRC2012_val_00009308.JPEG}&
% \includegraphics[width=.22\textwidth]{figures/vgg19_bn/orig_mask_ILSVRC2012_val_00009308_0437.JPEG}&
% \includegraphics[width=.22\textwidth]{figures/vgg19_bn/adv_ILSVRC2012_val_00009308.JPEG}&
% \includegraphics[width=.22\textwidth]{figures/vgg19_bn/adv_mask_ILSVRC2012_val_00009308_1_0072_0437.JPEG}\\
% (a) Original Image & (b) Grad-CAM Orig & (c) Adv Image & (d) Grad-CAM Adv \\
%  \end{tabular}
% \newline
% \newline
% Visualization for category: Agama (Lizard)
%  \begin{tabular}{ c c c c}
% \includegraphics[width=.22\textwidth]{figures/vgg19_bn/orig_ILSVRC2012_val_00005031.JPEG}&
% \includegraphics[width=.22\textwidth]{figures/vgg19_bn/orig_mask_ILSVRC2012_val_00005031_0042.JPEG}&
% \includegraphics[width=.22\textwidth]{figures/vgg19_bn/adv_ILSVRC2012_val_00005031.JPEG}&
% \includegraphics[width=.22\textwidth]{figures/vgg19_bn/adv_mask_ILSVRC2012_val_00005031_1_0034_0042.JPEG}\\
% (a) Original Image & (b) Grad-CAM Orig & (c) Adv Image & (d) Grad-CAM Adv\\
% \end{tabular}
% \newline
% \newline
% Visualization for category: Stopwatch
%  \begin{tabular}{ c c c c}
% \includegraphics[width=.22\textwidth]{figures/vgg19_bn/orig_ILSVRC2012_val_00005281.JPEG}&
% \includegraphics[width=.22\textwidth]{figures/vgg19_bn/orig_mask_ILSVRC2012_val_00005281_0826.JPEG}&
% \includegraphics[width=.22\textwidth]{figures/vgg19_bn/adv_ILSVRC2012_val_00005281.JPEG}&
% \includegraphics[width=.22\textwidth]{figures/vgg19_bn/adv_mask_ILSVRC2012_val_00005281_1_0048_0826.JPEG}\\
% (a) Original Image & (b) Grad-CAM Orig & (c) Adv Image & (d) Grad-CAM Adv\\
% \end{tabular}
% \newline
% \newline
% Visualization for category: Eggnog
%  \begin{tabular}{ c c c c}
% \includegraphics[width=.22\textwidth]{figures/vgg19_bn/orig_ILSVRC2012_val_00015882.JPEG}&
% \includegraphics[width=.22\textwidth]{figures/vgg19_bn/orig_mask_ILSVRC2012_val_00015882_0969.JPEG}&
% \includegraphics[width=.22\textwidth]{figures/vgg19_bn/adv_ILSVRC2012_val_00015882.JPEG}&
% \includegraphics[width=.22\textwidth]{figures/vgg19_bn/adv_mask_ILSVRC2012_val_00015882_1_0097_0969.JPEG}\\
% (a) Original Image & (b) Grad-CAM Orig & (c) Adv Image & (d) Grad-CAM Adv\\
% \end{tabular}
%   \caption{GradCAM results for the original and adversarial images generated by our method with VGG-19-BN. Our attack changes the GradCAM of the original prediction by a large amount.}
%   \label{fig_vgg19_2}
%  \end{center}
% \end{figure*}
% \begin{figure*}[h!]
% \begin{center}Visualization for category: Lobster
%   \begin{tabular}{ c c c c}
% \includegraphics[width=.22\textwidth]{figures/vgg16_bn/orig_ILSVRC2012_val_00000111.JPEG}&
% \includegraphics[width=.22\textwidth]{figures/vgg16_bn/orig_mask_ILSVRC2012_val_00000111_0122.JPEG}&
% \includegraphics[width=.22\textwidth]{figures/vgg16_bn/adv_ILSVRC2012_val_00000111.JPEG}&
% \includegraphics[width=.22\textwidth]{figures/vgg16_bn/adv_mask_ILSVRC2012_val_00000111_1_0070_0122.JPEG}\\
% (a) Original Image & (b) Grad-CAM Orig & (c) Adv Image & (d) Grad-CAM Adv
%  \end{tabular}
% \newline
% \newline
% Visualization for category: Hair Slide
%  \begin{tabular}{ c c c c}
% \includegraphics[width=.22\textwidth]{figures/vgg16_bn/orig_ILSVRC2012_val_00000477.JPEG}&
% \includegraphics[width=.22\textwidth]{figures/vgg16_bn/orig_mask_ILSVRC2012_val_00000477_0584.JPEG}&
% \includegraphics[width=.22\textwidth]{figures/vgg16_bn/adv_ILSVRC2012_val_00000477.JPEG}&
% \includegraphics[width=.22\textwidth]{figures/vgg16_bn/adv_mask_ILSVRC2012_val_00000477_1_0188_0584.JPEG}\\
% (e) Original Image & (f) Grad-CAM Orig & (g) Adv Image & (h) Grad-CAM Adv
% \end{tabular}
% \newline
% \newline
% Visualization for category: Ostrich
%  \begin{tabular}{ c c c c}
% \includegraphics[width=.22\textwidth]{figures/vgg16_bn/orig_ILSVRC2012_val_00001398.JPEG}&
% \includegraphics[width=.22\textwidth]{figures/vgg16_bn/orig_mask_ILSVRC2012_val_00001398_0009.JPEG}&
% \includegraphics[width=.22\textwidth]{figures/vgg16_bn/adv_ILSVRC2012_val_00001398.JPEG}&
% \includegraphics[width=.22\textwidth]{figures/vgg16_bn/adv_mask_ILSVRC2012_val_00001398_1_0079_0009.JPEG}\\
% (a) Original Image & (b) Grad-CAM Orig & (c) Adv Image & (d) Grad-CAM Adv\\
% \end{tabular}
% \newline
% \newline
% Visualization for category: Earthstar (fungus)
%  \begin{tabular}{ c c c c}
% \includegraphics[width=.22\textwidth]{figures/vgg16_bn/orig_ILSVRC2012_val_00049858.JPEG}&
% \includegraphics[width=.22\textwidth]{figures/vgg16_bn/orig_mask_ILSVRC2012_val_00049858_0995.JPEG}&
% \includegraphics[width=.22\textwidth]{figures/vgg16_bn/adv_ILSVRC2012_val_00049858.JPEG}&
% \includegraphics[width=.22\textwidth]{figures/vgg16_bn/adv_mask_ILSVRC2012_val_00049858_1_0191_0995.JPEG}\\
% (a) Original Image & (b) Grad-CAM Orig & (c) Adv Image & (d) Grad-CAM Adv\\
% \end{tabular}
%   \caption{GradCAM results for the original and adversarial images generated by our method with VGG-16-BN. Our attack changes the GradCAM of the original prediction by a large amount.}
%   \label{fig_vgg16_2}
%  \end{center}
% \end{figure*}
\end{document}

%{\small
%\bibliographystyle{ieee}
%\bibliography{egbib}
%}

%\end{document}
