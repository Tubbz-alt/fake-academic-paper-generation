\documentclass{article} % For LaTeX2e
%\usepackage{nips15submit_e,times}
\usepackage{nips15submit_09,times}
\usepackage{hyperref}
\usepackage{url}

\usepackage{epsfig}
\usepackage{epstopdf}
\usepackage{graphicx}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[ruled,vlined,boxed]{algorithm2e}
\usepackage{multirow}
\usepackage[table]{xcolor} 
\definecolor{LightCyan}{rgb}{0.88,1,1}


%% Dash line by BM
\usepackage{arydshln}
\hdashlinewidth=0.5pt
\hdashlinegap=0.8pt

%%% thick line by BM
%\makeatletter
%\def\hlinewd#1{%
%\noalign{\ifnum0=`}\fi\hrule \@height #1 %
%\futurelet\reserved@a\@xhline} 
%\makeatother

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09



% For making changes
%\newcommand{\changeBM}[1]{\textcolor{blue}{#1}}  % Colors \changeBM{ }
%\newcommand{\changeVB}[1]{\textcolor{red}{#1}}  % Colors \changeVB{ }

\newcommand{\changeBM}[1]{#1} % Does not color \changeBM{ }
\newcommand{\changeVB}[1]{#1} % Does not color \changeVB{ }




\title{Understanding symmetries in deep networks}


%\author{
%Vijay Badrinarayanan, Bamdev Mishra\thanks{Department of EECS, University of Li\`ege, 4000 Li\`ege, Belgium (\texttt{b.mishra@ulg.ac.be}).}, Roberto Cipolla\\
%%\thanks{ Use footnote for providing further information
%%about author (webpage, alternative address)---\emph{not} for acknowledging
%%funding agencies.} \\
%Department of Engineering\\
%Cambridge University, UK\\
%\texttt{vb292,bm458,cipolla@cam.ac.uk} 
%%\And
%%Bamdev Mishra \\
%%Department of Engineering\\
%%Cambridge University, UK \\
%%\texttt{bm458@cam.ac.uk} \\
%%\AND
%%Roberto Cipolla\\
%%Department of Engineering\\
%%Cambridge University, UK \\
%%\texttt{cipolla@cam.ac.uk} \\
%}

\author{Vijay Badrinarayanan\\
Department of Engineering\\
Cambridge University, UK\\
\texttt{\small vb292@cam.ac.uk}\\
\And
Bamdev Mishra\thanks{This work was initiated while the author was with the Department of Electrical Engineering and Computer Science, University of Li\`ege, 4000 Li\`ege, Belgium and was visiting the Department of Engineering (Control Group), University of Cambridge, Cambridge, UK.}\\
Amazon Development Centre India\\
Bangalore, India\\
\texttt{\small bamdevm@amazon.com}\\
\And
Roberto Cipolla\\
Department of Engineering\\
Cambridge University, UK\\
\texttt{\small cipolla@cam.ac.uk}
}



% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% BM lists
\newcommand{\Diag}{{\rm Diag}}
\newcommand{\diag}{{\rm diag}}
\newcommand{\Orth}{{\rm Orth}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\begin{abstract}
\changeBM{Recent works} have highlighted scale invariance or symmetry present in the weight space of a typical deep network and the adverse effect it has on the Euclidean gradient based stochastic gradient descent optimization. In this work, we show that a commonly used deep network, which uses convolution, batch normalization, reLU, max-pooling, and sub-sampling pipeline, possess more complex forms of symmetry arising from scaling-based reparameterization of the network weights. \changeVB{We propose to tackle the issue of the weight space symmetry by constraining the filters to lie on the unit-norm manifold. \changeBM{Consequently, training the network boils down to using stochastic gradient descent updates on the unit-norm manifold.} Our empirical evidence based on the MNIST dataset shows that the \changeBM{proposed updates} improve the test performance beyond what is achieved with batch normalization and without sacrificing the \changeBM{computational} efficiency of the weight updates.} 

\end{abstract}

\section{Introduction}
Stochastic gradient descent (SGD) has been the workhorse for optimization of deep networks \cite{Bottou}. The most well-known form uses the Euclidean gradients with a varying learning rate to optimize the weights. In this regard, \changeBM{the} recent work \cite{PathSGD} has brought to light scale invariance properties in \changeBM{the} weight space which commonly used deep networks possess. These symmetries or \changeBM{invariance to} reparameterizations of the weights \changeBM{imply} that even though the \changeBM{loss function} remains \changeBM{unchanged}, the Euclidean gradient varies based on the chosen parameterization. \changeVB{Consequently, optimization trajectories can vary significantly for different reparameterizations \cite{PathSGD}.} 

Although these issues have been raised recently, the precursor to these methods is the early work of Amari \cite{Amari}, who proposed the use of \textit{natural gradients} to tackle weight space symmetries in neural networks. The idea is to compute the steepest descent direction for the weight update on the manifold defined by these symmetries and use this direction to update the weights \cite{PascanuNaturalGradient, DesjardinsNaturalNN, OllivierRiemmanianMetricsI, OllivierRiemmanianMetricsII}. \changeVB{Most of theses proposals are either computationally expensive to implement or they need modifications to the architecture.} \changeBM{On the other hand, optimization over a manifold with symmetries or invariances has been a topic of much research and provides guidance to other simpler metric constructions \cite{absil08a, mishra14b, boumal15a, journee10a, absil04b, edelman98a, manton02a}.}

%  . \changeBM{Recently,} Pascanu \& Bengio  \changeBM{proposed} a second order method using natural gradients for deep networks, but their iterative method to compute the inverse of the natural metric is expensive. Natural neural networks, on the other hand, define a reparameterization of the network weights such that \changeBM{the} Euclidean and natural gradient based updates are the same . They use a block-diagonal approximation of the Fisher information matrix \cite{PascanuNaturalGradient} as an approximate \emph{natural metric} \changeBM{(a particular inner product)} to \changeBM{motivate} their \changeBM{proposed} reparameterization. The works of Ollivier  define several metrics that are also based on approximations of the Fisher information matrix or the Hessian of the loss function to perform scale invariant optimization. 
  
\changeBM{In Section \ref{Architecture},} our analysis into a commonly used network \changeBM{shows that} there \changeBM{exists} more complex forms of symmetries which can affect optimization, and hence there is a need to define \changeBM{\emph{simpler}} weight updates which take into account these invariances. \changeBM{Accordingly, in Section \ref{Optimization}, we look at \changeBM{one particular} way of resolving the symmetries} \changeVB{by constraining the filters to lie on \changeBM{the} unit-norm manifold. This results from a geometric viewpoint on the manifold of the search space. The proposed \changeBM{updates, shown in Table \ref{ProposedUpdates}, are symmetry-invariant and are} numerically efficient to implement.}


\changeBM{The stochastic gradient descent algorithms with the proposed updates are implemented in Matlab and Manopt \cite{ManOpt}. The codes are available at \url{http://bamdevmishra.com/codes/deepnetworks}.}



%\changeBM{In Section \ref{Architecture},} we analyze the weight space symmetries which exist in a deep architecture commonly for classification, where each layer is composed of a fully connected network, the output of which is batch normalized \cite{BN}, followed by \emph{reLU non-linearity} and a \emph{max-pooling-sub-sampling} step. Such an architecture in its convolutional form is currently been used for practical problems such as image segmentation \cite{SegNetarXiv} and our analysis is also applicable to them. \changeBM{Section \ref{Optimization} discusses manifold optimization techniques to address the symmetries and we propose simple weight updates and give a motivation behind those. \changeBM{The proposed updates are shown in Table \ref{ProposedUpdates}}. Finally, numerical experiments are discussed in Sections \ref{Experiments}, \ref{Results}, and \ref{DeepConvNets}.}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Architecture and symmetry analysis}
\label{Architecture}
A two layer deep architecture, ArchBN, is shown in Figure \ref{ArchBN}. Each layer in ArchBN has typical components commonly found in convolutional neural networks \cite{LeCunNature} such as multiplication with a trainable weight matrix ($W_{1},W_{2}$), a batch normalization layer ($b_{1},b_{2}$) \cite{BN}, element-wise rectification ReLU, $2\times1$ max-pooling with stride 2, and sub-sampling. The final layer is a K-way soft-max classifier $\theta$. The network is trained with a cross-entropy loss. \changeVB{The rows of the weight matrices $W_{1}$ and $W_{2}$ correspond to filters in layers $1$ and $2$, respectively. The dimension of each row corresponds to the input dimension of the layer. For the MNIST digits dataset, the input is a $784$ dimensional vector. With $64$ filters in each of the layers, the dimensionality of $W_{1}$ is $64\times784$ and of $W_{2}$ is $32\times64$. The dimension of $\theta$ is $10\times32$, where each row corresponds to a trainable class vector.} 

\changeVB{The batch normalization \cite{BN} layer normalizes each feature (element) in the $h_{1}$ and $h_{2}$ layers to have zero-mean unit variance over each mini-batch. Then a separate and trainable scale and shift is applied to the resulting features to obtain $b_{1}$ and $b_{2}$, respectively. This effectively models the distribution of the features in $h_{1}$ and $h_{2}$ as Gaussian whose mean and variance are learnt during training. Empirical results in  \cite{BN} show that this significantly improves convergence and our experiments also support this claim.} \changeBM{A key observation is that the} normalization of $h_{1}$ and $h_{2}$ allows for complex symmetries to exist in the network. \changeBM{To this end,} consider the reparameterizations
\begin{align}
\label{weightsreparamBN}
\tilde{W}_{1} =  \alpha W_{1} \quad {\rm and} \quad \tilde{W}_{2} =  \beta W_{2}, 
\end{align}
where $\mathbf{\alpha} = \Diag(\alpha_{1},\alpha_{2},\alpha_{3},\alpha_{4},\alpha_{5},\alpha_{6},\alpha_{7},\alpha_{8}) $ and $ \mathbf{\beta} = \Diag(\beta_{1},\beta_{2},\beta_{3},\beta_{4},\beta_{5},\beta_{6},\beta_{7},\beta_{8})$ and the elements of $\alpha$ and $\beta$ can be any real number. $\Diag(\cdot)$ is an operator which creates a diagonal matrix with its argument placed along the diagonal. Due to batch normalization which makes $h_1$ and $h_2$ unit-variance, $y$ is unchanged, and hence the loss is invariant to reparameterizations (\ref{weightsreparamBN}) of the weights. \changeBM{Equivalently}, there exists continuous symmetries or reparameterizations of $W_1$ and $W_2$, which leave the loss function unchanged. \changeBM{It should be stressed that our analysis differs from \cite{PathSGD}, where the authors deal with a simpler case of $\alpha = \alpha_{0}$, $\beta = \frac{1}{\alpha_{0}}$, and $\alpha_0$ is a non-zero scalar}. 


\begin{figure*}
\centering
\includegraphics[width=0.7\textwidth]{Figures/Arch2.pdf}
\caption{\footnotesize{ArchBN: a two layer deep architecture for classification with batch normalization \cite{BN}.}}
\label{ArchBN}
\end{figure*}

\changeVB{Unfortunately, the Euclidean gradient of the weights (used in standard SGD) is not invariant to reparameterizations of the weights \cite{PathSGD}. Consequently, optimization trajectories can vary significantly based on the chosen parameterizations. This issue can be resolved either by defining a suitable non-Euclidean gradient which is invariant to reparameterizations (\ref{weightsreparamBN}) or by placing appropriate constraints on the filter weights as we show in the following section.}


%. This requires defining an appropriate non-Euclidean metric \changeBM{(an inner product)} on the manifold resulting from the symmetries in weight space \cite[Chapter~3]{absil08a}.


%
%
%Consider the following reparameterization of the trainable parameters
%\begin{align}
%\label{weightsreparam}
%\tilde{W}_{1} =  \alpha_{0} W_{1}, \tilde{W}_{2} =  \beta W_{2},  
%\end{align}
%for the network in Figure \ref{Arch1} with $8$ filters per layer, where $\alpha_{0}$ is a positive scalar and $\mathbf{\beta} = \Diag (\beta{1},\beta{1},\beta{2},\beta{2},\beta{3},\beta{3},\beta{4},\beta{4})$,
%where $\beta{i} > 0$ for $i = \{1,2, 3,4 \}$ and $\Diag(\cdot)$ is an operator which creates a diagonal matrix with its arguments placed along the diagonal. \changeBM{It should be noted that there are repeating elements along the diagonal, which comes out because of the max-pooling operation.} \changeBM{Under these} reparameterizations, the changes to the other intermediate outputs in layer $1$ are $\tilde{h_{1}} = \alpha_{0} h_{1}$, $\tilde{r_{1}} = \alpha_{0} r_{1}$, and $\tilde{m_{1}} = \alpha_{0} m_{1}$. \changeBM{Subsequently,} the effect on layer 2 are
%$\tilde{h_{2}} = \alpha_{0} \beta h_{2}$, $\tilde{r_{2}} = \alpha_{0} \beta r_{1}$, and $\tilde{m_{2}} = \alpha_{0} \beta_{s} m_{2}$, where $\beta_{s} = \Diag(\beta{1},\beta{2},\beta{3},\beta{4})$. 
%
%Now, let us reparameterize the class vectors (rows) of the $\theta$ matrix as
%\begin{align}
%\label{thetareparam}
%\tilde{\theta}_{k} = \frac{1}{\alpha_{0}} \beta_{s}^{-1} \theta_{k}, k = 1:K, 
%\end{align}
%then evaluating the predicted class probabilities we have,

%\begin{align*}
%\tilde{y}_{k} =& \frac{ e^{ \tilde{\theta}_{k}^{T} \tilde{m}_{2}}} {\sum_{l=1:K}e^{ \tilde{\theta}_{l}^{T} \tilde{m}_{2}}} \nonumber \\
% =& \frac{e^{\theta_{k}^{T}\frac{1}{\alpha_{0}}\beta_{s}^{-T}\alpha_{0}\beta_{s} m_{2} }}
%         {\sum_{l=1:K} e^{\theta_{l}^{T} \frac{1}{\alpha_{0}} \beta_{s}^{-T} \alpha_{0} \beta_{s} m_{2} }} \nonumber \\
% =& y_{k}.
%\end{align*}
%
%\begin{align*}
%\tilde{y}_{k} = \frac{ e^{ \tilde{\theta}_{k}^{T} \tilde{m}_{2}}} {\sum_{l=1:K}e^{ \tilde{\theta}_{l}^{T} \tilde{m}_{2}}}  =  \frac{e^{\theta_{k}^{T}\frac{1}{\alpha_{0}}\beta_{s}^{-T}\alpha_{0}\beta_{s} m_{2} }}
%         {\sum_{l=1:K} e^{\theta_{l}^{T} \frac{1}{\alpha_{0}} \beta_{s}^{-T} \alpha_{0} \beta_{s} m_{2} }}  = y_{k}.
%\end{align*}

%This can be seen \changeBM{in the simple example of a function $f : \mathbb{R} \rightarrow \mathbb{R}: x \mapsto f(x)$ that is invariant under the transformation $\tilde{x} = \alpha x$ for all $\alpha \in \mathbb{R}$, i.e., $f(\tilde{x}) = f(x)$. Equivalently,}
%\begin{equation}\label{EuclideanGradient}
%\begin{array}{ll}
%\displaystyle \frac{\partial f(\tilde{x})}{\partial \tilde{x}} =  \frac{\partial f(\tilde{x})}{\partial x} \frac{\partial x}{\partial \tilde{x}}  =  \frac{\partial f(x)}{\partial x} \frac{1}{\alpha},
%\end{array}
%\end{equation}
%\changeBM{where $ {\partial f(x)}/{\partial x}$ is the Euclidean gradient of $f$ at $x$. As is clear in (\ref{EuclideanGradient}), the Euclidean gradient is not invariant to reparameterizations, i.e., it scales \emph{inversely} to the scaling of the variable $x$. Consequently, the optimization trajectory can vary significantly based on the chosen parameterization. On the other hand, a scale-invariant gradient scales \emph{proportionally} to that of the scaling of the variable.} This issue can be resolved by defining a suitable non-Euclidean gradient which is invariant to the chosen parameterization. This requires defining an appropriate non-Euclidean metric \changeBM{(an inner product)} on the manifold resulting from the symmetries in weight space \cite[Chapter~3]{absil08a}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Resolving symmetry issues using manifold optimization}
\label{Optimization}
An efficient way to \changeBM{resolve} the symmetries that exist in ArchBN is to constrain the weight vectors (filters) in $W_{1}$ and $W_{2}$ to lie on the \emph{oblique manifold} \cite{absil08a, ManOpt}, i.e., each filter in the fully connected layers is constrained to have \changeBM{unit} norm (abbreviated UN). \changeVB{Equivalently, we impose the constraints $\diag (W_1 W_1^T) = 1$ and $\diag (W_2 W_2^T) = 1$, where $\diag(\cdot)$ is an operator which extracts the diagonal elements of the argument matrix.} To this end, consider a weight vector $w \in \mathbb{R}^{n}$ with the constraint $w^Tw = 1$. (For example, $w^T$ is a row of $W_1$.) The steepest descent direction for a loss $\ell(w)$ \changeBM{with $w$ on the unit-norm manifold} is computed $\tilde{\nabla} \ell(w) = \nabla \ell(w) - (w^{T}\nabla \ell(w)) w$, \changeBM{where $\nabla \ell(w)$ is the Euclidean gradient and $\tilde{\nabla} \ell(w)$ is the Riemannian gradient on the unit-norm manifold \cite[Chapter~3]{absil08a}.} Effectively, the normal component of the Euclidean gradient, i.e., $(w^{T}\nabla \ell(w)) w$, is subtracted to result in the tangential (to the unit-norm manifold) component. \changeBM{Following the tangential direction takes the update out of the manifold, which is then pulled back to the manifold with a \emph{retraction} operation \cite[Example~4.1.1]{absil08a}.} \changeBM{Finally}, an update of $w$ on the unit-norm manifold is of the form
\begin{equation}\label{UN}
\small{
\begin{array}{lll}
\tilde{\nabla} \ell(w) &= & \nabla \ell(w) - (w^{T}\nabla \ell(w)) w \\
\tilde{w}^{t+1}& = & w^{t} - \lambda \tilde{\nabla} \ell(w) \\ 
w^{t+1} &= & \tilde{w}^{t+1} / \| \tilde{w}^{t+1} \|, 
\end{array}
}
\end{equation}
where $w^t$ is the current weight, $w^{t+1}$ is the updated weight, $\nabla \ell(w)$ is the Euclidean gradient, and $\lambda$ is the learning rate. \changeBM{It should be noted that when $W_1$ and $W_2$ are constrained, the $\theta$ variable is reparameterization free.}

\changeVB{The proposed weight update (\ref{UN}) can be used in a stochastic gradient descent (SGD) setting which we use in our experiments described in the following section. \changeBM{It should be emphasized that the proposed update is numerically efficient to implement. The formulas are shown in Table \ref{ProposedUpdates}. The convergence analysis of SGD on manifolds follows the developments in \cite{Bottou, bonnabel13a}.}}

%
%\changeBM{We propose two ways of resolving the symmetries that arise in \changeVB{ArchBN}. First, we follow the approach of \cite{Amari, edelman98a, absil08a} to equip the search space with a new non-Euclidean metric to resolve the symmetries present. Second, we break the symmetries by forcing the filter weights to be on the unit-norm manifold. In both these case, our updates are simple to implement in a stochastic gradient descent setting on manifolds.} 
%
%
%Consider a weight vector $w \in \mathbb{R}^{n}$, the \changeVB{non-Euclidean} squared length of a small incremental vector $dw$ connecting $w$ and $w+dw$ is given by \changeVB{the Riemannian metric},
%%\begin{align}
%%\label{EuclideanMetric}
%%\| dw \|^{2} = \sum_{i=1}^{n} ( dw_{i} )^{2}.
%%\end{align}
%%For a non-Euclidean coordinate system, however, the notion of squared distance is given by \changeBM{the Riemannian metric}
%\begin{align}
%\label{RiemannianMetric}
%\| dw \|_G^{2} = dw^{T} G dw, 
%\end{align}
%where the matrix $G$ is a positive definite matrix. \changeBM{If $G $ is the identity matrix,} then the coordinate system in (\ref{RiemannianMetric}) is Euclidean. The steepest descent direction for a loss function $L(w)$ under the metric (\ref{RiemannianMetric}) is given by $\tilde{\nabla} L(w) = G^{-1}\nabla L(w)$, \changeBM{where $\nabla L(w)$ is the Euclidean gradient and $\tilde{\nabla} L(w)$ is the Riemannian gradient under the metric (\ref{RiemannianMetric}).} Consequently, the first order weight update is of the form
%\begin{equation}\label{SM}
%\begin{array}{lll}
%%\tilde{\nabla} L(w) &=& G^{-1}\nabla L(w) \\
%w^{t+1} &=& w^{t} - \lambda \tilde{\nabla} L(w),
%\end{array}
%\end{equation}
%\changeBM{where $w^t$ is the current weight, $w^{t+1}$ is the updated weight, and $\lambda$ is the learning rate. Specifically, to resolve the symmetries discussed in Section \ref{Architecture}, we propose the \emph{novel} Riemannian metric}
%\begin{equation}
%\label{MetricDef}
%\begin{array}{lll}
%G_{W_{1}} =  (\Diag ( \diag( W_{1}W_{1}^{T} )))^{-1}, G_{W_{2}} = ( \Diag ( \diag( W_{2}W_{2}^{T})))^{-1}, 
%\end{array}
%\end{equation}
%where $\Diag(\cdot)$ is an operator which creates a diagonal matrix with its arguments placed along the diagonal and $\diag(\cdot)$ is an operator which extracts the diagonal elements of the argument matrix. \changeVB{To show the invariance of this metric, we consider the invariance for $W_{1}$ in ArchBN as an example.}

%\changeBM{It should be noted that $G$ is defined separately for the weights $W_1$, $W_2$ in (\ref{RiemannianMetric}), which is invariant to the reparameterizations shown in (\ref{weightsreparamBN}) for ArchBN. The motivation behind the metric choice in (\ref{MetricDef}) comes from the classical notion of \emph{right} and \emph{left} invariance in geometry, but now restricted to diagonal elements. To show the invariance of the metric, we consider the invariance for $W_{1}$ in ArchBN as an example.}
%\[
%\begin{array}{lll}
%\| d\tilde{W_{1}} \|_{G_{\tilde{W_{1}}}}^{2} & = & d\tilde{W_{1}}^{T} G_{\tilde{W_{1}}} d\tilde{W_{1}}\\
% & =  & ( \alpha dW_{1} )^{T}( \Diag( \diag(\alpha W_{1} W_{1}^{T} \alpha^{T})  ))^{-1} ( \alpha dW_{1} ) \\
% & = &  dW_{1}^{T}\alpha^{T} \alpha^{-1} (\Diag( \diag(W_{1} W_{1}^{T}) ))^{-1} \alpha^{-T} \alpha dW_{1} \\
%%& = &  dW_{1}^{T} (\Diag( \diag(W_{1} W_{1}^{T}) ))^{-1} dW_{1} \\
%& = &  \| dW_{1} \|_{G_{W_{1}}}^{2},
%\end{array}
%\]

%\[
%\small{
%\begin{array}{lll}
%\| d\tilde{W_{1}} \|_{G_{\tilde{W_{1}}}}^{2} & = & d\tilde{W_{1}}^{T} G_{\tilde{W_{1}}} d\tilde{W_{1}}\\
% & =  & ( \alpha dW_{1} )^{T}( \Diag( \diag(\alpha W_{1} W_{1}^{T} \alpha^{T})  ))^{-1} ( \alpha dW_{1} ) \\
% & = &  dW_{1}^{T}\alpha^{T} \alpha^{-T} (\Diag( \diag(W_{1} W_{1}^{T}) ))^{-1} \alpha^{-1} \alpha dW_{1} \\
%%& = &  dW_{1}^{T} (\Diag( \diag(W_{1} W_{1}^{T}) ))^{-1} dW_{1} \\
%& = &  \| dW_{1} \|_{G_{W_{1}}}^{2},
%\end{array}
%}
%\]
%and therefore, the squared length (\ref{RiemannianMetric}) is left unchanged (i.e., the metric is invariant) under the considered reparameterization of $W_{1}$ in (\ref{weightsreparamBN}) \changeVB{and similarly for $W_{2}$}. We term the \changeBM{proposed} metric in (\ref{RiemannianMetric}) with the definition in (\ref{MetricDef}), collectively as the \textit{scaled metric} (SM).

%\begin{table*}[t]%
%\center \small
%\begin{tabular}{c|c}
% Scaled metric (SM) & Unit norm (UN) \\
%\hline
% & \\
% $\begin{array}[t]{lll}
%W_1^{t+1} = W_1^t - \lambda  G_{W_{1}^t}^{-1} {\nabla}_{W_1} L (W_1^t, W_2^t, \theta^t)
%\end{array} $ &  $\begin{array}[t]{lll}
%\tilde{{\nabla}}_{W_1} L (W_1^t, W_2^t, \theta^t) = \Pi_{W_1^t}({{\nabla}}_{W_1} L (W_1^t, W_2^t, \theta^t))\\
%W_1^{t+1} = {\Orth}({W_1^t} - \lambda \tilde{{\nabla}}_{W_1} L (W_1^t, W_2^t, \theta^t))
%\end{array} $ \\
% & \\
% $\begin{array}[t]{lll}
%W_2^{t+1} = W_2^t - \lambda  G_{W_{2}^t}^{-1} {\nabla}_{W_2} L (W_1^t, W_2^t, \theta^t)
%\end{array} $ & $\begin{array}[t]{lll}
%\tilde{{\nabla}}_{W_2} L (W_1^t, W_2^t, \theta^t) = \Pi_{W_2^t}({{\nabla}}_{W_2} L (W_1^t, W_2^t, \theta^t))\\
%W_2^{t+1} = {\Orth}({W_2^t} - \lambda \tilde{{\nabla}}_{W_1} L (W_1^t, W_2^t, \theta^t))
%\end{array} $  \\
%%& \\
%% $\begin{array}[t]{lll}
%%\theta^{t+1} = \theta^t - \lambda  G_{{\theta}^t}^{-1} {\nabla}_{\theta} L (W_1^t, W_2^t, \theta^t)
%%\end{array} $ & $\begin{array}[t]{lll}
%%\theta^{t+1} = \theta^t - \lambda  {\nabla}_{\theta} L (W_1^t, W_2^t, \theta^t)
%%\end{array} $ \\
%% & \\
%\hline
%\end{tabular}%
%\caption{\footnotesize{The proposed symmetry-invariant updates for a loss function $L(W_1, W_2, \theta)$ in ArchBN type network. Here $(W_1^t, W_2^t$ is the current weight, $(W_1^{t+1} , W_2^{t+1})$ is the updated weight, $\lambda$ is the learning rate, and ${\nabla}_{W_1} L (W_1^t, W_2^t,\theta^t)$,  ${\nabla}_{W_2} L (W_1^t, W_2^t, \theta^t)$ are the partial derivatives of the loss $L$ with respect to $W_1$, $W_2$, respectively at $(W_1^t, W_2^t$. The matrices $G_{W_{1}^t}$, $G_{W_{2}^t}$ are defined in (\ref{MetricDef}) at $(W_1^t, W_2^t)$.} The operator $\Orth(\cdot)$ normalizes the rows of the input argument. $\Pi_W(\cdot)$ is the linear projection operation that projects an arbitrary matrix onto the tangent space of the oblique manifold at an element $W$. Specifically, it is defined as $\Pi_W(Z) = Z - \Diag(\diag((ZW^T))W$ \cite{ManOpt}, where $\Diag(\cdot)$ is an operator which creates a diagonal matrix with its arguments placed along the diagonal and $\diag(\cdot)$ is an operator which extracts the diagonal elements of the argument matrix.}
%\label{ProposedUpdates}
%\end{table*}

\begin{table*}[t]%
\center \scriptsize %   \small %  
\begin{tabular}{|c c|}
\hline
$\begin{array}[t]{lll}
\tilde{{\nabla}}_{W_1} L (W_1^t, W_2^t, \theta^t) = \Pi_{W_1^t}({{\nabla}}_{W_1} L (W_1^t, W_2^t, \theta^t))\\
W_1^{t+1} = {\Orth}({W_1^t} - \lambda \tilde{{\nabla}}_{W_1} L (W_1^t, W_2^t, \theta^t))
\end{array} $ 
&
 $\begin{array}[t]{lll}
\tilde{{\nabla}}_{W_2} L (W_1^t, W_2^t, \theta^t) = \Pi_{W_2^t}({{\nabla}}_{W_2} L (W_1^t, W_2^t, \theta^t))\\
W_2^{t+1} = {\Orth}({W_2^t} - \lambda \tilde{{\nabla}}_{W_1} L (W_1^t, W_2^t, \theta^t))
\end{array} $  \\


& \\
\multicolumn{2}{|c|}{
$\begin{array}[t]{lll}
\theta^{t+1} = \theta^t - \lambda  {\nabla}_{\theta} L (W_1^t, W_2^t, \theta^t) 
\end{array} $ } \\

%& \\
% $\begin{array}[t]{lll}
%\theta^{t+1} = \theta^t - \lambda  G_{{\theta}^t}^{-1} {\nabla}_{\theta} L (W_1^t, W_2^t, \theta^t)
%\end{array} $ & $\begin{array}[t]{lll}
%\theta^{t+1} = \theta^t - \lambda  {\nabla}_{\theta} L (W_1^t, W_2^t, \theta^t)
%\end{array} $ \\
% & \\
\hline
\end{tabular}%
\caption{\footnotesize{The proposed UN symmetry-invariant updates for a loss function $L(W_1, W_2, \theta)$ in ArchBN. Here $(W_1^t, W_2^t ,\theta_t)$ is the current weight, $(W_1^{t+1} , W_2^{t+1}, \theta^{t+1})$ is the updated weight, $\lambda$ is the learning rate, and ${\nabla}_{W_1} L (W_1^t, W_2^t,\theta^t)$,  ${\nabla}_{W_2} L (W_1^t, W_2^t, \theta^t)$, and ${\nabla}_{\theta} L (W_1^t, W_2^t, \theta^t)$ are the partial derivatives of the loss $L$ with respect to $W_1$, $W_2$, and $\theta$, respectively at $(W_1^t, W_2^t, \theta ^t)$. The operator $\Orth(\cdot)$ normalizes the rows of the input argument. $\Pi_W(\cdot)$ is the linear projection operation that projects an arbitrary matrix onto the tangent space of the oblique manifold at an element $W$. It is defined as $\Pi_W(Z) = Z - \Diag(\diag((ZW^T))W$ \cite{ManOpt}.}}
\label{ProposedUpdates}
\end{table*}



%where $\Diag(\cdot)$ is an operator which creates a diagonal matrix with its arguments placed along the diagonal and $\diag(\cdot)$ is an operator which extracts the diagonal elements of the argument matrix.
%Another way to \changeBM{resolve} the symmetries that exist in ArchBN is to constrain the weight vectors (filters) in $W_{1}$ and $W_{2}$ to lie on the \emph{oblique manifold} \cite{absil08a, ManOpt}, i.e., each filter in the fully connected layers is constrained to have \changeBM{unit} Euclidean norm.  %\changeVB{Equivalently, the constraints are $\diag (W_1 W_1^T) = 1$ and $\diag (W_2 W_2^T) = 1$.
%%, where $\diag(\cdot)$ is an operator which extracts the diagonal elements of the argument matrix.} 
%Consider a weight vector $w \in \mathbb{R}^{n}$ with the constraint $w^Tw = 1$. (For example, $w$ is a row of $W_1$.) The steepest descent direction for a loss $L(w)$ \changeBM{with $w$ on the unit-norm manifold} is computed $\tilde{\nabla} L(w) = \nabla L(w) - (w^{T}\nabla L(w)) w$, \changeBM{where $\nabla L(w)$ is the Euclidean gradient and $\tilde{\nabla} L(w)$ is the Riemannian gradient on the unit-norm manifold \cite[Chapter~3]{absil08a}.}
%Effectively, the normal component of the Euclidean gradient, i.e., $(w^{T}\nabla L(w)) w$, is subtracted to result in the tangential component (to the unit-norm manifold) as the steepest descent direction. \changeBM{Following the tangential direction takes the update out of the manifold, which is then pulled back to the manifold with a \emph{retraction} operation \cite[Example~4.1.1]{absil08a}.} \changeBM{Finally}, an update of the weight $w$ is of the form
%\begin{equation}\label{UN}
%\small{
%\begin{array}{lll}
%\tilde{\nabla} L(w) &= & \nabla L(w) - (w^{T}\nabla L(w)) w \\
%\tilde{w}^{t+1}& = & w^{t} - \lambda \tilde{\nabla} L(w) \\ 
%w^{t+1} &= & \tilde{w}^{t+1} / \| \tilde{w}^{t+1} \|,
%\end{array}
%}
%\end{equation}
%where $w^t$ is the current weight, $w^{t+1}$ is the updated weight, and $\lambda$ is the learning rate. \changeBM{It should be noted when $W_1$ and $W_2$ are constrained, the $\theta$ variable is reparameterization free.}
%
%Both the proposed weight updates, \changeBM{(\ref{SM}) that is based on the scaled metric (SM) and (\ref{UN}) that is based on the unit-norm (UN) constraint}, can be used in a stochastic gradient descent (SGD) setting which we use in our experiments described in the following section. \changeBM{It should be emphasized that the proposed updates are numerically efficient to implement. The formulas are shown in Table \ref{ProposedUpdates}. The convergence analysis of SGD on manifolds follows the developments in \cite{Bottou, bonnabel13a}.}







%
%
%The well known proximal form \cite{} of the weight update equation for a weight vector $w$,
%\begin{align}
%w^{t+1} = \argmin_{w} \left\lbrace w^{T}\nabla L + \lambda \frac{1}{2}(w - w^{t})^{T} G (w - w^{t})  \right\rbrace. \nonumber \\
%\end{align}
%Here, $\nabla L$ is the Euclidean gradient of the loss $L(w)$. $G$ is a inner product matrix which is used to measure the change between two weight vectors.  $\lambda$ is a tunable parameter used to control the distance between two consecutive iterates.
%
%Solving the above optimization problem we have,
%\begin{align}
%\label{Proximal}
%w^{t+1} = w^{t} - \lambda G^{-1} \nabla L. 
%\end{align}
%If $G$ is defined to be identity, then it is the standard Euclidean steepest descent based weight update. 
%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Experiments and results}
\label{Experiments}
We train both two and four layer deep ArchBN to perform digit classification on the MNIST dataset ($60$K training and $10$K testing images). We use 64 features per layer. The digit images are rasterized into a $784$ dimensional vector as input to the network(s). No input pre-processing \changeBM{is} performed. The weights in each layer are drawn from a standard Gaussian and each filter is unit-normalized. The class vectors are also drawn from a standard Gaussian and unit-normalized.  

%We use SGD based optimization as this is the most widely used technique for training deep networks. The three different weight updates \changeBM{that} we compare within this first-order framework are scaled metric (SM), unit-norm (UN), and balanced SGD (B-SGD), where B-SGD is simply the Euclidean update, but wherein the starting values of filters and class vectors are unit-normalized. B-SGD is also studied \changeBM{as a benchmark algorithm} in \cite{PathSGD}. We choose a mini-batch size \changeBM{of} $100$ samples.

We use SGD-based optimization and use choose the base learning rate from the set $10^{-p}$ \changeBM{for $p \in \{2, 3, 4, 5\}$} for each training run. For finding the base learning rate, we create a validation set of $500$ images from the training set. We then train the network with a fixed learning rate using a randomly chosen set of $1000$ images for $50$ epochs. At the start of each epoch, the training set is randomly permuted and mini-batches are sampled in a sequence ensuring each training sample is used only once within an epoch. We record the validation error measured as the error per training sample for each candidate base learning rate. We then choose the candidate rate which corresponds to the lowest validation error and use this for training the network on the full training set. We repeat this whole process for $10$ training runs for each network to measure the mean and variance of the test error. \changeBM{We ignore the runs where the validation error diverged.} \changeVB{For each full dataset training run, we use the bold-driver protocol \cite{HintonBoldDriver} to anneal the learning rate.} We choose $50000$ randomly chosen samples as the training set and the remaining $10000$ samples for validation. We train for a minimum of $25$ epochs and a maximum of $60$ epochs. Training is terminated if either the training error is less than $10^{-5}$ or the validation error increases with respect to the one measured before $5$ epochs or successive validation error measurements differ less than $10^{-5}$. 



%
%For training each network, we use the two well known protocols for learning rate annealing or decay, the bold-driver protocol \cite{HintonBoldDriver} and the exponential decay protocol. For the exponential decay protocol, we choose a decay factor of $0.95$ after each epoch. In all, for each network, we use two protocols, three different weight update strategies, and ten training runs for each combination thus totaling sixty training runs.


%\begin{table*}[t]%
%%\centering
%%\hspace*{-1.8cm}
%%\small{
%\scriptsize{
%\begin{tabular}{|c|c|c|c|c|c|c|}
%\hline
%& \multicolumn{3}{|c|}{Arch1} & \multicolumn{3}{|c|}{Arch2}  \\
%\hdashline
%Protocol & B-SGD & SM & UN & B-SGD & SM & UN \\
%\hline
%\multicolumn{7}{|c|}{2 layer deep network} \\
%%\hdashline
%%%\cline{4-6}
%%Exp. decay & 0.0263 $\pm$ 0.0079 & 0.0283 $\pm$ 0.0062 & \textbf{0.0220} $\pm$ 0.0057 & \textbf{0.0216} $\pm$ 0.0038 & 0.0228 $\pm$ 0.0051 & 0.0230 $\pm$ 0.0036  \\
%\hdashline
%Bold driver & 0.0240 $\pm$ 0.0026 & 0.0271 $\pm$ 0.0076 & \textbf{0.0228} $\pm$ 0.0038 & 0.0206 $\pm$ 0.0024 & \textbf{0.0186} $\pm$ 0.0024 & 0.0199 $\pm$ 0.0046 \\
%\hline
%\multicolumn{7}{|c|}{4 layer deep network}\\
%%\hdashline
%%Exp. decay & 0.0277 $\pm$ 0.0045 & 0.0256 $\pm$ 0.0038 & \textbf{0.0215} $\pm$ 0.0049 & 0.0218 $\pm$ 0.0028 & \textbf{0.0204} $\pm$ 0.0050 & 0.0224 $\pm$ 0.0065 \\
%\hdashline
%Bold driver & 0.0253 $\pm$ 0.0060 & 0.0264 $\pm$ 0.0056 &\textbf{ 0.0244} $\pm$ 0.0101 & 0.0204 $\pm$ 0.0027 & 0.0188 $\pm$ 0.0033 & \textbf{0.0179} $\pm$ 0.0025 \\
%\hline
%\end{tabular}}%
%\caption{\footnotesize{The mean and standard deviation of the test error after the last training epoch measured over $10$ runs on the MNIST dataset}. \changeBM{Our proposed updates show competitive performance while resolving the symmetries.} }
%\label{TestErr}
%\end{table*}



%\begin{table*}[t]%
%\centering
%%\hspace*{-1.8cm}
%\small{
%%\scriptsize{
%\begin{tabular}{|c|c|c|c|}
%%\hline
%%& \multicolumn{3}{|c|}{Arch1} & \multicolumn{3}{|c|}{Arch2}  \\
%\hdashline
%Depth & B-SGD & SM & UN \\
%\hline
%%\multicolumn{7}{|c|}{2 layer deep network} \\
%%\hdashline
%%%\cline{4-6}
%%Exp. decay & 0.0263 $\pm$ 0.0079 & 0.0283 $\pm$ 0.0062 & \textbf{0.0220} $\pm$ 0.0057 & \textbf{0.0216} $\pm$ 0.0038 & 0.0228 $\pm$ 0.0051 & 0.0230 $\pm$ 0.0036  \\
%%\hdashline
%2 &  0.0206 $\pm$ 0.0024 & \textbf{0.0186} $\pm$ 0.0024 & 0.0199 $\pm$ 0.0046 \\
%\hline
%%\multicolumn{7}{|c|}{4 layer deep network}\\
%%\hdashline
%%Exp. decay & 0.0277 $\pm$ 0.0045 & 0.0256 $\pm$ 0.0038 & \textbf{0.0215} $\pm$ 0.0049 & 0.0218 $\pm$ 0.0028 & \textbf{0.0204} $\pm$ 0.0050 & 0.0224 $\pm$ 0.0065 \\
%%\hdashline
%4 & 0.0204 $\pm$ 0.0027 & 0.0188 $\pm$ 0.0033 & \textbf{0.0179} $\pm$ 0.0025 \\
%\hline
%\end{tabular}}%
%\caption{\footnotesize{The mean and standard deviation of the test error after the last training epoch measured over $10$ runs on the MNIST dataset}. \changeBM{Our proposed updates show competitive performance while resolving the symmetries.} }
%\label{TestErr}
%\end{table*}

\begin{table*}[t]%
\centering 
%\hspace*{-1.8cm}
%\small{
\scriptsize{
\begin{tabular}{|c|c|c|c|}
%\hline
%& \multicolumn{3}{|c|}{Arch1} & \multicolumn{3}{|c|}{Arch2}  \\
\hline
Depth & B-SGD &  UN \\
\hline
%\multicolumn{7}{|c|}{2 layer deep network} \\
%\hdashline
%%\cline{4-6}
%Exp. decay & 0.0263 $\pm$ 0.0079 & 0.0283 $\pm$ 0.0062 & \textbf{0.0220} $\pm$ 0.0057 & \textbf{0.0216} $\pm$ 0.0038 & 0.0228 $\pm$ 0.0051 & 0.0230 $\pm$ 0.0036  \\
%\hdashline
2 &  0.0206 $\pm$ 0.0024 &  \textbf{0.0199} $\pm$ 0.0046 \\
\hline
%\multicolumn{7}{|c|}{4 layer deep network}\\
%\hdashline
%Exp. decay & 0.0277 $\pm$ 0.0045 & 0.0256 $\pm$ 0.0038 & \textbf{0.0215} $\pm$ 0.0049 & 0.0218 $\pm$ 0.0028 & \textbf{0.0204} $\pm$ 0.0050 & 0.0224 $\pm$ 0.0065 \\
%\hdashline
4 & 0.0204 $\pm$ 0.0027 & \textbf{0.0179} $\pm$ 0.0025 \\
\hline
\end{tabular}}%
\caption{\footnotesize{The test error after the last training epoch measured over $10$ runs on the MNIST dataset. \changeBM{The proposed UN update shows a competitive performance while resolving the symmetries.}} }
\label{TestErr}
\end{table*}


%
%
%For each training run on the full dataset, we choose $50000$ randomly chosen samples as the training set and the remaining $10000$ samples for validation. We train for a minimum of $25$ epochs and a maximum of $60$ epochs. When the bold driver protocol is used, we terminate the training if either the training error is less than $10^{-5}$ or the validation error increases with respect to the one measured before $5$ epochs or successive validation error measurements differ less than $10^{-5}$. 






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section{Results and Analysis}\label{Results}

%\begin{table*}[tch]%
%%\centering
%%\hspace*{-1.8cm}
%\begin{tabular}{cc}
%\includegraphics[width=0.5\textwidth]{Figures/Basic_Euclidean_Expdecay_depth4.eps}
%&
%\includegraphics[width=0.5\textwidth]{Figures/BN_Euclidean_Bolddriver_depth4.eps}
%\\
%Arch1: B-SGD Exp. decay & Arch2: B-SGD Bold driver
%\\
%\includegraphics[width=0.5\textwidth]{Figures/Basic_Natural1_Expdecay_depth4.eps}
%&
%\includegraphics[width=0.5\textwidth]{Figures/BN_Natural1_Bolddriver_depth4.eps}\\
%Arch1: SM Exp. decay & Arch2: SM Bold driver
%\\
%\includegraphics[width=0.5\textwidth]{Figures/Basic_Riemannian_Expdecay_depth4.eps}
%&
%\includegraphics[width=0.5\textwidth]{Figures/BN_Riemannian_Bolddriver_depth4.eps}\\
%Arch1: UN Exp. decay & Arch2: UN Bold driver
%\end{tabular}
%\caption{\footnotesize{Trajectories for four layer deep Arch1 and Arch2 networks which result in the lowest mean test error. The one standard deviation band is shown along with the mean trajectory dotted lines.}}
%\label{Trajectory}
%\end{table*}

%The mean and standard deviation of the test error for various training combinations are tabulated in Table. \ref{TestErr}. From these quantitative figures we can observe that Arch2 performs significantly better than Arch1, lending support to use batch normalization of feature maps \cite{BN}. Arch1 results are characterized by high mean and large standard deviation (std) values for all the forms of weight update. There is, however, no clear indication from the results of the superiority of one protocol over the other. While the bold driver protocol improves the performance of B-SGD, it worsens the performance of SM and UN in the four layer deep network. The exponential decay protocol produces the best result in combination with the UN weight update and has better performance for SM and UN for the four layer deep network. The \changeBM{good} performance of UN can be explained by the fact that it allows for better gradient back propagation since the norm of the filters are constrained to unity while this is not the case for SM. 
%
%The Arch2 network which includes batch normalization \cite{BN} helps improve gradient back propagation by constraining the scales of the inputs to each layer.  With this, it is clearly visible from our results that both SM and UN perform much better than B-SGD with the Bold driver protocol. Both the mean and std of the test error are the lowest for SM and UN when both two and the four layer deep networks are considered. The bold driver protocol performs better than the exponential decay protocol for all the forms of weight update. This annealing protocol seems more suitable when sufficient regularization is in place as in Arch2 with SM and UN.
\changeVB{
From Table \ref{TestErr} we see that for both two and four layer deep networks, the mean test error is lower for UN as compared to balanced SGD (B-SGD) which is simply the Euclidean update, but where the starting values of filters and class vectors are unit-normalized. The lowest mean and variance in the test error is obtained when UN weight update is used for training a four layer deep network. The difference between the B-SGD and UN updates is more significant for the four layer deep network, thereby highlighting the performance improvement over what is achieved by \changeBM{standard} batch normalization in deeper networks. The use UN can also be seen as \changeBM{a} way to regularize the weights of the network during training without introducing any hyper-parameters, \changeBM{e.g., a weight decay term}.} It should also be noted that the performance difference between the two and four layer networks is not very large. This raises the question for future research as to whether some deep networks necessarily have to be that deep or it can be made shallower (and efficient) by better optimization \cite{Caruana}. 

%The best results \changeBM{are} obtained by choosing the appropriate architecture (Arch2), weight update (SM and UN) and learning rate annealing protocol (Bold driver). SM and UN absorb the symmetries present in Arch2 and help improve performance over what is achieved by \changeBM{standard} batch normalization alone. 
%
%We show the mean trajectory of the test error over the training epochs for four layer deep Arch1 and Arch2 in Figure \ref{Trajectory}. For Arch1, we show the results using the exponential decay protocol and for Arch2 we show results using the bold driver protocol. When using Bold driver, the training is terminated based on the stopping criterion at most after 30 epochs. 


%The general pattern that can be observed is that the mean and std of SM and UN are high in the transition phase but reduces significantly as the trajectories begin to plateau. Also, the initial test error is significantly lower for Arch2 as compared to Arch1 for all the forms of weight update with B-SGD the lowest. How do we explain all this???

%The quantitative results show that SM for a two layer deep network with the Bold driver protocol performs better than using the B-SGD update for training a four layer deep network with exponential decay of the learning rate. It is also noteworthy to observe that the performance difference between the two and four layer deep Arch2 network is not very large. This raises the question for future research as to whether some deep networks necessarily have to be that deep \cite{Caruana} or it can be made shallower (and efficient) by better optimization. 
\begin{figure*}[t]
\centering
\includegraphics[width=0.6\textwidth]{Figures/CamVidQualitative.pdf}
\caption{\footnotesize{SGD with the proposed UN weight updates, shown in Table \ref{ProposedUpdates}, for training SegNet \cite{SegNetarXiv}. The quality of the predictions as compared to the ground truth indicates a successful training of SegNet.}}
\label{CamVidQualy}
\end{figure*}

\section{Application to image segmentation}
\label{DeepConvNets}
We \changeBM{apply SGD with the proposed UN weight updates in Table \ref{ProposedUpdates} for} training SegNet, a deep convolutional network proposed for road scene image segmentation into multiple classes \cite{SegNetarXiv}. This network, although convolutional, possesses the same symmetries as those analyzed for ArchBN in (\ref{weightsreparamBN}). The network \changeBM{is} trained for 100 epochs on the CamVid \cite{GabeDataset} training set of 367 images. The predictions on some sample test images from  CamVid are shown in Figure \ref{CamVidQualy}. These qualitative results indicate the usefulness of \changeBM{symmetry-invariant} weight updates for larger networks that arise in practice. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%\section{Application to Image Segmentation}
%\label{DeepConvNets}
%
%\begin{figure*}
%\centering
%\includegraphics[width=1\textwidth]{Figures/CamVidQualitative.pdf}
%\caption{\footnotesize{Using SGD with the proposed UN weight update, shown in Table \ref{ProposedUpdates}, for training SegNet \cite{SegNetarXiv}. The quality of the predictions as compared to the ground truth indicates a successful training of the network.}}
%\label{CamVidQualy}
%\end{figure*}
%
%We \changeBM{apply SGD with the proposed UN weight update in Table \ref{ProposedUpdates} for} training SegNet, a deep convolutional network proposed for road scene image segmentation into multiple classes \cite{SegNetarXiv}. This network, although convolutional, possesses the same symmetries as those analyzed for Arch2 in (\ref{weightsreparamBN}). The network \changeBM{is} trained for 100 epochs on the CamVid \cite{GabeDataset} training set of 367 images. The predictions of the trained SegNet on some sample test images from the dataset can be seen in Figure \ref{CamVidQualy}. These qualitative results indicate the usefulness of our analysis and \changeBM{symmetry-invariant} weight updates for larger networks that arise in practice. 






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}
\label{Conclusion}
We have highlighted the symmetries that exist in the weight space of deep neural network architectures which are currently popular. \changeVB{These symmetries can be absorbed into gradient descent by applying a unit-norm constraint on the filter weights. This takes into account the manifold structure on which the weights of the network reside. The empirical results show the test performance can be improved using our proposed weight update technique on a modern architecture}. \changeBM{As a future research direction,} we would \changeVB{like to explore other efficient symmetry-invariant weight update techniques and exploit them for deep convolutional neural network \changeBM{used in} practical applications}.



\subsubsection*{Acknowledgments}
Bamdev Mishra was supported as an FNRS research fellow (Belgian Fund for Scientific Research). The scientific responsibility rests with its authors.



\small
\bibliographystyle{unsrt}  
\bibliography{optarXiv_BMC}

%[1] Alexander, J.A. \& Mozer, M.C. (1995) Template-based algorithms
%for connectionist rule extraction. In G. Tesauro, D. S. Touretzky
%and T.K. Leen (eds.), {\it Advances in Neural Information Processing
%Systems 7}, pp. 609-616. Cambridge, MA: MIT Press.

%[2] Bower, J.M. \& Beeman, D. (1995) {\it The Book of GENESIS: Exploring
%Realistic Neural Models with the GEneral NEural SImulation System.}
%New York: TELOS/Springer-Verlag.
%
%[3] Hasselmo, M.E., Schnell, E. \& Barkai, E. (1995) Dynamics of learning
%and recall at excitatory recurrent synapses and cholinergic modulation
%in rat hippocampal region CA3. {\it Journal of Neuroscience}
%{\bf 15}(7):5249-5262.
%@ARTICLE{GabeDataset,
%  author = {G. Brostow and J. Fauqueur and R. Cipolla},
%  title = {Semantic object classes in video: A high-definition ground truth
%	database},
%  journal = {PRL},
%  year = {2009},
%  volume = {30(2)},
%  pages = {88-97},
%  owner = {vb292},
%  timestamp = {2010.04.27}
%}



\end{document}
