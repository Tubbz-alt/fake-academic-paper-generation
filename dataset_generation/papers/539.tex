\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xspace}
\usepackage{bm}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,citecolor=blue,linkcolor=blue,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{326} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% For figures
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{rotating}
\usepackage[export]{adjustbox}
\makeatletter
\newcommand{\mycaption}[2]{\caption{\textbf{#1.}\xspace#2}}

% For Tables
\usepackage{booktabs}
\usepackage{array, makecell, tabularx}
\usepackage{multirow}


\def\model{SPLATNet\xspace}
\def\modelthree{SPLATNet$_{\text{3D}}$\xspace}
\def\modeljoint{SPLATNet$_{\text{2D-3D}}$\xspace}

\newcommand{\vjcomment}[1]{{\bf \textcolor{red}{VJ: #1}}}
\newcommand{\hscomment}[1]{{\bf \textcolor{magenta}{HS: #1}}}
\newcommand{\camreadycomment}[1]{{\textcolor{red}{#1}}}
\usepackage[normalem]{ulem}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
% \title{Bilateral Point Cloud Networks for 3D Segmentation}

\title{SPLATNet: Sparse Lattice Networks for Point Cloud Processing}

\author{Hang Su\\
UMass Amherst
\and
Varun Jampani\\
NVIDIA
\and
Deqing Sun\\
NVIDIA
\and
Subhransu Maji\\
UMass Amherst
\and
Evangelos Kalogerakis\\
UMass Amherst
\and
Ming-Hsuan Yang\\
UC Merced
\and
Jan Kautz\\
NVIDIA
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
We present a network architecture for processing point clouds that directly operates on a collection of points represented as a sparse set of samples in a high-dimensional lattice.
Na\"ively applying convolutions on this lattice scales poorly, both in terms of memory and computational cost, as the size of the lattice increases.
Instead, our network uses sparse bilateral convolutional layers as building blocks.
These layers maintain efficiency by using indexing structures to apply convolutions only on occupied parts of the lattice, and allow flexible specifications of the lattice structure enabling hierarchical and spatially-aware feature learning, as well as joint 2D-3D reasoning. 
Both point-based and image-based representations can be easily incorporated in a network with such layers and the resulting model can be trained in an end-to-end manner.
We present results on 3D segmentation tasks where our approach outperforms existing state-of-the-art techniques.
\end{abstract}\vspace{-3mm}

%%%%%%%%% BODY TEXT
\section{Introduction}% Point clouds and CNNs
Data obtained with modern 3D sensors such as laser scanners is predominantly in the \emph{irregular} format
of point clouds or meshes.
Analysis of point clouds has several useful applications such as robot
manipulation and autonomous driving. In this work, we aim to develop a new neural network architecture
for point cloud processing.

% About CNNs for 3D data
A point cloud consists of a \emph{sparse} and \emph{unordered} set of 3D points. 
These properties of point clouds make it difficult to use traditional convolutional neural network (CNN)
architectures for point cloud processing. As a result, existing approaches that directly operate on
point clouds are dominated by hand-crafted features. One way to use CNNs for point clouds is by
first pre-processing a given point cloud in a form that is amenable to standard spatial convolutions.
Following this route, most deep architectures for 3D point cloud analysis require pre-processing
of irregular point clouds into either voxel representations (\eg, ~\cite{wu2015shapenets,riegler2017octnet,wang2017ocnn}) or 
2D images by view projection (\eg, ~\cite{su15mvcnn,qi2016volmv,kalogerakis2017shapepfcn,cao2017sphericalprojection}).
This is due to the ease of implementing convolution operations on regular 2D or 3D grids.
However, transforming point cloud representation to either 2D images or 3D voxels would often result in artifacts and
more importantly, a loss in some natural invariances present in point clouds.

% More recent works that directly work on point clouds
Recently, a few network architectures~\cite{qi2017pointnet,qi2017pointnetpp,zaheer2017deep} have been developed to directly work on point clouds.
One of the main drawbacks of these architectures is that they do not allow a flexible specification of
the extent of spatial connectivity across points (filter neighborhood). 
Both~\cite{qi2017pointnet} and~\cite{qi2017pointnetpp} use max-pooling to aggregate information across points either
globally~\cite{qi2017pointnet} or in a hierarchical manner~\cite{qi2017pointnetpp}.
This pooling aggregation may lose surface information because the spatial layouts of points are not explicitly considered.
It is desirable to capture spatial relationships in point clouds through more general convolution
operations while being able to specify filter extents in a flexible manner.

\begin{figure}
\begin{center}
\centerline{\includegraphics[width=1.0\columnwidth]{figures/splatnet_2d3d_compact.pdf}}
  \vspace{-0.2cm}
  \mycaption{From point clouds and images to semantics}{SPLATNet$_{\text{3D}}$ directly takes point cloud
  as input and predicts labels for each point. SPLATNet$_{\text{2D-3D}}$, on the other hand, jointly processes
  both point cloud and the corresponding multi-view images for better 2D and 3D predictions.}
  \label{fig:teaser}
\end{center}
\vspace{-0.8cm}
\end{figure}% Briefly about our technique
In this work, we propose a generic and flexible neural network architecture for processing point clouds
that alleviates some of the aforementioned issues with existing deep architectures.
Our key observation is that the bilateral convolution layers (BCLs) proposed in~\cite{jampani2016learning,kiefel:iclr:2015} have several favorable properties
for point cloud processing. BCL provides a systematic way of filtering unordered points while enabling 
%flexible specification of underlying lattice structure %\camreadycomment{flexible specifications of the underlying lattice structure}
flexible specifications of the underlying lattice structure
on which the convolution operates.
BCL smoothly maps input points onto a sparse lattice, performs 
%standard
convolutions on the sparse lattice
and then smoothly interpolates the filtered signal back onto the original input points.
With BCLs as building blocks, we propose a new neural network architecture,
which we refer to as \model~(SParse LATtice Networks), that does hierarchical and spatially-aware feature learning for unordered points.
% How is it better than existing methods?% The proposed \model has several advantages for point cloud processing: 
\vspace{-1mm}\begin{itemize}
    \setlength\itemsep{0em}
    \item \model~takes the point cloud as input and does not require any pre-processing to voxels or images.
    \item \model~allows an easy specification of filter neighborhood as in standard CNN architectures.
    \item With the use of hash table, our network can efficiently deal with sparsity in the input point cloud by convolving  only at locations where data is present.
    \item SPLATNet computes hierarchical and spatially-aware features of an input point cloud with sparse
    and efficient lattice filters.
    % \item BCL allows for separate specifications of input and also the lattice space on which the convolution operates. This decoupling between the input and the underlying lattice allows the projection of point clouds into higher-dimensional spaces (say, $XYZRGB$), which enables longer range connectivity between the points.%
    \item In addition, our network architecture allows an easy mapping of 2D points into 3D space and vice-versa. Following this,
    we propose a joint 2D-3D deep architecture that processes both the multi-view 2D images and the corresponding 3D point cloud in a single forward pass while being end-to-end learnable.
\end{itemize}\vspace{-1mm}% Briefly about the experiments%\camreadycomment{
The inputs and outputs of two versions of the proposed network, SPLATNet$_{\text{3D}}$ and SPLATNet$_{\text{2D-3D}}$, 
are depicted in Figure~\ref{fig:teaser}.
We demonstrate the above advantages with experiments on point cloud segmentation. 
% v0 - submission% Experiments on two different benchmark datasets of RueMonge2014~\cite{riemenschneider2014learning} for facade segmentation% and ShapeNet~\cite{yi2016scalable} for part segmentation % demonstrate the superior performance of our technique compared to state-of-the-art techniques, % while being computationally efficient.% Specifically, in the case of facade segmentation, \model~ significantly% outperforms prior state-of-the-art on both multi-view image labeling and point cloud labeling.% In addition, in the case of ShapeNet part segmentation, \model~outperforms existing state-of-the-art techniques.% v1 - still too verbose (HS)%Experiments on two different benchmarks %demonstrate the superior performance of our technique compared to state-of-the-art techniques, %while being computationally efficient.%Specifically, on the RueMonge2014 dataset~\cite{riemenschneider2014learning}, \model~ significantly%outperforms prior state-of-the-art methods on both multi-view image labeling and point cloud labeling.%In addition, in the case of ShapeNet~\cite{yi2016scalable} part segmentation, \model~outperforms existing state-of-the-art techniques.
Experiments on both RueMonge2014 facade segmentation~\cite{riemenschneider2014learning} and ShapeNet part segmentation~\cite{yi2016scalable} 
demonstrate the superior performance of our technique compared to state-of-the-art techniques, 
while being computationally efficient.
%}

\section{Related Work}\label{sec:related}

Below we briefly review existing deep learning approaches for 3D shape processing and explain differences with our work.

\vspace{-0.35cm}\paragraph{Multi-view and voxel networks.} Multi-view networks pre-process shapes into a set of 2D rendered images encoding surface depth and normals under various 2D projections~\cite{su15mvcnn,qi2016volmv,bai2016gift,kalogerakis2017shapepfcn,cao2017sphericalprojection,huang2018lmvcnn}. These networks take advantage of high resolution in the input rendered images and transfer learning through fine-tuning of 2D pre-trained image-based architectures. On the other hand, 2D projections can cause surface information loss due to self-occlusions, while viewpoint selection is often performed through heuristics that are not necessarily optimal for a given task.

Voxel-based methods convert the input 3D shape representation into a 3D volumetric grid. Early voxel-based architectures executed convolution in regular, fixed voxel grids, and were limited to low shape resolutions due to high memory and computation costs~\cite{wu2015shapenets,maturana2015voxnets,qi2016volmv,brock2016anothervoxnet,garciagarcia2016pointnetwhichisvoxnetactually,sedaghat2017orion}. Instead of using fixed grids, more recent approaches pre-process the input shapes into adaptively subdivided, hierarchical grids with denser cells placed near the surface ~\cite{riegler2017octnet,riegler2017OctNetFusion,klokov2017escape,wang2017ocnn,tatarchenko2017octree}. As a result, they have much lower computational and memory overhead.  On the other hand, convolutions are often still executed away from the surface, where most of the shape information resides. An alternative approach is to constrain the execution of volumetric convolutions only along the input sparse set of active voxels of the grid~\cite{graham2017ssnet}. 
%Our approach can be seen as a generalization of sparse grid convolutions to more general ones (permutohedral lattice convolutions). %\camreadycomment{
Our approach generalizes this idea to high-dimensional permutohedral lattice convolutions.
%}
In contrast to previous work, we do not require pre-processing points into voxels that may cause discretization artifacts and surface information loss. We smoothly map the input surface signal to our sparse lattice, perform convolutions over this lattice, and smoothly interpolate the filter responses back to the input surface. In addition, our architecture can easily incorporate feature representations originating from both 3D point clouds and rendered images within the same lattice, getting the best of both worlds. 

\vspace{-0.35cm}\paragraph{Point cloud networks.} Qi \etal~\cite{qi2017pointnet} pioneered another type of deep networks having the advantage of directly operating on point clouds. The networks learn spatial feature representations for each input point, then the point features are aggregated across the whole point set~\cite{qi2017pointnet}, or hierarchical surface regions~\cite{qi2017pointnetpp} through max-pooling. This aggregation may lose surface information since the spatial layout of points is not explicitly considered. In our case, the input points are mapped to a sparse lattice where convolution can be efficiently formulated and spatial relationships in the input data can be effectively captured through flexible filters. 
%In our experiments section, we demonstrate better performance than the latest hierarchical formulation of point-based nets. \vspace{-0.35cm}\paragraph{Non-Euclidean networks.} An alternative approach is to represent the input surface as a graph (\eg, a polygon mesh or point-based connectivity graph), convert the graph into its spectral representation, then perform convolution in the spectral domain~\cite{Bruna2013spectral,Henaff2015spectral,Defferrard2016spectral,Boscaini2015spectral}. 
%Shapes with different graph structures tend to have largely different spectral bases, thus these networks do not generalize well across structurally different shapes. % The shape basis functions can be aligned through a spectral transformer~\cite{yi2017syncspeccnn}, which, however, requires a robust initialization scheme.%\camreadycomment{%Structurally different shapes tend to have largely different spectral bases, and thus lead to poor generalization. The shape basis functions can be aligned through a spectral transformer~\cite{yi2017syncspeccnn}, which, however, requires a robust initialization scheme.}
However, structurally different shapes tend to have largely different spectral bases, and thus lead to poor generalization. 
Yi \etal~\cite{yi2017syncspeccnn} proposed aligning shape basis functions through a spectral transformer, which, however, requires a robust initialization scheme.
 Another class of methods embeds the input shapes into 2D parametric domains and then execute convolutions within these domains~\cite{Sinha2016,Maron2017CNN,Ezuz2017}. However, these embeddings can suffer from spatial distortions or require topologically consistent input shapes. Other methods parameterize the surface into local patches and execute surface-based convolution within these patches~\cite{masci2015geodesic,Boscaini2016,Monti2017}. Such non-Euclidean networks have the advantage of being invariant to surface deformations, yet this invariance might not always be desirable in man-made object segmentation and classification tasks where large deformations may change the underlying shape or part functionalities and semantics. We refer to Bronstein \etal~\cite{bronstein2017geometric} for an excellent review of spectral, patch- and graph-based methods.

\vspace{-0.35cm}\paragraph{Joint 2D-3D networks.} FusionNet~\cite{hedge2016fusionnet} combines shape classification scores from a volumetric and a multi-view network, yet this fusion happens at a late stage, after the final fully connected layer of these networks, and does not jointly consider their intermediate local and global feature representations. In our case, the 2D and 3D feature representations are mapped onto the same lattice, enabling end-to-end learning from both types of input representations.

% \paragraph{Sparse high-dimensional filtering.} % \vjcomment{Write about graph neural networks and sparse high-dimensional filters}

\section{Bilateral Convolution Layer}\label{sec:review_bcl}

In this section, we briefly review the Bilateral Convolution Layer (BCL) 
that forms the basic building block of our SPLATNet architecture for point clouds.
% BCL, propsoed in \cite{kiefel:iclr:2015,jampani2016learning}, 
BCL provides a way to incorporate sparse high-dimensional
filtering inside neural networks. In~\cite{jampani2016learning,kiefel:iclr:2015}, BCL was proposed as a learnable generalization
of bilateral filtering~\cite{tomasi1998bilateral,aurich1995non}, 
hence the name `Bilateral Convolution Layer'.
Bilateral filtering involves a projection of a given 2D image into
a higher-dimensional space (\eg, space defined by position and color) and
is traditionally limited to hand-designed filter kernels. BCL provides a way to learn filter
kernels in high-dimensional spaces for bilateral filtering.
BCL is also shown to be useful for information propagation across video 
frames~\cite{jampani2017video}.
We observe that BCL has several favorable properties to
filter data that is inherently sparse and high-dimensional, like point clouds.
Here, we briefly describe how a BCL works and then discuss its properties.

\begin{figure}
\vspace{-0.2cm}
\begin{center}
\centerline{\includegraphics[width=1.1\columnwidth]{figures/permutohedral_illustration.pdf}}
  \vspace{-0.2cm}
  \mycaption{Bilateral Convolution Layer}{\emph{Splat}: BCL first interpolates input features $F$ onto a $d_l$-dimensional permutohedral lattice defined by the lattice features $L$ at input points. \emph{Convolve}: BCL then does $d_l$-dimensional convolution over this sparsely populated lattice. \emph{Slice}: The filtered signal is then interpolated back onto the input signal. For illustration, input and output are shown as point cloud and the corresponding segmentation labels.}
  \label{fig:bcl}
\end{center}
\vspace{-.8cm}
\end{figure}%\vspace{-0.3cm}\subsection{Inputs to BCL}%\camreadycomment{
Let $F \in \mathbb{R}^{n \times d_f}$ be the given \emph{input features} to a BCL, where $n$ denotes
the number of input points and $d_f$ denotes the dimensionality of input features at each point.
%For point clouds, $n$ denotes the number of points and $k$ denotes the features at each point.
For 3D point clouds, input features can be low-level features such as color, position, \etc, and can also
% be high-level features generated by a neural network or some other feature learning technique.
be high-level features such as features generated by a neural network.
%}

One of the interesting characteristics of BCL is that it allows a flexible specification
of the lattice space in which the convolution operates. This is specified as \emph{lattice features} at 
each input point. Let $L \in \mathbb{R}^{n \times d_l}$ denote lattice features at input points
with $d_l$ denoting the dimensionality of the feature space in which convolution operates.
For instance, the lattice features can be 
% positional and color features ($XYZRGB$) 
point position and color ($XYZRGB$)
that define a 6-dimensional filtering space for BCL. For standard 3D spatial filtering of point clouds,
% $L$ is given as positional features ($XYZ$) at each point. $L$ is given as the position ($XYZ$) of each point.
Thus BCL takes input features $F$ and lattice features $L$ of input points and performs $d_l$-dimensional filtering of the points. 

\begin{figure*}[!ht]
    \vspace{-0.2cm}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/splatnet_2d3d_v3.pdf}
    \vspace{-0.2cm}
    \mycaption{SPLATNet}{Illustration of inputs, outputs and network architectures for SPLATNet$_{\text{3D}}$ and SPLATNet$_{\text{2D-3D}}$.}
    \label{fig:splatnet}
    \vspace{-0.25cm}
\end{figure*}%\vspace{-0.3cm}\subsection{Processing steps in BCL}
As illustrated in Figure~\ref{fig:bcl}, 
%BCL has three processing steps of \emph{splat}, \emph{convolve} and \emph{slice} that work as follows. 
BCL has three processing steps, \emph{splat}, \emph{convolve} and \emph{slice}, that work as follows.

\vspace{-0.35cm}\paragraph{Splat.} BCL first projects the input features $F$ onto the $d_l$-dimensional
lattice defined by the lattice features $L$, via barycentric interpolation. 
Following~\cite{adams2010fast}, BCL uses a permutohedral
lattice instead of a standard Euclidean grid for efficiency purposes. The size of lattice simplices
or space between the grid points is controlled by scaling the lattice features $\Lambda L$,
where $\Lambda$ is a diagonal $d_l\times d_l$ scaling matrix.

\vspace{-0.35cm}\paragraph{Convolve.} Once the input points are projected onto the $d_l$-dimensional lattice,
BCL performs $d_l$-dimensional convolution on the splatted signal with learnable filter kernels.
% This step can be thought of as a standard $s$-dimensional spatial convolution. 
Just like in standard spatial CNNs, BCL allows an easy specification of filter neighborhood in the $d_l$-dimensional space.

\vspace{-0.35cm}\paragraph{Slice.} The filtered signal is then mapped back to the input points via barycentric
interpolation. The resulting signal can be passed on to other BCLs for further processing. 
This step is called `slicing'. BCL allows slicing the filtered signal onto a different set
of points other than the input points. 
%This is achieved by a specification of different%set of lattice features $L^{out}\in \mathbb{R}^{m \times d_l}$ at $m$ output points of interest.
This is achieved by specifying a different
set of lattice features $L^{out}\in \mathbb{R}^{m \times d_l}$ at $m$ output points of interest.

All the above three processing steps in BCL can be written as matrix multiplications:
%\vspace{-0.2cm}\begin{equation}
\vspace{-0.2cm}
    \hat{F}_c = S_{slice} B_{conv} S_{splat} F_c,
\end{equation}%
where $F_c$ denotes the $c^{th}$ column/channel of the input feature $F$ and $\hat{F}_c$ denotes
the corresponding filtered signal.

%\vspace{-0.3cm}\subsection{Properties of BCL}

There are several properties of BCL that makes it particularly convenient for point cloud
processing. Here, we mention some of those properties:
\vspace{-1mm}\begin{itemize}
    \setlength\itemsep{0mm}
    \item The input points to BCL need not be ordered or lie on a grid as
    they are projected onto a $d_l$-dimensional grid defined by lattice features $L^{in}$.
    \item The input and output points can be different for BCL with 
    the specification of different input and output lattice features $L^{in}$ and $L^{out}$.
    \item Since BCL allows separate specifications of input and lattice
    features, input signals can be projected into a different dimensional space for filtering.
    For instance, a 2D image can be projected into 3D space for filtering.
    \item Just like in standard spatial convolutions, BCL allows an easy specification
    of filter neighborhood. 
%\camreadycomment{\sout{This allows for flexible neural network architectures.}}
    \item Since a signal is usually sparse in high-dimension, BCL uses
    hash tables to index the populated vertices and does convolutions only at those locations.
    This helps in efficient processing of sparse inputs.
\end{itemize}\vspace{-1mm}

Refer to~\cite{adams2010fast} for more information about sparse high-dimensional Gaussian filtering
on a permutohedral lattice and refer to~\cite{jampani2016learning} for more details on BCL.


\section{SPLATNet$_{\text{3D}}$ for Point Cloud Processing}\label{sec:bpcn_3d}We first introduce SPLATNet$_{\text{3D}}$, an instantiation of our proposed network architecture which operates directly on 3D point clouds and is readily applicable to many important 3D tasks. 
The input to SPLATNet$_{\text{3D}}$ is a 3D point cloud $P \in \mathbb{R}^{n \times d}$, where $n$ denotes the number of points and $d \ge 3$ denotes the number of feature dimensions including point locations $XYZ$. 
Additional features are often available either directly from 3D sensors 
or through pre-processing. These can be RGB color, surface normal, curvature, \etc. at the input points.
%\camreadycomment{
Note that input features $F$ of the first BCL and lattice features $L$ in the network each 
comprises a subset of the $d$ feature dimensions: $d_f\le d, d_l \le d$.
%}

As output, SPLATNet$_{\text{3D}}$ produces per-point predictions. Tasks like 3D semantic segmentation and 
3D object part labeling fit naturally under this framework. 
With simple techniques such as global pooling \cite{qi2017pointnet}, SPLATNet$_{\text{3D}}$ can be modified to 
produce a single output vector and thus can be extended to other tasks such as classification.
% scalar predictions are output instead. % We first describe different components that constitute our SPLATNet architecture and then explain% the network architecture.% Below we first go over the building blocks used in SPLATNet$_{\text{3D}}$, and then explain how they % are organized in our network. \vspace{-0.35cm}\paragraph{Network architecture.}
The architecture of SPLATNet$_{\text{3D}}$ is depicted in Figure~\ref{fig:splatnet}. The network starts with a single $1\times1$ CONV layer followed by a series of BCLs. The $1\times1$ CONV layer processes
each input point separately without any data aggregation. The functionality of BCLs is already explained
in Section~\ref{sec:review_bcl}. For SPLATNet$_{\text{3D}}$, we use $T$ BCLs each operating on
a 3D lattice ($d_l=3$) constructed using 
%3D positional features $XYZ$ at input points, 
3D point locations $XYZ$ as lattice features,
$L^{in} = L^{out} \in \mathbb{R}^{n \times 3}$. 
We note that different BCLs can use different lattice scales $\Lambda$.
Recall from Section~\ref{sec:review_bcl} that $\Lambda$ is a diagonal matrix that controls the spacing between the grid points in the lattice. For BCLs in \modelthree, we use the same lattice scales along each of the $X$, $Y$ and $Z$ directions, \ie, $\Lambda = \lambda I_3$, where $\lambda$ is a scalar and $I_3$ denotes a $3\times 3$ identity matrix.
We start with an initial lattice scale $\lambda_0$ for the first BCL and subsequently divide the lattice scale by a factor of 2 ($\lambda_t = \lambda_{t-1} / 2$) 
% until the lattice scale becomes 2 for $T^{th}$ BCL. 
for the next $T-1$ BCLs. 
In other words, \modelthree with $T$ BCLs use the following lattice scales: $(\Lambda_0, \Lambda_0/2, \dots, \Lambda_0/2^{T-1})$.
Lower lattice scales imply coarser lattices and larger receptive fields for the filters. 
Thus, in SPLATNet$_{\text{3D}}$, deeper BCLs have longer-range connectivity between input points compared 
to earlier layers. We will discuss more about the effects of different lattice spaces and their scales later.
Like in standard CNNs, SPLATNet allows an easy specification of filter neighborhoods.
For all the BCLs, we use filters operating on 
%1-neighborhoods (\ie, one-ring) 
one-ring neighborhoods
and refer to the supp. material
for details on the number of filters per layer.

The responses of the $T$ BCLs are concatenated and then passed through two additional $1\times1$ CONV layers. 
%Then the output of the final layer passes through a SoftMax layer, which produces point-wise class label probabilities. 
Finally, a softmax layer produces point-wise class label probabilities.
The concatenation operation aggregates information from BCLs operating at different lattice scales. 
% Similar techniques of concatenation from network layers at different depths 
Similar techniques of concatenating outputs from network layers at different depths
have been useful in 2D CNNs~\cite{hariharan2015hypercolumns}.
All parameterized layers, except for the last CONV layer, are followed by ReLU and BatchNorm.
More details about the network architecture are given in the supp. material.

% \subsection{Components of a SPLATNet}% Our 3D network builds on top of a handful of network layer types. Note that despite they are very % different operations, all of them operate directly on a set of points and output point-wise % responses. This allows them to be combined flexibly in the network. % \vspace{-0.2cm}% \paragraph{BCL} A BCL consumes three sets of features. The $i^{\text{th}}$ BCL in the network takes input feature $P_i \in \mathbb{R}^{n_i \times k_i}$, input lattice feature $L_i^{in} \in \mathbb{R}^{n_i \times s_i}$, and output lattice feature $L_i^{out} \in \mathbb{R}^{{n'}_i \times s_i}$. % In SPLATNet$_{\text{3D}}$, each BCL produces filtered signals on the given input points. This is achieved by using the same input lattice feature and output lattice feature throughout the network, \ie% $L_i^{in}=L_i^{out}=L_i$ for all BCLs in the network.  In general, input lattice feature and output lattice need not to be the same --- a such use case is explored in Sec.~\ref{sec:bpcn_2d3d}). % \vspace{-0.2cm}% \paragraph{$\mathbf{1} \times \mathbf{1}$ \textbf{CONV}} In this layer, each filter only looks at one point at a time without involving any neighborhood. This allows traditional convolution operation to be applied directly on unordered sets. % \vspace{-0.2cm}% \paragraph{ReLU, BatchNorm, SoftMax} These are standard neural network layers. Both ReLU and BatchNorm are applied after all parameterized layers except the last CONV layer. SoftMax is applied after the last CONV layer to generate label predictions. % \subsection{SPLATNet$_{\text{3D}}$ architecture}% SPLATNet$_{\text{3D}}$ is shown in the blue box in Figure~\ref{fig:splatnet}. After a $1 \times 1$ CONV layer, a sequence of $T$ BCLs are used consecutively. Their responses are then % concatenated together, and passed through two additional $1 \times 1$ CONV layers before % finally outputting pixel-wise label predictions. % All the layers in SPLATNet$_{\text{3D}}$ are differentiable, and so the layer parameters can be learned through back-prop with a standard cross-entropy loss. % All of the $T$ BCLs used in the network use the same set of lattice features, $L_{3D}$ (to be distinguished from $L_{2D}$, which will be described in Sec.~\ref{sec:bpcn_2d3d}). Each of them uses different lattice feature scales: % %% \begin{align*}%     (\Lambda_0, \frac{\Lambda_0}{2}, \dots, \frac{\Lambda_0}{2^{T-1}})% \end{align*}% By using gradually decreased lattice feature scales, the network turns to coarser lattice and % larger receptive fields. The skip connections brought by the concatenation operation pool information directly from all available feature scales. This technique is also known to be % useful in some 2D CNNs~\cite{hariharan2015hypercolumns}. \vspace{-0.35cm}\paragraph{Lattice spaces and their scales.}%One of the main distinguishing features of SPLATNet compared to existing 3D networks is %its use of convolutions on sparse lattices while still taking unordered point clouds as input.
The use of BCLs in SPLATNet allows easy specifications of lattice spaces via lattice features and also lattice scales via a scaling matrix.

Changing the lattice scales $\Lambda$ directly affects the resolution of the signal on which the convolution operates. This gives us direct control over the receptive fields of network layers.
Figure~\ref{fig:lattice_viz} shows lattice cell visualizations for different lattice spaces and scales.
%, with points falling in the same lattice cell shown with the same color.
Using coarser lattice can increase the effective receptive field of a filter.
Another way to increase the receptive field of a filter is by increasing its neighborhood size. 
%But, in high-dimensions, this will exponentially increase the number of filter weights. For instance, a standard Euclidean filter with 1-neighborhood in 3D space has $3^3=27$ parameters, whereas a filter with 7-neighborhood has $7^3=343$ parameters. 
But, in high-dimensions, this will significantly increase the number of filter parameters. For instance, 3D filters of size $3, 5, 7$ on a regular Euclidean grid have $3^3=27, 5^3=125, 7^3=343$ parameters respectively.
On the other hand, 
%increasing the receptive field of a filter by
making the lattice coarser would not increase the number of filter parameters leading to more computationally efficient network architectures.

We observe that it is beneficial to use finer lattices (larger lattice scales) earlier in the network, and then coarser lattices (smaller lattice scales) going deeper. This is consistent with the common knowledge in 2D CNNs: increasing receptive field gradually through the network can help build hierarchical representations with varying spatial extents and abstraction levels. 

Although we mainly experiment with $XYZ$ lattices in this work, BCL allows for other lattice spaces such as position and color space ($XYZRGB$) or normal space. Using different lattice spaces enforces different connectivity across input points that may be beneficial to the task. In one of the experiments, we experimented with a variant of SPLATNet$_{\text{3D}}$, where
we add an extra BCL with position and normal lattice features ($XYZn_xn_yn_z$) and observed minor
performance improvements.

% \subsection{BCL lattice features and feature scales}% A major advantage of BCL is its flexibility in specifying filter neighborhood by building different lattices. Each ${L_i}$ in the network can be in different features (\eg XYZ, XYZRGB). In comparison, a traditional 2D or 3D CNN can only operate on a fixed regular XY or XYZ grid, and can only occasionally change the grid scale with pooling layers.  % Choosing the right lattice features can serve multiple purposes: % \begin{itemize}%     \vspace{-0.2cm}%     \item Using extra features in addition to XYZ, a BCL can achieve ``edge-preserving" filtering similar %         to the behavior observed in bilateral image filtering. This can be particularly helpful for segmentation tasks where %         sharp boundaries are highly desired.  %     \vspace{-0.2cm}%     \item Using high-dimensional lattice features enables desirable long-range connectivity among points in the point-cloud. % \end{itemize}% To make SPLATNet$_{\text{3D}}$ applicable to the most generic usage scenarios, unless % otherwise noted, we build lattices using only point positions XYZ. However, there is still a great % amount of flexibility in lattice spaces by setting different feature scales $\Lambda_i$. % Changing lattice feature scales directly changes the resolution of the convolution operation. % This also gives us direct control over the receptive field of the layer. Coarse lattice in combination with small convolution neighborhood can provide large enough receptive field while avoiding the computation burden in larger convolution neighborhoods. We observe that it is % beneficial to use finer lattices (larger lattice feature scales) earlier in the network, and then coarser lattices (smaller lattice feature scales) later in the network. This is consistent % with the common knowledge in 2D CNNs that increasing receptive field gradually through the network can help build hierarchical representations with varying spatial spans and abstraction levels. % Visualizations of some different lattice features and feature scales are shown in Fig~\ref{fig:lattice_viz}. %\begin{figure*}[!ht]%    \vspace{-0.0cm}%    \centering%    \includegraphics[width=0.85\textwidth]{figures/bike_and_airpane_xyzs_norm_6d.pdf}%    \vspace{0.0cm}%    \mycaption{Example lattice feature choices} {XYZ, XYZ $\times 8$, XYZ $\times 64$, normals}%    \label{fig:lattice_viz}%    \vspace{-0.4cm}%\end{figure*}% \begin{figure*}[!ht]% \centering% \captionsetup[subfigure]{labelformat=empty}% \begin{subfigure}{.18\textwidth}% \centering% \includegraphics[height=3.5cm]{figures/lattice_viz/lattice_viz_xyz.pdf}% \caption{$(x,y,z), \Lambda=I_3$}% \end{subfigure}% \begin{subfigure}{.18\textwidth}% \centering% \includegraphics[height=3.5cm]{figures/lattice_viz/lattice_viz_xyz8.pdf}% %\caption{$(x\cdot8,y\cdot8,z\cdot8)$}% \caption{$(x,y,z), \Lambda=8 \cdot I_3$}% \end{subfigure}% \begin{subfigure}{.18\textwidth}% \centering% \includegraphics[height=3.5cm]{figures/lattice_viz/lattice_viz_xyz64.pdf}% %\caption{$(x\cdot64,y\cdot64,z\cdot64)$}% \caption{$(x,y,z), \Lambda=64 \cdot I_3$}% \end{subfigure}% \begin{subfigure}{.18\textwidth}% \centering% \includegraphics[height=3.5cm]{figures/lattice_viz/lattice_viz_norms.pdf}% %\caption{$(nx,ny,nz)$}% \caption{$(n_x,n_y,n_z), \Lambda = I_3$}% \end{subfigure}% \begin{subfigure}{.18\textwidth}% \centering% \includegraphics[height=3.5cm]{figures/lattice_viz/lattice_viz_6d.pdf}% %\caption{$(x,y,z,nx,ny,nz)$}% \caption{$(x,y,z,n_x,n_y,n_z), \Lambda=I_6$}% \end{subfigure}% \mycaption{Example lattice features and feature scales}{}% \label{fig:lattice_viz}% \end{figure*}\begin{figure}[!htb]
\vspace{1.5mm}
\centering
\captionsetup[subfigure]{labelformat=empty}
\begin{subfigure}{.155\textwidth}
\centering
\includegraphics[height=3.5cm]{figures/lattice_viz/lattice_viz_xyz.pdf}
\caption{$(x,y,z), I_3$}
\end{subfigure}
\begin{subfigure}{.155\textwidth}
\centering
\includegraphics[height=3.5cm]{figures/lattice_viz/lattice_viz_xyz8.pdf}
\caption{$(x,y,z), 8 I_3$}
\end{subfigure}
\begin{subfigure}{.155\textwidth}
\centering
\includegraphics[height=3.5cm]{figures/lattice_viz/lattice_viz_norms.pdf}
\caption{$(n_x,n_y,n_z), I_3$}
\end{subfigure}
\vspace{-0.1cm}
\mycaption{Effect of different lattice spaces and scales}{Visualizations for different
lattice feature spaces $L = (x,y,z), (x,y,z), (n_x,n_y,n_z)$ along with lattice scales
$\Lambda = I_3, 8I_3, I_3$. $(n_x,n_y,n_z)$ refers to point normals.
All points falling in the same lattice cell are colored the same.}
\label{fig:lattice_viz}
\vspace{-0.3cm}
\end{figure}

\section{Joint 2D-3D Processing with SPLATNet$_{\text{2D-3D}}$}\label{sec:bpcn_2d3d}

Oftentimes, 3D point clouds are accompanied by 2D images of the same target. For instance, many modern 3D
sensors capture RGBD streams and perform 3D reconstruction to obtain 3D point clouds, resulting
in both 2D images and point clouds of a scene together with point correspondences between 2D and 3D.
One could also easily sample point clouds along with 2D renderings from a given 3D mesh.
% Very often, 3D point clouds accompany by 2D images depicting the same scene: many modern 3D sensors capture RGBD streams % and perform 3D reconstruction, outputting both 2D images and point clouds, together with point registration; given 3D meshes, % it is also easy to sample point clouds and 2D renderings. 
When such aligned 2D-3D data is present, SPLATNet provides an extremely flexible framework for joint 
processing. We propose SPLATNet$_{\text{2D-3D}}$, another SPLATNet instantiation designed for such joint
processing.

The network architecture of the SPLATNet$_{\text{2D-3D}}$ is depicted in the green box of Figure~\ref{fig:splatnet}.
SPLATNet$_{\text{2D-3D}}$ encompasses SPLATNet$_{\text{3D}}$ as one of its components and adds extra computational 
modules for joint 2D-3D processing. Next, we explain each of these extra components of SPLATNet$_{\text{2D-3D}}$,
in the order of their computations.

% Shown in Figure~\ref{fig:splatnet} as the green box, SPLATNet$_{\text{2D-3D}}$ encompasses SPLATNet$_{\text{3D}}$ as % one of its components. It incorporates additional BCLs for 2D $\leftrightarrow$ 3D projections (BCL$_{3D\rightarrow2D}$, BCL$_{2D\rightarrow3D}$), two 2D image CNNs (CNN$_1$ and CNN$_2$), and a `2D-3D Fusion' stage. Each of the components is explained below in detail. % \paragraph{SPLATNet$_{\text{3D}}$} This architecture is described in Section~\ref{sec:bpcn_3d}. In the joint % framework, SPLATNet$_{\text{3D}}$ serves the purpose of extracting 3D features for each point in the point cloud. Filter weights can be initialized from pretraining on 3D only task. \vspace{-0.35cm}\paragraph{CNN$_1$.} First, we process the given multi-view 2D images using 
%a standard 2D segmentation CNN architecture, which we refer to as CNN$_1$. 
a 2D segmentation CNN, which we refer to as CNN$_1$.
%In our experiments, we use DeepLab~\cite{chen2014semantic} segmentation architecture 
In our experiments, we use the DeepLab~\cite{chen2014semantic} architecture
for CNN$_1$ and initialize the network weights with those pre-trained on
PASCAL VOC segmentation~\cite{Everingham15}.

% Given multi-view input images are first processed by a 2D CNN to extract 2D features. We use the DeepLab~\cite{chen2014semantic} architecture and initialize the filters from PASCAL VOC pretraining. \vspace{-0.35cm}\paragraph{BCL$_{\text{2D}\rightarrow\text{3D}}$.}%\camreadycomment{
CNN$_1$ outputs features of the image pixels, 
whose 3D locations often do not exactly correspond to points in the 3D point cloud. We project 
information from the pixels onto the point cloud using a BCL with only \emph{splat} and \emph{slice} operations. 
As mentioned in Section~\ref{sec:review_bcl}, one of the interesting properties of BCL is that it allows for different
input and output points by separate specifications of input and output lattice features, $L^{in}$ and $L^{out}$.
%}% old text in original submission% Once the output of CNN$_1$ is computed for given multi-view 2D images, we project them onto 3D point cloud using a BCL with only \emph{splat} and \emph{slice} operations. As mentioned in Section~\ref{sec:review_bcl}, one of the interesting properties of BCL is that it allows for different input and output points by separate specification of input and output lattice features, $L^{in}$ and $L^{out}$.
Using this property, we use BCL to \emph{splat} 2D features onto the 3D lattice space and then \emph{slice} the 3D splatted signal on the point cloud.
We refer to this BCL, without a convolution operation, as BCL$_{\text{2D}\rightarrow\text{3D}}$ 
as illustrated in Figure~\ref{fig:map_2d_3d}. Specifically, we use 3D locations of the image pixels
as input lattice features, $L^{in}=L_{2D} \in \mathbb{R}^{m \times 3}$, where $m$ denotes the number
of input image pixels. In addition, we use 3D locations of points in the point cloud as output lattice
features, $L^{out}=L_{3D} \in \mathbb{R}^{n \times 3}$, which are the same lattice features used in SPLATNet$_{\text{3D}}$.
The lattice scale, $\Lambda_a$, controls the smoothness of the projection and can be adjusted
according to the sparsity of the point cloud.

% A BCL is used for projecting 2D image features into a 3D point cloud. The layer has only `splat' and `slice' operations (skipping the `convolve' step). Unlike the BCLs in SPLATNet$_{\text{3D}}$, input lattice features and output lattice features are different here: % input lattice features, $L^{in}=L_{2D} \in \mathbb{R}^{(H\times W)\times 3}$, specify the 3D locations of each image pixels; while $L^{out}=L_{3D}$ is the same as in SPLATNet$_{\text{3D}}$, specifying all the points of interest in a 3D point cloud.  Lattice feature scales, $\Lambda_a$, is a hyper-parameter which controls the smoothness of the projection, and can be adjusted according to the sparsity of the points. The projection process is illustrated in Figure~\ref{fig:map_2d_3d}. \begin{figure}[!t]
\vspace{-0.5cm}
\begin{center}
\centerline{\includegraphics[width=1.05\columnwidth]{figures/splat_2d_slice_3d.pdf}}
  \vspace{-0.2cm}
  \mycaption{2D to 3D projection}{Illustration of 2D to 3D projection using \emph{splat} and \emph{slice}usingsplatandsliceoperations. Given input features of 2D images, pixels are projected onto a 3D permutohedral lattice defined by 3D positional lattice features. The splatted signal
  is then sliced onto the points of interest in a 3D point cloud.}
    \label{fig:map_2d_3d}
\end{center}
\vspace{-0.7cm}
\end{figure}\vspace{-0.35cm}\paragraph{2D-3D Fusion.} At this point, we have the result of CNN$_1$ projected onto 3D points and also
the intermediate features from SPLATNet$_{\text{3D}}$ that exclusively operates on the input point cloud. Since both
of these signals are embedded in the same 3D space, we concatenate these two signals and then use a series of $1\times1$ CONV layers for further processing. The output of the `2D-3D Fusion' module is 
passed on to a softmax layer to compute class probabilities at each input point of the point cloud.

% Once both 2D features and 3D features are embedded in the same lattice space, they are combined through a \emph{2D-3D Fusion} process. 2D features and 3D features are first concatenated together. Additional network layers can be applied on the concatenated features before % outputting 3D predictions. A single $1 \times 1$ CONV layer is found to be sufficient. \vspace{-0.35cm}\paragraph{BCL$_{\text{3D}\rightarrow\text{2D}}$.} Sometimes, we are also interested in segmenting 2D images and
want to leverage relevant 3D information for better 2D segmentation. For this purpose, we back-project
the 3D features computed by the `2D-3D Fusion' module onto the 2D images by a BCL$_{\text{2D}\rightarrow\text{3D}}$ module. This is the reverse operation of BCL$_{\text{2D}\rightarrow\text{3D}}$, where the
input and output lattice features are swapped. Similarly, a hyper-parameter $\Lambda_b$ controls the smoothness of the projection.

% Features obtained in the 2D-3D Fusion stage can be projected to 2D pixels to facilitate 2D tasks. % This is a % reverse operation of BCL$_{2D\rightarrow3D}$: note that input lattice features and output lattice features are swapped, \ie $L^{in}=L_{3D}, L^{out}=L_{2D}$. Similarly, a hyper-parameter $\Lambda_b$ controls the smoothness of the projection. \vspace{-0.35cm}\paragraph{CNN$_2$.} We then concatenate the output from CNN$_1$, input images and the output
of BCL$_{\text{3D}\rightarrow\text{2D}}$, and pass them through another 2D CNN, CNN$_2$, to obtain refined
2D semantic predictions. In our experiments, we find that a simple 2-layered network is good enough
for this purpose.

% An additional 2D network provides 2D predictions given projected 2D features. Since the pixel-wise features at this stage is already % quite refined, we use a relatively simple network with a few standard CONV-BatchNorm-ReLU blocks for CNN$_2$. 

All components in this 2D-3D joint processing framework are differentiable, and can be trained end-to-end. Depending on the availability of 2D or 3D ground-truth labels, loss functions can be defined on either one of the two domains, or on both domains in a multi-task learning setting. More details of the network architecture
are provided in the supp. material. We believe that this joint processing capability offered by SPLATNet$_{\text{2D-3D}}$ can result in better predictions for both 2D images and 3D point clouds. For 2D images, leveraging 3D features helps in view-consistent predictions across multiple viewpoints. 
For point clouds, incorporating 2D CNNs help leverage powerful 2D deep CNN features computed on high-resolution images.

% With SPLATNet$_{\text{2D-3D}}$, 2D and 3D data can be integrated seamlessly. These are the potential usage scenarios: % \paragraph{a) Filtering 2D data in 3D space} Because of occlusion and depth discrepancy, 2D positions often do not % faithfully reflect the real spatial relationships between objects. BCL allows % filtering 2D data in 3D space. Given a set of images $I_{1\dots N}$ where $I_i \in \mathbb{R}^{H\times W \times C}$, % and the corresponding 3D locations of the pixels $L_{1\dots N}$ where $L_i \in \mathbb{R}^{H\times W\times 3}$, % the pixels can be \emph{splatted} and \emph{convolved} in 3D, and subsequently \emph{sliced} back. % \paragraph{b) Projecting 2D data into 3D space} When the goal is to extract 3D features or to output 3D predictions, % 2D data filtered in 3D space need not be sliced back into 2D. Instead, 3D features can be read off the layer by % slicing with a set of 3D positions in interest: $L^{out} \in \mathbb{R}^{n \times 3}$.% \paragraph{c) Filtering 3D data in 2D space} The operations discussed above can also be applied in the reversed direction. % Given a set of 3D points, it might be desirable to do 2D filtering on the point cloud. Possible reasons for such usage % include: resorting to cheaper 2D filtering instead of 3D filtering, utilizing pre-trained 2D networks, \etc.% \paragraph{d) Projecting 3D data into 2D space} Similarly, to assist 2D tasks, 3D data can be splatted into 2D, and then % subsequently sliced to a set of 2D positions in interest, \eg dense 2D pixels in the task of 2D segmentation. % \subsection{Joint 2D-3D segmentation}% We present a joint 2D-3D \model (Figure~\ref{}). It illustrates the use b) and d) at the same time.  % The input of the network consists of three parts: % \begin{itemize}% \item a point cloud $F \in \mathbb{R}^{n \times d}$% \item a set of 2D images $\{I_i \in \mathbb{R}^{H \times W \times C}\}$% \item aligned 3D positions of the image pixels $\{L_i \in \mathbb{R}^{H\times W\times 3}\}$% \end{itemize}% The task is to output label prediction for 2D image pixels, the point cloud, or both. % In Figure~\ref{}, $\text{NET}^{\text{2D}}_1$ $\text{NET}^{\text{2D}}_2$ are 2D CNNs, % while $\text{NET}^{\text{3D}}$ is a 3D \model which operates directly on the 3D point cloud. % BCL$_{2D\rightarrow 3D}$ is a BCL for interpolating 2D pixel-wise features of $I_i$ into 3D space with $\lambda_1 L_i$ as lattice features. % BCL$_{3D\rightarrow 2D}$ is a BCL for interpolating 3D point-wise features of the point cloud into 2D space with $\lambda_2 L_i$ as lattice features.  % We use a DeepLab network architecture~\cite{} for $\text{NET}^{\text{2D}}_1$. It can be pretrained in two stages: % \textbf{stage 1}) generic 2D pretraining, \eg on PASCAL VOC 2D segmentation benchmark; and % \textbf{stage 2}) domain specific pretraining when 2D ground truth is available. % We use the 3D \model described in Section~\ref{sec:bpcn_3d} for $\text{NET}^{\text{3D}}$. The network can also be pretrained % when 3D ground truth is present. % \hscomment{explain about 5 components}% \begin{align}% L &= \alpha_1 L_{\text{2D}} + \alpha_2 L_{\text{3D}} \nonumber \\% L_{\text{2D}} &= -\sum_v^V \sum_{i,j} \log Prob^{2D}_{vij} \nonumber\\% L_{\text{3D}} &= -\sum_i^N \log Prob^{3D}_{i} \nonumber% \end{align}

\section{Experiments}\label{sec:exp}

We evaluate \model on tasks on two different benchmark datasets of
RueMonge2014~\cite{riemenschneider2014learning} and ShapeNet~\cite{yi2016scalable}.
On RueMonge2014, we conducted experiments on the tasks of 3D point cloud labeling
and multi-view image labeling. On ShapeNet, we evaluated \model on 3D part segmentation. We use Caffe~\cite{jia2014caffe} neural network framework for all the experiments.
%and use Adam stochastic optimization~\cite{kingma2014adam} for training the networks}. 
Full code and trained models are publicly available on our project website\footnote{\url{http://vis-www.cs.umass.edu/splatnet}}.

\subsection{RueMonge2014 facade segmentation}\label{sec:exp_facade}\begin{table}[!tb]
    \scriptsize
    \mycaption{Results on facade segmentation}{Average IoU scores and approximate runtimes
    for point cloud labeling and 2D image labeling using different techniques. Runtimes indicate the time taken to segment the entire test data (202 images sequentially for 2D and a point cloud for 3D).}
    \centering
    \small
    \begin{tabular}{p{3.4cm}>{\centering\arraybackslash}p{1.7cm}>{\centering\arraybackslash}p{2.0cm}}
    % {@{}lcc@{}}
        \toprule
        Method & Average IoU & Runtime (min) \\
        \midrule
        \multicolumn{2}{l}{\textit{With only 3D data}} & \\
        OctNet~\cite{riegler2017octnet}              &  59.2  & - \\
        Autocontext$_{\text{3D}}$~\cite{gadde2017efficient}     &  54.4  & 16 \\
        \modelthree (Ours)      & \textbf{65.4}  & 0.06 \\
        \midrule
        \multicolumn{2}{l}{\textit{With both 2D and 3D data}} &  \\
        Autocontext$_{\text{2D-3D}}$~\cite{gadde2017efficient}   &  62.9  &  87\\
        \modeljoint (Ours) & \textbf{69.8} & 1.20 \\
        \bottomrule
    \end{tabular}
    \subcaption{Point cloud labeling}
    \label{tab:facade_results_3d}
    \begin{tabular}{p{3.4cm}>{\centering\arraybackslash}p{1.7cm}>{\centering\arraybackslash}p{2.0cm}}
    % {@{}lcc@{}}
        \toprule
        Method & Average IoU & Runtime (min) \\
        \midrule
        Autocontext$_{\text{2D}}$~\cite{gadde2017efficient}     &  60.5  & 117 \\
        Autocontext$_{\text{2D-3D}}$~\cite{gadde2017efficient}     &  62.7  & 146\\
        DeepLab$_{\text{2D}}$~\cite{chen2014semantic} & 69.3 & 0.84\\
        \modeljoint (Ours)   & \textbf{70.6} &  4.34 \\
        \bottomrule
    \end{tabular}
    \subcaption{Multi-view image labeling}
    \label{tab:facade_results_2d}
    \vspace{-3mm}
\end{table}\begin{figure*}[!htb]
\captionsetup[subfigure]{labelformat=empty}
\begin{subfigure}{.24\textwidth}
\centering
\includegraphics[height=3.5cm]{figures/facade_results/pcl.png}
\caption{Input Point Cloud}
\end{subfigure}
\begin{subfigure}{.24\textwidth}
\centering
\includegraphics[height=3.5cm]{figures/facade_results/pcl_gt.png}
\caption{Ground truth}
\end{subfigure}
\begin{subfigure}{.24\textwidth}
\centering
\includegraphics[height=3.5cm]{figures/facade_results/pcl_result_only3d.png}
\caption{\modelthree}
\end{subfigure}
\begin{subfigure}{.24\textwidth}
\centering
\includegraphics[height=3.5cm]{figures/facade_results/pcl_result_2d_3d.png}
\caption{\modeljoint}
\end{subfigure}
\vspace{-.1cm}
\mycaption{Facade point cloud labeling}{Sample visual results of \modelthree and \modeljoint.}
\vspace{-.2cm}
\label{fig:facade_3d_visuals}
\end{figure*}Here, the task is to assign semantic label to every point in a point cloud and/or corresponding
multi-view 2D images.

\vspace{-0.35cm}\paragraph{Dataset.} RueMonge2014~\cite{riemenschneider2014learning} provides
a standard benchmark for 2D and 3D facade segmentation and also inverse procedural
modeling. The dataset consists of 428 high-resolution and multi-view images obtained
from a street in Paris. A point cloud with approximately 1M points is reconstructed
using the multi-view images. A ground-truth labeling with seven semantic classes of
door, shop, balcony, window, wall, sky and roof are provided for both 2D images and
the point cloud. Sample point cloud sections and 2D images with their corresponding ground truths
are shown in Figure~\ref{fig:facade_3d_visuals} and \ref{fig:facade_2d_visuals}
respectively. For evaluation, Intersection over Union (IoU) score is computed for
each of the seven classes and then averaged to get a single overall IoU.

\vspace{-0.35cm}\paragraph{Point cloud labeling.} We use our \modelthree architecture for the task
of point cloud labeling on this dataset. We use 5 BCLs followed by a couple of 
$1\times1$ CONV layers. Input features to the network
comprise of a 7-dimensional vector at each point representing RGB color, normal and height above the ground. For all the BCLs, we use $XYZ$ lattice space ($L_{\text{3D}}$) with $\Lambda_0 = 64I_3$. Experimental results with average IoU and runtime are shown in 
Table~\ref{tab:facade_results_3d}. Results show that, with only 3D data, 
our method achieves an IoU of 65.4 which is a considerable improvement (6.2 IoU $\uparrow$) over the state-of-the-art deep network, OctNet~\cite{riegler2017octnet}.

Since this dataset comes with multi-view 2D images, one could leverage the
information present in 2D data for better point cloud labeling. 
We use \modeljoint to leverage 2D information and obtain better 3D segmentations. Table~\ref{tab:facade_results_3d} shows the experimental results when using
both the 2D and 3D data as input. \modeljoint obtains an average IoU of 69.8 outperforming
the previous state-of-the-art by a large margin (6.9 IoU $\uparrow$), thereby setting up a new
state-of-the-art on this dataset. This is also a significant improvement from the IoU obtained
with \modelthree demonstrating the benefit of leveraging 2D and 3D information in a joint
framework. Runtimes in Table~\ref{tab:facade_results_3d} also indicate that
our \model approach is much faster compared to traditional Autocontext techniques.
Sample visual results for 3D facade labeling are shown in Figure~\ref{fig:facade_3d_visuals}.

\vspace{-0.35cm}\paragraph{Multi-view image labeling.} As illustrated in Section~\ref{sec:bpcn_2d3d}, we extend
%the 2D CNN$_1$ with CNN$_2$ to obtain better multi-view image segmentation. 
2D CNNs with \modeljoint to obtain better multi-view image segmentation.
Table~\ref{tab:facade_results_2d} shows the results of multi-view image labeling on this dataset
using different techniques. Using DeepLab (CNN$_1$) already outperforms existing state-of-the-art
by a large margin. Leveraging 3D information via \modeljoint boosts
the performance to 70.6 IoU. An increase of 1.3 IoU from only using CNN$_1$ demonstrates
the potential of our joint 2D-3D framework in leveraging 3D information for better 2D segmentation.

% We evaluate SPLATNet on the task of 3D scene semantic segmentation. Given a point % cloud depicting a scene, the task is to assign object category label to each point. % We experiment on the RueMonge2014 dataset~\cite{riemenschneider2014learning}, which provides a 3D point cloud of % several facades along Rue Monge in Paris, comprising \textasciitilde 1 million 3D points in total. % For input features, each point is represented by a 7-dim vector of the RGB color, the normal vector and % the height above the ground. We use 5 BCLs followed by 2 $1 \times 1$ \emph{conv} layers. The lattice features % for all BCLs are the locations of the points, XYZ. % Example segmentation results are shown in Figure~\ref{fig:facade}. Our method achieves 6.3\% improvement over OctNet~\cite{riegler2017octnet}, % a state-of-the-art 3D network on this data, and 2.6\% over Auto-Context~\cite{gadde2017efficient}, a method relying on both 2D and 3D data. \begin{figure}[!ht]
\captionsetup[subfigure]{labelformat=empty}
\begin{subfigure}{.155\textwidth}
\centering
\includegraphics[height=3.5cm]{figures/facade_results/IMG_5602.jpg}
\end{subfigure}
\begin{subfigure}{.155\textwidth}
\centering
\includegraphics[height=3.5cm]{figures/facade_results/IMG_5602_gt.png}
\end{subfigure}
\begin{subfigure}{.155\textwidth}
\centering
\includegraphics[height=3.5cm]{figures/facade_results/IMG_5602_result.png}
\end{subfigure} \\[3pt]% \addlinespace[3pt]
% \begin{subfigure}{.155\textwidth}
% \centering
% \includegraphics[height=3.5cm]{figures/facade_results/IMG_5559.jpg}
% \end{subfigure}
% \begin{subfigure}{.155\textwidth}
% \centering
% \includegraphics[height=3.5cm]{figures/facade_results/IMG_5559_gt.png}
% \end{subfigure}
% \begin{subfigure}{.155\textwidth}
% \centering
% \includegraphics[height=3.5cm]{figures/facade_results/IMG_5559_result.png}
% \end{subfigure} \\[3pt]% \addlinespace[3pt]
\begin{subfigure}{.155\textwidth}
\centering
\includegraphics[height=3.5cm]{figures/facade_results/IMG_5519.jpg}
\caption{Input}
\end{subfigure}
\begin{subfigure}{.155\textwidth}
\centering
\includegraphics[height=3.5cm]{figures/facade_results/IMG_5519_gt.png}
\caption{Ground truth}
\end{subfigure}
\begin{subfigure}{.155\textwidth}
\centering
\includegraphics[height=3.5cm]{figures/facade_results/IMG_5519_result.png}
\caption{\modeljoint}
\end{subfigure}
\vspace{-1mm}
\mycaption{2D facade segmentation}{Sample visual results of \modeljoint.}
\label{fig:facade_2d_visuals}
\vspace{-2mm}
\end{figure}%\vspace{-5mm}% \begin{figure}% \footnotesize% \stackunder[5pt]{\includegraphics[width=2in,height=.7in]{figures/facade_results/pcl.png}{Input}%% \hspace{1cm}%% \stackunder[5pt]{\includegraphics[width=2in,height=.7in]{figures/facade_results/pcl_gt.png}}{Groundtruth}% \mycaption{Sample Results of 3D Facade Labelling}{}% \end{figure}\subsection{ShapeNet part segmentation}\label{sec:exp_shapenet}\begin{table*}[th!]
\scriptsize
    \mycaption{Results on ShapeNet part segmentation}{Class average mIoU, instance average mIoU and mIoU scores for all the categories on the task of point cloud labeling using different techniques.}
    \centering
    \small
\setlength{\tabcolsep}{2pt}
%\begin{tabular}{lllllllllllllllllll}
\begin{tabular}{ccccccccccccccccccc}
\toprule
\#instances                         &                                 &                                    & 2690      & 76   & 55   & 898  & 3758  & 69       & 787    & 392   & 1547 & 451    & 202        & 184  & 283    & 66     & 152        & 5271  \\ \hline
	\multicolumn{1}{l|}{}               & \multicolumn{1}{c|}{class} & \multicolumn{1}{c|}{instance} & \small{air-} & \small{bag}  & \small{cap}  & \small{car}  & \small{chair} & \small{ear-} & \small{guitar} & \small{knife} & \small{lamp} & \small{laptop} & \small{motor-} & \small{mug}  & \small{pistol} & \small{rocket} & \small{skate-} & \small{table} \\
	\multicolumn{1}{l|}{}               & \multicolumn{1}{c|}{avg.} & \multicolumn{1}{c|}{avg.} & \small{plane} & & & & & \small{phone} & & & & & \small{bike} & & & & \small{board} & \\
	\midrule
\multicolumn{1}{l|}{Yi \etal{~\cite{yi2016scalable}}} & \multicolumn{1}{c|}{79.0}     & \multicolumn{1}{c|}{81.4}         & 81.0      & 78.4 & 77.7 & 75.7 & 87.6  & 61.9     & 92.0   & 85.4  & 82.5 & 95.7   & 70.6     & 91.9 & \textbf{85.9}   & 53.1   & 69.8 & 75.3 \\
\multicolumn{1}{l|}{3DCNN{~\cite{qi2017pointnet}}}          & \multicolumn{1}{c|}{74.9}     & \multicolumn{1}{c|}{79.4}         & 75.1      & 72.8 & 73.3 & 70.0 & 87.2  & 63.5     & 88.4   & 79.6  & 74.4 & 93.9   & 58.7       & 91.8 & 76.4   & 51.2   & 65.3       & 77.1  \\
\multicolumn{1}{l|}{Kd-network{~\cite{klokov2017escape}}}         & \multicolumn{1}{c|}{77.4}     & \multicolumn{1}{c|}{82.3}         & 80.1      & 74.6 & 74.3 & 70.3 & 88.6  & 73.5     & 90.2   & \textbf{87.2}  & 81.0 & 94.9   & 57.4       & 86.7 & 78.1   & 51.8   & 69.9       & 80.3  \\
\multicolumn{1}{l|}{PointNet{~\cite{qi2017pointnet}}}       & \multicolumn{1}{c|}{80.4}     & \multicolumn{1}{c|}{83.7}         & \textbf{83.4}      & 78.7 & 82.5 & 74.9 & 89.6  & 73.0     & 91.5   & 85.9  & 80.8 & 95.3   & 65.2       & 93.0 & 81.2   & 57.9   & 72.8       & 80.6  \\
\multicolumn{1}{l|}{PointNet++{~\cite{qi2017pointnetpp}}}       & \multicolumn{1}{c|}{81.9}     & \multicolumn{1}{c|}{85.1}         & 82.4      & 79.0 & 87.7 & 77.3 & \textbf{90.8}  & 71.8     & 91.0   & 85.9  & 83.7 & 95.3   & 71.6       & 94.1 & 81.3   & 58.7   & 76.4       & \textbf{82.6}  \\
\multicolumn{1}{l|}{SyncSpecCNN{~\cite{yi2017syncspeccnn}}}       & \multicolumn{1}{c|}{82.0}     & \multicolumn{1}{c|}{84.7}         & 81.6 & 81.7 & 81.9 & 75.2 & 90.2 & 74.9 & \textbf{93.0} & 86.1 & \textbf{84.7} & 95.6 & 66.7 & 92.7 & 81.6 & 60.6 & \textbf{82.9} & 82.1  \\
\midrule
\multicolumn{1}{l|}{\modelthree}       & \multicolumn{1}{c|}{82.0}     & \multicolumn{1}{c|}{84.6}         & 81.9 & 83.9 & 88.6 & 79.5 & 90.1 & 73.5 & 91.3 & 84.7 & 84.5 & \textbf{96.3} & 69.7 & 95.0 & 81.7 & 59.2 & 70.4 & 81.3  \\
\multicolumn{1}{l|}{\modeljoint}       & \multicolumn{1}{c|}{\textbf{83.7}}     & \multicolumn{1}{c|}{\textbf{85.4}}         & 83.2 & \textbf{84.3} & \textbf{89.1} & \textbf{80.3} & 90.7 & \textbf{75.5} & 92.1 & 87.1 & 83.9 & \textbf{96.3} & \textbf{75.6} & \textbf{95.8} & 83.8 & \textbf{64.0} & 75.5 & 81.8  \\
\bottomrule
\end{tabular}
\label{tab:seg-results}
\end{table*}The task of part segmentation is to assign a part category label to each point in a point cloud representing a 3D object.
% Given the point cloud corresponding to a 3D object, the task of part segmentation is to assign a % part category label to each of input points. % We evaluate both \modelthree and \modeljoint for this task. \vspace{-0.35cm}\paragraph{Dataset.}
The ShapeNet Part dataset~\cite{yi2016scalable} is a subset of ShapeNet, which contains 16681 objects from 16 categories, each with 2-6 part labels. The objects are consistently aligned and scaled to fit into a unit cube, and the ground-truth annotations are provided on sampled points on the shape surfaces. 
It is common to assume that the  category of the input 3D object is known, narrowing the possible part labels to the ones specific to the given object category.
We report standard IoU scores for evaluation of part segmentation. An IoU score
is computed for each object and then averaged within the objects in a category to compute mean IoU (mIoU)
for each object category. In addition to reporting mIoU score for each object category, we also report
`class average mIoU' which is the average mIoU across all object categories, and also `instance
average mIoU', which is the average mIoU across all objects.
% To evaluate, mean IoU is computed for each object, and averaged within the object categories to provide mIoU for each object category. We also report class average mIoU, the average mIoU across all object categories, and instance average mIoU, the average mIoU across all objects.% \begin{figure}% \begin{center}% \centerline{\includegraphics[width=1.0\columnwidth]{figures/shapenet_seg_gt_joint.pdf}}%   \vspace{-0.5em}%   \mycaption{ShapeNet part segmentation} {Sample visual results of \modeljoint.}%     \label{fig:teaser}%   \vspace{-1.5em}% \end{center}% \vspace{-0.4cm}% \end{figure}\begin{figure}
\vspace{-1mm}
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/shapenet_seg_gt_3d_joint_wide.pdf}}
  \vspace{-0mm}
  \mycaption{ShapeNet part segmentation}{Sample visual results of \modelthree and \modeljoint.}
    \label{fig:shapenet_seg_samples}
\end{center}
\vspace{-8mm}
\end{figure}\vspace{-0.35cm}\paragraph{3D part segmentation.}
We evaluate both \modelthree and \modeljoint for this task. First, we discuss the architecture and results with \modelthree that uses only 3D point clouds as input.
Since the category of the input object is assumed to be known, we train separate networks
for each object category.
\modelthree network architecture for this taks is also composed of 5 BCLs.
Point locations $XYZ$ are used as input features as well as lattice features $L$ for all the BCLs
and the lattice scale for the first BCL layer is $\Lambda_0 = 64I_3$.
Experimental results are shown in Table~\ref{tab:seg-results}. \modelthree obtains a class average mIoU of 82.0 and an instance average mIoU of 84.6, which is on-par with the best networks that only take point clouds as input (PointNet++~\cite{qi2017pointnetpp} uses surface normals as additional inputs).

% We train a separate network for each object category. We use a \modelthree with 5 BCLs. % Point locations $XYZ$ are used as input features as well as the lattice features $L$ for all the BCLs. We use a starting lattice feature scale $\Lambda_0=64 I_3$ for the first BCL, and reduce the scale by half after each BCL. Experimental results are shown in Table~\ref{tab:seg-results}. From the results, \modelthree performs better or on par with the best existing methods taking only the 3D point clouds as inputs (PointNet++~\cite{qi2017pointnetpp} uses surface normals as additional input features). % \vspace{-0.3cm}% \paragraph{2D-3D joint part segmentation.}
We also adopt our \modeljoint network, which operates on both 2D and 3D data, for this task.
For the joint framework to work, we need rendered 2D views and corresponding 3D locations for each pixel in the renderings. 
We first render 3-channel images: Phong shading~\cite{Phong:1975:ICG}, depth, and height from ground. Cameras are placed on the 20 vertices of a dodecahedron from a fixed distance, pointing towards the object's center. 
The 2D-3D correspondences can be generated by carrying the $XYZ$ coordinates of 3D points into the rendering rasterization pipeline so that each pixel also acquires coordinate values from the surface point projected onto it. Results in Table~\ref{tab:seg-results} show that incorporating 
2D information allows \modeljoint to improve noticeably from \modelthree with 1.7 and 0.8 increase in class and instance average mIoU respectively. 
%\modeljoint obtains a class average IoU of 83.7 outperforming existing state-of-the-art approaches, while performing on par with state-of-the-art in terms of instance average IoU (85.4).\modeljoint obtains a class average IoU of 83.7 and an instance average IoU of 85.4, outperforming existing state-of-the-art approaches.

%\camreadycomment{
On one Nvidia GeForce GTX 1080 Ti, \modelthree{} runs at $9.4$ shapes$/$sec, while \modeljoint{} is slower at $0.4$ shapes$/$sec due to a relatively large 2D network
% (DeepLab~\cite{chen2014semantic}) 
operating on 20 high-resolution ($512\times512$) views, which takes up more than $95\%$ of the computation time. In comparison, PointNet++ runs at $2.7$ shapes$/$sec on the same hardware\footnote{We use the public implementation released by the authors (\url{https://github.com/charlesq34/pointnet2}) with settings: $\text{model}=\text{`pointnet2\_part\_seg\_msg\_one\_hot'}$, $\text{VOTE\_NUM}=12$, $\text{num\_point}=3000$ (in consistence with our experiments).}.
%}\vspace{-0.35cm}\paragraph{Six-dimensional filtering.} 
We experiment with a variant of \modelthree where an 
additional BCL with 6-dimensional position and normal lattice features ($XYZn_xn_yn_z$) is added between the last two $1\times1$ CONV layers. This modification gave only a marginal improvement of
$0.2$ IoU over standard \modelthree in terms of both class and instance average mIoU scores.

% The layer is initialized with 6D Gaussian filters, producing bilateral % filtered responses. Without any fine-tuning of the network parameters, the network achieves% $0.2\%$ improvement over standard \modelthree in both class and instance average mIoU. Fine-tuning the network provides no noticeable further improvement. \hscomment{offer some explaination?}

\section{Conclusion}\label{sec:conclusion}

In this work, we propose the SPLATNet architecture for point cloud processing. SPLATNet directly takes point clouds as input and computes hierarchical and spatially-aware features with sparse and efficient lattice filters. In addition, SPLATNet allows an easy mapping of 2D information into 3D and vice-versa, resulting in a novel network architecture for joint processing of point clouds and multi-view images. Experiments on two different benchmark datasets show that the proposed networks compare favorably against state-of-the-art approaches for segmentation tasks. In the future, we would like to explore the use of additional input features (\eg, texture) and also the use of other high-dimensional lattice spaces in our networks.

% we propose a novel neural network architecture, SPLATNet, which provides flexible and efficient learning and processing of point cloud data. A variant of our model, SPLATNet$_{\text{2D-3D}}$, further enables seamless integration of 2D and 3D data, allowing information from the 2 domains to be shared and jointly processed. In the future, it would be interesting to explore additional input signals (\eg, texture) to SPLATNet for 3D CAD shape analysis.



{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

%%%%%%%%% APPENDIX
\newpage
\clearpage
\appendix{\raggedleft{} \bf \Large Supplementary}
\vspace{0.3cm}


\renewcommand\thesection{\Alph{section}}

In this supplementary material, we provide additional details and explanations to help readers gain 
a better understanding of our technique. 

% \section{Overview in Video}

% Included in the supplementary material is a 6-minute video, \href{overview.mp4}{overview.mp4}, showing an overview of our techniques and results.
% % VJ
% We recommend the reviewers to check this video to get a quick walk-through of the paper.
% % It is a quick walk-through of the paper, and the reviewers are encouraged to watch to gain a better understanding of our techniques. 

\section{Point Cloud Density Normalization}%\camreadycomment{
BCL has a normalization scheme to deal with uneven point density, or more specifically, the fact that some lattice vertices are supported by more data points than others.
% Input signals are first filtered directly with the learned filter kernels, and are then filtered again with a Gaussian kernel for another round with their values replaced by $1$s. The filter responses in the second round are then used for normalizing responses from the first round. 
Input signals are filtered directly with the learnable filter kernels, and are also filtered in a separate second round with their values replaced by $1$s with a Gaussian kernel. The filter responses in the second round are then used for normalizing responses from the first round.
This is similar
% equivalent 
to using homogeneous coordinates, which are widely adopted in bilateral filtering implementations such as \cite{adams2010fast}.
%}

\section{RueMonge2014 Facade Segmentation}\paragraph{Network architecture of \modelthree.} We use 5 BCLs ($T=5$) 
% VJ
followed by 
2 $1\times 1$ CONV layers in \modelthree for the facade segmentation task. 
We omit the initial $1\times 1$ CONV layer 
% VJ - \remove{since  removing it shows no impact on performance.} 
since we find it has no effect on the overall performance.
% VJ - The number of filters in each layer are: 
The number of output channels in each layer are: 
% \vjcomment{Strictly speaking, 64, 128 are not exactly the number of filters, but the output channels. The number of filters is input channels times output channels. Change `the number of filters in each layer' to `the number of output channels after each layer'?}{\verb B64-B128-B128-B128-B64-C64-C7 }.
Note that although written as a linear structure, the network has skip connections from all BCLs 
(layers start with `{\verb B }') to the penultimate $1\times 1$ CONV layer. We use an initial scale 
$\Lambda_0=32I_3$ for scaling lattice features $XYZ$, and divide the scale in half after each BCL: 
$(32I_3, 16I_3, 8I_3, 4I_3, 2I_3)$. 
The unit of raw input features $XYZ$ is meter, with $Y$ 
(aligned with gravity axis) having a range of $7.1$ meters.
For all the BCLs, we use filters operating on one-ring neighborhoods on the lattice.
% VJ - \vjcomment{It would be good to also comment on filter sizes for both BCL and CONV layers.}\vspace{-0.3cm}\paragraph{Network architecture of \modeljoint.} We use \modelthree as described above as the 3D component 
of our 2D-3D joint model. The `2D-3D Fusion' component has 2 $1\times 1$ CONV layers: {\verb C64-C7 }. 
DeepLab~\cite{chen2014semantic} segmentation architecture is used as CNN$_1$. 
CNN$_2$ is a small network with 2 CONV layers: {\verb C32-C7 }, where the first layer has 
% VJ - 32 $3\times 3$ filters \vjcomment{(again, these are not exactly filter number.)}$3\times 3$ filters and 32 output channels, and the second one has $1\times 1$ filters and 7 output channels. 
% VJ - and the last one has 7 $1\times 1$ filters. 
We use $\Lambda_a=64$ and $\Lambda_b=1000$ 
for 2D$\leftrightarrow$3D projections with BCLs. Note that the dataset provides one-to-many mappings from 3D points 
to pixels. By using a very large scale (\ie, $\Lambda_b=1000$), 3D unaries are directly mapped to 
the corresponding 2D pixel locations without any interpolation. 

\vspace{-0.3cm}\paragraph{Training.} We randomly sample facade segments of 60k points and use a batch size of 4 when 
training \modelthree. 
CNN$_1$ is initialized with Pascal VOC~\cite{Everingham15} pre-trained weights 
% VJ - \remove{(which themselves start from ImageNet pre-trained weights)}
and fine-tuned for 2D 
% VJ - \remove{on the facade images.} 
facade segmentation.
% We freeze weights in \modelthree and CNN$_1$ and only adapt the `2D-3D Fusion' component and CNN$_2$ when training \modeljoint. % \vjcomment{This statement is not completely correct as we did not freeze the weights of 3D model. I would remove this sentence}.
Adam optimizer~\cite{kingma2014adam} with an initial learning rate 
of $0.0001$ is used for training both \modelthree and \modeljoint. 
% Learning rate is dropped to $0.00002$ once validation loss plateaus when training \modelthree.% VJ - \remove{During training, small color, position, and rotation jitterings are applied to the point clouds, as well as small color jittering to the images.}
Since the training data is small, we augment point cloud training data with random rotations, 
translations, and small color perturbations. We also augment 2D image data with small color 
perturbations during training.


\section{ShapeNet Part Segmentation}\begin{figure*}[ht]
    \vspace{-1mm}
    \centering
    \begin{subfigure}[t]{.4\textwidth}
        \centering
        \includegraphics{figures/errors/skateboard_w_caption.pdf}
        \caption{Incorrect labels}
    	\label{fig:errors_wrong}
    \end{subfigure}
    \begin{subfigure}[t]{.42\textwidth}
        \centering
        \includegraphics{figures/errors/rocket_w_caption.pdf}
        \caption{Incomplete labels}
    	\label{fig:errors_incomplete}
    \end{subfigure}\\
    \begin{subfigure}[t]{.4\textwidth}
        \centering
        \includegraphics{figures/errors/airplane_w_caption.pdf}
        \caption{Inconsistent labels}
    	\label{fig:errors_inconsistent}
    \end{subfigure}
    \begin{subfigure}[t]{.42\textwidth}
        \centering
        \includegraphics{figures/errors/earphone_w_caption.pdf}
        \caption{Confusing labels}
    	\label{fig:errors_confusing}
    \end{subfigure}
    \mycaption{Labeling issues in the ShapeNet Part dataset}{Four types of labeling issues are shown here. 
Two examples from the test set are given for each type, where the first row shows the ground-truth labels 
and the second row shows our predictions with \modeljoint. Our predictions 
% VJ - \remove{are actually} 
appear to be
more accurate than the ground truth in some cases (see the skateboard axles in \ref{fig:errors_wrong} and the rocket fins in \ref{fig:errors_incomplete}).}
    \vspace{-1mm}
    \label{fig:errors}
\end{figure*}\paragraph{Network architecture of \modelthree.} We use a $1\times 1$ CONV layer in the beginning,
% VJ - 5 BCLs ($T=5$), and 2 $1\times 1$ CONV layers at the end in 
followed by 5 BCLs ($T=5$), and then 2 $1\times 1$ CONV layers in 
\modelthree for the ShapeNet part segmentation task. 
The number of output channels in each layer are: 
{\verb C32-B64-B128-B256-B256-B256-C128-Cx }. 
% VJ - \vjcomment{(change to `channels' here as well?)}. 
`x' in the last CONV layer denotes the number of part categories, and 
ranges from 2-6 for different object categories.  
We use an initial scale 
$\Lambda_0=64I_3$ for scaling lattice features $XYZ$, and divide the scale in half after each BCL: 
$(64I_3, 32I_3, 16I_3, 8I_3, 4I_3)$. 

\vspace{-0.3cm}\paragraph{Network architecture of \modeljoint.} We use \modelthree as described above as the 3D component 
of the joint model. The `2D-3D Fusion' component has 2 $1\times 1$ CONV layers: {\verb C128-Cx }. 
The same DeepLab architecture is used for CNN$_1$. 
We use $\Lambda_a=32$ in BCL$_{\text{2D}\rightarrow\text{3D}}$. 
Since 2D prediction is not needed, CNN$_2$ and 
BCL$_{\text{3D}\rightarrow\text{2D}}$ are omitted. 

\vspace{-0.3cm}\paragraph{Training.} We train separate models for each object category. 
CNN$_1$ is initialized the same way as in the facade experiment. 
% We freeze weights in \modelthree and CNN$_1$ and only adapt the `2D-3D Fusion' component when training \modeljoint. % VJ - \vjcomment{I think, we also shouldn't freeze the 3D model weights here to have more model flexibility}.
Adam optimizer with an initial learning rate 
of $0.0001$ is used. 
% Learning rate is dropped to $0.00002$ once validation loss plateaus when training \modelthree.% VJ - \remove{During training, small random position, rotation, and stretching are applied to the 3D point clouds.} % For training, we augment point cloud training data with random rotations, translations, and scalings.
We augment point cloud data with random rotations, translations, and scalings during training. 

%\camreadycomment{
We train our networks until validation loss plateaus. Training \modelthree{} and \modeljoint{} take about $2.5$ and $3$ days respectively. 
% Using the default setting of $200$ epochs, 
With default settings, 
training PointNet++ takes $3.5$ days on the same hardware.
%}\vspace{-0.3cm}\paragraph{Dataset labeling issues.}

We observed a few types of labeling issues in the ShapeNet Part dataset: 
\begin{itemize}
\vspace{-0.1cm}
\item Some object part categories are frequently labeled incorrectly. 
\Eg, skateboard axles are often mistakenly labeled as `deck' or `wheel' (Figure~\ref{fig:errors_wrong}). 
\vspace{-0.1cm}
\item Some object parts, \eg `fin' of some rockets, have incomplete range or coverage 
(Figure~\ref{fig:errors_incomplete}).
\vspace{-0.1cm}
\item Some object part categories are labeled inconsistently between shapes. \Eg, airplane landing gears 
are seen labeled as `body', `engine', or `wings' (Figure~\ref{fig:errors_inconsistent}). 
\vspace{-0.1cm}
\item Some categories have 
% VJ - a part label that covers `others', which can be quite confusing for the classifier. \Eg 
parts that are labeled as `other', which can be confusing for the classifier as these parts do not have
clear semantic meanings or structures. \Eg, in the case of earphones,
anything that is not `headband' or `earphone' are given the same label 
% VJ 
(`other')
(Figure~\ref{fig:errors_confusing}). 
\end{itemize}

The first two issues make evaluations and comparisons on the benchmark less reliable, while the other two  make 
learning ill-posed or unnecessarily hard for the networks. 
% Pushing the performance further thus poses a potential risk of overfitting the dataset, and therefore we suggest a careful inspection beyond mere quantitative evaluations on this benchmark.% \vjcomment{This last statement sounds strong to me, as we did not do any analysis on how much errors we and other state-of-the-art techniques make are due to labelling errors. May be, there is still a good scope for improvement on this dataset (even after accounting for labelling errors)}.


\end{document}
