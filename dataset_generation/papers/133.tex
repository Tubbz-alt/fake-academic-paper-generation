
%% bare_conf.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


\documentclass[journal]{IEEEtran}
% Some Computer Society conferences also require the compsoc mode option,
% but others use the standard conference format.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
%\documentclass[conference]{IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
\usepackage[english]{babel}


\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{color}
\definecolor{g}{gray}{0.6}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\usepackage{multirow}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{longtable}

\usepackage{soul}

% For draft
\usepackage{soul}
% 
\usepackage{booktabs}
\usepackage{arydshln}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{supertabular}
\usepackage{arydshln}

\usepackage{hyperref}
% \usepackage{booktabs}       % professional-quality tables
% \usepackage{amsfonts}       % blackboard math symbols
% \usepackage{nicefrac}       % compact symbols for 1/2, etc.
% \usepackage{microtype}      % microtypography
% \usepackage{algorithm}
% \usepackage{algpseudocode}
\usepackage{subcaption}
\usepackage{multicol}
\usepackage{graphicx}




% *** GRAPHICS RELATED PACKAGES ***
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{graphics}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Inverting The Generator Of A Generative Adversarial Network}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
% \author{\IEEEauthorblockN{Antonia Creswell}
% \IEEEauthorblockA{BICV\\
% Imperial College London\\
% \\
% Email: ac2211@ic.ac.uk}
% \and
% \IEEEauthorblockN{Anil Anthony Bharath}
% \IEEEauthorblockA{BICV\\
% Imperial College London}}

\author{Antonia~Creswell and Anil~A~Bharath, Imperial College London
\thanks{e-mail: ac2211@ic.ac.uk.}}


% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}

% Generative adversarial networks (GANs) learn to synthesise new samples from a high-dimensional distribution by passing samples drawn from a latent space through a generative network. When the high-dimensional distribution describes images of a particular data set, the network should learn to generate visually similar image samples for latent variables that are close to each other in the latent space. For tasks such as image retrieval and image classification, it may be useful to exploit the arrangement of the latent space by projecting images into it, and using this as a representation for discriminative tasks. GANs often consist of multiple layers of non-linear computations, making them very difficult to invert. This paper introduces techniques for projecting image samples into the latent space using any pre-trained GAN, provided that the computational graph is available. {\color{red} We evaluate these techniques on several datasets including MNIST, Omniglot, Shoes and CelebA}. In the case of MNIST digits, we show that projections into the latent space maintain information about the style and the identity of the digit. In the case of Omniglot characters, we show that even characters from alphabets that have not been seen during training may be projected well into the latent space; this suggests that this approach may have applications in one-shot learning. We provide code at \url{https://github.com/ToniCreswell/InvertingGAN}.

Generative adversarial networks (GANs) learn a deep generative model that is able to synthesise novel, high-dimensional data samples. New data samples are synthesised by passing latent samples, drawn from a chosen prior distribution, through the generative model. Once trained, the latent space exhibits interesting properties, that may be useful for down stream tasks such as classification or retrieval. Unfortunately, GANs do not offer an ``inverse model'', a mapping from data space back to latent space, making it difficult to infer a latent representation for a given data sample. In this paper, we introduce a technique, \textit{inversion}, to project data samples, specifically images, to the latent space using a pre-trained GAN. Using our proposed \textit{inversion} technique, we are able to identify which attributes of a dataset a trained GAN is able to model and quantify GAN performance, based on a reconstruction loss. We demonstrate how our proposed \textit{inversion} technique may be used to quantitatively compare performance of various GAN models trained on three image datasets. We provide code for all of our experiments\footnote{\url{https://github.com/ToniCreswell/InvertingGAN}}.

% Our proposed \textit{inversion} technique provides an essential tool for better understanding the representation power of a GAN.

% We evaluate these techniques on three datasets including: Omniglot, Shoes and CelebA.\footnote{We provide code at \url{https://github.com/ToniCreswell/InvertingGAN}}.


\end{abstract}

% no keywords

% {\color{blue} We provide an essential debugging tool and show several examples of how our approach may be used to "trouble shoot"}

% {\color{blue} By inverting data samples we are able to identify which attributes of a dataset the trained GAN is able to model, and providing us with a method to quantify GAN performance.}




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



% \section{Introduction}
% % no \IEEEPARstart
% This demo file is intended to serve as a ``starter file''
% for IEEE conference papers produced under \LaTeX\ using
% IEEEtran.cls version 1.8b and later.
% % You must have at least 2 lines in the paragraph with the drop letter
% % (should never be an issue)
% I wish you the best of success.

% \hfill mds
 
% \hfill August 26, 2015

% \subsection{Subsection Heading Here}
% Subsection text here.


% \subsubsection{Subsubsection Heading Here}
% Subsubsection text here.


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.




\section{Introduction}

Generative adversarial networks (GANs) \cite{radford2015unsupervised, goodfellow2014generative} are a class of generative model which are able to synthesise novel, realistic looking images of faces, digits and street numbers \cite{radford2015unsupervised}. GANs involve two networks: a generator, $G$, and a discriminator, $D$. The generator, $G$, is trained to generate synthetic images, taking a random vector, $z$, drawn from a prior distribution, $P(Z)$, as input. The prior is often chosen to be a normal or uniform distribution.

Radford et al. \cite{radford2015unsupervised} demonstrated that generative adversarial networks (GANs) learn a ``rich linear structure", meaning that algebraic operations in $Z$-space often lead to semantically meaningful synthetic samples in image space. Since images represented in $Z$-space are often meaningful, direct access to a $z \in Z$ for a given image, $x \in X$ may be useful for discriminative tasks such as retrieval or classification. Recently, it has also become desirable to be able to access $Z$-space in order to manipulate original images \cite{zhu2016generative}. Thus, there are many reasons we may wish to invert the generator.
% Further, inverting the generator provides interesting insights to highlight what a trained GAN model has learned. Thus, there are many reasons that we may want map data samples back to latent space.
% Inversion is achieved by finding a vector $z \in Z$ which when passed through the generator produces an image that is very similar to the target image.

Typically, inversion is achieved by finding a vector $z \in Z$ which when passed through the generator produces an image that is very similar to the target image. If no suitable $z$ exists, this may be an indicator that the generator is unable to model either the whole image or certain attributes of the image. We give a concrete example in Section \ref{rec:shoes_rec}. Therefore, inverting the generator, additionally, provides interesting insights to highlight what a trained GAN has learned.
% , which may be very useful  


% However, traditional GAN training does not allow direct access to $Z$ for a given image. In order to exploit this structure many images often have to be generated for pre-chosen $z$-values and then grouped, based on desired attributes to discover meaningful vectors in z-space. For example to discover a "turn" vector Radford et al. \cite{radford2015unsupervised} generated many random samples and selected a number of samples with left facing faces and right facing faces, took the $z$ values that generated them and performed operations in $Z$-space to obtain the "turn vector". This process of generating samples, and searching through them manually to find left facing and right facing faces is tedious and time consuming. A more efficient approach would be to obtain $z$ vectors directly from real image samples, which in the case of CelebA (- the dataset used by Radford et al. \cite{radford2015unsupervised}- ) has labels indicating the facial orientation.

Mapping an image, from image space, $X$, to $Z$-space is non-trivial, as it requires inversion of the generator, which is often a many layered, non-linear model \cite{radford2015unsupervised, goodfellow2014generative, chen2016infogan}. Dumoulin et al. \cite{dumoulin2016adversarially} (ALI) and Donahue et al. (BiGAN) \cite{donahue2016adversarial} proposed learning a third, decoder network alongside the generator and discriminator to map image samples back to $Z$-space. Collectively, they demonstrated results on MNIST, ImageNet, CIFAR-10 and SVHN and CelebA. However, reconstructions of inversions are often poor. Specifically, reconstructions of inverted MNIST digits using methods of Donahue et al. \cite{donahue2015long}, often fail to preserve style and character class. Recently, Li et al. \cite{li2017alice} proposed method to improve reconstructions, however drawbacks to these approaches \cite{li2017alice, donahue2016adversarial, dumoulin2016adversarially} include the need to train a third network which increases the number of parameters that have to be learned, increasing the chances of over-fitting. The need to train an extra network, along side the GAN, also means that inversion cannot be performed on pre-trained networks.

A more serious concern, when employing a decoder model to perform inversion, is that its value as a diagnostic tool for evaluating GANs is hindered. GANs suffer from several pathologies including over-fitting, that we may be able to detect using inversion. If an additional encoder model is trained to perform inversion \cite{li2017alice, donahue2016adversarial, dumoulin2016adversarially,luo2017learning}, the encoder itself may over-fit, thus not portraying the true nature of a trained GAN. Since our approach does not involve training an additional encoder model, we may use our approach for ``trouble shooting'' and evaluating different pre-trained GAN models. 

% In this paper, we demonstrate several of ways in which our proposed inversion technique may be used to both qualitatively and quantitatively compare GAN models.


% We make the following contributions:

% \begin{itemize}
%     \item We propose a novel inversion technique to infer the $Z$-space representations that, when passed through a generator, produce samples that are visually similar to the images from which they were inferred. 
% \end{itemize}

% In summary, we propose an alternative approach to generator inversion which makes the following improvements:

In this paper, we make the following contributions:
\begin{itemize}
    % \item We infer the $Z$-space representations that, when passed through the generator, produce samples that are visually similar to the images from which they were inferred. 
    \item We propose a novel approach to invert the generator of any pre-trained GAN, provided that the computational graph for the generator network is available (Section \ref{sec:method}).
    \item We demonstrate that, we are able to infer a $Z$-space representation for a target image, such that when passed through the GAN, it produces a sample visually similar to the target image (Section \ref{sec:results}).
    \item We demonstrate several ways in which our proposed inversion technique may be used to \textbf{both qualitatively}  (Section \ref{rec:shoes_rec})  \textbf{and quantitatively} compare GAN models (Section \ref{sec:troubleshooting}).
    % \item We demonstrate how our propose inversion method may be used to both qualitatively (Section \ref{sec:results}) and quantitatively (Section \ref{sec:troubleshooting}) compare previously proposed GAN models.
    \item Additionally, we show that batches of $z$ samples can be inferred from batches of image samples, which improves the efficiency of the inversion process by allowing multiple images to be inverted in parallel (Section \ref{sec:batch}).
\end{itemize}  


We begin, by describing our proposed inversion technique.



% {\color{blue} Choose inversion parameters on a leave out dataset}

%Auto-encoders also allow you to 

\section{Method: Inverting The Generator}
\label{sec:method}

For a target image, $x \in \Re^{m \times m}$ we want to infer the $Z$-space representation, $z \in Z$, which when passed through the trained generator produces an image very similar to $x$. We refer to the process of inferring $z$ from $x$ as \textit{inversion}. This can be formulated as a minimisation problem:

\begin{equation} \label{cost}
    z^* = \min_z   {- \mathbb E}_x \log[G(z)] 
\end{equation}
%\[ z^* = \min_z  {\mathbb E}_x log[G(z)]  \]

Provided that the computational graph for $G(z)$ is known, $z^*$ can be calculated via gradient descent methods, taking the gradient of $G$ w.r.t. $z$. This is detailed in Algorithm \ref{min}.

% \begin{algorithm}[H]
% $z \sim P_z(Z)$ \# Initialise z \\
% \For{Number of training iterations}
% {
% $z \leftarrow z + a*G$
% }
% \label{min}
% \caption{Algorithm for infering $z^* \in \Re^n$, the latent representation for an image $x \in \Re^{m \times m}$.}
% \end{algorithm}

% \begin{algorithm}
% \caption{Algorithm for inferring $z^* \in \Re^d$, the latent representation for an image $x \in \Re^{m \times m}$.}\label{min}
% \begin{algorithmic}[1]
% \Procedure{Infer}{$x$} %\Comment{Infer $z^* \in \Re^d$ from $x \in \Re^{m \times m}$}
%   \State{$z^* \sim P_z(Z)$} \\%\Comment{Initialise z by sampling the prior distribution}
%   \While{NOT converged}
  
%       \State{$L \gets -( x \log [G(z^*)] + (1-x) \log [1-G(z^*)])$}  %\Comment{Calculate the error}
%       \State{$z^*\gets z^* - \alpha \nabla_z L $} %\Comment{Apply gradient descent}
    
%   \label{euclidendwhile}
%   \State \textbf{return} $z^*$
% \EndProcedure
% \end{algorithmic}
% \end{algorithm}

\begin{algorithm}

\caption{Algorithm for inferring $z^* \in \Re^d$, the latent representation for an image $x \in \Re^{m \times m}$.}\label{min}
\KwResult{Infer($x$)}
$z^* \sim P_z(Z)$ \;
\While{NOT converged}{
    $L \gets -( x \log [G(z^*)] + (1-x) \log [1-G(z^*)])$\;
    $z^*\gets z^* - \alpha \nabla_z L $\;
}
\textbf{return} $z^*$ \;

\end{algorithm}

Provided that the generator is deterministic, each $z$ value maps to a single image, $x$. A single $z$ value cannot map to multiple images. However, it is possible that a single $x$ value may map to several $z$ representations, particularly if the generator has collapsed \cite{salimans2016improved}. This suggests that there may be multiple possible $z$ values to describe a single image. This is very different to a discriminative model, where multiple images, may often be described by the same representation vector \cite{mahendran2015understanding}, particularly when a discriminative model learns representations tolerant to variations.

The approach described in Algorithm \ref{min} is similar in spirit to that of Mahendran et al. \cite{mahendran2015understanding}, however instead of inverting a representation to obtain the image that was responsible for it, we invert an image to discover the latent representation that generated it. %Unlike the method of Mahendran et al.\cite{mahendran2015understanding} we did not need to use any regularisation to recover the $Z$ space representation -- did use [0,1].

% \subsection{Effects Of Batch Normalisation}
% {\color{blue} check this again, can turn batch norm to eval mode}

% GAN training is non-trivial because the optimal solution is a saddle point rather than a minimum \cite{salimans2016improved}. It is suggested by Radford et al. \cite{radford2015unsupervised} that to achieve more stable GAN training it is necessary to use batch normalisation \cite{ioffe2015batch}. Batch normalisation involves calculating a mean and standard deviation over a batch of outputs from a convolutional layer and adjusting the mean and standard deviation using learned weights. If a single $z$ value is passed through a batch normalisation layer, the output of the layer may be meaningless. To prevent this problem, it would be ideal to use virtual batch normalisation \cite{salimans2016improved}, where statistics are calculated over a separate batch. However, we want to allow this technique to be applied to any pre-trained network - where virtual batch normalisation may not have been employed. To counteract the effects of batch normalisation, we propose inverting a mixed batch of image samples at a time. This not only has the desired effect of dealing with problems caused when using batch normalisation, but also allows multiple image samples to be inverted in parallel.

%The effects of batch normalisation can be easily counteracted by inverting a batch of image samples at a time. 

\subsection{Inverting A Batch Of Samples}
\label{sec:batch}

% Not only does inverting a batch of samples make sense when networks use batch normalisation, it is also a practical way to invert many images at once. We will now show that this approach is a legitimate way to update many $z^*$ values in one go.

Algorithm \ref{min} shows how we can invert a single data sample, however it may not be efficient to invert single images at a time, rather, a more practical approach is to invert many images at once. We will now show that we are able to invert batches of examples.

Let \textbf{z}$_b \in \Re^{B \times n}$, \textbf{z}$_b=\{z_1, z_2, ... z_B\}$ be a batch of $B$ samples of $z$. This will map to a batch of image samples \textbf{x}$_b \in \Re^{B \times m \times m}$, \textbf{x}$_b=\{x_1, x_2, ... x_B\}$. For each pair $(z_i, x_i)$, $i \in \{1...B\}$, a loss $L_i$, may be calculated. The update for $z_i$ would then be $z_i \gets z_i - \alpha \frac{d L_i}{d z_i}$ 

If reconstruction loss is calculated over a batch, then the batch reconstruction loss would be $\sum_{i=\{1,2...B\}} L_i$, and the update would be: 
\begin{equation}
    \nabla_{\textbf{z}_b} L = \frac{\partial \sum_{i \in \{1,2,...B\}} L_i}{\partial (\textbf{z}_b)}
\end{equation}
\begin{equation}
    =\frac{\partial(L_1+L_2...+L_i)}{\partial(\textbf{z}_b)}
\end{equation}
\begin{equation}
    = \frac{dL_1}{dz_1}, \frac{dL_2}{dz_2}, ... \frac{dL_B}{dz_B}
\end{equation}
% \[\nabla_z L = \frac{\sum_{i=\{1,2...B\}} L_i}{\textbf{z}_b} \]
% \[=\frac{\partial(L_1+L_2...+L_i)}{\partial(\textbf{z}_b)} \]
% \[= \frac{dL_1}{dz_1}, \frac{dL_2}{dz_2} ... \frac{dL_B}{dz_B} \]

Each reconstruction loss depends only on $G(z_i)$, so $L_i$ depends only on $z_i$, which means $\frac{\partial L_i}{\partial z_j}=0$, for all $i\not=j$. This shows that $z_i$ is updated only by reconstruction loss $L_i$, and the other losses do not contribute to the update of $z_i$, meaning that it is valid to perform updates on batches.
% making it feasible to perform updates on batches.
% making the use of batch updates a valid approach.

% Note that this may not strictly be true when batch normalisation \cite{ioffe2015batch} is applied to outputs of convolutional layers in the generative model, since batch statistics are used to normalise these outputs. However, provided that the size of the batch is sufficiently large we assume that the statistics of a batch are approximately constant parameters for the dataset, rather than being dependant on the specific $z_{j=1,...B}$ values in the batch.

\subsection{Using Prior Knowledge Of P(Z)}

A GAN is trained to generate samples from a $z \in Z$ where the distribution over $Z$ is a chosen prior distribution, $P(Z)$. $P(Z)$ is often a multivariate Gaussian or uniform distribution. If $P(Z)$ is a multivariate uniform distribution, $\mathcal{U}[a,b]$, then after updating $z^*$, it can be clipped to be between $[a,b]$. This ensures that $z^*$ lies in the probable regions of $Z$. If $P(Z)$ is a multivariate Gaussian Distribution, $\mathcal{N}[\mathbf{\mu},\mathbf{\sigma}^2]$, regularisation terms may be added to the cost function, penalising samples that have statistics that are not consistent with $P(Z)=\mathcal{N}[\mathbf{\mu},\mathbf{\sigma}^2]$.


If $z \in Z$ is a vector of length $d$ and each of the $d$ elements in $z \in \Re^d$ are drawn independently and from identical distributions, we may be able to add a regularisation term to the loss function. For example, if $P(Z)$ is a multivariate Gaussian distribution, then elements in a single $z$ are independent and identically drawn from a Gaussian distribution. Therefore we may calculate the likelihood of an encoding, z, under a multivariate Gaussian distribution by evaluating:

\[ \log P(z) = \log P(z^1, ..., z^d) = \frac{1}{d}\sum_{i=0}^d \log \mathcal{P}(z^i) \] 
where $z^i$ is the $i^{th}$ element in a latent vector z and $\mathcal{P}$ is the probability density function of a (univariate) Gaussian, which may be calculated analytically. Our new loss function may be given by:


% For instance, if $P(Z)$ is a distribution with mean, $\mu$ and standard deviation $\sigma$, we get the new loss function:

% %\[ L(z,x) = {\mathbb E}_x log[G(z)] + \gamma_1|| \mu - \hat{\mu} || + \gamma_2|| \sigma - \hat{\sigma}|| \]

\begin{equation} \label{eqn:regLoss}
     L(z,x) = {\mathbb E}_x \log[G(z)] - \beta \log P(z)
\end{equation}

by minimising this loss function (Equation \ref{eqn:regLoss}), we encourage $z^*$ to come from the same distribution as the prior.

% {\color{blue} Change to variance rather than std}

% where $\hat{\mu}$ is the mean value of elements in $z$, $\hat{\sigma}$ is the standard deviation of elements in $z$ and $\gamma_{1,2}$ are weights.

% Since $d$ is often quite small (e.g. $100$ \cite{radford2015unsupervised}), it is unrealistic to expect the statistics of a single $z$ to match those of the prescribed prior. However, since we are able to update a batch of samples at a time, we can calculate $\hat{\mu}$ and $\hat{\sigma}$ over many samples in a batch to get more meaningful statistics.

\section{Relation to Previous Work}

In this paper, we build on our own work\footnote{Creswell, Antonia, and Anil Anthony Bharath. "Inverting The Generator Of A Generative Adversarial Network." arXiv preprint arXiv:1611.05644 (2016). This paper was previously accepted at the NIPS Workshop on Adversarial Training, which was made available as a non archival pre-print only}. We have augmented the paper, by performing additional experiments on a shoe dataset \cite{luo2017learning} and CelebA, as well as repeating experiments on the Omniglot dataset using the DCGAN model proposed by Radford et al. \cite{radford2015unsupervised} rather than our own network \cite{creswell2016task}. In addition to proposing a novel approach for mapping data samples to their corresponding latent representation, we show how our approach may be used to quantitatively and qualitatively compare models.

%The purpose of the inversion process in Zhu et al. \cite{zhu2016generative} was to map samples to the latent space, manipulate the latent space, and map batch into image space to produce a manipulated version of the original image. 

Our approach to inferring $z$ from $x$ is similar to the previous work of Zhu et al. \cite{zhu2016generative}, however we make several additional contributions. 

% This approach of inferring $z$ from $x$ bears similarities to work of Zhu et al. \cite{zhu2016generative}; we now highlight the differences between the two approaches and the benefits of our approach over that of Zhu et al. \cite{zhu2016generative}. {\color{red} Primarily, our approach does not require an encoder to be trained, making our approach useful for "trouble shooting"}.

% Primarily, we address issues related to batch normalisation by showing that a mixed batch of image samples can be inverted to obtain latent $z$ encodings. Potential problems encountered when using batch normalisation are not discussed by Zhu et al. \cite{zhu2016generative}.

%By design, some $z \in Z$ values are more probable than others, according to the prior $P(Z)$ that $z$ samples are drawn from during GAN training.

% {\color{blue} Move or remove this paragraph}(To keep or not?)
% The generator of a GAN is trained to generate image samples $x \in X$ from a $z \in Z$ drawn from a prior distribution $P(Z)$. This means that some $z$ values are more probable that other $z$ values. It makes sense, then, that the inferred $z$'s are also from (or at least near) $P(Z)$. We introduce hard and soft constraints to be used during the optimisation process, to encourage inferred $z$'s to be likely under the prior distribution $P(Z)$. Two common priors often used when training GAN's are the uniform and normal distribution; we show that our method copes with both of these priors.

%(This is addressed by Donahue et al. and Dumolin et al. \cite{donahue2016adversarial, dumoulin2016adversarially}, however requires and extra network to be trained).

Specifically, Zhu et al. \cite{zhu2016generative} calculate reconstruction loss, by comparing the features of $x$ and $G(z^*)$ extracted from layers of AlexNet, a CNN trained on natural scenes. This approach is likely to fail if generated samples are not of natural scenes (e.g. Omniglot handwritten characters). Our approach considers pixel-wise loss, providing an approach that is generic to the dataset. Further, if our intention is to use the inversion to better understand the GAN model, it is essential not to incorporate information from other pre-trained networks in the inversion process.

% {\color{blue} Put the regularisation techniques here.}

An alternative class of inversion methods involves training a separate encoding network to learn a mapping from image samples, $x$ to latent samples $z$. Li et al \cite{li2017alice}, Donahue et al. \cite{donahue2016adversarial} and Dumoulin et al. \cite{dumoulin2016adversarially} propose learning the encoder along side the GAN. Unfortunately, training an additional encoder network may encourage over-fitting, resulting in poor image reconstruction. Further, this approach may not be applied to pre-trained models. 

On the other hand, Luo et al.\footnote{Luo et al. cite our pre-print} \cite{luo2017learning}, train an encoding network after the GAN has been trained, which means that their approach may be applied to pre-trained models. One concern about the approach of Luo et al. \cite{luo2017learning}, is that it may not be an accurate reflection of what the GAN has learned, since the learned decoder may over-fit to the examples it is trained on. For this reason, the approach of Luo et al. \cite{luo2017learning} may not be suitable for inverting image samples that come from a different distribution to the training data. Evidence of over-fitting may be found in Luo et al. \cite{luo2017learning}-Figure 2, where ``original'' image samples being inverted are synthesised samples, rather than samples from a test set data; in other words Luo et al. \cite{luo2017learning} show results (Figure 2) for inverting synthesised samples, rather than \textit{real} image samples from a test set. 

In contrast to Luo et al. \cite{luo2017learning}, we demonstrate our inversion approach on data samples drawn from test sets of real data samples. To make inversion more challenging, in the case of the Omniglot dataset, we invert image samples that come from a different distribution to the training data. We invert image samples from the Omniglot handwritten characters dataset that come from a different set of alphabets to the set used to train the (Omniglot) GAN  (Figure \ref{fig:omni_rec}). Our results will show that, using our approach, we are still able to recover a latent encoding that captures \textbf{most} features of the test data samples.
%In this paper we demonstrate that our approach works well, even when we invert image samples that come from a different distribution to the training data, by inverting image samples from the Omniglot hand written characters dataset that come from a different set of alphabets to the set used to pre-train the (Omniglot) GAN  (Figure \ref{inv_omni}).

% {\color{blue} Be careful here -- Omniglot samples are no longer so good!}

Finally, previous inversion approaches that use learned encoder models \cite{li2017alice, donahue2016adversarial, dumoulin2016adversarially, luo2017learning} may not be suitable for ``trouble shooting", as symptoms of the GAN may be exaggerated by an encoder that over-fits. We discuss this in more detail in Section \ref{sec:troubleshooting}.

% Finally, we emphasise that while there are other techniques that provide inversion, ours is the only one that is \textbf{both} (a) immune to over-fitting, in other words we do not train an encoder network that may itself over-fit, and (b) can be applied to any pre-trained GAN model provided that the computational graph is available.


% , as we do with the Omniglot dataset, we may not be confident }

%  Luo et al. \cite{luo2017learning} do not test their approach on the case where the test data comes from a different distribution to the training data, as we do with the Omniglot dataset, we may not be confident }

% While this approach is different to those proposed \cite{li2017alice, donahue2016adversarial, dumoulin2016adversarially}
% {\color{red} Recent developments in the GAN literature has ... augmenting GANs with encoder networks, these include the ALI \cite{dumoulin2016adversarially}} %--already included
% Depends on the fine grain sampling of the latent space...it is done randomly in the paper...}

% However, this approach does not take into account knowledge of the prior distribution}
% {\color{red} Papers to include (that cite the NIPS WS paper):
% \begin{itemize}
%     \item \cite{luo2017learning}
% \end{itemize}
% }

% \section{Generative vs. Discriminative Representations}

% One image per representation
% Perhaps there is more that one representation per image however?



% \section{Applications}
% Mapping samples to a more compact representation

\section{``Pre-trained'' Models}
% We train four models on two different datasets, MNIST and Omniglot \cite{lake2015human}. In order to compare the effects of regularisation or clipping when using a normal or uniform prior distribution respectively, we train networks on each dataset, using each prior - totalling four models. {\color{red} Additionally, we train two models on a shoe dataset \cite{yu2014fine} each using different resolution images}.
In this section we discuss the training and architecture details of several different GAN models, trained on $3$ different datasets, which we will use for our inversion experiments detailed in Section \ref{sec:experiments}. We show results on total of $10$ trained models (Sections \ref{sec:results} and \ref{sec:troubleshooting}).

% \subsection{MNIST}
% The MNIST dataset consists of $60$k samples of hand written digits, $0$ to $9$. The dataset is split into $50$k samples for training and $10$k samples for testing. Both the training and testing dataset contains examples of all digits between $0$ to $9$. The architecture for the generator and discriminator networks trained on MNIST digits are detailed in Table \ref{MNIST_arch}. We trained two GANs, one with a multivariate Gaussian prior, the other with a multivariate uniform prior. The GANs were trained for $500$ iterations with batch size $128$, learning rate $0.002$ using Adam updates. The networks are trained on $50$k MNIST training samples, covering all $10$ categories. Figure \ref{gen} shows examples of synthetic MNIST samples.

% \begin{table}
% \centering
% \begin{tabular}{cc}\toprule
%     G(z) & D(x) \\ \hline
%     input: $z \in \Re^{100}$ & input: $x \in \Re^{28 \times 28}$ \\
%     fully connected 1024 units + batch norm + relu & conv  64,5,5 + upsample + batch norm + leaky relu(0.2) \\
%     fully connected 6272 units + batch norm + relu & conv  128,5,5 + upsample + batch norm + leaky relu(0.2) \\
%     reshape(128,7,7) & reshape(6272) \\
%     conv 64,5,5 + down-sample + batch norm + relu & fully connected 1024 units + leaky relu(0.2) \\
%     conv 1,5,5 + down-sample + batch norm + relu & fully connected 1 unit + leaky relu(0.2)\\
%     \bottomrule
% \end{tabular}
% \caption{\textbf{MNIST Architecture}: Model architecture for learning to generate MNIST characters}
% \label{MNIST_arch}
% \end{table}
% \begin{table}
% \centering
% \begin{tabular}{cc}\toprule
%     G(z) \vspace{0.2cm} \\ \hline %& D(x) \\ \hline 
%     input: $z \in \Re^{100}$ \\ %& input: $x \in \Re^{28 \times 28}$ \\
%     fully connected 1024 units + batch norm + relu \\ % & conv  64,5,5 + upsample + batch norm + leaky relu(0.2) \\
%     fully connected 6272 units + batch norm + relu \\ %& conv  128,5,5 + upsample + batch norm + leaky relu(0.2) \\
%     reshape(128,7,7) \\ % & reshape(6272) \\
%     conv 64,5,5 + down-sample + batch norm + relu \\ % & fully connected 1024 units + leaky relu(0.2) \\
%     conv 1,5,5 + down-sample + batch norm + relu \\ %& fully connected 1 unit + leaky relu(0.2)\\
%     \toprule
%     D(x) \vspace{0.2cm} \\ \hline
    
%     input: $x \in \Re^{28 \times 28}$ \\
%     conv  64,5,5 + upsample + batch norm + leaky relu(0.2) \\
%     conv  128,5,5 + upsample + batch norm + leaky relu(0.2) \\
%     reshape(6272) \\
%     fully connected 1024 units + leaky relu(0.2) \\
%     fully connected 1 unit + leaky relu(0.2)\\
%     \bottomrule
% \end{tabular}
% \caption{\textbf{MNIST Architecture}: Model architecture for learning to generate MNIST characters}
% \label{MNIST_arch}
% \end{table}
  
  
% \begin{figure}
% \centering 
%     \begin{subfigure}{\columnwidth}
%     \includegraphics[width=0.9\columnwidth]{graphics/omni_gen_GAN.png} %Ex_15
%     \caption{\textbf{Synthetic Omniglot Samples from a GAN}}
%     \end{subfigure}
%     \begin{subfigure}{\columnwidth}
%     \includegraphics[width=0.9\columnwidth]{graphics/omni_gen_WGAN_smallNZ.png}
%     \caption{\textbf{Synthetic Omniglot Samples from a WGAN, latent encoding size, nz=100}} %Ex_24
%     \end{subfigure}
%     \begin{subfigure}{\columnwidth}
%     \includegraphics[width=0.9\columnwidth]{graphics/omni_gen_WGAN_largeNZ_temp.png} %Ex_25
%     \caption{\textbf{Synthetic Omniglot Samples from a WGAN, latent encoding size, nz=500}}
%     \end{subfigure}
% \centering
% \caption{\textbf{Synthesised Omniglot Samples} using a GAN and WGAN {\color{red} possibly compare nz instead}}
% \label{gen}
% \end{figure}

% {\color{blue} Compare to other methods to quantify over-fitting}
  
\subsection{Omniglot}
The Omniglot dataset \cite{lake2015human} consists of characters from $50$ different alphabets, where each alphabet has at least $14$ different characters. The Omniglot dataset has a background dataset, used for training and a test dataset. The background set consists of characters from $30$ writing systems, while the test dataset consists of characters from the other $20$. Note, characters in the training and testing dataset come from different writing systems. We train both a DCGAN \cite{radford2015unsupervised} and a WGAN \cite{arjovsky2017wasserstein} using a latent representation of dimension, $d=100$. The WGAN \cite{arjovsky2017wasserstein} is a variant of the GAN that is easier to train and less likely to suffer from mode collapse; mode collapse is where synthesised samples look similar to each other. All GANs are trained with additive noise whose standard deviation decays during training \cite{arjovsky2017towards}.  Figure \ref{fig:omni_gen} shows Omniglot samples synthesised using the trained models. Though it is clear from Figure \ref{fig:omni_gen}(a), that the GAN has collapsed, because the generator is synthesising similar samples for different latent codes, it is less clear to what extent the WGAN  (Figure \ref{fig:omni_gen}(b)) may have collapsed or over-fit. It is also unclear from Figure \ref{fig:omni_gen}(b) what representative power, the (latent space of the)  WGAN has. Results in sections \ref{sec:results} and \ref{sec:troubleshooting} will provide more insight into the representations learned by these models.

% The generator and discriminator networks for learning Omniglot characters \cite{lake2015human} are the same as those used in previous work \cite{creswell2016task}. We trained two GANs, one with a multivariate Gaussian prior, the other with a multivariate uniform prior. The GANs were trained only on the background dataset, for $2000$ iterations with random batches of size $128$, using Adam updates with learning rate $0.002$. The latent encoding has dimension, $d=100$. Figure \ref{gen} shows synthetic Omniglot data samples.

\begin{figure}
\centering 
    \begin{subfigure}{\columnwidth}
    \includegraphics[width=0.9\columnwidth]{graphics/omni_gen_GAN.png} %Ex_9
    \caption{\textbf{Synthetic Omniglot Samples from a GAN}.}
    \end{subfigure}
    \begin{subfigure}{\columnwidth}
    \includegraphics[width=0.9\columnwidth]{graphics/omni_gen_WGAN.png}
    \caption{\textbf{Synthetic Omniglot Samples from a WGAN}.} %Ex_3 Ex197
    \end{subfigure}
\centering
\caption{\textbf{Synthetic Omniglot samples:} Shows samples synthesised using a (a) GAN and (b) WGAN.}
\label{fig:omni_gen}
\end{figure}

\subsection{Shoes}
The shoes dataset \cite{yu2014fine} consists of c.$50,000$ examples of shoes in RGB colour, from $4$ different categories and over $3000$ different subcategories. The images are of dimensions $128\times128$. We leave $1000$ samples out for testing and use the rest for training. We train two GANs using the DCGAN \cite{radford2015unsupervised} architecture. We train one DCGAN with full sized images and the second we train on $64\times64$ images. The networks were trained according to the setup described by Radford et al. \cite{radford2015unsupervised}, using a multivariate Gaussian prior. We also train a WGAN \cite{arjovsky2017wasserstein} on full sized images. All GANs are trained with additive noise whose standard deviation decays during training \cite{arjovsky2017towards}. Figure \ref{fig:shoe_gen} shows samples randomly synthesised using the DCGAN models trained on shoes. The samples look quite realistic, but again, they do not tell us much about the representations learned by the GANs.

\begin{figure}
\begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{graphics/shoes_gen_64.png}
    \caption{\textbf{Synthetic Shoe Samples from a DCGAN $64\times64$}.}
    \label{fig:my_label}
\end{subfigure}
\begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{graphics/shoes_gen_128.png}
    \caption{\textbf{Synthetic Shoe Samples from a DCGAN $128\times128$}.}
    \label{fig:my_label}
\end{subfigure}
\begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{graphics/shoes_gen_WGAN.png} %Ex_7
    \caption{\textbf{Synthetic Shoe Samples from a WGAN $64\times64$}.}
    \label{fig:my_label}
\end{subfigure}
\caption{\textbf{Shoe samples synthesised using GANs:} Shows samples from DCGANs trained on  (a) lower resolution ($64\times64$) images, (b) higher resolution images ($128\times128$) and (c) samples from a WGAN. }
\label{fig:shoe_gen}
\end{figure}

\subsection{CelebA}
The CelebA dataset consists of $250,000$ celebrity faces, in RGB colour. The images are of dimensions $64\times64$ pixels. We leave $1000$ samples out for testing and use the rest for training. We train three models, a DCGAN and WGAN trained with decaying noise \cite{arjovsky2017towards} and a DCGAN trained without noise. The networks are trained according to the setup described by Radford et al. \cite{radford2015unsupervised}. Figure \ref{fig:face_gen} shows examples of faces synthesised with and without noise. It is clear from Figure \ref{fig:face_gen}(a) that the GAN trained without noise has collapsed, synthesising similar examples for different latent codes. The WGAN produces the sharpest and most varied samples. However, these samples do not provide sufficient information about the representation power of the models.

\begin{figure}
\begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{graphics/face_gen_noise.png} %Ex_19
    \caption{\textbf{Synthetic Face Samples from GAN trained with noise.}}
    \label{fig:my_label}
\end{subfigure}
\begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{graphics/face_gen_GAN_noNoise.png} %Ex_20
    \caption{\textbf{Synthetic Face Samples from a GAN trained without noise.}}
    \label{fig:my_label}
\end{subfigure}
% \begin{subfigure}{\columnwidth}
%     \centering
%     \includegraphics[width=\columnwidth]{} %Ex_20 (monal)
%     \caption{\textbf{Synthetic CelebA Face Samples from a WGAN trained without noise}}
%     \label{fig:my_label}
% \end{subfigure}
\begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{graphics/face_gen_WGAN_noise_epoch54.png} %Ex_23
    \caption{\textbf{Synthetic Face Samples from a WGAN trained with noise.}}
    \label{fig:face_gen}
\end{subfigure}
\caption{\textbf{Celebrity faces synthesised using GANs:} Shows samples from DCGANs trained (a) without noise and (b) with noise, and (c) samples from a WGAN.}
\label{fig:shoe_gen}
\end{figure}

% Note that although the cost function 

\section{Experiments}
\label{sec:experiments}
% \subsection{Evaluation Methods}
To obtain latent representations, $z^*$ for a given image $x$ we apply our proposed inversion technique to a batch of randomly selected test images, $x \in X$. To invert a batch of image samples, we minimised the cost function described by Equation (\ref{eqn:regLoss}). In most of our experiments we use $\beta=0.01$, unless stated otherwise, and update candidate $z^*$ using an RMSprop optimiser, with a learning of rate $0.01$.

A valid inversion process should map a target image sample, $x\in X$ to a $z^* \in Z$, such that when $z^*$ is passed through the generative part of the GAN, it produces an image, $G(z^*)$, that is close to the target image, $x$. However, the quality of the reconstruction, depends heavily on the latent representation that the generative model has learned. In the case where a generative model is only able to represent some attributes of the target image, $x$, the reconstruction, $G(z^*)$ may only partially reconstruct $x$.

Thus, the purpose of our experiments is two fold:
\begin{enumerate}
    \item To demonstrate qualitatively, through reconstruction, ($G(z^*)$), that for most well trained GANs, our inversion process is able to recover a latent code, $z^*$, that captures \textbf{most of the important features} of a target image (Section \ref{sec:results}).
    \item To demonstrate how our proposed inversion technique may be used to both qualitatively (Section \ref{rec:shoes_rec}). and \textbf{quantitatively} compare GAN models (Section \ref{sec:troubleshooting}).
\end{enumerate}

% A valid inversion process should map a target image sample, $x\in X$ to a $z^* \in Z$, such that when $z^*$ is passed through the generative part of the GAN, it produces an image, $G(z^*)$, that is close to the target image, $x$. {\color{red} In the case where a generative model is only able to represent some attributes of the image, $x$, the reconstruction, $G(z^*)$ may only partially reconstruct $x$.}

% The quality of the latent code, $z^*$, obtained for a target image, $x$, may be assessed by looking at a reconstructed image, $G(z^*)$. However, the quality of the reconstruction, depends heavily on the latent representation that the generative model has learned.

% {\color{red} These experiments are designed, in part, to evaluate the proposed inversion process and, more importantly, to demonstrate qualitatively which features of an image each GAN model is able (and not able) to represent. A quantitative evaluation is given in the next Section.} 

% In our experiments, we apply our proposed inversion technique to a batch of randomly selected test images, $x \in X$. To invert a batch of image samples, we minimised the cost function described by Eqn. \ref{cost}. 
% For this reason, we propose to use our inversion approach to evaluate generative models

% {\color{blue} Comments on smoothness of latent space, a good generative model may have a smooth latent space?}

% To quantitatively evaluated the quality of image reconstruction by taking the mean absolute pixel error across all reconstructions for each of the reconstruction methods. For qualitative evaluation, we show pairs of $x$ and their reconstruction, $G(z^*)$. By visualising the inversions, we can assess to what extent the the digit or character identity is preserved.

% {\color{red} When comparing models, we used the same optimisation hyper-parameters to find $z^*$}

\section{Reconstruction Results}
\label{sec:results}

\subsection{Omniglot}
The Omniglot inversions are particularly challenging, as we are trying to find a set of $z^*$'s for a set of characters, $x$, from alphabets that were not in the training data. The inversion process will involve finding representations for data samples from alphabets that it has not seen before, using information about alphabets that it has seen. The original and reconstructed samples are shown in Figure \ref{fig:omni_rec}.

% This is challenging the inversion process to invert samples from alphabets that it has not seen before, using information about alphabets that it has seen. The original and reconstructed samples are shown in Figure \ref{fig:omni_rec}.

In our previous work,\footnote{https://arxiv.org/abs/1611.05644} we showed that given the ``correct'' architecture, we are able to find latent representations that lead to excellent reconstructions. However, here we focus on evaluating standard models \cite{radford2015unsupervised} and we are particularly interested in detecting (and quantifying) where models fail, especially since visual inspection of synthesised samples may not be sufficient to detect model failure. 

% In our previous work,\footnote{https://arxiv.org/abs/1611.05644} we showed that given the ``correct'' architecture, we are able to find latent representations that lead to excellent reconstructions. However, here we focus on evaluating standard models \cite{radford2015unsupervised} and we are particularly interested in detecting (and quantifying) where models fail, especially when it may be less apparent from their synthesised samples.

It is clear from Figure \ref{fig:omni_gen} that the GAN has over-fit, however it was less clear whether or not the WGAN has, since samples appeared to be more varied. By attempting to perform inversion, we can see that the WGAN has indeed over-fit, as it is only able to partially reconstruct the target data samples. In the next section (Section \ref{sec:troubleshooting}), we quantitatively compare the extent to which the GAN and WGAN trained on the Omniglot dataset have over-fit.

%Something about different distributions...eval and training data...hence why regularisation does not help.
% \begin{figure}[h]
%     \begin{subfigure}{\columnwidth}
%         \includegraphics[width=\columnwidth]{graphics/omni_original.png}
%         \caption{Omniglot handwritten characters, $x$, from alphabets different to those seen during training}
%     \end{subfigure}
%     \begin{subfigure}{\columnwidth}
%         \includegraphics[width=\columnwidth]{graphics/omni_rec_GAN.png} %Ex_15
%         \caption{Reconstructed data samples, $G(z^*)$, using a GAN ($d=100$)}
%     \end{subfigure}
%     \begin{subfigure}{\columnwidth}
%         \includegraphics[width=\columnwidth]{graphics/omni_rec_WGAN_smallNZ.png} %Ex_24
%         \caption{Reconstructed data samples, $G(z^*)$, using a WGAN, ($d=100$)}
%     \end{subfigure}
%     \begin{subfigure}{\columnwidth}
%         \includegraphics[width=\columnwidth]{} %Ex_25
%         \caption{Reconstructed data samples, $G(z^*)$, using a WGAN, ($d=500$)}
%     \end{subfigure}
%     \caption{\textbf{Reconstruction of Omniglot handwritten characters}}
%     \label{fig:omni_rec}
% \end{figure}

\begin{figure}[h]
\centering
    \begin{subfigure}{\columnwidth}
    \centering
        \includegraphics[width=\columnwidth]{graphics/omni_original.png}
        \caption{ Target Omniglot handwritten characters, $x$, from alphabets different to those seen during training.}
    \end{subfigure}
    \begin{subfigure}{\columnwidth}
        \includegraphics[width=\columnwidth]{graphics/omni_rec_GAN.png} %Ex_9
        \caption{Reconstructed data samples, $G(z^*)$, using a GAN.}
    \end{subfigure}
    % \begin{subfigure}{\columnwidth}
    %     \includegraphics[width=\columnwidth]{graphics/omni_rec_WGAN.png} %Ex_25
    %     \caption{Reconstructed data samples, $G(z^*)$, using a WGAN}
    % \end{subfigure}
    % \begin{subfigure}{\columnwidth}
    %     \includegraphics[width=\columnwidth]{graphics/omni_rec_WGAN_overlay.png}
    %     \caption{Reconstructed data samples, $G(z^*)$, using a WGAN overlaid with $x$.}
    % \end{subfigure}
    % \begin{subfigure}{\columnwidth}
    %     \includegraphics[width=\columnwidth]{graphics/omni_rec_WGAN.png} %Ex_31
    %     \caption{Reconstructed data samples, $G(z^*)$, using a WGAN}
    % \end{subfigure}
    % \begin{subfigure}{\columnwidth}
    \begin{subfigure}{\columnwidth}
        \includegraphics[width=\columnwidth]{graphics/omni_rec_WGAN.png} %Ex_3
        \caption{Reconstructed data samples, $G(z^*)$, using a WGAN.}
    \end{subfigure}
    \begin{subfigure}{\columnwidth}
        \includegraphics[width=\columnwidth]{graphics/one_on_top.png}
        \caption{Reconstructed data samples, $G(z^*)$, using a WGAN overlaid with $x$.}
    \end{subfigure}
    \caption{\textbf{Reconstruction of Omniglot handwritten characters.}}
    \label{fig:omni_rec}
\end{figure}


\subsection{Shoes}
\label{rec:shoes_rec}
In Figure \ref{fig:shoes_rec} we compare shoe reconstructions using a DCGAN trained on low and high resolution images. By comparing reconstructions in Figures \ref{fig:shoes_rec} (b) and (c) (particularly the blue shoe on the top row) we see that the lower resolution model has failed to capture some structural details, while the higher resolution model has not. This suggests that the model trained on higher resolution images is able to capture more structural details than the model trained on lower resolution images. Using our inversion technique to make comparisons between models is just one example of how inversion may also be used to ``trouble shoot'' and identify which features of a dataset our models are not capturing.

Additionally, we may observe that while the GAN trained on higher resolution images preserves more structure than the GAN trained on lower resolution images, it still misses certain details. For example the reconstructed red shoes do not have laces (top left Figure \ref{fig:shoes_rec}(b,c)) laces. This suggests that the representation is not able to distinguish shoes with laces from those without. This may be important when designing representations for image retrieval, where a retrieval system using this representation may be able to consistently retrieve red shoes, but less consistently retrieve red shoes with laces. This is another illustration of how a good inversion technique may be used to better understand what representation is learned by a GAN.

Figure \ref{fig:shoes_rec}(d) shows reconstructions using a WGAN trained on low resolution images. We see that the WGAN is better able to model the blue shoe, and some ability to model the ankle strap, compared to the GAN trained on higher resolution images. It is however, difficult to asses from reconstructions, which model best represents the data. In the next Section (\ref{sec:troubleshooting}), we show how our inversion approach may be used to quantitatively compare these models, and determine which learns a better (latent) representation for the data.
% This suggests that if this model were, for example, to be used for retrieval, it may be able to consistently retrieve red shoes, but less consistently retrieve red shoes with laces, since this may not be encoded in the representation. This is another example of how our inversion technique may be used to better understand what representation learned by a GAN.}

Finally, we found that while regularisation of the latent space may not always improve reconstruction fidelity, it can be helpful for ensuring that latent encodings, $z^*$, found through inversion, correspond to images, $G(z^*)$ that look more like shoes. Our results in Figure \ref{fig:shoes_rec} were achieved using $\beta=0.01$.

\begin{figure}[h]
    \begin{subfigure}{\columnwidth}
        \includegraphics[width=\columnwidth]{graphics/shoes_128_original.png}
        \caption{Shoe data samples, $x$, from a test set}
    \end{subfigure}
    \begin{subfigure}{\columnwidth}
        \includegraphics[width=\columnwidth]{graphics/shoes_128_rec.png}
        \caption{Reconstructed data samples, $G(z^*)$ using a GAN at resolution $128\times128$}
    \end{subfigure}
    \begin{subfigure}{\columnwidth}
        \includegraphics[width=\columnwidth]{graphics/shoes_64_rec.png}
        \caption{Reconstructed data samples, $G(z^*)$ using a GAN at resolution $64\times64$}
    \end{subfigure}
    \begin{subfigure}{\columnwidth}
        \includegraphics[width=\columnwidth]{graphics/shoes_rec_WGAN.png} %Ex_7
        \caption{Reconstructed data samples, $G(z^*)$ using a WGAN at resolution $64\times64$}
    \end{subfigure}
    \caption{\textbf{Reconstruction of Shoes.} By comparing reconstructions, particularly of the blue shoe, we see that the higher resolution model (b) is able to capture some structural details, specifically the shoe's heel, that the lower resolution model (c) does not. Further, the WGAN (d) is able to capture additional detail, including the blue shoe's strap. These results demonstrate how inversion may be a useful tool for comparing which features of a dataset each model is able to capture.}
    \label{fig:shoes_rec}
\end{figure}


\subsection{CelebA}

Figure \ref{fig:face_rec} shows reconstructions using three different GAN models. Training GANs can be very challenging, and so various modifications may be made to their training to make them easier to train. Two examples of modifications are (1) adding corruption to the data samples during training \cite{arjovsky2017towards} and (2) a reformulation of the cost function to use the Wasserstein distance. While these techniques are known to make training more stable, and perhaps also prevent other pathologies found in GANs for example, mode collapse, we are interested to compare the (latent) representations learned by these models.

The most faithful reconstructions appear to be those from the WGAN Figure \ref{fig:face_rec} (b). This will be confirmed quantitatively in the next section. By observing reconstruction results across all models in Figure \ref{fig:face_rec}, it is apparent that all three models fail to capture a particular mode of the data; all three models fail to represent profile views of faces.

%could argue representations have been compared through semi-supervised training...

% {\color{red} It is common to add noise during the training of GANs \cite{} as this improves training stability. By applying inversion to a GAN trained with noise and a GAN trained with out noise, we are able quantify ... }
% {\color{blue} Look for other claims...}

% {\color{red} Observing inversion results shown in Figure \ref{fig:face_rec}(b) we may identify at least one mode that the GAN has failed to capture, this would profile views of faces.}

\begin{figure}[h]
    \begin{subfigure}{\columnwidth}
        \includegraphics[width=\columnwidth]{graphics/face_original.png}
        \caption{CelebA faces, $x$, from a test set}
    \end{subfigure}
    \begin{subfigure}{\columnwidth}
        \includegraphics[width=\columnwidth]{graphics/face_rec_WGAN_noise_Ex23epoch54.png}
        \caption{Reconstructed data samples, $G(z^*)$, using a WGAN}
    \end{subfigure}
    \begin{subfigure}{\columnwidth}
        \includegraphics[width=\columnwidth]{graphics/face_rec_noise_highReg.png}
        \caption{Reconstructed data samples, $G(z^*)$, using a GAN+noise}
    \end{subfigure}
    \begin{subfigure}{\columnwidth}
        \includegraphics[width=\columnwidth]{graphics/face_rec_GAN_noNoise.png}
        \caption{Reconstructed data samples, $G(z^*)$, using a GAN}
    \end{subfigure}
    \caption{\textbf{Reconstruction of celebrity faces}}
    \label{fig:face_rec}
\end{figure}


% {\color{red} Our approach may also be used to detect mode dropping and over-fitting. In less well trained models, we are able... For a given problem it would be possible to design a test set that included a diverse selection of data samples, and to evaluate a model based on how well it represented each of those models. These are shown in the appendix}

\section{Quantitatively Comparing Models}

% \section{``Trouble Shooting'' Via Inversion}
\label{sec:troubleshooting}


Failing to represent a mode in the data is commonly referred to a ``mode dropping'', and is just one of three common problems exhibited by GANs. For completeness, common problems exhibited by trained GANs include the following: (1) mode collapse, this is where similar image samples are synthesised for different inputs, (2) mode dropping (more precisely), this is where the GAN only captures certain regions of high density in the data generating distribution and (3) training sample memorisation, this is where the GAN memorises and reproduces samples seen in the training data. If a model exhibits these symptoms, we say that it has over-fit, however these symptoms are often difficult to detect.

% {\color{blue}Each of these symptoms may be difficult to detect. We refer to models that exhibit these symptoms, as models that have over-fit.} 

% Collectively, we refer to these symptoms as over-fitting.}

\begin{table}[h!]
    \centering
    \caption{\textbf{Comparing Models Using Our Inversion Approach} MSE is reported across all test samples for each model trained with each dataset. A smaller MSE suggests that the model is better able to represent test data samples.}
    \label{tab:compare}
    \begin{tabular}{c c c c}
        \textbf{Model}  & CelebA & Shoes & Omniglot\\ \toprule 
        GAN \cite{radford2015unsupervised} & 0.118 & 0.059 & 0.588\\
        GAN$+$noise \cite{arjovsky2017towards} & 0.109 & 0.029 & 0.305\\
        WGAN \cite{arjovsky2017wasserstein} & 0.042 &  0.020 & 0.082\\ \hdashline[1pt/5pt]
        High Res. & -  & 0.016 & - \\ \bottomrule
    \end{tabular}
\end{table}

If a GAN is trained well and exhibits none of the above three problems, it should be possible to preform inversion to find suitable representations for most test samples using our technique.

However, if a GAN does exhibit any of the three problems listed above, inversion becomes challenging, since certain regions of high density in the data generating distribution may not be represented by the GAN. Thus, we may compare GAN models, by evaluating reconstruction error using our proposed inversion process. A high reconstruction error, in this case mean squared error (MSE), suggests that a model has possibly over-fit, and is not able to represent data samples well. By comparing MSE between models, we can compare the extent to which one model has over-fit compared to another. 

Table \ref{tab:compare} shoes how our inversion approach may be used to quantitatively compare $3$ models ($4$ in the case of the shoes dataset) across three datasets, CelebA, shoes and Omniglot. The Table shows mean squared reconstruction error on a large \footnote{CelebA:100 samples, Shoes and Omniglot:500 samples} batch of test samples.

From Table \ref{tab:compare} we may observe the following:

\subsubsection{CelebA}
The (latent) representation learned by the WGAN generalises to test samples, better than either the GAN or the GAN trained with noise. Results also suggests that training a GAN with noise helps to prevent over-fitting. These conclusions are consistent with both empirical and theoretical results found in previous work \cite{arjovsky2017wasserstein, arjovsky2017towards}, suggesting that this approach for quantitatively comparing models is valid.

\subsubsection{Shoes}
Using inversion to quantify the quality of a representation allows us to make fine grain comparisons between models. We see that training a model using higher resolution images reduces reconstruction error by almost a factor of two, in the case of the GAN$+$noise, compared to a similar model trained at a lower resolution. We know from earlier observations (Figure \ref{rec:shoes_rec}), that this is because the model trained at higher resolution, captures finer grain details, that the model trained on lower resolution images. 


% If we compare models using a classification accuracy, 
% a discriminative model learns only the features needed to perform classification.
Comparing models using our proposed inversion approach, in addition to classifier based measures \cite{salimans2016improved}, helps to detect fine grain differences between models, that may not be detected using classification based measures alone. A ``good'' discriminative model learns many features that help to make decisions about which class an object belongs to. However, any information in an image that does not aid classification is likely to be ignored (e.g. when classifying cars and trucks, the colour of a car is not important \footnote{Bottom left of Figure 10 \cite{mahendran2015understanding}, shows that the 1st layer of a discriminatively trained CNN ignores the colours of the input image.}). Yet, we may want to compare representations that encode information that a classifier ignores (e.g. colour of the car). For this reason, using only a classification based measure \cite{salimans2016improved} to compare representations learned by different models may not be enough, or may require very fine grain classifiers to detect differences.


%inception or training model as a classifier -- applies to both. Use inception score as this is applied to generated samples, rather than classifier, which is typically put on the end of the discriminator -- can make so many args about why this is not a good idea.

% {\color{blue} If using classification accuracy, might not be able to get such significant differences....}

\subsubsection{Omniglot}
From Figure \ref{fig:omni_rec}, it was clear that both models trained on the Omniglot dataset had over-fit, but not to the same extent. Here, we are able to quantify the degree to which each model has over-fit. We see that the WGAN has over-fit to a lesser extent compared to the GAN trained with noise, since the WGAN has a smaller MSE. Quantifying over-fitting can be useful when developing new architectures, and training scheme, to objective compare models.

In this section we have demonstrated how our inversion approach may be used to quantitatively compare representations learned by GANs. We intend this approach to provide a useful, quantitative approach for evaluating and developing new GAN models and architectures for representation learning.

Finally, we emphasise that while there are other techniques that provide inversion, ours is the only one that is \textbf{both} (a) immune to over-fitting, in other words we do not train an encoder network that may itself over-fit, and (b) can be applied to any pre-trained GAN model provided that the computational graph is available.





% {\color{blue} Quantifying GAN performance is difficult...}


% However, when attempting to perform inversion, using a (test) dataset that is diverse, these symptoms become easier to detect. In the case of (1) mode collapse, most inverted data samples look similar and nothing like the target. In the case of (2) mode dropping, some samples are well reconstructed while others . In the case of 

% In GAN training a common problem is mode collapse \cite{}, this is where similar image samples are synthesised for different inputs. Another issue is over-fitting, where the GAN memorises and reproduces samples seen in the training data. Both of these things can be difficult to detect. 

% Our inversion technique may be used to detect

% In the experimental section of our paper, we showed two examples of how our proposed inversion technique may be used to detect


% \section{Trade off: Sample Quality vs. }
% {\color{red} In our experiments we have chosen to keep $\alpha$ small. This encourages the inversion to find more faithful representations at the cost of sample quality being reduced. If however, we were more interested in samples quality, and wish to find a representation for a target image, $x$, that capture most of its features, while also producing higher quality samples, we may choose to weight $\alpha$ higher.}
% {\color{blue} Approach applies to any latent generative model}

\section{Conclusion}

The generator of a GAN learns the mapping $G: Z \rightarrow X$. It has been shown that $z$ values that are close in $Z$-space produce images that are visually similar in image space, $X$ \cite{radford2015unsupervised}. We propose an approach to map data, $x$ samples back to their latent representation, $z^*$ (Section \ref{sec:method}).

For a generative model, in this case a GAN, that is trained well and given target image, $x$, we should be able to find a representation, $z^*$, that when passed through the generator, produces an image, $G(z^*)$, that is similar to the target image. However, it is often the case that GANs are difficult to train, and there only exists a latent representation, $z^*$, that captures some of the features in the target image. When $z^*$ only captures some of the features, this results in, $G(z^*)$, being a partial reconstruction, with certain features of the image missing. Thus, our inversion technique provides a tool, to provide qualitative information about what features are captured by in the (latent) representation of a GAN. We showed several visual examples of this in Section \ref{sec:results}.

Often, we want to compare models quantitatively. In addition to providing a qualitative way to compare models, we show how we may use mean squared reconstruction error between a target image, $x$ and $G(z^*)$, to quantitatively compare models. In our experiments, in Section \ref{sec:troubleshooting}, we use our inversion approach to quantitatively compare $3$ models trained on $3$ datasets. Our quantitative results support claims from previous work that suggests, that certain modified GANs are less likely to over-fit.

We expect that our proposed inversion approach may be used as a tool to asses and compare various proposed modifications to generative models, and aid the development of new generative approaches to representation learning.

% We expect that our proposed inversion approach may be used as a tool, to asses and compare various proposed modification to generative models, and aid the development of new generative approaches to representation learning.


% \section{Future Work}
% We hope that our proposed inversion approach may be used as a tool, to asses and compare various proposed modification to generative models. Here we layout three (additional) ideas for how inversion may be used for assessing models:

% It may be possible to carefully curate a finite set of test data samples, that, combined, exhibit the features that a users wishes a representation to capture, (N.B. not every combination of features must be seen, but simply all features). Given this test set, one could apply our inversion technique to asses how well different models represent these features.

% It may also be possible to detect over-fitting to specific data samples, by comparing latent representations of synthetic samples, to latent representations of data samples. 

% Our approach is general and may be applied to any latent variable generative model for which the computational graph is available. This means that our inversion technique may be applied to autoencoder models and bi-directional GANs \cite{dumoulin2016adversarially, donahue2016adversarial}. Inversion may be used to asses to what degree a decoder has over-fit to the certain synthesised samples from the generator, by comparing reconstruction from the autoencoder, to inverted data samples. 


 



\section*{Acknowledgements}
We like to acknowledge the Engineering and Physical Sciences Research Council for funding through a Doctoral Training studentship.

\bibliographystyle{abbrv}
\bibliography{bib}

% \appendix



\end{document}

%This inversion technique may also be used to understand how transformations to an image are manifest in latent space.


%This inversion process shows that the latent encoding preserves certain attributes such as style etc... that a discriminative model may not preserve. This suggests that the latent encoding may be useful for tasks beyond digit classification. 
%Our approach shows what the latent encoding preserves important attributes about the training data, which we hope motivates further investigation into more efficient methods for inverting the generator.

%Our results show that both the identity and the style of the digit is preserved in the inversion process, which suggests that this inversion process has potential to be used in applications beyond just digit classification because of the ability of the latent space to encode the style too. From another view point - this inversion process show that the latent space preserves both identity and style, suggesting 


%Investigate the need for regularisation


%This inversion reveals several interesting properties of generative model, which make them distinct from discriminative models.  
