\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
 \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage[final]{nips_2017} % \usepackage{nips_2017}
\usepackage{amsmath}
% to compile a camera-ready version, add the [final] option, e.g.:
%\usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage[font=small,labelfont=bf]{caption}




\title{Deep Learning Improves Template Matching by Normalized Cross Correlation}
% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.
\author{
  Davit Buniatyan, Thomas Macrina, Dodam Ih, Jonathan Zung, Sebastian Seung \\
  Neuroscience Institute and Computer Science Dept.\\
  Princeton University, Princeton, NJ 08544\\
  \texttt{\{davit, tmacrina, dih, jzung, sseung\}@princeton.edu} \\
}


\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
Template matching by normalized cross correlation (NCC) is widely used for finding image correspondences. We improve the robustness of this algorithm by preprocessing images with "siamese" convolutional networks trained to maximize the contrast between NCC values of true and false matches. The improvement is quantified using patches of brain images from serial section electron microscopy. Relative to a parameter-tuned bandpass filter, siamese convolutional networks significantly reduce false matches. Furthermore, all false matches can be eliminated by removing a tiny fraction of all matches based on NCC values. The improved accuracy of our method could be essential for connectomics, because emerging petascale datasets may require billions of template matches to assemble 2D images of serial sections into a 3D image stack. Our method is also expected to generalize to many other computer vision applications that use NCC template matching to find image correspondences.
\end{abstract}

\section{Introduction}

Template matching by normalized cross correlation (NCC) is widely used in computer vision applications such as image registration, stereo matching, motion estimation, object detection and localization, and visual tracking \cite{, avants2008symmetric,  heo2011robust, lewis1995fast, luo2010fast, smeulders2014visual}. Here we show that the robustness of the algorithm can be improved by applying deep learning. Namely, if the template and source images are preprocessed by a convolutional network, the rate of false matches can be significantly reduced, and NCC output becomes more useful for rejecting suspect matches. The training of the convolutional network follows the "siamese network" method of learning a measure of similarity from pairs of input images \cite{chopra2005learning}. The learning is only weakly supervised, in the sense that a true match to the template should exist somewhere in the source image, but the location of that match is not an input to the learning procedure.  If NCC already works fairly well for template matching with raw images, then the incorporation of deep learning is expected to improve its accuracy further.

We test the power of our technique using images acquired by serial section electron microscopy (EM). NCC template matching is commonly applied to image patches in the course of assembling a 3D image stack from 2D images of individual sections \cite{preibisch2009bead, saalfeld2012elastic}. Achieving highly precise alignment between successive sections is critical for the accuracy of the subsequent step of tracing fine neurites through the image volume. Erroneous matches may arise because sections are deformed, distorted, or damaged during collection, and defects may also arise during imaging \cite{saalfeld2012elastic}. An image of a (0.1 mm)$^3$ brain volume is roughly a teravoxel \cite{lichtman2014bigdata}, and a high quality assembly could require up to 100 million template matches \cite{saalfeld2012elastic}. Every false match leads to tracing errors in multiple neurites, so even a small error rate across so many matches can have devastating consequences.

% An error rate of 1\% would translate into 1,000,000 false matches, each of which could lead to tracing errors in multiple neurites. 

In empirical tests, we find that the error rate of template matching on raw serial EM images is on the order of 1 to 3\%. Preprocessing with a bandpass filter lowers the error rate \cite{berg2001geometric}, and substituting convolutional networks improves upon that error rate by a factor of 2-7x. The overall result is an error rate of 0.05 to 0.30\%. A common strategy for reducing false matches is to reject those that are suspect according to some criteria \cite{saalfeld2012elastic}. This can be problematic if too many true matches are rejected and there are not enough matches to describe the deformation in a given region, which can also lead to tracing errors in multiple neurites. We show that NCC output provides superior rejection efficiency once deep learning is incorporated. To achieve zero false matches under our most accurate conditions, we need only reject 0.12\% of the true matches based on NCC output, an improvement of 3.5x over the efficiency based on a bandpass filter.

The idea of using deep learning to improve NCC template matching for image correspondences is simple and obvious, but has been little explored as far as we know. The closest antecedent of our work introduced an NCC layer inside a network used for the person identification problem \cite{subramaniam2016deep}. Recent work applying deep learning to image correspondences avoids template matching and instead trains a convolutional network to directly output a vector field \cite{dosovitskiy2015flownet, long2014convnets, pathak2016learning}. This approach is well-suited for computing dense correspondences, while template matching makes sense for computing sparse sets of corresponding points.

An advantage of the template matching approach is its interpretability. The height of an NCC peak provides information about the goodness of a match, and the width of an NCC peak provides information about the accuracy of spatial localization. Furthermore, one can examine the convolutional network output to see what image features are being used to compute matches. In our application to serial EM images, it appears that the network detects mitochondria. It learns to suppress image defects that arise from brightness-contrast fluctuations and damaged sections. It also learns to suppress high contrast edges of blood vessels. When such edges are present, they tend to produce a strong NCC peak, but the peak is very wide due to the near straightness of the edges, leading to imprecise spatial localization.

%The remaining false matches can be eliminated by rejecting those that are suspect. State-of-the-art software packages implement various heuristic criteria for rejection, but it may be necessary to reject a large fraction of matches to achieve a low error rate on the remaining ones. As will be shown by our empirical results, the 


%Instead of relying on intensity values, \citet{saalfeld2010rigid} uses SIFT descriptors and a geometric consistency constraint for globally minimizing reconstruction error. Subsequently, for high resolution reconstruction of large specimen imaged from a series of ultra-thin microscopy cuts, the method described in \citet{saalfeld2012elastic} includes construction of elastic sheets by treating each section as moving targets in a template-free global alignment. Each microscopy section is a triangular spring mesh consisted from vertices that are aligned across neighbouring sections through local pairwise block matching. Vertices along with neighbouring sections are connected with zero-length springs. Relaxing the system results in series of smoothly aligned sections. 

%Within those methods, pairwise block matching is either done using normalized cross-correlation in local neighbourhood of sections \citet{lewis1995fast} or using feature correspondence across sections includes landmark extraction exploiting invariant local image features \citet{lowe2004distinctive}. By the nature of source images and due to human data intervention, both methods used for template matching introduce substantial amount of ambiguous results that are further filtered based on local and global features of matches. Unfiltered errors, propagate through later stages of pipeline and produce significant error e.g. in connectomic 3D reconstruction of neurons. 

\begin{figure}[h]
  \centering
  
  \includegraphics[width=\linewidth]{examples_original.jpg}
  \caption{Three examples of template matching by 3D NCC correlograms. The large image in each example is the source image, and the small inset is the template. The correlogram plots the Pearson correlation coefficient $r$ values for all possible translations of the template across the source. The first image shows a correlogram with a well-localized maximum ("peak"). The second and third images show correlograms that fail to produce a clearly distinct maximum.}
  \label{ncc_examples}
\end{figure}


\section{Methods}

\subsection{Weakly Supervised Similarity Metric Learning by Siamese Convolutional Nets}
% corresponding location of a ing pixels is computtemplate $t$ in an image $f$. The traditional technique for block matching is Sum of Squared Differences (SSD) or its variant Normalized Cross Correlation (NCC) which can be considered as the cosine similarity of a normalized image pair. Despite sparse feature extraction and correspondence techniques such as SIFT (\citet{lowe1999object}, \citet{liu2011sift}), NCC is widely used in biomedical image alignment procedures (\citet{saalfeld2012elastic}). Due to repetitive nature of biomedical data and image artifacts introduced during data captioning, NCC produces ambiguous matches with multiple correspondences that influence alignment and further analysis. The goal of this paper is to transform the image space of the input such that NCC will produce unique correspondence. 

The inputs to the NCC are a template image and a larger source image. If the template image is placed somewhere inside the borders of the source image, the template pixels are in one-to-one correspondence with a subset of the source pixels, and the Pearson correlation coefficient can be computed for the pixel pairs. This computation can be done for all placements of the template image inside the source image, yielding an output image called the normalized cross-correlogram or correlogram for short (see Fig. \ref{ncc_examples}). The location in the correlogram with the largest Pearson coefficient is considered the location at which the template matches the source. 

Ideally, the correlogram should have a high and narrow peak only at the location of a true match, and should be low at all other locations. There should be no peak at all if there is no good match between the template and source. In practice, there can be peaks at spurious locations, leading to false matches. Another failure mode is a wide peak near a true match, leading to imprecise spatial localization.

To reduce the failure rate, one could apply preprocessing to the template and source images prior to computing the NCC. The preprocessing step can be trained from data using standard methods for supervised learning of a similarity metric \cite{kulis2013metric, yang2006distance}. Given pairs of points in a space $X$ that are known to be similar or dissimilar, and a similarity measure $S:\mathbb{R}^n\times\mathbb{R}^n\to \mathbb{R}$, the method is to learn an embedding $\psi:X\rightarrow \mathbb{R}^n$ such that $S(\psi(x),\psi(y))$ is large for similar $(x,y)$ and small for dissimilar $(x,y)$. If the embedding function $\psi$ is a neural network, then the technique is known as "siamese networks"\cite{bromley1993signature} because identical networks are applied to both $x$ and $y$ \cite{chopra2005learning}. 

%The convolution network learns how to transform the input space such that the difference between first and second peaks of the correlogram will be maximized. 

%Interestingly enough, CNN in unsupervised or weakly-supervised fashion learns how to capture a visual feature-based sparse representation by introducing some form of mitochondria detector, neglecting damaged and local brightness-contrast defects to optimize the metric. We further quantitatively analyze the improvement over large scale alignment. 

%Suppose we are given a set of objects $X=\{x_1,\hdots,x_N\}$. Suppose furthermore that some pairs of objects constituting the set $S=\{(x_{i_1}, x_{j_1}),(x_{i_2},x_{j_2}),\hdots\}$ are known to be similar, while other pairs of objects constituting the set $D$ are known to be dissimilar. The problem of metric learning is to find an embedding $\psi:X\rightarrow \mathbb{R}^n$ such that $||\psi(x)-\psi(y)||$ is small for $(x,y) \in S$ and $||\psi(x)-\psi(y)||$ is large for $(x,y) \in D$. Moreover, we would like to do this given only a few samples from $S$ and $D$. To be more precise, we might formulate the following optimization problem:

% \begin{equation*}
% \begin{aligned}
% & \underset{\psi \in \Psi}{\text{min}}
% & & \sum_{(x,y) \in S} d(\psi (x), \psi (y)) \\
% & \text{s.t.}
% & &  \sum_{(x,y) \in D} d(\psi (x), \psi (y)) \geq 1.
% \end{aligned}
% \end{equation*}
% where $\Psi$ is some class of embeddings (in our application, a class of neural networks) and $d$ is Euclidean distance.

We train siamese convolutional networks by repeating the following for template-source pairs that are known to contain a true match:
\begin{enumerate}
\item Compute the correlogram for source and template image. 
\item Find the peak of the correlogram. 
\item Make a gradient update to the convolutional net that increases the height of the peak. 
\item Draw a small box around the peak of the correlogram.
\item Find the maximum of the correlogram outside the box, and call this the "secondary peak."
\item Make a gradient update to the convolutional net that decreases the secondary peak.
\end{enumerate}
The cost function for the above algorithm is the difference in the heights of the primary and secondary peaks, which we will call the "correlation gap." The cost function has two purposes, depending on the shape of the correlogram (Fig. \ref{loss_function_2D}). If the primary peak is wider than the box, then the secondary peak will not actually be a local maximum (Fig. \ref{loss_function_2D}). In this case, the cost function encourages narrowing of the primary peak, which is good for precise spatial localization. The size of the box in the algorithm represents the desired localization accuracy. In other cases, the secondary peak will be a true local maximum, in which case the purpose of the cost function is to suppress peaks corresponding to false matches. 

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{plots.jpg}
  \caption{Loss function intuition. (a) 2D toy example. Left, make a correlogram with a wide peak more narrow. Right, promote the first peak and diminish the second peak. (b) Real 3D example. Generate NCC correlogram from template and source, then promote the first peak and diminish the second peak.}
  \label{loss_function_2D}
\end{figure}

The above algorithm corresponds to similarity metric learning if the primary peak indeed represents a true match and the secondary peak represents a false match. In fact, the NCC does have a nonzero error rate, which means that some of the examples in the training have incorrect labeling. However, if the error rate starts out small, one can hope that the similarity metric learning will make it even smaller. Our algorithm requires supervision, in the sense that a good match should exist between each source-template pair.  However, the location of the match is not required as an input to the learning, so the supervision is fairly weak.

By itself, the above algorithm may lead to pathological solutions in which the network is able to minimize the cost function by ignoring the input image. To avoid these solutions, one can additionally train on source-template pairs that are known to contain no good match. Since these are dissimilar pairs, the goal of learning is to reduce the peak of the NCC.
\begin{enumerate}
\item Compute the correlogram for source and template image. 
\item Find the peak of the correlogram. 
\item Make a gradient update to the convolutional net that decreases the height of the peak. 
\end{enumerate}
Dissimilar pairs can be artificially generated by permuting the source and template images within a batch. 

% One might consider the first part of the loss function to be the similar pair and second peak as dissimilar. Our optimization, in most cases, maximizes the similarity (or minimizes the distance) of similar patch and relatively minimizes the similarity (maximizes the distance) of dissimilar patch. Then this process is iterated over the dataset of $\{f,t\}$ pairs. In result, we have unsupervised metric learning setting with the assumption that our original distance function is good enough and correspondence of template $t$ is contained in $f$.

\label{gen_inst}

%\subsection{Deep Learning}

%Early works in unsupervised image alignment introduced by \citet{kim2003visual} takes the approximation of Mutual Information (MI) between pairs of images and incorporates into energy-based minimization framework. The method called Congealing takes the stack of pixels across sections and minimizes the entropy of each column. Instead of directly working on intensity values, the method described by \citet{huang2007unsupervised} takes SIFT descriptors and cluster the feature vectors with soft assignment before applying congealing. Further, improvement to this method by \citet{cox2008least} changing the energy-based framework into least squares %, extensively surveyed in \citet{smeulders2014visual} paper. 

 

%    In biomedical applications like ours, U-Net has proven to be a highly effective neural network architecture \cite{ronneberger2015u}. At each level similar to Fig. \ref{architecture}b, the input passes through two 3x3 convolution layers followed by a non-linearity and a 2x2 max-pooling with downsampling and doubling feature channels. After repeating this process $n$ times, upsampling and further convolutions are applied in symmetric fashion. At each level there is also a skip connection that is directly concatenated with the upsampled output. This network uses skip connections to preserve low-level features that are vital in biomedical data. In this paper, we use FusionNet \citet{hegde2016fusionnet} which is a variant of U-Net. Instead of convolution blocks it uses residual blocks consisting of three convolution layers and a skip connection from the first layer to the last. Instead of concatenation at each level, the output of the left-side is summed with the right-side. This network also enforces symmetric input-output resolution.

%    Another application of deep metric learning described in \citet{osendorfer2013convolutional} is learning local feature descriptors. \citet{huang2012learning} used deep nets with weak supervision on the similarity metric applied on face alignment problem has shown the success of the approach in unsupervised alignment and unsupervised feature extraction. Deep learning approaches applied to tracking the objects across video frames as seen by \citet{wang2013learning} and \citet{pathak2016learning} demonstrated that feature extraction methods transferred to a classification problem improves the results. Correspondence learning with convolutional nets reported in \citet{long2014convnets} outperform conventional feature extraction methods.

\begin{figure}[h]
  \centering
  
  \includegraphics[width=\linewidth]{architecture.jpg}
  \caption{Architecture diagram of two channel neural network with NCC block as a bridge. a) Perspective view of the network. Gray boxes represent residual blocks that are either connected to each other through skip connections, max-pooling and convolution layers. b) Left view where U-Net architecture can be easily seen. c) Top view shows two Siamese networks with weight-sharing.}
  \label{architecture}
\end{figure}

\subsection{Implementation} 
  
 Fig. \ref{architecture} depicts siamese convolution networks, i.e., two networks with the same architecture and weight-sharing between the networks. The architecture is FusionNet \citet{hegde2016fusionnet}, which is a variant of U-Net. Instead of convolution blocks it uses residual blocks consisting of three convolution layers and a skip connection from the first layer to the last. Instead of concatenation at each level, the output of the left-side is summed with the right-side. This network also enforces symmetric input-output resolution.
 
 The input size of both networks plays a crucial role because it defines the sparseness of features that the network will preserve for optimizing the NCC. The template and the source image are both squares with sizes 160px and 512px. We consistently use $3\times 3$ convolution layers with $tanh$ non-linearity. At each level the number of features is doubled starting with 8 up to 64 channels. The output of FusionNet is passed through another convolution layer that feeds to the NCC layer. 

 The NCC layer is implemented in TensorFlow using FFT \citet{lewis1995fast} and can handle batches and multiple channels effectively. The loss layer takes as input the NCC correlogram, computes the maximum peak, removes a 20px square window centered at the peak, then computes the next maximum value that represents the second peak. The initial framework for the loss is defined to maximize the difference between the first and second peaks during training. 
 
%  Instead of difference, one can take the ratio. In order to enhance the learning effect of pathological cases (e.g. where the distance is low) one can minimize the inverse or take the log. Also we can choose between taking the minimum or the mean over the batch.

 Training alternated between a batch of eight source-template pairs and then the same batch with randomly permuted source-template pairings. Gradient descent used the Adam optimizer with learning rate of 0.0005. Training converged within 10,000 iterations. The training data consisted of pairs of inputs sampled from an affine-aligned stack of images that contained non-affine deformations. It is recommended either to choose a dataset with enhanced pathological cases that the network is expected to handle or to use data augmentation for covering the problem space of possible damages and deformations. During the training we randomly cropped the source and template images such that the position of the peak is randomly distributed. Also, to increase the size of the training dataset we used random rotations of both inputs by 90, 180, 270 degrees. 
 
 
% If the network's architecture has many features or is very deep, overfitting can be avoided to avoid over fitting by tricking the normalized cross-correlation,


\section{Experiments}

We validated our model on 95 serial images from the training set, an unpublished EM dataset with a resolution of 7x7x40nm$^3$. Each image was 15,000x15,000px, and had been roughly aligned with an affine model but still contained considerable non-affine distortions up to 250px (full resolution).

From the serial images, we produced three datasets: raw images (\textit{raw}), images preprocessed with a circular Gaussian bandpass filter that was optimally tuned to produce a low number of false matches (\textit{bandpass}), and images preprocessed with our convolutional net by applying the larger convolutional channel across the entire image, upsampling and blending accordingly (\textit{convnet}). We then varied the parameters of our template matching procedure, varying the template image size between small and large (\textit{160px} and \textit{224px}), and matching between neighboring images (\textit{adjacent}) as well as the next-nearest neighbors (\textit{across}). For matching between next-nearest neighbors, a slightly different bandpass parameter was used, as the optimal filter for next-nearest neighbors differed from the filter for neighboring images. The network used was identical in all experiments, having been trained on 160px template and 512px source patches from adjacent sections. Table \ref{table:parameters} summarizes the training and experiment parameters.

\begin{table}[h]
    \caption{Image parameters for training and testing. Unless otherwise noted, resolutions are given after 3x downsampling where 1px represents 21nm.}
    \centering
     \small
    \begin{tabular}{l*{6}{c}r}
    
        \toprule
        & \multicolumn{1}{c}{Training} & \multicolumn{2}{c}{Adjacent}   &  \multicolumn{2}{c}{Across}\\
        \cmidrule{2-6}
        
        Template size   & 160px & 160px & 224px & 160px & 224px \\
        Source size     & 512px &\multicolumn{2}{c}{512px} &\multicolumn{2}{c}{512px} \\
        Section depth   & 40nm &\multicolumn{2}{c}{40nm} &\multicolumn{2}{c}{80nm} \\
        \cmidrule{2-6}
        Section size (full res.)    & ~33,000px & \multicolumn{2}{c}{15,000px} & \multicolumn{2}{c}{15,000px} \\
        No. of sections & 1040 & \multicolumn{2}{c}{95} & \multicolumn{2}{c}{48}\\
        No. of matches & 10,000 & \multicolumn{2}{c}{~144,000} & \multicolumn{2}{c}{~72,000}\\
        \cmidrule{2-6}
        Bandpass $\sigma$ (full res.) & N/A & \multicolumn{2}{c}{2.0-12.0px} & \multicolumn{2}{c}{2.5-25.0px}\\
       
        \bottomrule
    \end{tabular}
\label{table:parameters}
\end{table}

% We compared the performance of our template matching procedure using three different inputs: raw images, images preprocessed with a bandpass filter, and images preprocessed with our convnet. We also varied the parameters of our template matching procedure: varying the template image size between small and large (160x160px vs 224x224px), and matching between neighboring images (adjacent) and matching between next-nearest neighboring images (across). The network used was identical in all experiments, having been trained on 160x160px template and 512x512px source patches from adjacent sections.

In each experiment, both the template and the source images were downsampled by a factor of 3 before NCC, so that 160px and 224px templates were 480px and 672px at full resolution, while the source image was fixed at 512px downsampled (1,536px full resolution). The template matches were taken in a triangular grid covering the image, with an edge length of 400px at full resolution (Fig. \ref{vectorfield} shows the locations of template matches across an image).

\begin{figure}[h]
  \centering
  
  \includegraphics[width=\linewidth]{vectorfield.jpg} 
  \caption{Example displacement vector fields for each image condition. Representation of output of template matching in a regular triangular grid (edge length 400px at full resolution) across a 15,000x15,000px image. Each node represents the centerpoint of a template image used in the template matching procedure. Each vector represents the displacement of that template image to its matching location in its source image. Matches shown are based on 224px template size on across (next-nearest neighbor) sections. Raw: matches on the raw images. Bandpass: matches on images filtered with a Gaussian bandpass filter; Convnet: matches on the output of the convolutional network processed image.}
  \label{vectorfield}
\end{figure}

Our first method to evaluate performance was to compare error rates. Errors were detected manually, using a tool that allowed human annotators to inspect the template matching inputs and outputs. The tool is based on the visualization of the displacement vectors that result from each template match across a section, as shown in Fig. \ref{vectorfield}. Any match that significantly differed (over 50px) from its neighbors were rejected, and matches that differed from neighbors but not significantly were individually inspected for correctness by visualizing a false color overlay of the template over the source at the match location. The latter step was needed as there were many true matches that deviated prominently from its neighbors: the template patch could contain neurites or other features parallel to the sectioning plane, resulting in large motions of specific features in a random direction that may not be consistent with the movement of the larger area around the template (see Fig. \ref{difficult_match} for an example of this behavior). Table \ref{table:error_rates} summarizes the error counts in each experiment.

\begin{figure}[h]
  \centering
  \includegraphics[width=250pt]{difficult_match.jpg}
  \caption{Manual inspection difficulties. a) The vector field around a match (circled in red) that prominently differs from its neighbors. b) The template for the match, showing many neurites parallel to the sectioning plane. c) The false color overlay of the template (green) over the source image (red) at the matched location, establishing the match as true.}
  \label{difficult_match}
\end{figure}

 

\begin{table}[h]
    \caption{False matches for each image condition across experiments. Total possible adjacent matches: 144,500. Total possible across matches: 72,306.}
    \centering
     \small
    \begin{tabular}{l*{9}{c}r}
    
        \toprule
        &  \multicolumn{4}{c}{Adjacent}   &  \multicolumn{4}{c}{Across}\\
        \cmidrule{2-9}
        Template size    & \multicolumn{2}{c}{160px}  & \multicolumn{2}{c}{224px} & \multicolumn{2}{c}{160px} & \multicolumn{2}{c}{224px} \\
        \hline
        Raw             & 1,778 & 1.23\% & 827 & 0.57\% & 2,105 & 2.91\% & 1,068 & 1.48\% \\
        Bandpass        & 480 & 0.33\% & 160 & 0.11\% & 1,504 & 2.08\% & 340 & 0.47\% \\
        Convnet         & \textbf{261} & \textbf{0.18\%} & \textbf{69} & \textbf{0.05\%} & \textbf{227} & \textbf{0.31\%} & \textbf{45} & \textbf{0.06\%} &  \\
        \bottomrule
    \end{tabular}
\label{table:error_rates}
\end{table}

To ensure that fewer false matches were not coming at the expense of true matches, we evaluated the overlap between true match sets created by the bandpass images and our convnet images. Table \ref{table:true_overlap} summarizes how many true matches were unique to the bandpass, convnet, or neither.

\begin{table}[h]
\caption{Dissociation of true matches set between the bandpass and convnet. Counts of true matches per category. Total possible adjacent matches: 144,500. Total possible across matches: 72,306.}
\centering
\small
    \begin{tabular}{l*{9}{c}r}
        \toprule
        &  \multicolumn{4}{c}{Adjacent}   &  \multicolumn{4}{c}{Across}\\
        \cmidrule{2-9}
        Template size    & \multicolumn{2}{c}{160px}  & \multicolumn{2}{c}{224px} & \multicolumn{2}{c}{160px} & \multicolumn{2}{c}{224px} \\
        \hline
        Neither         & 144 & 0.10\% & 54 & 0.04\% & 162 & 0.22\% & 33 & 0.05\% &  \\
        Bandpass only             & 117 & 0.08\% & 15 & 0.01\% & 65 & 0.09\% & 12 & 0.02\% \\
        Convnet only        & 336 & 0.23\% & 106 & 0.07\% & 1342 & 1.86\% & 307 & 0.42\% \\
        \bottomrule
    \end{tabular}

\label{table:true_overlap}
\end{table}

To assess how easily false matches could be removed, we evaluated matches with the following criteria:
\begin{itemize}
    \item \textit{norm}: The Euclidean norm of the displacement required to move the template image to its match location in the source image, at full resolution.
    \item \textit{r max}: The first peak of the correlogram serves as a proxy for confidence in the match.
    \item \textit{r delta}: The difference between the first peak and second peak (after removing a 5px square window surrounding the first peak) of the correlogram provides some estimate of the certainty there is no other likely match in the source image, and the criteria the convnet was trained to optimize.
\end{itemize}

These criteria can serve as useful heuristics to accept or reject matches to approximate the unknown partitions for the true and erroneous matches. The less overlap between the actual distributions when projected onto the criterion dimension, the more useful that criterion. Fig. \ref{criteria_distributions} plots these three criteria across the three image conditions.

\begin{figure}[h]
  \centering
  
  \includegraphics[width=\linewidth]{error_metrics.jpg}
  \caption{Match criteria for adjacent 224px experiment. (a) Distributions of the three template match criteria for each image condition. Red bars represent counts of false matches that were manually identified. Green bars represent counts of true matches. (b) Percentage of true matches that must be rejected to reduce the error rate when varying the \textit{r delta} criterion. See the Appendix for distributions \& rejection curves from other experiments.}
  \label{criteria_distributions}
\end{figure}

%Once trained, we use the large  convolutional channel and post-process each slice with the grid of 10x10 and then upsample to the original size. As the corner outputs of the network are not consistent with the center, we take overlapping regions and use blending for continuity within sections. Then, already post-processed sections are taken as a usual input to the alignment procedure. Hence, the pre-processing can be considered as a separate step in the global alignment problem without any intervention.
%It is often the case that during alignment one or more sections are substantially damaged or completely missing. This is commonly overcome by taking the other nearest section. Across sections the data is more varied and, in the result, NCC is producing more ambiguous results. We also experiment with the harder setup where middle section is missing. For generalization, we also train the network across sections pairs to capture the variability, however as we shall see it doesn't produce better results than those that are trained on adjacent ones neither in standard nor in across experiments. In total, we have 4 experiments with two template sizes (240x240 and 160x160) and two modes (across and adjacent).  Each experiment is evaluated on four datasets: Original ($orignal$), Bandpass filtered ($bandpass$), Net trained on adjacent ($net\_adj$) and across sections ($net\_across$).
%Make sure to note that sigma values are scaled to the original

\section{Discussion}

% In all experiments, NCC was vastly more accurate with convnet images than on raw of bandpass images, producing far less errors. Furthermore, the distributions of various properties calculated for each correlogram differed significantly between the datasets, specifically with the convnet providing distributions more suitable for differentiation between true and erroneous matches.

In all experiments, the images preprocessed by our convnet consistently produced fewer false matches than the other two sets of images with a reduction factor of 2-7x (see Table \ref{table:error_rates}). The convnet produced matches in the vast majority of cases that the bandpass produced matches. It did introduce some false matches that the bandpass did not, but it correctly identified 3-20 times as many additional true matches relatively (see Table \ref{table:true_overlap}). The majority of the false matches in the convnet output were also present in the bandpass case, which establishes the convnet as superior to and not merely different from bandpass.

Notably, the reduction in error from applying the convnet generalized well to both harder (across sections at 160px and 224px template sizes) and easier (adjacent sections at 224px template size) tasks. In fact, the convnet provided larger gains in experiments other than on the task on which it was trained. This ability to generalize is crucial to applications as different template matching parameters are often needed at different stages of the alignment process. The results suggest that a single convnet may be used throughout the range of speed-accuracy tradeoffs (smaller-larger template size) as well as in dealing with missing sections (across).

Inspecting the filtered image, the convnet seems to identify keypoints (small dark objects that localize well, such as mitochondria) and suppress objects that do not localize well (e.g. lines, such as cell membranes, or consistently patterned regions, such as regions inside cell bodies and blood vessels). See Fig. \ref{ncc_examples2} for examples from the convnet image set. The convnet fails when the template does not contain the keypoints it has learned to identify. The last column in Fig. \ref{ncc_examples2} contains a template that is almost completely occupied by a cell body, and the convnet failed to find the true match. The raw image can be more useful in those cases, because it can match on corner-like edges (see Sup. Fig. 8 in the Appendix). This can be improved by biasing the training set with more of these pathological examples.

\begin{figure}[h]
  \centering
  
  \includegraphics[width=\linewidth]{examples.jpg}
  \caption{Difficult examples from the dataset with damaged areas \& local brightness changes. Correlograms are projected to be 2D with white pixels having higher \textit{r} values. The last column is an example of failure by the convnet.}
  \label{ncc_examples2}
\end{figure}

Fortunately, when these false matches do occur with the convnet, we can reject them efficiently using our match criteria. The convnet transformed the true match distributions for \textit{r max} and \textit{r delta} to be more left-skewed, while the erroneous match distribution for \textit{r delta} remain with lower values (see Fig. \ref{criteria_distributions}a), resulting in a distribution more amenable to accurate error rejection. For the case of adjacent sections with 224px templates, we can remove every error in our convnet output by rejecting matches with an \textit{r delta} below 0.05, which removes only 0.12\% of the true matches. The same threshold also removes all false matches in the bandpass outputs, but removes 0.40\% of the true matches (see Fig. \ref{criteria_distributions}b). This 3.5x improvement in rejection efficiency is critical to balancing the trade-off between complete elimination of false matches and retaining as many true matches as possible.

The improvement in rejection efficiency also generalized well across experiments, as evident in the Appendix, Sup. Fig. 15. Achieving a 0.1\% error rate on the most difficult task we tested (across, 160px template size) required rejecting 20\% of the true matches on bandpass, while less than 1\% rejection of true matches was sufficient with the convnet.


% Convnet: remove less and get more.
% Bandpass: remove more and get less.

% In particular, both $r\_max$ and $r\_delta$ values for true matches displayed a left-skewed distribution in $net\_adj$ and $net_across$ relative to those for $original$ and $bandpass$, while the values for intrue matches were better confined to a smaller range near zero. The result was a smaller overlap between the true and erroneous outputs allowing for far more accurate.

% profiles of matches property for each kernel. In particular, the overlap in the ranges of r\_max value for true and incorrect matches is significantly smaller for net outputs. 

% mode of the r\_max value for incorrect matches 

% It is notable that the net trained on adjacent sections consistently performed superior to the output from the net trained across sections. 

% Failure modes when the network fails \\
% SIFT vs Network features \\
% Why is it important to have smooth \\

\section{Conclusions}

Combining NCC with deep learning reduces false matches from template matching. It also improves the efficiency by which those false matches can be removed so that a minimal number of true matches are rejected. This is a very promising technique that offers us the ability to significantly increase the throughput of our alignment process while maintaining the precision we require. We expect this technique to serve well in other areas that demand such high-quality template matching.

We would like to explore how well this technique generalizes from one EM dataset to another, as well as to investigate if there is transfer learning that could benefit the segmentation convolutional network that follows the alignment process.

% NCC + deep learning helped reduce error rates and increase filtering efficiency
% Appear to be a very useful method to use in our alignment process

% Explore generalization to other EM datasets
% Explore transfer learning to the segmentation convnet


% Taking normalized cross-correlation under the metric learning framework in unsupervised alignment problem setting turned out to be useful for learning sparse features in correspondence problem. Empirically, it was demonstrated that the number of misalignments have been halved in adjacent settings. In the across section, it produced much better results then existing methods. Furthermore, the architecture of the network could be considered as kernelized version of normalized cross-correlation trained under unsupervised or semi-supervised setting. 

% Alternative view to the metric learning framework via normalized cross-correlation is to consider learning process of a child. Given a single digit image and a 10x10 table of digits, we ask the child to find corresponding number in the table. Repeating this process many times, an interesting result would be that the child will learn to distinguish digits having low-embedding in his head even though he can't recognize numbers.

% Further advancements of this research include exploring multiple feature outputs from the convolutional network to the normalized cross-correlation layer motivated by \citet{fisher1995multi}. From feature learning perspective, it could be beneficial to validate the transfer learning either from or to the segmentation task by pre-training the network. Alternative approaches for loss metric have been considered but not advanced. It could be also beneficial to take the softmax beforehand or minimize the entropy of normalized cross-correlogram. Also it could be interesting to use data augmentation for rotation/scale invariance to make NCC comparable with feature extraction methods in other domains. 

% This research combined both classical methods and recent deep learning advancements in computer vision applied on biomedical image alignment problem. However one might consider to explore further generalizations in face alignment, visual tracking and optical flow domains by having, for example, additional layers after the normalized cross-correlation to compute the optical flow for a patch. 


% Commented out for submission
%\subsubsection*{Acknowledgments}
%We would like to thank Ashwin Vishwanathan for kind use of his unpublished EM dataset. Maybe we'll align it for him one day.

\medskip

\small

\bibliography{sources}
\bibliographystyle{plainnat}

\newpage
%\section*{Appendix}
% \subsection*{More template matches}
% Specific examples drawn from dataset. First and second examples demonstrate how the NCC on networks can outperform the original NCC by ignoring brightness / contrast changes and image defects. On the third example both methods fail to produce the true match, and on the fourth example the original outperforms the convolutional net.
%\begin{figure}[h]

 % \centering
  
  %\includegraphics[height=535pt]{samples.jpg} 
  %\caption{Examples of NCC results with adversarial cases}
  %\label{ncc_examples3}

%\end{figure}

% \subsection*{More vector fields}
% Several adversarial examples of vector fields are presented for completeness. It can be seen that sometimes bandpass achieves better or near the same quality. 
%\begin{figure}[h]
%  \centering
  
%  \includegraphics[width=\linewidth]{vectorfield_bad.jpg} 
%  \caption{Bandpass vector field has no false matches}
  
%  \includegraphics[width=\linewidth]{vectorfield_comparable.jpg} 
%  \caption{Comparable quality}
  
%  \includegraphics[width=\linewidth]{vectorfield_ok.jpg} 
%  \caption{Convnet output is slightly better}
%\end{figure}

%\begin{figure}[h]
  %\centering
  
  %\includegraphics[width=\linewidth]{adjacent_160_criteria.jpg}
  %\caption{Match criteria for adjacent, 160px.}
  %\includegraphics[width=\linewidth]{across_160_criteria.jpg}
  %\caption{Match criteria for across, 160px.}
  %\includegraphics[width=\linewidth]{across_224_criteria.jpg}
%  \caption{Match criteria for across, 224px.}
%  \label{additional_distributions}
%\end{figure}

% \subsection*{Rejection curves for all experiments}
%\begin{figure}[h] 
% \centering 
%  \includegraphics[width=\linewidth]{reject_percentage_all.jpg}
%  \caption{Rejection curves by \textit{r delta} for all experiments.}
%  \label{rejection_all}
%\end{figure}

\end{document}


