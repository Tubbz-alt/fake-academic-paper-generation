% Template for IGARSS-2016 paper; to be used with:
%          spconf.sty  - LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass[journal]{IEEEtran}
\usepackage{amsmath,epsfig,amssymb}
\usepackage[american]{babel}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{array}% http://ctan.org/pkg/array
\usepackage{tabularx}
\usepackage{caption}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage[noabbrev]{cleveref}
\usepackage[T1]{fontenc}
\usepackage{color}
\ifCLASSOPTIONcompsoc
\captionsetup[subfigure]{subrefformat=simple,labelformat=simple,listofformat=subsimple}
\renewcommand\thesubfigure{(\alph{subfigure})}
\usepackage[caption=false, font=normalsize, labelfont=sf, textfont=sf]{subfig}
\else
\usepackage[caption=false, font=footnotesize]{subfig}
\fi
% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------

%
% Single address.
% ---------------
%\name{Dao-Yu Lin, Yang Wang, Guang-Luan Xu, Kun Fu\thanks{This work was supported in part by the National Natural Science Foundation of China under Grant No.41501485}}
%\address{Institute of Electronics, Chinese Academy of Sciences, Beijing, China\\
%	Email:lindaoyu15@mails.ucas.ac.cn}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
\begin{document}
%\topmargin=0mm
%\ninept
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{MARTA GANs: Unsupervised Representation Learning for
	Remote Sensing Image Classification}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{
	Daoyu Lin, Kun Fu, Yang Wang, Guangluan Xu, and Xian Sun 
	\thanks{This work was supported in part by the National Natural Science Foundation of China under Grant No.41501485 and No.61331017. (Corresponding author: Kun Fu.)}
	\thanks{The authors are with the Key Laboratory of Technology in Geo-spatial
	Information Processing and Application System, Chinese Academy of Sciences, Beijing, 100190, China (e-mail: lindaoyu15@mails.ucas.ac.cn; fukun@mail.ie.ac.cn; primular@163.com; gluanxu@mail.ie.ac.cn; sunxian@mail.ie.ac.cn).}

	
	%Michael~Shell,~\IEEEmembership{Member,~IEEE,}
%	John~Doe,~\IEEEmembership{Fellow,~OSA,}
%	and~Jane~Doe,~\IEEEmembership{Life~Fellow,~IEEE}% <-this % stops a space
	
%	\thanks{M. Shell was with the Department
%		of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
%		GA, 30332 USA e-mail: (see http://www.michaelshell.org/contact.html).}% <-this % stops a space
%	\thanks{J. Doe and J. Doe are with Anonymous University.}% <-this % stops a space
%	\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}

}
% The paper headers

%\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%

%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}

% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}
\maketitle
%
%\vspace{-6mm}
\begin{abstract}
With the development of deep learning, supervised learning has frequently been adopted to classify remotely sensed images using convolutional networks (CNNs). However, due to the limited amount of labeled data available, supervised learning is often difficult to carry out. Therefore, we proposed an unsupervised model called multiple-layer feature-matching generative adversarial networks (MARTA GANs) to learn a representation using only unlabeled data. MARTA GANs consists of both a generative model $G$ and a discriminative model $D$. We treat $D$ as a feature extractor. To fit the complex properties of remote sensing data, we use a fusion layer to merge the mid-level and global features. $G$ can produce numerous images that are similar to the training data; therefore, $D$ can learn better representations of remotely sensed images using the training data provided by $G$. The classification results on two widely used remote sensing image databases show that the proposed method significantly improves the classification performance compared with other state-of-the-art methods.

\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
	Unsupervised representation learning, generative adversarial networks, scene classification.
\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



%section{Introduction}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
%\IEEEPARstart{T}{his} demo file is intended to serve as a ``starter file''
%for IEEE journal papers produced under \LaTeX\ using
%IEEEtran.cls version 1.8b and later.
%% You must have at least 2 lines in the paragraph with the drop letter
%% (should never be an issue)
%I wish you the best of success.
%
%\hfill mds
% 
%\hfill August 26, 2015
%
%\subsection{Subsection Heading Here}
%Subsection text here.
%
%% needed in second column of first page if using \IEEEpubid
%%\IEEEpubidadjcol
%
%\subsubsection{Subsubsection Heading Here}
%Subsubsection text here.


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.








% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


%\appendices
%\section{Proof of the First Zonklar Equation}
%Appendix one text goes here.
%
%% you can choose not to have a title for an appendix
%% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here.
%
%
%% use section* for acknowledgment
%\section*{Acknowledgment}
%
%
%The authors would like to thank...

\section{Introduction}
\label{introduction}
% Image transformation as a general problem
As satellite imaging techniques improve, an ever-growing number of high-resolution satellite images provided by special satellite sensors have become available. It is urgent to be able to interpret these massive image repositories in automatic and accurate ways. In recent decades, scene classification has become a hot topic and is now a fundamental method for land-resource management and urban planning applications. Compared with other images, remote sensing images have several special features. For example, even in the same category, the objects we are interested in usually have different sizes, colors and angles. Moreover, other materials around the target area cause high intra-class variance and low inter-class variance. Therefore, learning robust and discriminative representations from remotely sensed images is difficult.

Previously, the bag of visual words (BoVW)~\cite{sivic2003video} method was frequently adopted for remote sensing scene classification. BoVW includes the following three steps: feature detection, feature description, and codebook generation. 
%After feature detection, each image is abstracted by several local patches. Feature representation methods include the scale-invariant feature transform (SIFT)
%~\cite{lowe2004distinctive} 
%and the histogram of oriented gradients (HOG). These methods
%~\cite{dalal2005histograms}
%represent the patches as numerical vectors and then convert the %vector-represented patches to codewords. 
To overcome the problems of the orderless bag-of-features image representation, the spatial pyramid matching (SPM) model~\cite{lazebnik2006beyond} was proposed, which works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The above-mentioned methods have comprised the state of the art for several years in the remote sensing community~\cite{lienou2010semantic}, but they are based on hand-crafted features, which are difficult, time-consuming, and require domain expertise to produce.
\begin{figure}[t]
	%  \hspace{20mm} Style
	%  \hspace{15.5mm} Content
	%  \hspace{5.5mm} Gatys \etal~\cite{gatys2015neural}
	%  \hspace{6.5mm} Ours
	%  \vspace{-2.5mm}
	\begin{center}
		\includegraphics[width=0.5\textwidth]{images/overview.pdf}
	\end{center}
	%  \vspace{-3mm}\cite{castelluccio2015land}
	%  \hspace{14mm} Ground Truth
	%  \hspace{7mm} Bicubic
	%  \hspace{8mm} SRCNN~\cite{dong2014learning}
	%  \hspace{2mm} Perceptual loss
	%  \vspace{-1mm}
	\vspace{-1em}
	\caption{ \label{overview}Overview of the proposed approach. The discriminator~(2) learns to make classifications between real and synthesized images, while the generator~(1) learns to fool the discriminator.
		\vspace{-3mm}
	%	Adversarial net takes label map as input and produces class label (1=real, or 0=synthetic ).
	}
\end{figure}


Deep learning algorithms
%~\cite{lecun2015deep}
 can learn high-level semantic features automatically rather than requiring handcrafted features. Some approaches~\cite{penatti2015deep,nogueira2017towards} based on convolutional neural networks (CNNs)~\cite{krizhevsky2012imagenet} have achieved success in remote sensing scene classification, but those methods usually require an enormous amount of labeled training data or are fine-tuned from pre-trained CNNs.

Several unsupervised representation learning algorithms have been based on the autoencoder~\cite{vincent2010stacked, makhzani2013k}, which receives corrupted data as input and is trained to predict the original, uncorrupted input. Although training the autoencoder requires only unlabeled data, input reconstruction may not be the ideal metric for learning a general-purpose representation. The concept of Generative Adversarial Networks (GANs)~\cite{Goodfellow2014} is one of the most exciting unsupervised algorithm ideas to appear in recent years; its purpose is to learn a generative distribution of data through a two-player minimax game. In subsequent work, a deep convolutional GAN (DCGAN)~\cite{Radford2015} achieved a high level of performance on image synthesis tasks, showing that its latent representation space captures important variation factors.

\begin{figure*}[t]
	\centering
	\subfloat[generator]{\label{fig:gen}
		\includegraphics[width=0.4\linewidth]{images/Doc1.pdf}}
	\hfill
	\subfloat[discriminator]{\label{fig:disc}
		\includegraphics[width=0.58\linewidth]{images/Doc2.pdf}}
		
	\caption{Network architectures of a generator and a discriminator: (a) a MARTA GANs generator is used for the UC-Merced Land-use dataset. The input is a 100-dimensional uniform distribution $p_z(z)$ and the output is a $256 \times 256$-pixel RGB image; (b) a MARTA GANs discriminator is used for the UC-Merced Land-use dataset. The discriminator is treated as a feature extractor to extract features from the multi-feature layer.}
	\label{model} 
	\vspace{-4mm}
\end{figure*}

GANs is a promising unsupervised learning method, yet thus far, it has rarely been applied in the remote sensing field. Due to the tremendous volume of remote sensing images, it would be prohibitively time-consuming and expensive to label all the data. To tackle this issue, GANs would be the excellent choice because it is an unsupervised learning method in which the required quantities of training data would be provided by its generator. Therefore, in this paper, we propose a multiple-layer feature-matching generative adversarial networks 
(MARTA GANs) model to learn the representation of remote sensing images using unlabeled data.
% We also proposed a \emph{multi-feature layer} in the discriminator which performs max pooling from multiple layers to fuse coarse, semantic and local appearance information.

Although based on DCGAN, our approach is rather different in the following aspects. 1) DCGAN can, at most, produce images with a $64\times64$ resolution, while our approach can produce remote sensing images with a resolution of $256\times256$ by adding two deconvolutional layers in the generator; 
%model and two corresponding convolutional layers in the discriminator to extract the features
2) To avoid the problem of such deconvolutional layers producing checkerboard artifacts, the kernel sizes of our networks are $4\times4$, while those in DCGAN are $5\times5$; 3) We propose a multi-feature layer to aggregate the mid- and high-level information; 4) We combine both the perceptual loss and feature matching loss to produce more accurate fake images. Based on the improvements above, our method can realize the better representation of remote sensing images among all methods. Fig.~\ref{overview} shows the overall model.

%vspace：vertical space 竖直间距
%hspace：horizontal space 水平间距



% We bridge the gap between the two
The contributions of this paper are the following:

\begin{enumerate}
	\item To our knowledge, this is the first time that GANs have been applied to classify unsupervised remote sensing images.
% Senior Editor: Please note that the following sentence was changed into a separate bullet point because its content is completely different that of the preceding item.
	\item The results of experiments on the UC-Merced Land-use and Brazilian Coffee Scenes datasets showed that the proposed algorithm outperforms state-of-the-art unsupervised algorithms in terms of overall classification accuracy.
	%In the same time, the generator $G$ got the unprecedented resolution ($256\times256$) optical remote-sensing images and green-red-infrared remote-sensing images.
	\item We propose a multi-feature layer by combining perceptual loss and loss of feature matching to learn better image representations.
%	\item This method outperformed state-of-the-art algorithms in terms of overall accuracy in UC-Merced Land-use dataset and Brazilian Coffee Scenes dataset.
\end{enumerate}


%The outline of the paper will be discussed.
%The rest of this paper is organized as follows. First, we present the training approach and architecture of MARTA GANs in Section \ref{sec:method}. Experimental results are discussed in Section \ref{subsect:experiments}. Finally, Section \ref{sec:discussion} provides conclusions.






%\section{Related Work}
%\label{related}
%\textbf{Unsupervised representation learning.}
%The purpose of unsupervised representation learning is to use unlabeled data to learn a representation that exposes important semantic features as easily decodable factors.  In earlier times, several unsupervised representation learning algorithms were based on autoencoder. Denoising autoencoders \cite{vincent2010stacked, makhzani2013k} receives a corrupted data as input and is trained to predict the original uncorrupted input. Sparse autoencoder can learn useful structures in the input data by imposing sparsity on the hidden units during training. Stacked autoencoders \cite{makhzani2013k} are also used by repeating the process of training greedily layer by layer. All these three algorithms were very simple, but reconstruction of input may not be ideal metric for learning a general-purpose representation. A lot of promising recent work originates from the Skip-gram model \cite{Mikolov2013}, which inspired the skip-thought vectors \cite{Kingma2015} and several techniques for unsupervised feature learning of images \cite{Doersch2015}.
%
%\textbf{ Generative adversarial networks.}
%Generative adversarial networks were first introduced in 2014. The goal of adversarial approach is to learn deep generative models. The generator network directly produces samples ${x}=G({z};{\theta_g})$. However, the discriminator network endeavors to make a distinction between samples drawn from the training data and samples drawn from the generator. The discriminator produces a probability value given by $D({x}; {\theta_g})$, demonstrating the probability that ${x}$ is a real training example instead of a fake sample drawn from the model. In follow-up work, a deep convolutional GAN (DCGAN) \cite{Radford2015} performed very well in image synthesis tasks, and showed that it's latent representation space captures important factors of variation. Improving GANs \cite{Salimans2016} provided a great variety of new architectural features and training procedures, including feature matching and mini batch discrimination, which can produce very clean and sharp images and learn codes that contain valuable information about these textures. InfoGAN \cite{Chen2016}, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner.

% Kim et al have extended this work to very deep~\cite{kim2015accurate} and
% deeply-recursive networks~\cite{kim2015deeply}, both trained with per-pixel Euclidean loss.

\section{Method}


\label{sec:method}
A GAN is most straightforward to apply when the involved models are both multilayer perceptrons; however, to apply a GAN to remote sensing images, we used CNNs for both the generator and discriminator in this work. The generator network directly produces samples $x=G(z;\theta_g)$ with parameters $\theta_g$ and $z$, where $z$ obeys a prior noise distribution $p_z(z$). Its adversary, the discriminator network, attempts to distinguish between samples drawn from the training data and samples created by the generator. The discriminator emits a probability value denoted by $D(x;\theta_d)$ with parameters $\theta_d$, indicating the probability that $x$ is a real training example rather than a fake sample drawn from the generator. During the classification task, the discriminative model $D$ is regarded as the feature extractor. Then, additional training data so that the discriminator can learn a better representation is provided by the generative model $G$.
\vspace{-1em}
\subsection{Training the discriminator}
When training the discriminator, the weights of the generator are fixed. The goals of training the discriminator $D(x)$ are as follows:

\begin{enumerate}
	\item Maximize $D(x)$ for every image from the real training examples.
	\item Minimize $D(x)$ for every image from the fake samples drawn from the generator.
\end{enumerate}

Therefore, the objective function of training discriminator is to maximize:
\begin{equation}{\mathbb E}_{x\sim p_{\rm data}(x)} \log D(x) +
{\mathbb E}_{z\sim p_z(z)}[\log (1-D(G(z)))] \,.
\label{eq:minmax}
\end{equation}


\begin{figure*} 
	\centering
	\subfloat[real images]{\label{fig:uc_real}
		\includegraphics[width=0.48\linewidth]{images/real.pdf}}
	\hfill
	\subfloat[fake images]{\label{fig:uc_fake}
		\includegraphics[width=0.48\linewidth]{images/fake.pdf}}
	
	\caption{Part of exemplary images. (a) Ten random images from UC-Merced data set. (b) Exemplary images produced by generator trained on UC-Merced using the $\ell _{final}$ (Eqn.~\ref{eq:finalloss}) objective.}
	\label{ucland} 
	\vspace{-2mm}
\end{figure*}
\vspace{-1em}
\subsection{Training the generator}
When training the generator, the weights of the discriminator are fixed. The goal of training the generator $G(z)$ is to produce samples that fool $D$. The output of the generator is an image that can be used as the input for the discriminator. Therefore, the generator wants to maximize $D(G(z))$ (or equivalently, minimize $1-D(G(z))$) because $D$ is a probability estimate that ranges only between 0 and 1. We call this concept perceptual loss; it encourages the reconstructed image to be similar to the samples drawn from the training set by minimizing the perceptual loss.
\begin{equation}
\ell_{perceptual}={\mathbb E}_{{z}\sim p_z({z})}[\log (1-D(G({z})))].
\label{eq:perceptual}
\end{equation}

In summary, the discriminator $D$ is shown an image produced from the generator $G$ and adjusts its parameters to make its output, $D(G(Z))$, larger. But $G(Z)$ will train itself to produce images that fool $D$ into thinking they are real. It does this by getting the gradient of $D$ with respect to each sample it produces. In other words, the $G$ is trying to minimize the output while $D$ is trying to maximize it; consequently, it is a minimax game that is defined as follows:
\vspace{-1.5mm}
\begin{align}
\min_G \max_D V(D,G)=&{\mathbb E}_{x\sim p_{\rm data}(x)} \log D(x) +\nonumber\\
&{\mathbb E}_{z\sim p_z(z)}[\log (1-D(G(z)))]\,.
\label{eq:minmax}
\end{align}

To make the images generated by generator more similar to the real images, we train the generator to match the expected values of the features in the multi-feature layer of the discriminator. Letting $f(x)$ denote activations on the multi-feature layer of the discriminator, the loss of feature matching for the generator is defined as follows:
\begin{equation}
\ell_{feature\_match}=||{\mathbb E}_{x\sim p_{\rm data}(x)} f(x)-{\mathbb E}_{z\sim p_z(z)}f(G(z))||_2^2 \,.
\label{eq:featurematchloss}
\end{equation}
Therefore, our final object (the combination of Eqn. \ref{eq:perceptual} and Eqn. \ref{eq:featurematchloss}) for training the generator is to minimize Eqn. \ref{eq:finalloss}~. 
%where $\lambda$ is a hyper-parameter that controls how import the loss of feature matching.
\begin{equation}
%\ell_{final}=\ell_{perceptual}+\lambda \ell_{feature\_matching}
\ell_{final}=\ell_{perceptual}+ \ell_{feature\_matching}
\label{eq:finalloss}.
\end{equation}








\subsection{Network architectures}
\label{subsect:net arc}
%We proposed one kind of network architectures that was similar to DCGAN~\cite{Radford2015}. 


%To fit the complex properties of remote sensing datasets, we have made the following innovations to the network structures. For the generator, we use more deconvolutional layers whose kernel sizes are $4\times4$ to produce higher resolution images and to use both the perceptual loss and feature matching loss to train the generator to produce more accurate fake images. For the discriminator, the multi-feature layer is used to fuse coarse, semantic and local appearance information from the remote sensing images.

The details of the generator and discriminator in MARTA GANs are as follows:


The generator takes 100 random numbers drawn from a uniform distribution as input.
%It is called fully connected as it is just a matrix multiplication.
Then, the result is reshaped into a four-dimensional tensor. We used six deconvolutional layers in our generator to learn its own spatial upsampling and upsample the $4\times 4$ feature maps to $256\times256$ remote sensing images.
% Except for using tanh function in the output layer, other layers all used relu activation in the generator. 
Fig.~\ref{fig:gen} shows a visualization of the generator.




%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.35\textwidth]{images/G.pdf}
%
%	\caption{MARTA GANs generator used for UC-Merced Land-use dataset. The input is a 100 dimensional uniform distribution $p_z(z)$ and the ouput is a $256 \times 256$ pixel RGB image.
%	}
%	\label{fig:gen}
%	\vspace{-3mm}
%\end{figure}

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=0.5\textwidth]{images/D.pdf}
%%	\vspace{-3mm}
%	\caption{MARTA GANs discriminator used for UC-Merced Land-use dataset.
%	}
%	\label{fig:disc}
%	\vspace{-3mm}
%\end{figure}

For the discriminator, the first layer takes input images, including both real and synthesized images. We use convolutions in our discriminator which allows it to learn its own spatial downsampling. As shown in Fig.~\ref{fig:disc}, by performing $4\times4$ max pooling, $2\times2$ max pooling and the identity function separately in the last three convolutional layers, we can produce feature maps that have the same spatial size, $4\times4$. Then, we concatenate the  $4\times4$ feature maps through channel dimension in the multi-feature layer.
%We use LeakyReLU activation in the discriminator for all layers.
Finally, the  multi-feature layer is flattened and fed into a single sigmoid output.
The multi-feature layer includes two functions: 1) the features used for classification are extracted from the flatted multi-feature layer; 2) when training the generator, we use feature matching loss (Eqn.~\ref{eq:featurematchloss}) to evaluate the similarities of the features between the fake and real images in the flatted multi-feature layer. %Fig.~\ref{fig:disc} shows a visualization of the discriminator.

We set the kernel sizes to $4\times4$ and the stride to 2 in all the convolutional and deconvolutional layers, because the deconvolutional layers can avoid uneven overlap when the kernel size is divisible by the stride~\cite{odena2016deconvolution}. In the generator, all layers use ReLU activation except for the output layer, which uses the tanh function. We use LeakyReLU activation in the discriminator for all the convolutional layers; the slope of the leak was set to 0.2.
%The multi-feature layer is flattened and then fed into a single sigmoid output.
We used batch normalization in both the generator and the discriminator, and the decay factor was 0.9.
%We used batch normalization \cite{ioffe2015batch} for both generator and discriminator which stabilizes learning by normalizing the input to each unit to have zero mean and unit variance. This helps to deal with training problems that arise due to poor initialization and helps gradient flow in deeper models. 


%We used batch normalization \cite{ioffe2015batch} for both generator and discriminator which stabilizes learning by normalizing the input to each unit to have zero mean and unit variance. This helps to deal with training problems that arise due to poor initialization and helps gradient flow in deeper models. 








\section{Experiments}
\label{subsect:experiments}
To verify the effectiveness of the proposed method, we trained MARTA GANs on two datasets: the UC Merced Land Use dataset~\cite{yang2010bag} and the Brazilian Coffee Scenes dataset~\cite{penatti2015deep}. We carried out experiments on both datasets using a 5-fold cross-validation protocol and a regularized linear L2-SVM as a classifier. We implemented MARTA GANs in TensorLayer~\footnote{\url{http://tensorlayer.readthedocs.io/en/latest/}}, a deep learning and reinforcement learning library extended from Google TensorFlow \cite{abadi2016tensorflow}. We scaled the input image to the range of [-1, 1] before training. All the models were trained by SGD with a batch size of 64, and we used the Adam optimizer with a learning rate of 0.0002 and a momentum term $\beta_1$ of 0.5.

%In addition to scale the range [-1,1], no other pre-processing was done to train images. We trained all models with mini-batch stochastic gradient descent and the batch size is 64. We set the slope of the leak to 0.2 in the Leaky ReLU. To accelerate training, we used the Adam optimizer \cite{Kingma2015} with tuned hyper parameters and the learning rate was set to 0.0002. Additionally, we set the momentum term $\beta_1=0.5$ to help stabilize training. And we set BatchNormLayer decay factor 0.9 for Exponential Moving Average.

\begin{table*}
	\centering
	\scriptsize 
	\caption{ Classification accuracy (\%) in the form of the means $\pm$ standard deviation bars of DCGAN and MARTA GAN for every class.. The class labels are as follows: 1 = Mobile home park, 2 = Beach, 3 = Tennis courts, 4 = Airplane, 5 = Dense residential, 6 = Harbor, 7 = Buildings, 8= Forest, 9 = Intersection, 10 = River, 11 = Sparse residential, 12 = Runway, 13 = Parking lot, 14 = Baseball diamond, 15 = Agricultural, 16 = Storage tanks, 17 = Chaparral, 18 = Golf course, 19 = Freeway, 20 = Medium residential, and 21 = Overpass.}
	\vspace{-0.5em}
	\label{tab:compare_acc}
	\begin{tabular}{|c|ccccccccccc|}
     \hline
		
		Class&1& 2 &3& 4 & 5 & 	6& 7 &8 & 9 &10&11\\ 
		\hline
		DCGAN &$85\pm5.0$ & $94\pm2.2$ & $89\pm4.2$ & $95\pm3.5$ & $82\pm2.7$ & 
			$91\pm2.2$& $78\pm2.7$ & $83\pm2.7$ & $88\pm2.7$ & $90\pm0.0$ & $79\pm2.2$ \\ 
		
		MARTA GAN &$95\pm3.5$& $100\pm0.0$ & $96\pm4.2$ & $100\pm0.0$ &$89\pm4.2$&
			$99\pm2.2$& $86\pm6.5$ & $97\pm2.7$ & $98\pm2.7$ &$94\pm2.2$& $89\pm2.2$
			\\ 
	\hline
		Class&12 & 13 & 14 & 15 & 16 &17 &18 &19 &20 &21 &\\ 
	\hline	
		DCGAN& $89\pm4.2$ & $88\pm2.7$ & $95\pm3.5$ & $78\pm4.5$ & 
		$93\pm2.7$& $88\pm2.7$ & $97\pm2.7$ & $77\pm2.7$ & $95\pm5.0$ & 	$89\pm4.2$&\\ 
		MARTA GAN&  $94\pm4.2$ & $98\pm2.7$ & $100\pm0.0$ &$85\pm5.0$
		&	$100\pm0.0$& $93\pm2.7$ & $100\pm0.0$ & $87\pm5.7$ &$97\pm5.5$&$95\pm5.0$ &

	\\ 
	\hline
	\end{tabular}
\vspace{-6mm}
\end{table*}

%\begin{table*}
%	\centering
%	\caption{ Classification accuracy (\%) in the form of the means $\pm$ standard deviation bars of DCGAN and MARTA GAN for every class. }
%		\vspace{-0.5em}
%		\label{tab:compare_acc}
%	\begin{tabular}{ccccc|ccccc}
%		\toprule
%		\multicolumn{5}{c|}{\textbf{DCGAN}}& \multicolumn{5}{c}{\textbf{MARTA GAN}} \\ \midrule
%		
%		$85\pm5.0$& $94\pm2.2$ & $89\pm4.2$ & $95\pm3.5$ & $82\pm2.7$ & 
%		
%		$95\pm3.5$& $100\pm0.0$ & $96\pm4.2$ & $100\pm0.0$ &$89\pm4.2$\\ 
%		
%		$91\pm2.2$& $78\pm2.7$ & $83\pm2.7$ & $88\pm2.7$ & $90\pm0.0$ & 
%		
%		$99\pm2.2$& $86\pm6.5$ & $97\pm2.7$ & $98\pm2.7$ &$94\pm2.2$\\ 
%		
%		$79\pm2.2$& $89\pm4.2$ & $88\pm2.7$ & $95\pm3.5$ & $78\pm4.5$ & 
%		
%		$89\pm2.2$& $94\pm4.2$ & $98\pm2.7$ & $100\pm0.0$ &$85\pm5.0$\\ 
%		
%		$93\pm2.7$& $88\pm2.7$ & $97\pm2.7$ & $77\pm2.7$ & $95\pm5.0$ & 
%		
%		$100\pm0.0$& $93\pm2.7$ & $100\pm0.0$ & $87\pm5.7$ &$97\pm5.5$\\ 
%		
%		$89\pm4.2$&  &   &   & &
%		
%		$95\pm5.0$&  &   &   & \\ 
%		\bottomrule
%	\end{tabular}
%	\vspace{-6mm}
%\end{table*}

\subsection{UC Merced dataset}

This dataset consists of images of 21 land-use classes (100 $256\times256$-pixel images for each class).
%selected from aerial optical images acquired by the US Geological Survey. We manually selected 100 $256\times256$-pixel images for each of the 21 land use classes (100 images for each class). 
Some of the images from this dataset are shown in Fig.~\ref{fig:uc_real}. We used a moderate data augmentation in this dataset via flipping images horizontally and vertically and rotating them by 90 degrees to increase the effective training set size. Training takes approximately 4 hours on a single NVIDIA GTX 1080 GPU.
%More information are available in the original paper \cite{yang2010bag}.



%\begin{figure}[h]
%	\centering
%	\includegraphics[width=0.49\textwidth]{images/compare_acc.eps}
%	\vspace{-3mm}
%	\caption{Performance comparison uses different fusion layers.   Features of the $n$ column are extracted from the last $n$ strided convolutional layers in the discriminator. For example, features of the second column are extracted from the last two strided convolutional layers in the discriminator, and so on. We also compared the impacts of data augmentation and loss function on accuracy. }
%	\vspace{-3mm}
%	\label{fig:compare_acc}
%\end{figure}
%\begin{table}[h]
%	\centering
%	\caption{Classification accurary (\%) of representation extracted by DCGANs and MARTA GANS on the UC-Merced dataset. Best result in bold.}
%	\label{tab:uc_gan}
%	\begin{tabular}{llc}
%		\toprule
%		Architectures & Design & Accuracy \\
%		\midrule
%		DCGANs  & Without data augmentation & 80.36  \\
%		& With data augmentation &  87.01\\
%		MARTA GANs & Without data augmentation&  85.37\\
%		& With data augmentation, $\ell_{perceptual}$  only & 93.57\\
%		& With data augmentation, use $\ell_{final}$ & \textbf{95.00}\\
%		\bottomrule
%	\end{tabular}
%\end{table}

\begin{figure}[t]
	\centering
	\subfloat[]{
		\includegraphics[width=0.24\linewidth]{images/compare_acc1.pdf}}
	%\hfill
	\subfloat[ ]{
		\includegraphics[width=0.24\linewidth]{images/compare_acc2.pdf}}
	%\hfill
	%	\vspace{-3mm}
	\subfloat[ ]{
		\includegraphics[width=0.24\linewidth]{images/compare_acc3.pdf}}
	%\hfill
	\subfloat[ ]{
		\includegraphics[width=0.24\linewidth]{images/compare_acc4.pdf}}
	\\
	\vspace{-2mm}
	\caption{The performance comparison uses different features. (a) $f_1$; (b) $f_2$; (c) $f_3$;
		(d) $f_4$. 
		The red curves: training with $\ell_{final}$ and with data augmentation;
		Cyan curves: training with $\ell_{perceptual}$ and with data augmentation;
		Yellow curves: training with $\ell_{final}$ and without data augmentation;
		Green curves: training with $\ell_{perceptual}$ and without data augmentation. }
	\vspace{-2mm}
	\label{fig:compare_acc}
\end{figure}




To evaluate the quality of the representations learned by the multi-feature layer, we trained on the UC-Merced data and extracted the features from different multi-feature layers.
%to combine features from different convolutional layers. 
To improve the clarity of the expression, we use $f_1$ to denote the features from the last convolutional layer, $f_2$ to denote features combined from the last two convolutional layers' features, and so on. Based on the results shown in Fig.~\ref{fig:compare_acc}, we found that $f_3$ achieved the highest accuracy. These results can be explained by two reasons. First, $f_3$ has the same high-level information as $f_1$ and $f_2$, but it has more mid-level information compared with $f_1$ and $f_2$. However, $f_4$ has too much low-level information, which leads to the "curse of dimensionality." Therefore, the features extracted from the last three convolutional layers in the discriminator resulted in the highest accuracy. As shown in Fig.~\ref{fig:compare_acc}, data augmentation is an effective way to reduce overfitting when training a large deep network. Augmentation generates more training image samples by rotating and flipping patches from original images. We also evaluated the performance between two types of loss: $\ell_{perceptual}$ (Eqn. \ref{eq:perceptual}) and $\ell_{final}$ (Eqn.~\ref{eq:finalloss}) and found that using $\ell_{final}$ achieved the best performance. Synthesized remote sensing images when using $\ell_{final}$ are shown in Fig.~\ref{fig:uc_fake}.




%These features are then flattened and concatenated to a 14336 dimensional vector and a regularized linear L2-SVM classifier is trained on top of them. This achieves 95.00\% accuracy which outperforms all unsupervised representation Learning methods on this dataset.





Fig.~\ref{fig:conf_matix} depicts the confusion matrix of classification results for the two GAN architectures, DCGAN and MARTA GAN. DCGAN and MARTA GAN reached an overall accuracy of $87.76\pm0.64$\% and $94.86\pm0.80$\%, respectively. MARTA GAN is approximately 7\% better because it used the multi-feature layer to merge the mid-level and global features. To improve the comparison, the accuracy classification performances of the methods for each class are shown in Table~\ref{tab:compare_acc}. Compared to DCGAN, MARTA GAN achieves 100.00\% accuracy in some scene categories (e.g., Beach, Airplane, etc.). Moreover, MARTA GAN also achieves higher accuracy in some very close classes, such as dense residential, building, medium residential, sparse residential.

%The features extraction from multiple feature layer in discriminator results in 100.00\% accuracy for some scene categories. But comparing the presence with other very close classes, like dense residential, building, medium residential, mobile home park, get about 88.33\% accuracy, see in Fig.~\ref{ucclose}, even human can not distinguish between them sometimes.



%The data augmentation is an effective way to reduce overfitting when training a large deep network, which generates more training image samples by rotation and flipping these patches from original images. We also evaluate the performance of the generator network for two losses $\ell_{perceptual}$ (Eqn. \ref{eq:perceptual})and $\ell_{final}$ (Eqn. \ref{eq:featurematchloss}), combined this two losses (in Eqn. \ref{eq:finalloss} we use $\lambda = 1.0$) achieved the best performance, and generated remote sense examples provided in Fig. \ref{fig:uc_data}.






%\begin{figure*}[t]
%	
%	\begin{center}
%		
%		\includegraphics[width=0.09\textwidth]{images/ug4.png}
%		\includegraphics[width=0.09\textwidth]{images/ug2.jpg}
%		\includegraphics[width=0.09\textwidth]{images/ug5.png}
%		\includegraphics[width=0.09\textwidth]{images/ug13.png}
%		\includegraphics[width=0.09\textwidth]{images/ug18.png} \\
%		\vspace{1.5mm}
%		\includegraphics[width=0.09\textwidth]{images/ug16.png}
%		\includegraphics[width=0.09\textwidth]{images/ug17.png}
%		\includegraphics[width=0.09\textwidth]{images/ug14.png}
%		\includegraphics[width=0.09\textwidth]{images/ug8.png}
%		\includegraphics[width=0.09\textwidth]{images/ug6.png}
%	\end{center}
%	\caption{Exemplary images produced by a generator trained on UC-Merced using the $\ell _{fianl}$ (Eqn. \ref{eq:finalloss}) objective. Generator sampled with unprecedented resolution ($256\times256$) Remote Sensing Images.
%	}
%	\label{fig:uc_data}
%\end{figure*}
%
%\begin{figure*}[t]
%	
%	\begin{center}
%		
%		\includegraphics[width=0.09\textwidth]{images/1.jpg}
%		\includegraphics[width=0.09\textwidth]{images/2.jpg}
%		\includegraphics[width=0.09\textwidth]{images/5.jpg}
%		\includegraphics[width=0.09\textwidth]{images/3.jpg}
%		\includegraphics[width=0.09\textwidth]{images/8.jpg} \\
%		\vspace{1.5mm}
%		\includegraphics[width=0.09\textwidth]{images/6.jpg}
%		\includegraphics[width=0.09\textwidth]{images/7.jpg}
%		\includegraphics[width=0.09\textwidth]{images/4.jpg}
%		\includegraphics[width=0.09\textwidth]{images/9.jpg}
%		\includegraphics[width=0.09\textwidth]{images/10.jpg}
%	\end{center}
%	\caption{Exemplary images produced by a generator trained on UC-Merced using the $\ell _{fianl}$ (Eqn. \ref{eq:finalloss}) objective. Generator sampled with unprecedented resolution ($256\times256$) Remote Sensing Images.
%	}
%	\label{fig:uc_data}
%\end{figure*}




%\begin{figure}[h]
%	\centering
%	\includegraphics[width=0.48\textwidth]{images/con_matix.pdf}
%	\caption{Confusion matrices on the UC-Merced dataset using features extracted from MARTA GAN.
%	}
%	\vspace{-3mm}
%	\label{fig:conf_mat}
%\end{figure}


%\begin{figure} 
%	\centering
%	\subfloat[] {\label{fig:real}
%		\includegraphics[width=0.18\linewidth]{images/cd.jpg}}
%	\hfill
%	\subfloat[ ]{\label{fig:fake}
%		\includegraphics[width=0.18\linewidth]{images/cb.jpg}}
%	\hfill
%	\subfloat[ ]{\label{fig:fake}
%		\includegraphics[width=0.18\linewidth]{images/cm.jpg}}
%	\hfill
%	\subfloat[ ]{\label{fig:fake}
%		\includegraphics[width=0.18\linewidth]{images/co.jpg}}
%	\\
%	\caption{Exemplary images of close classes from the UC Merced dataset. (a) dense residential; (b) building;
%		(c) medium residential; (d) mobile homepark.}
%	\vspace{-2mm}
%	\label{ucclose} 
%\end{figure}


%\begin{figure}[t]
%	
%	\begin{center}
%		\hspace{0mm} \verb|dense residential|
%		\hspace{4mm} \verb|building|
%		\hspace{8mm} \verb|medium residential|
%		\hspace{2mm} \verb|mobile homepark|
%		\includegraphics[width=0.09\textwidth]{images/cd.jpg}
%		\hspace{4mm}
%		\includegraphics[width=0.09\textwidth]{images/cb.jpg}
%		\hspace{4mm}
%		\includegraphics[width=0.09\textwidth]{images/cm.jpg}
%		\hspace{4mm}
%		\includegraphics[width=0.09\textwidth]{images/co.jpg}
%	\end{center}
%
%	\caption{Exemplary images of close classes from the UC Merced dataset.
%	}
%	\label{fig:close}
%\end{figure}
\begin{figure}[t]
	\centering
	\subfloat[]{\label{fig:real}
		\includegraphics[width=0.48\linewidth]{images/con_matix0.pdf}}
	\hfill
	\subfloat[]{\label{fig:fake}
		\includegraphics[width=0.48\linewidth]{images/con_matix1.pdf}}
	%	\hfill
	%	\subfloat[]{\label{fig:fake}
	%		\includegraphics[width=0.48\linewidth]{images/acc.pdf}}
	\\
	\caption{Confusion matrix of (a):DCGAN, (b):MARTA GAN. The class labels are same as Table~\ref{tab:compare_acc}.}
	\label{fig:conf_matix} 
		\vspace{-5mm}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.49\textwidth]{images/t_sne.pdf}
	\vspace{-1.5em}
	\caption{2-D feature visualization of image global representations of the UC-Merced dataset.  The class labels are same as Table~\ref{tab:compare_acc}.}
	\vspace{-4mm}
	\label{fig:t_sne}
\end{figure}


\begin{figure*} 
	\centering
	\subfloat[real images]{\label{fig:coffee_real}
		\includegraphics[width=0.48\linewidth]{images/coffee_real.pdf}}
	\hfill
	\subfloat[fake images]{\label{fig:coffe_fake}
		\includegraphics[width=0.48\linewidth]{images/coffee_fake.pdf}}
	\\
	\caption{Parts of exemplary images: (a) ten random images from the Brazilian Coffee Scenes dataset; (b) exemplary images produced by a generator trained on the Brazilian Coffee Scenes dataset using the $\ell _{final}$ (Eqn. \ref{eq:finalloss}) objective.}
	\label{coffee}
		\vspace{-2mm} 
\end{figure*}

In addition, we visualized the global image representations encoded via MARTA GANs features of the UC-Merced dataset. We computed the features for all the scenes of the dataset and then used the t-SNE algorithm to embed the high-dimensional features in 2-D space. The final results are shown in Fig. \ref{fig:t_sne}. This visualization shows that features extracted from the multi-feature layer contain abstract semantic information because those close classes are also very close in 2-D space.



%In recent years, some approaches have been developed for remote sensing scene classification and most of them have been verified on the UC-Merced dataset. Therefore, a lot of data available can be compared with our results in this paper.


Compared with the results of other tested methods, the method proposed in this work achieves the highest classification accuracy among the unsupervised methods. As shown in Table~\ref{tab:uc_ref}, our method outperforms the SCMF~\cite{sheng2012high} (a sparse coding based multiple-feature fusion method) by 3.82\%. When the classification accuracy of our method is compared with  LRFF~\cite{hu2015unsupervised} (an improved unsupervised feature learning algorithm based on spectral clustering), our method  outperforms LRFF by more than 4\%. While some of the supervised methods~\cite{penatti2015deep, nogueira2017towards} achieved an accuracy above 99\%, these methods are fine-tuned from pre-trained models, which are usually trained with a large amount of labeled data (such as ImageNet). Compared with those methods, our unsupervised method requires fewer parameters.

%On the other hand,  Multiview Deep Learning (MDL)~\cite{luus2015multiview} has obtained the best result by using the multiscale input strategy in supervised learning.

%In table \ref{tab:uc_ref} we report the overall accuracies for some comparable methods test in this dataset, as they appear in the original papers, together with the accuracy of our best MARTA GANs method. In all the supervised learning methods, Multi View Deep Learning (MDL)~\cite{luus2015multiview} has obtained the best result by using the multiscale input strategy.
%The proposed method guarantees a better performance gain w.r.t. to all references.



%\begin{center}
%
%\begin{table*}
%	\footnotesize
%	\centering
%\begin{tabular}{|c|p{1.1cm}<{\centering}|c|c|c|c|c|c|c|c|c|c|}
%	\hline 
%	Label& 1 & 2 &3  & 4 & 5 & 6 & 7 & 8 & 9 & 10 &11 \\ 
%	\hline 
%	85.12+0.42&85.12+0.42& \textbf{85.12+0.42} & 85.12+0.42 &  85.12+0.42& 85.12+0.42 & 85.12+0.42 &85.12+0.42 & 85.12+0.42 & 85.12+0.42 &85.12+0.42&85.12+0.42 \\ 
%	\hline 
%	MARTA&  &  &  &  &  &  &  \\ 
%	\hline 
%	label& 12 & 13 &14  & 15 & 16 & 17& 18 & 19 & 20 & 21 & \\ 
%	\hline 
%	DCGAN& &  &  &  &  &  &  \\ 
%	\hline 
%	MARTA &  &  &  &  &  &  &  \\ 
%	\hline 
%\end{tabular} 
%\end{table*}
%\end{center}


\begin{table}[t]
	\centering
	\scriptsize
	\caption{Overall classification accuracy (\%) of reference and proposed methods on the UC-Merced dataset and Coffee Scenes dataset. Our result is in bold.}
	\label{tab:uc_ref}
	
	\begin{tabular}{|p{1.2cm}<{\centering} | p{2.25cm} p{1.2cm} p{0.7cm}<{\centering}  p{1.4cm}<{\centering}|}
		%\toprule
		\hline
		 DataSet&Method & Description&Parameters &Accuracy \\
		%\midrule
		\hline
		 \multirow{5}{*}{UC-Merced}&SCMF~\cite{sheng2012high}& Unsupervised&- & $91.03\pm0.48$ \\
		&UFL-SC~\cite{hu2015unsupervised}&Unsupervised&-& $90.26\pm1.51$  \\
		%PSR~\cite{chen2015pyramid}&Unsupervised&-& 89.10 \\
		%Fine-tuned GoogLeNet~\cite{castelluccio2015land}&Supervised&5M& 97.10\\
		&OverFeat$_L$ + Caffe~\cite{penatti2015deep}&Supervised&205M&$99.43\pm0.27$\\
		&GoogLeNet~\cite{nogueira2017towards}&Supervised&5M&$99.47\pm0.50$\\
		%2015&ConvNet~\cite{nogueira2015improving}&Supervised&& 89.39 \\
		%2015&MDL~\cite{luus2015multiview}&Supervised&& 93.48 \\
%		2015&GoogLeNet~\cite{castelluccio2015land}&Supervised, fine-tuning& {97.10}\\
		&\textbf{MARTA GANs}&\textbf{Unsupervised}&\textbf{2.8M}& \textbf{$94.86\pm0.80$}\\
		\hline
		 \multirow{4}{*}{Coffee}	&BIC~\cite{penatti2015deep}& Unsupervised&- & $87.03\pm1.07$ \\
	&OverFeat$_L$+OverFeat$_S$~\cite{penatti2015deep}&Supervised&289M& $83.04\pm2.00$  \\
		%GoogLeNet~\cite{castelluccio2015land} &Supervised&5M& 91.80  \\
		&CaffeNet~\cite{nogueira2017towards}&Supervised&60M& $94.45\pm1.20$  \\
		%2015&ConvNet~\cite{nogueira2015improving}&Supervised& {89.79}\\
		&\textbf{MARTA GANs}&\textbf{Unsupervised}&\textbf{0.18M}& \textbf{$89.86\pm0.98$}\\
		%\bottomrule
		\hline
	\end{tabular}
\vspace{-1em}
\end{table}




%\begin{figure*}[t]
%	
%	\begin{center}
%		
%		\includegraphics[width=0.09\textwidth]{images/cg11.png}
%		\includegraphics[width=0.09\textwidth]{images/cg12.png}
%		\includegraphics[width=0.09\textwidth]{images/cg13.png}
%		\includegraphics[width=0.09\textwidth]{images/cg4.png}
%		\includegraphics[width=0.09\textwidth]{images/cg5.png} \\
%		\vspace{1.5mm}                        
%		\includegraphics[width=0.09\textwidth]{images/cg6.png}
%		\includegraphics[width=0.09\textwidth]{images/cg7.png}
%		\includegraphics[width=0.09\textwidth]{images/cg8.png}
%		\includegraphics[width=0.09\textwidth]{images/cg9.png}
%		\includegraphics[width=0.09\textwidth]{images/cg10.png}
%	\end{center}
%	\caption{Exemplary images produced by a generator trained on UC-Merced using the $\ell _{fianl}$ (Eqn. \ref{eq:finalloss}) objective. Generator sampled with unprecedented resolution ($256\times256$) Remote Sensing Images.
%	}
%	\label{fig:uc_data}
%\end{figure*}
%
%\begin{figure*}[t]
%	
%	\begin{center}
%		
%		\includegraphics[width=0.09\textwidth]{images/1.jpg}
%		\includegraphics[width=0.09\textwidth]{images/2.jpg}
%		\includegraphics[width=0.09\textwidth]{images/5.jpg}
%		\includegraphics[width=0.09\textwidth]{images/3.jpg}
%		\includegraphics[width=0.09\textwidth]{images/8.jpg} \\
%		\vspace{1.5mm}
%		\includegraphics[width=0.09\textwidth]{images/6.jpg}
%		\includegraphics[width=0.09\textwidth]{images/7.jpg}
%		\includegraphics[width=0.09\textwidth]{images/4.jpg}
%		\includegraphics[width=0.09\textwidth]{images/9.jpg}
%		\includegraphics[width=0.09\textwidth]{images/10.jpg}
%	\end{center}
%	\caption{Exemplary images produced by a generator trained on UC-Merced using the $\ell _{fianl}$ (Eqn. \ref{eq:finalloss}) objective. Generator sampled with unprecedented resolution ($256\times256$) Remote Sensing Images.
%	}
%	\label{fig:uc_data}
%\end{figure*}


%\begin{figure}[t]
%	
%	\begin{center}
%		
%		\includegraphics[width=0.18\textwidth]{images/cg11.png}
%		\includegraphics[width=0.18\textwidth]{images/cg12.png}
%		\includegraphics[width=0.18\textwidth]{images/cg13.png}
%		\includegraphics[width=0.18\textwidth]{images/cg4.png}
%		\includegraphics[width=0.18\textwidth]{images/cg5.png} \\
%		\vspace{2.5mm}
%		\includegraphics[width=0.18\textwidth]{images/cg6.png}
%		\includegraphics[width=0.18\textwidth]{images/cg7.png}
%		\includegraphics[width=0.18\textwidth]{images/cg8.png}
%		\includegraphics[width=0.18\textwidth]{images/cg9.png}
%		\includegraphics[width=0.18\textwidth]{images/cg10.png}
%	\end{center}
%	\vspace{-5mm}
%	\caption{Exemplary images produced by a generator trained on Brazilian Coffee dataset using the $\ell _{fianl}$ (Eqn. \ref{eq:finalloss}) objective. Is the first time generator sampled with green-red-infrared  remote sensing images.
%	}
%	\label{fig:coffee_data}
%\end{figure}




\subsection{Brazilian Coffee Scenes dataset}
\label{subsect:coffee}
To evaluate the generalization power of our model, we also performed experiments using the Brazilian Coffee Scenes dataset~\cite{penatti2015deep}, which is a composition of scenes taken by the SPOT sensor in the green, red, and near-infrared bands. This dataset has 2,876 multispectral high-resolution scenes. It includes 1,438 tiles of coffee and 1,438 tiles of non-coffee with a $64\times64$-pixel resolution. Fig.~\ref{fig:coffee_real} shows some examples of this dataset. We did not use data augmentation on this dataset because it contains sufficient data to train the network.

%
%
%The whole image set of each county was partitioned into multiple tiles of $64\times64$ pixels. For this dataset, it was considered only the green, red, and near-infrared bands, which are the most useful and representative ones for discriminating vegetation areas, 
% The creation of the dataset is performed as follows: tiles with at least 85\% of coffee pixels were assigned to the coffee class; tiles with less than 10\% of coffee pixels were assigned to the non-coffee class, it has 1438 tiles of coffee and 1438 tiles of uncoffee. Fig. \ref{fig:coffee_data} shows some examples produced by a generator trained on this dataset.


%
%\vspace{-7.5mm}
%\begin{table}[h]
%	\centering
%	\caption{Classification accurary (\%) of representation extracted by DCGANs and MARTA GANS on the Coffee Scenes dataset. Best result in bold.}
%	\label{tab:coffee_gan}
%	\begin{tabular}{llc}
%		\toprule
%		Architectures & Design & Accuracy \\
%		\midrule
%		DCGANs  & Without data augmentation & 85.36  \\
%		& With data augmentation &  85.01\\
%		MARTA GANs & Without data augmentation&  87.69\\
%		& With data augmentation, $\ell_{perceptual}$  only & 87.73\\
%		& With data augmentation, use $\ell_{final}$ & \textbf{88.36}\\
%		\bottomrule
%	\end{tabular}
%\end{table}

Table~\ref{tab:uc_ref} shows the results obtained with the proposed method. In general, the results are significantly worse than those on the UC-Merced dataset, despite reducing the classification from a 21-class to a 2-class problem. Brazilian Coffee Scenes is a challenging dataset because of the high intra-class variability caused by different crop management techniques, different plant ages and spectral distortions and shadows. Nevertheless, our results are better than that of BIC~\cite{penatti2015deep}.


%\begin{table}[t]
%	\centering
%	\caption{Overall classification accuracy (\%) of reference and proposed methods on the Coffee Scenes dataset. Our result is in bold.}
%	\label{tab:coffee_ref}
%	\begin{tabular}{ p{3.05cm}p{1.4cm}cp{1.5cm}<{\centering}}
%	     \toprule
%	 Method & Description&Parameters& Accuracy \\
%		\midrule
%	BIC~\cite{penatti2015deep}& Unsupervised&- & $87.03\pm1.07$ \\
%	OverFeat$_L$+OverFeat$_S$~\cite{penatti2015deep}&Supervised&289M& $83.04\pm2.00$  \\
%	GoogLeNet~\cite{castelluccio2015land} &Supervised&5M& 91.80  \\
%	Fine-tuned CaffeNet~\cite{nogueira2017towards}&Supervised&60M& $94.45\pm1.20$  \\
%		2015&ConvNet~\cite{nogueira2015improving}&Supervised& {89.79}\\
%	\textbf{MARTA GANs}&\textbf{Unsupervised}&\textbf{0.18M}& \textbf{$89.86\pm0.98$}\\
%		\bottomrule
%	\end{tabular}
%	\vspace{-2em}
%\end{table}





\vspace{-2mm}
\section{CONCLUSION}
\label{sec:discussion}
\vspace{-2mm}
This paper introduced a representation learning algorithm called MARTA GANs. In contrast to previous approaches that require supervision, MARTA GANs is completely unsupervised; it can learn interpretable representations even from challenging remote sensing datasets. In addition, MARTA GANs introduces a new multiple-feature-matching layer that learns multi-scale spatial information for high-resolution remote sensing. Other possible future extensions to the work described in this paper include: producing high-quality samples of remote sensing images using the generator and classifying remote sensing images in a semi-supervised manner to improve classification accuracy.




% Below is an example of how to insert images. Delete the ``\vspace'' line,
% uncomment the preceding line ``\centerline...'' and replace ``imageX.ps''
% with a suitable PostScript file name.
% -------------------------------------------------------------------------



% To start a new column (but not a new page) and help balance the last-page
% column length use \vfill\pagebreak.
% -------------------------------------------------------------------------
%\vfill
%\pagebreak





% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEtran}
\bibliography{paper}

\end{document}
