% last updated in April 2002 by Antje Endemann
% Based on CVPR 07 and LNCS, with modifications by DAF, AZ and elle, 2008 and AA, 2010, and CC, 2011; TT, 2014; AAS, 2016

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} % define this before the line numbering.
% \usepackage{ruler}
\usepackage{color}
\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}
\usepackage{multirow}

\makeatletter
\newcommand{\printfnsymbol}[1]{%
  \textsuperscript{\@fnsymbol{#1}}%
}
\makeatother

\begin{document}
\title{Supplementary of Mask TextSpotter}
\titlerunning{Mask TextSpotter}
% Replace with a meaningful short version of your title
%
\author{Pengyuan Lyu\thanks{Authors contribute equally.}\inst{1} \and
Minghui Liao\printfnsymbol{1}\inst{1} \and
Cong Yao\inst{2} \and
Wenhao Wu\inst{2} \and \\
Xiang Bai\thanks{Corresponding author.}\inst{1}}
%
%Please write out author names in full in the paper, i.e. full given and family names. 
%If any authors have names that can be parsed into FirstName LastName in multiple ways, please include the correct parsing, in a comment to the volume editors:
%\index{Lastnames, Firstnames}
%(Do not uncomment it, because you may introduce extra index items if you do that, we will use scripts for introducing index entries...)
\authorrunning{Pengyuan Lyu, Minghui Liao, Cong Yao, Wenhao Wu, Xiang Bai}
% Replace with shorter version of the author list. If there are more authors than fits a line, please use A. Author et al.
%

\institute{
% School of Electronic Information and Communications, \\
Huazhong University of Science and Technology \and
Megvii (Face++) Technology Inc. \\
% \email{lncs@springer.com}\\
% \url{http://www.springer.com/gp/computer-science/lncs} \and
% ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
% \email{\{abc,lncs\}@uni-heidelberg.de}
\email{lvpyuan@gmail.com, mhliao@hust.edu.cn, yaocong2010@gmail.com, wwh@megvii.com, xbai@hust.edu.cn}
}
\maketitle
\section{Ablation Experiments}
%
\begin{table}
\begin{centering}
\caption{Ablation experimental results. ``Ours (a)":  without character annotations from the real images; ``Ours (b)": without weighted edit distance.}
\label{tab_discussion}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline 
\multirow{3}{*}{Method} & \multicolumn{6}{c|}{ICDAR2013} & \multicolumn{6}{c|}{ICDAR2015}\tabularnewline
\cline{2-13} 
 & \multicolumn{3}{c|}{Word Spotting} & \multicolumn{3}{c|}{End-to-End} & \multicolumn{3}{c|}{Word Spotting} & \multicolumn{3}{c|}{End-to-End}\tabularnewline
\cline{2-13} 
 & S & W & G & S & W & G & S & W & G & S & W & G\tabularnewline
\hline 
Ours(a) &91.8 &90.3 &85.9 &90.7 &89.4 &84.6 &76.9 &71.6 &61.6 &76.6 &69.9 &59.8 \tabularnewline
\hline 
Ours(b) &91.4  &90.5  &84.3  &91.3  &89.9  &83.8  &75.9  &67.5  &56.8  &76.1  &67.1  &56.7 \tabularnewline
\hline 
Ours &\textbf{92.5}  &\textbf{92.0}  &\textbf{88.2} &\textbf{92.2}  &\textbf{91.1}  &\textbf{86.5}  &\textbf{79.3}  &\textbf{74.5}  &\textbf{64.2}  &\textbf{79.3}  &\textbf{73.0}  &\textbf{62.4} \tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\end{table}

\noindent\textbf{With or without character maps} We train a model named ``Ours(det only)" which removes the subnet of the character maps from the original network to explore the effect of training detection and recognition jointly. As shown in Table 3 in the paper, the detection results of ``Ours" exceed ``Ours(det only)" by $0.7\%$ and $2.6\%$ on ICDAR2013 and ICDAR2015 respectively,  which demonstrate that the detection task can  benefit from the recognition task when jointly training.
\\
\textbf{With or without real-world character annotation} The experiment without real-world character annotations is also conducted. As shown in Table~\ref{tab_discussion},  although ``Ours(a)" is trained without any real-world character annotation, it still achieves competitive performances. More specifically, for horizontal text (ICDAR2013), it decrease ``Ours", which is trained with a few real-world character annotations, by  $0.7\%-2.3\%$ on various settings; on ICDAR2015, ``Ours(a)" still outperforms all other previous methods by a large margin.
\\
\textbf{With or without weighted edit distance} We conduct experiments to verify the effectiveness of our proposed weighted edit distance. The method of using original edit distance is named ``Ours(b)" and the results are shown in Table~\ref{tab_discussion}. As shown, weighted edit distance can boost the performance by at most 7.4 points of all experiments. %In general, the larger lexicon leads to more performance gains.

\end{document}
