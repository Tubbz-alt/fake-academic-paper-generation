% This template has been tested with LLNCS DOCUMENT CLASS -- version 2.20 (24-JUN-2015)

%"runningheads" enables:
%  - page number on page 2 onwards
%  - title/authors on even/odd pages
%This is good for other readers to enable proper archiving among other papers and pointing to
%content. Even if the title page states the title, when printed and stored in a folder, when
%blindly opening the folder, one could hit not the title page, but an arbitrary page. Therefore,
%it is good to have title printed on the pages, too.
\documentclass[runningheads,a4paper]{llncs}[2015/06/24]

%cmap has to be loaded before any font package (such as cfr-lm)
\usepackage{cmap}
\usepackage[T1]{fontenc}

\usepackage{graphicx}

%Even though `american`, `english` and `USenglish` are synonyms for babel package (according to https://tex.stackexchange.com/questions/12775/babel-english-american-usenglish), the llncs document class is prepared to avoid the overriding of certain names (such as "Abstract." -> "Abstract" or "Fig." -> "Figure") when using `english`, but not when using the other 2.
%english has to go last to set it as default language
\usepackage[ngerman,english]{babel}
%Hint by http://tex.stackexchange.com/a/321066/9075 -> enable "= as dashes
\addto\extrasenglish{\languageshorthands{ngerman}\useshorthands{"}}

%better font, similar to the default springer font
%cfr-lm is preferred over lmodern. Reasoning at http://tex.stackexchange.com/a/247543/9075
\usepackage[%
rm={oldstyle=false,proportional=true},%
sf={oldstyle=false,proportional=true},%
tt={oldstyle=false,proportional=true,variable=true},%
qt=false%
]{cfr-lm}
%
%if more space is needed, exchange cfr-lm by mathptmx
%\usepackage{mathptmx}

%for demonstration purposes only
\usepackage[math]{blindtext}

%Sorts the citations in the brackets
%It also allows \cite{refa, refb}. Otherwise, the document does not compile.
%  Error message: "White space in argument"
\usepackage{cite}


%% If you need packages for other papers,
%% START COPYING HERE
%% COPY ALSO cmap and fontenc from lines 10 to 12

%extended enumerate, such as \begin{compactenum}
\usepackage{paralist}

%put figures inside a text
%\usepackage{picins}
%use
%\piccaptioninside
%\piccaption{...}
%\parpic[r]{\includegraphics ...}
%Text...

%for easy quotations: \enquote{text}
\usepackage{csquotes}

%enable margin kerning
\usepackage{microtype}

\usepackage{subcaption}

%tweak \url{...}
\usepackage{url}
%\urlstyle{same}
%improve wrapping of URLs - hint by http://tex.stackexchange.com/a/10419/9075
\makeatletter
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\makeatother
%nicer // - solution by http://tex.stackexchange.com/a/98470/9075
%DO NOT ACTIVATE -> prevents line breaks
%\makeatletter
%\def\Url@twoslashes{\mathchar`\/\@ifnextchar/{\kern-.2em}{}}
%\g@addto@macro\UrlSpecials{\do\/{\Url@twoslashes}}
%\makeatother

%diagonal lines in a table - http://tex.stackexchange.com/questions/17745/diagonal-lines-in-table-cell
%slashbox is not available in texlive (due to licensing) and also gives bad results. This, we use diagbox
%\usepackage{diagbox}

%required for pdfcomment later
\usepackage{xcolor}

%enable nice comments
%this also loads hyperref
\usepackage{pdfcomment}
%enable hyperref without colors and without bookmarks
\hypersetup{hidelinks,
   colorlinks=true,
   allcolors=black,
   pdfstartview=Fit,
   breaklinks=true}
%enables correct jumping to figures when referencing
\usepackage[all]{hypcap}

\newcommand{\commentontext}[2]{\colorbox{yellow!60}{#1}\pdfcomment[color={0.234 0.867 0.211},hoffset=-6pt,voffset=10pt,opacity=0.5]{#2}}
\newcommand{\commentatside}[1]{\pdfcomment[color={0.045 0.278 0.643},icon=Note]{#1}}

%compatibality with packages todo, easy-todo, todonotes
\newcommand{\todo}[1]{\commentatside{#1}}
%compatiblity with package fixmetodonotes
\newcommand{\TODO}[1]{\commentatside{#1}}

%enable \cref{...} and \Cref{...} instead of \ref: Type of reference included in the link
\usepackage[capitalise,nameinlink]{cleveref}
%Nice formats for \cref
\crefname{section}{Sect.}{Sect.}
\Crefname{section}{Section}{Sections}

\usepackage{xspace}
%\newcommand{\eg}{e.\,g.\xspace}
%\newcommand{\ie}{i.\,e.\xspace}
\newcommand{\eg}{e.\,g.,\ }
\newcommand{\ie}{i.\,e.,\ }

\usepackage{bm}
\usepackage{amsmath}
\usepackage{mathtools}

%introduce \powerset - hint by http://matheplanet.com/matheplanet/nuke/html/viewtopic.php?topic=136492&post_id=997377
\DeclareFontFamily{U}{MnSymbolC}{}
\DeclareSymbolFont{MnSyC}{U}{MnSymbolC}{m}{n}
\DeclareFontShape{U}{MnSymbolC}{m}{n}{
    <-6>  MnSymbolC5
   <6-7>  MnSymbolC6
   <7-8>  MnSymbolC7
   <8-9>  MnSymbolC8
   <9-10> MnSymbolC9
  <10-12> MnSymbolC10
  <12->   MnSymbolC12%
}{}
\DeclareMathSymbol{\powerset}{\mathord}{MnSyC}{180}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

%% END COPYING HERE

\begin{document}

\title{Automatic Instrument Segmentation in Robot-Assisted Surgery Using Deep Learning}
%If Title is too long, use \titlerunning
\titlerunning{Deep Learning for Surgical Instrument Segmentation}

% %Single insitute
% \author{Firstname Lastname \and Firstname Lastname}
% %If there are too many authors, use \authorrunning
% %\authorrunning{First Author et al.}
% \institute{Institute}

%Multiple insitutes
%Currently disabled

\author{Alexey A. Shvets\inst{1}, Alexander Rakhlin\inst{2}, Alexandr A. Kalinin\inst{3}, \\ \and Vladimir I. Iglovikov\inst{4}}
% %If there are too many authors, use \authorrunning
 \authorrunning{Shvets et al.}

 \institute{
  Massachusetts Institute of Technology, Cambridge, MA 02142, USA\\ \email{shvets@mit.edu}
  \and Neuromation OU, Tallinn, 10111 Estonia\\ \email{rakhlin@neuromation.io} 
  \and University of Michigan, Ann Arbor, MI 48109, USA\\ \email{akalinin@umich.edu}
  \and Lyft Inc., San Francisco, CA 94107, USA\\ \email{iglovikov@gmail.com}  
  }

% \fi
\maketitle

\begin{abstract}
Semantic segmentation of robotic instruments is an important problem for the robot-assisted surgery. One of the main challenges is to correctly detect an instrument's position for the tracking and pose estimation in the vicinity of surgical scenes. Accurate pixel-wise instrument segmentation is needed to address this challenge. In this paper we describe our deep learning-based approach for robotic instrument segmentation. Our approach demonstrates an improvement over the state-of-the-art results using several novel deep neural network architectures. It addressed the binary segmentation problem, where every pixel in an image is labeled as an instrument or background from the surgery video feed. In addition, we solve a multi-class segmentation problem, in which we distinguish between different instruments or different parts of an instrument from the background. In this setting, our approach outperforms other methods for automatic instrument segmentation thereby providing state-of-the-art results for these problems.
The source code for our solution is made publicly available.
% The source code for our solution is made publicly available at \url{https://github.com/ternaus/robot-surgery-segmentation}
\end{abstract}

\begin{keywords}
Medical imaging, Robot-assisted surgery, Computer vision, Image segmentation, Deep learning
\end{keywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \blindtext\todo{Refine me}

Research in robotics promises to revolutionize surgery towards safer, more consistent and minimally invasive intervention \cite{burgner2015continuum, munzer2018content}. New developments continues on the way to robot-assisted systems and moving toward a future with fully autonomous robotic surgeons. Thus far, the most widespread surgical system is the da Vinci robot, which has already proved its favor via remote controlled laparoscopic surgery in gynecology, urology, and general surgery \cite{burgner2015continuum}.

%Despite its huge potential for remote control and automation, its current clinical use is limited to operation by a human surgeon in the same room. Interestingly, robotic surgery breakthrough was made recently when a bot stitched up a pig's small intestines using its own vision, tools, and intelligence to carry out the procedure. Moreover, the Smart Tissue Autonomous Robot (STAR) did a better job on the operation than human surgeons who were given the same task. It is reasonable to suggest that in the nearest future we can anticipate a critical moment when robots are able to plan and perform entire operations without the input of human surgeons. 

Information in a surgical console of a robot-assisted surgical system includes valuable details for intra-operative guidance that can help the decision making process. This information is usually represented as 2D images or videos that contain surgical instruments and patient tissues. Understanding these data is a complex problem that involves the tracking and pose estimation for surgical instruments in the vicinity of surgical scenes. A critical component of this process is semantic segmentation of the instruments in the surgical console. Semantic segmentation of robotic instruments is a difficult task by the virtue of light changes such as shadows and specular reflections, visual occlusions such as blood and camera lens fogging, and due to the complex and dynamic nature of background tissues. Segmentation masks can be used to provide a reliable input to instrument tracking systems. Therefore, there is a compelling need for the development of accurate and robust computer vision methods for semantic segmentation of surgical instruments from operational images and video. 

There is a number of vision-based methods developed for the robotic instrument detection and tracking \cite{munzer2018content}. Instrument-background segmentation can be treated as a binary or instance segmentation problem for which classical machine learning algorithms have been applied using color and/or texture features  \cite{speidel2006tracking, doignon2007segmentation}. Later applications addressed this problem as semantic segmentation, aiming to distinguish between different instruments or their parts \cite{pezzementi2009articulated, bouget2015detecting}.

Recently, deep learning-based approaches demonstrated performance improvements over conventional machine learning methods for many problems in biomedicine \cite{ching2017opportunities, kalinin2018deep}. In the domain of medical imaging, convolutional neural networks (CNN) have been successfully used, for example, for breast cancer histology image analysis \cite{rakhlin2018deep}, bone disease prediction \cite{tiulpin2018automatic} and age assessment \cite{iglovikov2017pediatric}, and other problems \cite{ching2017opportunities}. Previous deep learning-based applications to robotic instrument segmentation have demonstrated competitive performance in binary segmentation \cite{garcia2017toolnet, attia2017surgical} and promising results in multi-class segmentation \cite{pakhomov2017deep}.
%Therefore, it is still on the way to provide utility for qualitative instrument tracking and their pose estimation.

In this paper, we present a deep learning-based solution for robotic instrument semantic segmentation that achieves state-of-the-art results in both binary and multi-class setting. We used this method to produce a submission to the MICCAI 2017 Endoscopic Vision SubChallenge: Robotic Instrument Segmentation \cite{miccai2017} to achieve one of the top results. Here we describe the details of the solution based on a modification of the U-Net model \cite{ronneberger2015u, iglovikov2017satellite}. Moreover, we provide further improvements over this solution utilizing recent deep architectures: TernausNet \cite{iglovikov2018ternausnet} and a modified LinkNet \cite{chaurasia2017linknet}.

\begin{figure}[!t]
\centering
% \includegraphics[width=8cm]{./figures/instruments_4classes.png} %width=\linewidth
\includegraphics[width=\linewidth]{./figures/instruments_4classes_hor.png}
\caption{A snapshot from a robotic surgical video that contains robotic instruments and patient tissues: (1) original video frame; (2) binary segmentation of robotic instruments shown in blue and tissue that serves as a background; (3) multi-class segmentation of robotic instruments where each class corresponds to a different part of the robotic instrument (3 classes: rigid shaft, articulated wrist and claspers); and (4) multi-class segmentation of robotic instruments where each class corresponds to a different robotic instrument (7 classes).}
\label{fig::classes}
\end{figure}

\section{Methods}
\subsection{Dataset}
The training dataset consists of $8\times225$-frame sequences of high resolution stereo camera images acquired from a da Vinci Xi surgical system during several different porcine procedures, see \cite{miccai2017}. Training sequences are provided with 2 Hz frame rate to avoid redundancy. Every video sequence consists of two stereo channels taken from left and right cameras and has a $1920\times1080$ pixel resolution in RGB format. To remove black canvas and extract original $1280\times1024$ camera images from the frames, an image has to be cropped starting from the pixel at the $(320, 28)$ position. Ground truth labels are provided for left frames only, therefore only left channel images are used for training. The articulated parts of the robotic surgical instruments, such as a rigid shaft, an articulated wrist and claspers have been hand labelled in each frame. Ground truth labels are encoded with numerical values $(10, 20, 30, 40, 0)$ and assigned to each part of an instrument or background. Furthermore, there are instrument type labels that categorize instruments in following categories: left/right prograsp forceps, monopolar curved scissors, large needle driver, and a miscellaneous category for any other surgical instruments.

The test dataset consists of $8\times75$-frame sequences containing footage sampled immediately after each training sequence and 2 full 300-frame sequences, sampled at the same rate as the training set. Under the terms of the challenge, participants should exclude the corresponding training set when evaluating on one of the 75-frame sequences.


%\begin{figure*}[!ht]
%% \includegraphics[width=\textwidth,height=6cm]{./figures/ternausnet16.png}
%\includegraphics[width=\textwidth]{./figures/ternausnet16.png}
%\caption{Encoder-decoder neural network architecture also known as U-Net \cite{ronneberger2015u} with VGG16 %\cite{simonyan2014vgg} achitecture (without fully connected layers) as an encoder. Each blue rectangular block represents %a multi-channel features map passing through a series of transformations. The height of the rod shows a relative map size %(in pixels), while their widths are proportional to the number of channels (the number is explicitly subscribed to the %corresponding rod). The number of channels increases stage by stage on the left part while decrease stage by stage on the %right decoding part. The arrows on top show transfer of information from each encoding layer and concatenating it to a %corresponding decoding layer.}
%\label{fig::unetvgg16}
%\end{figure*}

\subsection{Network architectures}
In this work we evaluate 4 different deep architectures for segmentation: U-Net \cite{ronneberger2015u, iglovikov2017satellite}, 2 modifications of TernausNet \cite{iglovikov2018ternausnet}, and a modification of LinkNet \cite{chaurasia2017linknet}.

In general, a U-Net-like architecture consists of a contracting path to capture context and of a symmetrically expanding path that enables precise localization (for example, see Fig. \ref{fig::linknet34}). The contracting path follows the typical architecture of a convolutional network with alternating convolution and pooling operations and progressively downsamples feature maps, increasing the number of feature maps per layer at the same time. Every step in the expansive path consists of an upsampling of the feature map followed by a convolution. Hence, the expansive branch increases the resolution of the output. In order to localize, upsampled features, the expansive path combines them with high-resolution features from the contracting path via skip-connections \cite{ronneberger2015u}. The output of the model is a pixel-by-pixel mask that shows the class of each pixel. We use slightly modified version of the original U-Net model that previously proved itself very useful for segmentation problems with limited amounts of data, for example, see \cite{iglovikov2017satellite, iglovikov2017pediatric}. Our submission to the MICCAI 2017 Endoscopic Vision SubChallenge: Robotic Instrument Segmentation \cite{miccai2017} was produced using this architecture.

\begin{figure*}[!b]
\includegraphics[width=\linewidth]{./figures/ternausnet16_sm.png}
\includegraphics[width=\textwidth]{./figures/break.png}
\includegraphics[width=\linewidth]{./figures/LinkNet34_sm.png}
\caption{These segmentation networks are based on encoder-decoder network of U-Net family. TernausNet uses pre-trained VGG16 network as an encoder, while LinkNet-34 uses pre-trained ResNet34. Each box corresponds to a multi-channel feature map. The number of channels is pointed below the box. The height of the box represents a feature map resolution. The blue arrows denote skip-connections where information is transmitted from the encoder to the decoder.}
\label{fig::linknet34}
\end{figure*}

%U-Net is capable of learning from a relatively small training set. In most cases, datasets for image segmentation consist of at most thousands of images, since manual preparation of the masks is a very costly procedure. Typically U-Net is trained from scratch starting with randomly initialized weights. It is well known that training network without over-fitting the dataset should be relatively large, millions of images. Networks that are trained on the ImageNet \cite{russakovsky2014imagnet} dataset are widely used as a source of the initialization for network weights in other tasks. In this way, the learning procedure can be done for non-pre-trained several layers of the network (sometimes only for the last layer) to take into account features of the date set.

As an improvement over U-Net, we use similar networks with pre-trained encoders. TernausNet \cite{iglovikov2018ternausnet} is a U-Net-like architecture that uses relatively simple pre-trained VGG11 or VGG16 \cite{simonyan2014vgg} networks as an encoder (see Fig. \ref{fig::linknet34}). VGG11 consists of seven convolutional layers, each followed by a ReLU activation function, and five max polling operations, each reducing feature map by $2$. All convolutional layers have $3\times3$ kernels. TernausNet16 has a similar structure and uses VGG16 network as an encoder (see Fig. \ref{fig::linknet34}).

In contrast, LinkNet \cite{chaurasia2017linknet} model uses an encoder based on a ResNet-type architecture \cite{he2016resnet}. In this work, we use pre-trained ResNet34, see Fig. \ref{fig::linknet34}. The encoder starts with the initial block that performs convolution with a kernel of size $7\times7$ and stride $2$. This block is followed by max-pooling with stride $2$. The later portion of the network consists of repetitive residual blocks. In every residual block, the first convolution operation is implemented with stride $2$ to provide downsampling, while the rest convolution operations use stride $1$. In addition, the decoder of the network consists of several decoder blocks that are connected with the corresponding encoder block. In this case, the transmitted block from the encoder is added to the corresponding decoder block. Each decoder block includes $1\times1$ convolution operation that reduces the number of filters by $4$, followed by batch normalization and transposed convolution to upsample the feature map.

\subsection{Training}
We use Jaccard index (Intersection Over Union) as the evaluation metric. It can be interpreted as a similarity measure between a finite number of sets. For two sets $A$ and $B$, it can be defined as following:
\begin{equation}
\label{jaccard_iou}
    J(A, B) = \frac{|A\cap B|}{|A\cup B|} = \frac{|A\cap B|}{|A|+|B|-|A\cap B|}
\end{equation}
Since an image consists of pixels, the last expression can be adapted for discrete objects in the following way:
\begin{equation}
\label{dicrjacc}
J=\frac{1}{n}\sum\limits_{i=1}^n\left(\frac{y_i\hat{y}_i}{y_{i}+\hat{y}_i-y_i\hat{y}_i}\right)
\end{equation}
where $y_i$ and $\hat{y}_i$ are a binary value (label) and a predicted probability for the pixel $i$, correspondingly.

Since image segmentation task can also be considered as a pixel classification problem, we additionally use common classification loss functions, denoted as $H$. For a binary segmentation problem $H$ is a binary cross entropy, while for a multi-class segmentation problem $H$ is a categorical cross entropy.

The final expression for the generalized loss function is obtained by combining (\ref{dicrjacc}) and $H$ as following:
\begin{equation}
\label{free_en}
L=H-\log J
\end{equation}
By minimizing this loss function, we simultaneously maximize probabilities for right pixels to be predicted and maximize the intersection $J$ between masks and corresponding predictions \cite{iglovikov2017satellite}.

As an output of a model, we obtain an image, in which each pixel value corresponds to a probability of belonging to the area of interest or a class. The size of the output image matches the input image size. For binary segmentation, we use $0.3$ as a threshold value (chosen using validation dataset) to binarize pixel probabilities. All pixel values below the specified threshold are set to $0$, while all values above the threshold are set to $255$ to produce final prediction mask. For multi-class segmentation we use similar procedure, but we set different integer numbers for each class as was noted above.
%and it is pretty universal for our generalized loss function and many different image datasets. For different loss function this number is different and should be found independently.
%Then, multiplying every pixel by $255$, we produce the final black-and-white predicted mask.

\begin{figure}[!t]
\centering
%\includegraphics[width=12cm]{./figures/grid_legend.png} %width=\linewidth
\includegraphics[width=\linewidth]{./figures/grid_legend_filled.png}
\caption{Qualitative comparison between several  neural network architectures implemented for a binary and multi-class segmentation.}
\label{fig::grid_legend}
\end{figure}

\section{Results}
The qualitative comparison of our models both for a binary and multi-class segmentation is presented in Fig. \ref{fig::grid_legend} and Table \ref{table:segmentation}. For the binary segmentation task the best results is achieved by TernausNet-16 providing $IoU = 0.836$ and $Dice = 0.901$. These values are the best reported in the literature up to now \cite{pakhomov2017deep, garcia2017toolnet}. Next, we consider multi-class segmentation of different parts of instruments. As before, the best results reveals TernausNet-16 providing $IoU = 0.655$ and $Dice = 0.760$. For the multi-class class instrument segmentation task the results look less optimistic. In this case the best model is TernausNet-11 that achieves $IoU = 0.346$ and $Dice= 0.459$ for segmentation on 7 classes. Lower performance can be explained by the relatively small dataset size. There are 7 classes and several classes appear just few times in the training dataset.
% Despite that we showed the best performance in the competition in this sub-category too
The results suggests that this results can be  improved by increasing the dataset size for the corresponding problem.
% In such a way our models can provide a comparable performance with the results obtained for the binary instrument segmentation.

\begin{table}[t!]
\caption{Segmentation results per task. Intersection over Union (IoU) and Dice coefficient (Dice) are in $\%$ and inference time (Time) is in $ms$.}
\label{table:segmentation}
\centering   
\begin{tabular}{|c | c | c | c | c | c | c | c | c | c|}
\hline\noalign{\smallskip}
&
\multicolumn{3}{c}{Binary segmentation} &
\multicolumn{3}{|c|}{Parts segmentation} &
\multicolumn{3}{c|}{Instrument segmentation} \\
\hline
% \hline\noalign{\smallskip}
Model & IOU & Dice & Time & IOU & Dice & Time & IOU & Dice & Time \\
\hline
U-Net & 75.44 & 84.37 & 93 & 48.41 & 60.75 & 106 & 15.80 & 23.59 & \textbf{122}\\
TernausNet-11 & 81.14 & 88.07 & 142 & 62.23 & 74.25 & 157 & \textbf{34.61} & \textbf{45.86} & 173\\
TernausNet-16 & \textbf{83.60} & \textbf{90.01} & 184 & \textbf{65.50} & \textbf{75.97} & 202 & 33.78 & 44.95 & 275\\
LinkNet-34 & 82.36 & 88.87 & \textbf{88} & 34.55 & 41.26 & \textbf{97} & 22.47 & 24.71 & 177\\

%  Model & Mean IOU [\%] \,& \, Mean Dice [\%]\,& \,Inference[ms]\, \\
% \multicolumn{4}{c}{Binary Segmentation} \\
% \hline
% \noalign{\smallskip}
% U-Net & 75.44 & 84.37 & 93\\
% TernausNet-11 & 81.14 & 88.07 & 142\\
% TernausNet-16 & 83.60 & 90.01 & 184\\
% LinNet-34 & 82.36 & 88.87 & 88\\
% \hline
% \multicolumn{4}{c}{Parts Segmentation}\\
% \hline
% U-Net & 48.41 & 60.75 & 106\\
% TernausNet-11 & 62.23 & 74.25 & 157\\
% TernausNet-16 & 65.50 & 75.97 & 202\\
% LinkNet-34 & 34.55 & 41.26 & 97\\
% \hline
% \multicolumn{4}{c}{Instrument Segmentation}\\
% \hline
% U-Net & 15.80 & 23.59 & 122\\
% TernausNet-11 & 34.61 & 45.86 & 173\\
% TernausNet-16 & 33.78 & 44.95 & 275\\
% LinkNet-34 & 22.47 & 24.71 & 177\\
\hline
\end{tabular}
\end{table}

When compared by the inference time, LinkNet-34 is the fastest model due to the light encoder. In the case of a binary segmentation task this network takes around 90 $ms$ for $1280\times1024$ pixel image and more than twice as fast as TernausNet. The inference time was measured using one NVIDIA GTX 1080Ti GPU. A detailed comparison for the binary and multi-class tasks can be found in our GitHub repository at \url{https://github.com/ternaus/robot-surgery-segmentation}.

Suggested approach demonstrated state-of-the-art level of performance when compared to other deep learning-based solutions within to the MICCAI 2017 Endoscopic Vision SubChallenge: Robotic Instrument Segmentation \cite{miccai2017}.

\section{Conclusions}
In this paper, we describe our solution robotic instrument segmentation and demonstrate comparative analysis of various deep network models. Our approach is originally based on U-Net network architecture that we improved using state-of-the-art semantic segmentation neural networks known as LinkNet and TernausNet. Our results shows competitive performance for a binary as well as for multi-class robotic instrument segmentation. All of these networks make up end-to-end pipeline, performing efficient analysis on the whole image resolution. We believe that our methods can lay a good foundation for similar problems of real-time surgical instrument position detection. This, in turn, can be used for the tracking and pose estimation in the vicinity of surgical scenes.    

% use section* for acknowledgement
% \subsubsection*{Acknowledgments}
%  Authors thank Evgeny Nizhibitsky, Alexander Buslaev, and the rest of Open Data Science community (\url{ods.ai}) for useful suggestions and other help aiming to the development of this work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\bibliographystyle{ieeetr}
\bibliographystyle{splncs03}
\bibliography{paper}

% All links were last followed on October 5, 2014.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
