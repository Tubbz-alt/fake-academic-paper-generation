\documentclass{article} % For LaTeX2e
\usepackage{iclr2019_conference,times}
\linespread{0.95}


% Optional math commands from https://github.com/goodfeli/dlbook_notation.
%%%%% NEW MATH DEFINITIONS %%%%%\usepackage{amsmath,amsfonts,bm}% Mark sections of captions for referring to divisions of figures\newcommand{\figleft}{{\em (Left)}}\newcommand{\figcenter}{{\em (Center)}}\newcommand{\figright}{{\em (Right)}}\newcommand{\figtop}{{\em (Top)}}\newcommand{\figbottom}{{\em (Bottom)}}\newcommand{\captiona}{{\em (a)}}\newcommand{\captionb}{{\em (b)}}\newcommand{\captionc}{{\em (c)}}\newcommand{\captiond}{{\em (d)}}% Highlight a newly defined term\newcommand{\newterm}[1]{{\bf #1}}% Figure reference, lower-case.\def\figref#1{figure~\ref{#1}}% Figure reference, capital. For start of sentence\def\Figref#1{Figure~\ref{#1}}\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}% Section reference, lower-case.\def\secref#1{section~\ref{#1}}% Section reference, capital.\def\Secref#1{Section~\ref{#1}}% Reference to two sections.\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}% Reference to three sections.\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}% Reference to an equation, lower-case.\def\eqref#1{equation~\ref{#1}}% Reference to an equation, upper case\def\Eqref#1{Equation~\ref{#1}}% A raw reference to an equation---avoid using if possible\def\plaineqref#1{\ref{#1}}% Reference to a chapter, lower-case.\def\chapref#1{chapter~\ref{#1}}% Reference to an equation, upper case.\def\Chapref#1{Chapter~\ref{#1}}% Reference to a range of chapters\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}% Reference to an algorithm, lower-case.\def\algref#1{algorithm~\ref{#1}}% Reference to an algorithm, upper case.\def\Algref#1{Algorithm~\ref{#1}}\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}% Reference to a part, lower case\def\partref#1{part~\ref{#1}}% Reference to a part, upper case\def\Partref#1{Part~\ref{#1}}\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}\def\ceil#1{\lceil #1 \rceil}\def\floor#1{\lfloor #1 \rfloor}\def\1{\bm{1}}\newcommand{\train}{\mathcal{D}}\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}\def\eps{{\epsilon}}% Random variables\def\reta{{\textnormal{$\eta$}}}\def\ra{{\textnormal{a}}}\def\rb{{\textnormal{b}}}\def\rc{{\textnormal{c}}}\def\rd{{\textnormal{d}}}\def\re{{\textnormal{e}}}\def\rf{{\textnormal{f}}}\def\rg{{\textnormal{g}}}\def\rh{{\textnormal{h}}}\def\ri{{\textnormal{i}}}\def\rj{{\textnormal{j}}}\def\rk{{\textnormal{k}}}\def\rl{{\textnormal{l}}}% rm is already a command, just don't name any random variables m\def\rn{{\textnormal{n}}}\def\ro{{\textnormal{o}}}\def\rp{{\textnormal{p}}}\def\rq{{\textnormal{q}}}\def\rr{{\textnormal{r}}}\def\rs{{\textnormal{s}}}\def\rt{{\textnormal{t}}}\def\ru{{\textnormal{u}}}\def\rv{{\textnormal{v}}}\def\rw{{\textnormal{w}}}\def\rx{{\textnormal{x}}}\def\ry{{\textnormal{y}}}\def\rz{{\textnormal{z}}}% Random vectors\def\rvepsilon{{\mathbf{\epsilon}}}\def\rvtheta{{\mathbf{\theta}}}\def\rva{{\mathbf{a}}}\def\rvb{{\mathbf{b}}}\def\rvc{{\mathbf{c}}}\def\rvd{{\mathbf{d}}}\def\rve{{\mathbf{e}}}\def\rvf{{\mathbf{f}}}\def\rvg{{\mathbf{g}}}\def\rvh{{\mathbf{h}}}\def\rvu{{\mathbf{i}}}\def\rvj{{\mathbf{j}}}\def\rvk{{\mathbf{k}}}\def\rvl{{\mathbf{l}}}\def\rvm{{\mathbf{m}}}\def\rvn{{\mathbf{n}}}\def\rvo{{\mathbf{o}}}\def\rvp{{\mathbf{p}}}\def\rvq{{\mathbf{q}}}\def\rvr{{\mathbf{r}}}\def\rvs{{\mathbf{s}}}\def\rvt{{\mathbf{t}}}\def\rvu{{\mathbf{u}}}\def\rvv{{\mathbf{v}}}\def\rvw{{\mathbf{w}}}\def\rvx{{\mathbf{x}}}\def\rvy{{\mathbf{y}}}\def\rvz{{\mathbf{z}}}% Elements of random vectors\def\erva{{\textnormal{a}}}\def\ervb{{\textnormal{b}}}\def\ervc{{\textnormal{c}}}\def\ervd{{\textnormal{d}}}\def\erve{{\textnormal{e}}}\def\ervf{{\textnormal{f}}}\def\ervg{{\textnormal{g}}}\def\ervh{{\textnormal{h}}}\def\ervi{{\textnormal{i}}}\def\ervj{{\textnormal{j}}}\def\ervk{{\textnormal{k}}}\def\ervl{{\textnormal{l}}}\def\ervm{{\textnormal{m}}}\def\ervn{{\textnormal{n}}}\def\ervo{{\textnormal{o}}}\def\ervp{{\textnormal{p}}}\def\ervq{{\textnormal{q}}}\def\ervr{{\textnormal{r}}}\def\ervs{{\textnormal{s}}}\def\ervt{{\textnormal{t}}}\def\ervu{{\textnormal{u}}}\def\ervv{{\textnormal{v}}}\def\ervw{{\textnormal{w}}}\def\ervx{{\textnormal{x}}}\def\ervy{{\textnormal{y}}}\def\ervz{{\textnormal{z}}}% Random matrices\def\rmA{{\mathbf{A}}}\def\rmB{{\mathbf{B}}}\def\rmC{{\mathbf{C}}}\def\rmD{{\mathbf{D}}}\def\rmE{{\mathbf{E}}}\def\rmF{{\mathbf{F}}}\def\rmG{{\mathbf{G}}}\def\rmH{{\mathbf{H}}}\def\rmI{{\mathbf{I}}}\def\rmJ{{\mathbf{J}}}\def\rmK{{\mathbf{K}}}\def\rmL{{\mathbf{L}}}\def\rmM{{\mathbf{M}}}\def\rmN{{\mathbf{N}}}\def\rmO{{\mathbf{O}}}\def\rmP{{\mathbf{P}}}\def\rmQ{{\mathbf{Q}}}\def\rmR{{\mathbf{R}}}\def\rmS{{\mathbf{S}}}\def\rmT{{\mathbf{T}}}\def\rmU{{\mathbf{U}}}\def\rmV{{\mathbf{V}}}\def\rmW{{\mathbf{W}}}\def\rmX{{\mathbf{X}}}\def\rmY{{\mathbf{Y}}}\def\rmZ{{\mathbf{Z}}}% Elements of random matrices\def\ermA{{\textnormal{A}}}\def\ermB{{\textnormal{B}}}\def\ermC{{\textnormal{C}}}\def\ermD{{\textnormal{D}}}\def\ermE{{\textnormal{E}}}\def\ermF{{\textnormal{F}}}\def\ermG{{\textnormal{G}}}\def\ermH{{\textnormal{H}}}\def\ermI{{\textnormal{I}}}\def\ermJ{{\textnormal{J}}}\def\ermK{{\textnormal{K}}}\def\ermL{{\textnormal{L}}}\def\ermM{{\textnormal{M}}}\def\ermN{{\textnormal{N}}}\def\ermO{{\textnormal{O}}}\def\ermP{{\textnormal{P}}}\def\ermQ{{\textnormal{Q}}}\def\ermR{{\textnormal{R}}}\def\ermS{{\textnormal{S}}}\def\ermT{{\textnormal{T}}}\def\ermU{{\textnormal{U}}}\def\ermV{{\textnormal{V}}}\def\ermW{{\textnormal{W}}}\def\ermX{{\textnormal{X}}}\def\ermY{{\textnormal{Y}}}\def\ermZ{{\textnormal{Z}}}% Vectors\def\vzero{{\bm{0}}}\def\vone{{\bm{1}}}\def\vmu{{\bm{\mu}}}\def\vtheta{{\bm{\theta}}}\def\va{{\bm{a}}}\def\vb{{\bm{b}}}\def\vc{{\bm{c}}}\def\vd{{\bm{d}}}\def\ve{{\bm{e}}}\def\vf{{\bm{f}}}\def\vg{{\bm{g}}}\def\vh{{\bm{h}}}\def\vi{{\bm{i}}}\def\vj{{\bm{j}}}\def\vk{{\bm{k}}}\def\vl{{\bm{l}}}\def\vm{{\bm{m}}}\def\vn{{\bm{n}}}\def\vo{{\bm{o}}}\def\vp{{\bm{p}}}\def\vq{{\bm{q}}}\def\vr{{\bm{r}}}\def\vs{{\bm{s}}}\def\vt{{\bm{t}}}\def\vu{{\bm{u}}}\def\vv{{\bm{v}}}\def\vw{{\bm{w}}}\def\vx{{\bm{x}}}\def\vy{{\bm{y}}}\def\vz{{\bm{z}}}\def\vphi{{\bm{\phi}}}\def\vvphi{{\bm{\varphi}}}% Elements of vectors\def\evalpha{{\alpha}}\def\evbeta{{\beta}}\def\evepsilon{{\epsilon}}\def\evlambda{{\lambda}}\def\evomega{{\omega}}\def\evmu{{\mu}}\def\evpsi{{\psi}}\def\evsigma{{\sigma}}\def\evtheta{{\theta}}\def\eva{{a}}\def\evb{{b}}\def\evc{{c}}\def\evd{{d}}\def\eve{{e}}\def\evf{{f}}\def\evg{{g}}\def\evh{{h}}\def\evi{{i}}\def\evj{{j}}\def\evk{{k}}\def\evl{{l}}\def\evm{{m}}\def\evn{{n}}\def\evo{{o}}\def\evp{{p}}\def\evq{{q}}\def\evr{{r}}\def\evs{{s}}\def\evt{{t}}\def\evu{{u}}\def\evv{{v}}\def\evw{{w}}\def\evx{{x}}\def\evy{{y}}\def\evz{{z}}% Matrix\def\mA{{\bm{A}}}\def\mB{{\bm{B}}}\def\mC{{\bm{C}}}\def\mD{{\bm{D}}}\def\mE{{\bm{E}}}\def\mF{{\bm{F}}}\def\mG{{\bm{G}}}\def\mH{{\bm{H}}}\def\mI{{\bm{I}}}\def\mJ{{\bm{J}}}\def\mK{{\bm{K}}}\def\mL{{\bm{L}}}\def\mM{{\bm{M}}}\def\mN{{\bm{N}}}\def\mO{{\bm{O}}}\def\mP{{\bm{P}}}\def\mQ{{\bm{Q}}}\def\mR{{\bm{R}}}\def\mS{{\bm{S}}}\def\mT{{\bm{T}}}\def\mU{{\bm{U}}}\def\mV{{\bm{V}}}\def\mW{{\bm{W}}}\def\mX{{\bm{X}}}\def\mY{{\bm{Y}}}\def\mZ{{\bm{Z}}}\def\mBeta{{\bm{\beta}}}\def\mPhi{{\bm{\Phi}}}\def\mLambda{{\bm{\Lambda}}}\def\mSigma{{\bm{\Sigma}}}% Tensor\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}\def\tA{{\tens{A}}}\def\tB{{\tens{B}}}\def\tC{{\tens{C}}}\def\tD{{\tens{D}}}\def\tE{{\tens{E}}}\def\tF{{\tens{F}}}\def\tG{{\tens{G}}}\def\tH{{\tens{H}}}\def\tI{{\tens{I}}}\def\tJ{{\tens{J}}}\def\tK{{\tens{K}}}\def\tL{{\tens{L}}}\def\tM{{\tens{M}}}\def\tN{{\tens{N}}}\def\tO{{\tens{O}}}\def\tP{{\tens{P}}}\def\tQ{{\tens{Q}}}\def\tR{{\tens{R}}}\def\tS{{\tens{S}}}\def\tT{{\tens{T}}}\def\tU{{\tens{U}}}\def\tV{{\tens{V}}}\def\tW{{\tens{W}}}\def\tX{{\tens{X}}}\def\tY{{\tens{Y}}}\def\tZ{{\tens{Z}}}% Graph\def\gA{{\mathcal{A}}}\def\gB{{\mathcal{B}}}\def\gC{{\mathcal{C}}}\def\gD{{\mathcal{D}}}\def\gE{{\mathcal{E}}}\def\gF{{\mathcal{F}}}\def\gG{{\mathcal{G}}}\def\gH{{\mathcal{H}}}\def\gI{{\mathcal{I}}}\def\gJ{{\mathcal{J}}}\def\gK{{\mathcal{K}}}\def\gL{{\mathcal{L}}}\def\gM{{\mathcal{M}}}\def\gN{{\mathcal{N}}}\def\gO{{\mathcal{O}}}\def\gP{{\mathcal{P}}}\def\gQ{{\mathcal{Q}}}\def\gR{{\mathcal{R}}}\def\gS{{\mathcal{S}}}\def\gT{{\mathcal{T}}}\def\gU{{\mathcal{U}}}\def\gV{{\mathcal{V}}}\def\gW{{\mathcal{W}}}\def\gX{{\mathcal{X}}}\def\gY{{\mathcal{Y}}}\def\gZ{{\mathcal{Z}}}% Sets\def\sA{{\mathbb{A}}}\def\sB{{\mathbb{B}}}\def\sC{{\mathbb{C}}}\def\sD{{\mathbb{D}}}% Don't use a set called E, because this would be the same as our symbol% for expectation.\def\sF{{\mathbb{F}}}\def\sG{{\mathbb{G}}}\def\sH{{\mathbb{H}}}\def\sI{{\mathbb{I}}}\def\sJ{{\mathbb{J}}}\def\sK{{\mathbb{K}}}\def\sL{{\mathbb{L}}}\def\sM{{\mathbb{M}}}\def\sN{{\mathbb{N}}}\def\sO{{\mathbb{O}}}\def\sP{{\mathbb{P}}}\def\sQ{{\mathbb{Q}}}\def\sR{{\mathbb{R}}}\def\sS{{\mathbb{S}}}\def\sT{{\mathbb{T}}}\def\sU{{\mathbb{U}}}\def\sV{{\mathbb{V}}}\def\sW{{\mathbb{W}}}\def\sX{{\mathbb{X}}}\def\sY{{\mathbb{Y}}}\def\sZ{{\mathbb{Z}}}% Entries of a matrix\def\emLambda{{\Lambda}}\def\emA{{A}}\def\emB{{B}}\def\emC{{C}}\def\emD{{D}}\def\emE{{E}}\def\emF{{F}}\def\emG{{G}}\def\emH{{H}}\def\emI{{I}}\def\emJ{{J}}\def\emK{{K}}\def\emL{{L}}\def\emM{{M}}\def\emN{{N}}\def\emO{{O}}\def\emP{{P}}\def\emQ{{Q}}\def\emR{{R}}\def\emS{{S}}\def\emT{{T}}\def\emU{{U}}\def\emV{{V}}\def\emW{{W}}\def\emX{{X}}\def\emY{{Y}}\def\emZ{{Z}}\def\emSigma{{\Sigma}}% entries of a tensor% Same font as tensor, without \bm wrapper\newcommand{\etens}[1]{\mathsfit{#1}}\def\etLambda{{\etens{\Lambda}}}\def\etA{{\etens{A}}}\def\etB{{\etens{B}}}\def\etC{{\etens{C}}}\def\etD{{\etens{D}}}\def\etE{{\etens{E}}}\def\etF{{\etens{F}}}\def\etG{{\etens{G}}}\def\etH{{\etens{H}}}\def\etI{{\etens{I}}}\def\etJ{{\etens{J}}}\def\etK{{\etens{K}}}\def\etL{{\etens{L}}}\def\etM{{\etens{M}}}\def\etN{{\etens{N}}}\def\etO{{\etens{O}}}\def\etP{{\etens{P}}}\def\etQ{{\etens{Q}}}\def\etR{{\etens{R}}}\def\etS{{\etens{S}}}\def\etT{{\etens{T}}}\def\etU{{\etens{U}}}\def\etV{{\etens{V}}}\def\etW{{\etens{W}}}\def\etX{{\etens{X}}}\def\etY{{\etens{Y}}}\def\etZ{{\etens{Z}}}% The true underlying data generating distribution\newcommand{\pdata}{p_{\rm{data}}}% The empirical distribution defined by the training set\newcommand{\ptrain}{\hat{p}_{\rm{data}}}\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}% The model distribution\newcommand{\pmodel}{p_{\rm{model}}}\newcommand{\Pmodel}{P_{\rm{model}}}\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}% Stochastic autoencoder distributions\newcommand{\pencode}{p_{\rm{encoder}}}\newcommand{\pdecode}{p_{\rm{decoder}}}\newcommand{\precons}{p_{\rm{reconstruct}}}\newcommand{\laplace}{\mathrm{Laplace}}% Laplace distribution\newcommand{\E}{\mathbb{E}}\newcommand{\Ls}{\mathcal{L}}\newcommand{\R}{\mathbb{R}}\newcommand{\emp}{\tilde{p}}\newcommand{\lr}{\alpha}\newcommand{\reg}{\lambda}\newcommand{\rect}{\mathrm{rectifier}}\newcommand{\softmax}{\mathrm{softmax}}\newcommand{\sigmoid}{\sigma}\newcommand{\softplus}{\zeta}\newcommand{\KL}{D_{\mathrm{KL}}}\newcommand{\Var}{\mathrm{Var}}\newcommand{\standarderror}{\mathrm{SE}}\newcommand{\Cov}{\mathrm{Cov}}% Wolfram Mathworld says $L^2$ is for function spaces and $\ell^2$ is for vectors% But then they seem to use $L^2$ for vectors throughout the site, and so does% wikipedia.\newcommand{\normlzero}{L^0}\newcommand{\normlone}{L^1}\newcommand{\normltwo}{L^2}\newcommand{\normlp}{L^p}\newcommand{\normmax}{L^\infty}\newcommand{\parents}{Pa}% See usage in notation.tex. Chosen to match Daphne's book.\DeclareMathOperator*{\argmax}{arg\,max}\DeclareMathOperator*{\argmin}{arg\,min}\DeclareMathOperator{\sign}{sign}\DeclareMathOperator{\Tr}{Tr}\let\ab\allowbreak

\usepackage{hyperref}
\usepackage{url}
%\usepackage[dvips]{graphicx} 
\usepackage{graphicx} 
%\usepackage[demo]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\usepackage{wrapfig}


\title{Graph HyperNetworks for \\ Neural Architecture Search}

\author{Chris Zhang$^{1,2}$, Mengye Ren$^{1,3}$ \& Raquel Urtasun$^{1,3}$\\
$^{1}$Uber Advanced Technologies Group, $^{2}$University of Waterloo, $^{3}$University of Toronto\\
\texttt{cjzhang@edu.uwaterloo.ca, \{mren3,urtasun\}@uber.com}\\
}
% \author{Chris Zhang$^{1,2}$\\
% $^{1}$Uber Advanced Technologies Group\\
% $^{2}$University of Waterloo
% \And
% Mengye Ren$^{1,3}$ \& Raquel Urtasun$^{1,3}$\\
% $^{3}$University of Toronto\\
% \texttt{\{chrisz,mren3,urtasun\}@uber.com}\\
% }

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\note}[1]{\textcolor{red}{#1}}
\newcommand{\raquel}[1]{\textcolor{red}{[RAQUEL: #1]}}
\newcommand{\mengye}[1]{\textcolor{blue}{[MENGYE: #1]}}
\newcommand{\chris}[1]{\textcolor{brown}{[CHRIS: #1]}}

\newif\ifarxiv
\arxivfalse
\def\arxiv{\arxivtrue}

%\arxiv
\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}
\maketitle

\hyphenpenalty=1000
% !TEX root = top.tex\begin{abstract}
Neural architecture search (NAS) automatically finds the best task-specific neural network topology,
outperforming many manual architecture designs. However, it can be prohibitively expensive as the
search requires training thousands of different networks, while each can last for hours. In this
work, we propose the Graph HyperNetwork (GHN) to amortize the search cost: given an architecture, it
directly generates the weights by running inference on a graph neural network. GHNs model the
topology of an architecture and therefore can predict network performance more accurately than
regular hypernetworks and premature early stopping. To perform NAS, we randomly sample architectures
and use the validation accuracy of networks with GHN generated weights as the surrogate search
signal. GHNs are fast -- they can search nearly 10$\times$ faster than other random search methods
on CIFAR-10 and ImageNet. GHNs can be further extended to the anytime prediction setting, where they
have found networks with better speed-accuracy tradeoff than the state-of-the-art manual designs.
\end{abstract}
% !TEX root = top.tex\section{Introduction}
The success of deep learning marks the transition from manual feature engineering to automated
feature learning. However, designing effective neural network architectures requires expert domain
knowledge and repetitive trial and error. Recently, there has been a surge of interest in {\it
neural architecture search} (NAS), where neural network architectures are automatically optimized.

One approach for architecture search is to consider it as a nested optimization problem, where the
inner loop finds the optimal parameters $w^*$ for a given architecture $a$ w.r.t. the training loss
$\gL_{train}$, and the outer loop searches the optimal architecture w.r.t. a validation loss
$\gL_{val}$:
\begin{equation}
\label{eq:inner}
w^*(a) = \argmin_w \gL_{train}(w, a)
\end{equation}\begin{equation}
\label{eq:outer}
a^* = \argmin_a \gL_{val}(w^*(a),a)
\end{equation}
Traditional NAS is expensive since solving the  inner optimization in Eq.~\ref{eq:inner}  requires a
lengthy optimization process (e.g. stochastic gradient descent (SGD)). Instead,  we propose to learn
a parametric function approximation referred to as a hypernetwork
\citep{ha2016hypernetworks,brock2017smash}, which attempts to \textit{generate} the network weights
directly.  Learning a hypernetwork is an amortization of the cost of solving Eq.~\ref{eq:inner}
repeatedly for multiple architectures. A trained hypernetwork is well correlated with SGD and can
act as a much faster substitute.

Yet, the architecture of the hypernet itself is still to be determined. Existing methods have
explored a variety of tactics to represent architectures, such as an ingenious 3D tensor encoding
scheme \citep{brock2017smash}, or a string serialization processed by an LSTM
\citep{zoph2016neural,zoph2017learning,pham2018efficient}. In this work, we advocate for a
\textit{computation graph} representation as it allows for the topology of an architecture to be
explicitly modeled. Furthermore, it is intuitive to understand and can be easily extensible to
various graph sizes.

To this end, in this paper we propose the \textit{Graph HyperNetwork} (GHN), which can aggregate
graph level information by directly learning on the graph representation. Using a hypernetwork to
guide architecture search, our approach requires significantly less computation when compared to
state-of-the-art methods. The computation graph representation allows GHNs to be the first
hypernetwork to generate all the weights of arbitrary CNNs rather than a subset (e.g.
\cite{brock2017smash}), achieving stronger correlation and thus making the search more efficient and
accurate.

While the validation accuracy is often the primary goal in architecture search, networks must also
be resource aware in real-world applications. Towards this goal, we exploit the flexibility of the
GHN by extending it to the problem of \textit{anytime prediction}. Models capable of anytime
prediction progressively update their predictions, allowing for a prediction at any time. This is
desirable in settings such as real-time systems, where the computational budget available for each
test case may vary greatly and cannot be known ahead of time. Although anytime models have
non-trivial differences to classical models, we show the GHN is amenable to these changes.
 
We summarize our main contributions of this work:
\vspace{-0.1cm}\begin{enumerate}
\setlength{\itemsep}{1pt}
\item We propose Graph HyperNetwork that predicts the parameters of unseen neural networks by
directly operating on their computational graph representations.
\item Our approach achieves highly competitive results with state-of-the-art NAS methods on both
CIFAR-10 and ImageNet-mobile and is 10$\times$ faster than other random search methods.
\item We demonstrate that our approach can be generalized and applied in the domain of
anytime-prediction, previously unexplored by NAS programs, outperforming the existing manually
designed state-of-the-art models.
\end{enumerate}
% !TEX root = top.tex\section{Related Work}
Various search methods such as reinforcement learning~\citep{zoph2016neural,
baker2016designing,zoph2017learning}, evolutionary
methods~\citep{real2017large,miikkulainen2017evolving,xie2017genetic,liu2017hierarchical,real2018regularized}
and gradient-based methods~\citep{liu2018darts,luo2018neural} have been proposed to address the
outer optimization (Eq.~\ref{eq:outer}) of NAS, where an agent learns to sample architectures that
are more likely to achieve higher accuracy. Different from these methods, this paper places its
focus on the inner-loop: inferring the parameters of a given network (Eq.~\ref{eq:inner}). Following
\cite{brock2017smash,bender2018understanding}, we opt for a simple random search algorithm to
complete the outer loop.

While initial NAS methods simply train candidate architectures for a brief period with SGD to obtain
the search signal, recent approaches have proposed alternatives in the interest of computational
cost. \cite{baker2017accelerating} propose directly predicting performance from the learning curve,
and \cite{deng2017peephole} propose to predict performance directly from the architecture without
learning curve information. However, training a performance predictor requires a ground truth, thus
the expensive process of computing the inner optimization is not avoided.
\cite{pham2018efficient,bender2018understanding,liu2018darts} use parameter sharing, where a
``one-shot'' model containing all possible architectures in the search space is trained. Individual
architectures are sampled by deactivating some nodes or edges in the one-shot model. In this case,
predicting $w^*(a)$ can be seen as using a selection function from the set of parameters in the
one-shot model.

Prior work has shown the feasibility of predicting parameters in a network with a function
approximator \citep{denil2013predictparams}. \cite{Schmidhuber92Learning,schmidhuber1993self}
proposed ``fast-weights'', where one network produces weight changes for another. HyperNetworks
\citep{ha2016hypernetworks} generate the weights of another network and show strong results in
large-scale language modeling and image classification experiments. SMASH~\citep{brock2017smash}
applied HyperNetworks to perform NAS, where an architecture is encoded as a 3D tensor using a memory
channel scheme. In contrast, we encode a network as a computation graph and use a graph neural
network. While SMASH predicts a subset of the weights, our graph model is able to predict
\textit{all} the free weights.

While earlier NAS methods focused on standard image classification and language modeling, recent
literature has extended NAS to search for architectures that are computationally efficient
~\citep{tan2018mnasnet,dong2018dpp,hsu2018monas,elsken2018multi,zhou2018resource}. In this work, we
applied our GHN based search program on the task of anytime prediction, where we not only optimize
for the final speed but the entire speed-accuracy trade-off curve.

% !TEX root = top.tex\section{Background}
We review the two major building blocks of our model: graph neural networks and hypernetworks.

\paragraph{Graph Neural Network:}
A graph neural network \citep{ScarselliGTHM09,li2015gated,KipfW16} is a collection of nodes and
edges $(\gV, \gE)$, where each node is a recurrent neural network (RNN) that individually sends and
receives messages along the edges, spanning over the horizon of message passing. Each node $v$
stores an internal node embedding vector $\vh_v^{(t)} \in \mathbb{R}^D$, and is updated recurrently:
\begin{equation}
\label{eq:gnn_prop}
\vh_v^{(t+1)} = 
\begin{cases}
U \left(\vh_v^{(t)}, \vm_v^{(t)} \right) \ \ & \text{if node $v$ is active},\\
\vh_v^{(t)} \ \ & \text{otherwise},
\end{cases}
\end{equation}
where $U$ is a recurrent cell function and $\vm_v^{(t)}$ is the message received by $v$ at time step
$t$:
\begin{equation}
\vm_v^{(t)}=\sum_{u\in N_{in}(v)} M \left(\vh_u^{(t)} \right),
\end{equation}
with $M$ the message function and $N_{in}(v)$ the set of neighbors with incoming edges pointing
towards $v$. $U$ is often modeled with a long short-term memory (LSTM) unit \citep{hochreiter97lstm}
or gated recurrent unit (GRU) \citep{cho14gru}, and $M$ with an MLP. Given a graph $\gA$, we define
the GNN operator $G_\gA$ to be a mapping from a set of initial node embeddings $\{\vh_v^{(0)}\}$ to
a set of different node embeddings $\{\vh_v^{(t)}\}$, parameterized by some learnable parameters
$\vphi$:
\begin{equation}
\left\{\vh_v^{(t)} | v \in \gV\right\} = 
G_\gA^{(t)} \left(\left\{\vh_v^{(0)} | v \in \gV \right\}; \vphi \right).
\end{equation}
Throughout propagation the node embeddings $\vh_v^{(t)}$ continuously aggregate graph level
information, which  can be used for tasks such as node prediction and   graph prediction by further
aggregation. Similar to RNNs, GNNs are typically learned using backpropagation through time (BPTT)
\citep{bptt}.

\paragraph{Hypernetwork:}
A hypernetwork \citep{ha2016hypernetworks} is a neural network that generates the parameters of
another network. For a typical deep feedforward network with $D$ layers, the parameters of the
$j$-th layer $W_j$ can be generated by a learned function $H$:
\begin{equation}
W_j = H(z_j), \ \ \forall j = 1, \dots, D,
\end{equation}
where $z_j$ is the layer embedding, and $H$ is shared for all layers. The output dimensionality of
the hypernetwork is fixed, but it's possible to accommodate predicting weights for layers of varying
kernel sizes by concatenating multiple kernels of the fixed size. Varying spatial sizes can also be
accommodated by slicing in the spatial dimensions. Hypernetworks have been found effective in
standard image recognition and text classification problems, and can be viewed as a relaxed weight
sharing mechanism. Recently, they have shown to be effective in accelerating architecture search
\citep{brock2017smash}.

\section{Graph Hypernetworks for Neural Architectural Search}
Our proposed Graph HyperNetwork (GHN) is a composition of a graph neural network and a hypernetwork.
It takes in a computation graph (CG) and generates all free parameters in the graph. During
evaluation, the generated parameters are used to evaluate the fitness of a random architecture, and
the top performer architecture on a separate validation set is then selected. This allows us to
search over a large number of architectures at the cost of training a single GHN. We refer the
reader to Figure~\ref{fig:main} for a high level system overview.

\begin{figure}
\vspace{-0.9cm}
\includegraphics[width=\linewidth]{figures/main3.pdf}
\vspace{-1cm}
\caption{Our system diagram. \textbf{A}: A neural network architecture
is randomly sampled, forming a GHN. \textbf{B}: After graph propagation, each node in the GHN
generates its own weight parameters. \textbf{C}: The GHN is trained to minimize the training loss of
the sampled network with the generated weights. Random networks are ranked according to their
performance using GHN generated weights. }
\label{fig:main}
\vspace{-0.3cm}
\end{figure}\subsection{Graphical Representation}
We represent a given architecture as a directed acyclic graph $\gA = (\gV, \gE)$, where each node $v
\in \gV$  has an associated computational operator $f_v$ parametrized by $w_v$, which produces an
output activation tensor $x_v$. Edges $e_{u \mapsto v} = (u, v) \in \gE$ represent the flow of
activation tensors from node $u$ to node $v$. $x_v$ is computed by applying its associated
computational operator on each of its inputs and taking summation as follows
\begin{equation}
\label{eq:compute_node}
x_v = \sum_{e_{u \mapsto v} \in \gE} f_v(x_u; w_v), \ \ \forall v \in \gV.
\end{equation}\subsection{Graph Hypernetwork}
Our proposed Graph Hypernetwork is defined as a composition of a GNN and a hypernetwork. First,
given an input architecture, we used the graphical representation discussed above to form a graph
$\gA$. A parallel GNN $G_\gA$ is then constructed to be \textit{homomorphic} to $\gA$ with the exact
same topology. Node embeddings are initialized to one-hot vectors representing the node's
computational operator. After graph message-passing steps, a hypernet uses the node embeddings to
generate each node's associated parameters. Let $\vh_v^{(T)}$ be the embedding of node $v$ after $T$
steps of GNN propagation, and let $H \left(\cdot; \vvphi\right)$ be a hypernetwork parametrized by
$\vvphi$, the generated parameters $\tilde{\vw}_v$ are:
\begin{equation}
\tilde{\vw}_v = H \left(\vh_v^{(T)}; \vvphi\right).
\end{equation}
For simplicity, we implement $H$ with a multilayer perceptron (MLP). It is important to note that
$H$ is shared across all nodes, which can be viewed as an output prediction branch in each node of
the GNN. Thus the final set of generated weights of the entire architecture $\tilde{\vw}$ is found
by applying $H$ on all the nodes and their respective embeddings which are computed by $G_\gA$:
\begin{align}
\tilde{\vw}=\left\{\tilde{\vw}_v | \ v \in \gV  \right\}
   &= \left\{H\left(\vh_v^{(T)}; \vvphi\right) \big| \ v \in \gV  \right\} \\
  &=  \left\{H\left(\vh; \vvphi\right) \big| \ \vh \in G_\gA^{(T)}\left(\left\{\vh_v^{(0)} \big| v \in \gV \right\}; \vphi\right)\right\} \\
  &= GHN\left(\gA; \vphi, \vvphi\right).
\end{align}\subsection{Architectural Motifs and Stacked GNNs}\label{section:graph_cells}\begin{wrapfigure}[]{r}{0.33\textwidth}
\vspace*{-0.5cm}
\includegraphics[width=\linewidth]{figures/graph_cells.pdf}
\caption{Stacked GHN along the depth dimension.}
\label{fig:graph_cells}
\end{wrapfigure}
The computation graph of some popular CNN architectures often spans over hundreds of nodes
\citep{he2016deep,huang2017densely}, which makes the search problem scale poorly. Repeated
architecture motifs are originally exploited in those architectures where the computation of each
computation block at different resolutions is the same, e.g. ResNet \citep{he2016resnet}. Recently,
the use of architectural motifs also became popular in the context of neural architecture search,
e.g. \citep{zoph2017learning, pham2018efficient}, where a small graph module with a fewer number of
computation nodes is searched, and the final architecture is formed by repeatedly stacking the same
module. \cite{zoph2017learning} showed that this leads to stronger performance due to a reduced
search space; the module can also be transferred to larger datasets by adopting a different
repeating pattern.

Our proposed method scales naturally with the design of repeated modules by stacking the same graph
hypernetwork along the depth dimension. Let $\gA$ be a graph composed of a chain of repeated modules
$\{\gA_i\}_{i=1}^N$. A graph level embedding $\vh_{\gA_i}$ is computed by taking an average over all
node embeddings after a full propagation of the current module, and passed onwards to the input node
of the next module as a message before graph propagation continues to the next module.
\begin{align}
\vh_{\gA_0} &= 0,\\
\vh_{\gA_i} &= \frac{1}{|\gV_i|}\sum_{v\in \gV_i} \left\{\vh_v^{(T)} | v \in \gV_i \right\} \label{eq:agg}\\
            &= \frac{1}{|\gV_i|}\sum G_{\gA_i}^{(T)}\left(\left\{\vh_v^{(0)} | v \in \gV_i \right\}, \vh_{\gA_{i-1}}; \vphi\right) \ \ \forall i > 0 \label{eq:next_cell}
\end{align}
Note that $G_{\gA_i}$ share parameters for all $\gA_i$.  Please see Figure~\ref{fig:graph_cells} for an overview.

\subsection{Forward-backward GNN message passing}\label{sec:prop_scheme}
Standard GNNs employ the \textit{synchronous propagation scheme}\citep{li2015gated}, where the node
embeddings of all nodes are updated simultaneously at every step (see Equation~\ref{eq:gnn_prop}).
Recently, \cite{liao2018graph} found that such propagation scheme is inefficient in passing
long-range messages and suffers from the vanishing gradient problem as do regular RNNs. To mitigate
these shortcomings they proposed \textit{asynchronous propagation} using graph partitions. In our
application domain, deep neural architectures are chain-like graphs with a long diameter; This can
make synchronous message passing difficult. Inspired by the backpropagation algorithm, we propose
another variant of asynchronous propagation scheme, which we called \textit{forward-backward}
propagation, that directly mimics the order of node execution in a backpropagation algorithm.
Specifically, let $s$ be a topological sort of the nodes in the computation graph in a forward pass,
\begin{equation}
\label{eq:gnn_prop2}
\vh_v^{(t+1)} = 
\begin{cases}
U \left(\vh_v^{(t)}, \vm_v^{(t)} \right) \ \ & \text{if } s(t) = v \text{ and } 1 \le t \le |\gV|\\
  \ \ & \text{or if } s(2|\gV| - t) = v \text{ and } |\gV| + 1 \le t < 2|\gV|,\\
\vh_v^{(t)} \ \ & \text{otherwise}.
\end{cases}
\end{equation}
The total number of propagation steps $T$ for a full forward-backward pass will then become $2|\gV|-1$. Under the synchronous scheme,  propagating information across a graph with diameter $|\gV|$ would require $O(|\gV|^2)$ messages. This is reduced to $O(|\gV|)$ under the forward-backward scheme.

\subsection{Learning}
Learning a graph hypernetwork is straightforward since $\tilde{\vw}$ are directly generated by a
differentiable network. We compute gradients of the graph hypernetwork parameters $\vphi, \vvphi$
using the chain rule:
\begin{equation}
\nabla_{\vphi, \vvphi}{\gL_{train}(\tilde{\vw})} = \nabla_{\tilde{\vw}}{
\gL_{train}(\tilde{\vw})} \cdot \nabla_{\vphi, \vvphi}{\tilde{\vw}}
\end{equation}
The first term is the gradients of standard network parameters, the second term is decomposed as\begin{align}
 \nabla_\vphi{\tilde{\vw}} &= \left\{ \nabla_\vh H( \vh; \vvphi) \cdot \nabla_\vphi \vh \ \big| \ \vh \in G^{(T)} \left( \{\vh_v^{(0)}\}, \gA, \vphi \right) \right\}, \label{eq:gnn_grad}  \\ 
 \nabla_\vvphi{\tilde{\vw}} &= \left\{ \nabla_\vvphi H( \vh_v^{(T)}; \vvphi) \ \big| \ v \in \gV \right\} \label{eq:hypernet_grad}
\end{align}
where (Eq. \ref{eq:gnn_grad}) is the contribution from GNN module $G$ and (Eq.
\ref{eq:hypernet_grad}) is the contribution from the hypernet module $H$. Both $G$ and $H$ are
jointly learned throughout training.

% !TEX root = top.tex\section{Experiments}
In this section, we use our proposed GHN to search for the best CNN architecture for image
classification. First, we evaluate the GHN on the standard CIFAR \citep{krizhevsky2009cifar} and
ImageNet \citep{russakovsky2015imagenet} architecture search benchmarks. Next, we apply GHN on an
``anytime prediction'' task where we optimize the speed-accuracy tradeoff that is key for many
real-time applications. Finally, we benchmark the GHN's  predicted-performance correlation and
explore various factors in an ablation study.
% !TEX root = top.tex\subsection{NAS benchmarks}\begin{table}[t]
\vspace{-0.7cm}
\caption{Comparison with image classifiers found by state-of-the-art NAS methods which employ a random search on CIFAR-10. Results shown are mean $\pm$ standard deviation.}
\label{table:Results1}
\footnotesize
\begin{center}
\begin{tabular}{ c c c c } 
Method & Search Cost (GPU days) & Param $\times 10^6$ & Accuracy   \\ 
\hline
SMASHv1 \citep{brock2017smash} &? & 4.6 & 94.5 \\
SMASHv2 \citep{brock2017smash} & 3 & 16.0 & 96.0\\
One-Shot Top (F=32) \citep{bender2018understanding} & 4  & 2.7 $\pm$ 0.3 & 95.5 $\pm$ 0.1\\
One-Shot Top (F=64) \citep{bender2018understanding} & 4 & 10.4 $\pm$ 1.0 & 95.9 $\pm$ 0.2\\
\hline
\hline
Random (F=32) & - & 4.6 $\pm$ 0.6 & 94.6 $\pm$ 0.3\\ 
GHN Top (F=32) &  0.42  & 5.1 $\pm$ 0.6 & 95.7 $\pm$ 0.1\\ 
\end{tabular}
\end{center}
%\footnotesize{$^*$ \cite{bender2018understanding} 16 GPU hours training + 80 GPU hours evaluation. }
\end{table}\begin{table}[t]
\vspace{-0.5cm}
\caption{Comparison with image classifiers found by state-of-the-art NAS methods which employ advanced search methods on CIFAR-10. Results shown are mean $\pm$ standard deviation.}
\label{table:Results2}
\vspace{-0.2cm}
\footnotesize
\begin{center}
\begin{tabular}{ c c c c } 
Method & Search Cost (GPU days) & Param $\times 10^6$ & Accuracy   \\ 
\hline
NASNet-A  \citep{zoph2017learning} & 1800 & 3.3 & 97.35 \\
%AmoebaNet-A  + CutOut \cite{real2018regularized} & 1 GPU, 0.45 days & 4.6 & 97.11 \\
%AmoebaNet-B + CutOut \cite{real2018regularized} & 1 GPU, 0.45 days & 4.6 & 97.11 \\
ENAS Cell search  \citep{pham2018efficient} & 0.45  & 4.6 & 97.11 \\
DARTS (first order)  \citep{liu2018darts} &  1.5  & 2.9  & 97.06  \\
DARTS (second order) \citep{liu2018darts} & 4  & 3.4 & 97.17 $\pm$ 0.06\\
\hline
\hline
GHN Top-Best, 1K (F=32) & 0.84  & 5.7 & 97.16 $\pm$ 0.07 \\
%GHN Top-Best, 20K (F=32) & 4.17 & 4.7 & 97.24 $\pm$ 0.05 \\
\end{tabular}
\end{center}
\end{table}% !TEX root = top.tex\begin{table}[t]
\vspace{-0.5cm}
\caption{Comparison with image classifiers found by state-of-the-art NAS methods which employ advanced search methods on ImageNet-Mobile.}
\vspace{-0.2cm}
\label{table:Results3}
\footnotesize
\begin{center}
\begin{tabular}{ c c c c c c} 
Method & Search Cost & Param & FLOPs  & \multicolumn{2}{c}{Accuracy}   \\ 
& (GPU days) & $\times 10^6$  & $\times 10^6$ & Top 1 & Top 5 \\
\hline
%One-Shot Top (F=24) \citep{bender2018understanding} & 3.3+? $^*$ & 6.8 $\pm$ 0.9 & 70.7 $\pm$ 0.6\\
%One-Shot Top (F=32) \citep{bender2018understanding} & 3.3+? $^*$ & 11.9 $\pm$ 1.5 & 72.6 $\pm$ 0.4\\
NASNet-A  \citep{zoph2017learning} & 1800 & 5.3 & 564& 74.0 & 91.6 \\
%NASNet-B  \citep{zoph2017learning} & 1800 & 5.3 & 72.8 \\
NASNet-C  \citep{zoph2017learning} & 1800 & 4.9 & 558 & 72.5 & 91.0 \\
AmoebaNet-A \citep{real2018regularized} & 3150 & 5.1 & 555& 74.5  & 92.0 \\
%AmoebaNet-B \citep{real2018regularized} & 3150 & 5.3 & 74.0  \\
AmoebaNet-C \citep{real2018regularized} & 3150 & 6.4 & 570 & 75.7 & 92.4 \\
PNAS \citep{liu2017progressive} & 225 & 	5.1 &  588 & 74.2 & 91.9 \\
DARTS (second order) \citep{liu2018darts} & 4  & 4.9 & 595 &73.1 & 91.0\\
\hline
\hline
GHN Top-Best, 1K & 0.84  & 6.1  &569& 73.0 & 91.3  \\
%GHN Top-Best, 20K & 4.17  & 5.0  & 73.2  \\
\end{tabular}
\end{center}
\vspace{-0.5cm}
\end{table}\subsubsection{CIFAR-10}\label{section:cifar10}
We conduct our initial set of experiments on CIFAR-10 \citep{krizhevsky2009cifar}, which contains 10
object classes and 50,000 training images and 10,000 test images of size 32$\times$32$\times$3. We
use 5,000 images split from the training set as our validation set.

\vspace{-0.25cm}\paragraph{Search space:} 
Following existing NAS methods, we choose to search for optimal blocks rather than the entire
network. Each block contains 17 nodes, with 8 possible operations. The final architecture is formed
by stacking 18 blocks. The spatial size is halved and the number of channels is doubled after blocks
6 and 12. These settings are all chosen following recent NAS methods
\citep{zoph2016neural,pham2018efficient,liu2018darts}, with details in the Appendix.

\vspace{-0.25cm}\paragraph{Training:}
For the GNN module, we use a standard GRU cell \citep{cho14gru} with hidden size 32 and 2
layer MLP with hidden size 32 as the recurrent cell function $U$ and message function $M$
respectively. The shared hypernetwork $H \left(\cdot; \vvphi\right)$ is a 2-layer MLP with hidden
size 64. From the results of ablations studies in Section~\ref{section:ablations}, the GHN is
trained with blocks with $N=7$ nodes and $T=5$ propagations under the forward-backward scheme, using
the ADAM optimizer \citep{kingma2015adam}. Training details of the final selected architectures are
chosen to follow existing works and can be found in the Appendix.
\vspace{-0.25cm}\paragraph{Evaluation:}
First, we compare to similar methods that use random search with a  hypernetwork or a one-shot model
as a surrogate search signal. We randomly sample 10 architectures and train until convergence for
our random baseline. Next, we randomly sample 1000 architectures, and select the top 10 performing
architectures with GHN generated weights, which we refer to as GHN Top. Our reported search cost
includes both the GHN training and evaluation phase. Shown in Table~\ref{table:Results1}, the GHN
achieves competitive results with nearly an order of magnitude reduction in search cost.

In Table~\ref{table:Results2}, we compare with methods which use more advanced search methods, such
as reinforcement learning and evolution. Once again, we sample 1000 architectures and use the GHN to
select the top 10. To make a fair comparison for random search, we train the top 10 for a short
period before selecting the best to train until convergence. The accuracy reported for GHN Top-Best
is the average of 5 runs  of the same final architecture. Note that all methods in
Table~\ref{table:Results2} use CutOut~\citep{devriescutout17}. GHN achieves very competitive results
with a simple random search algorithm, while only using a fraction of the total search cost. Using
advanced search methods with GHNs may bring further gains.

\subsubsection{ImageNet-Mobile}
We also run our GHN algorithm on the ImageNet dataset \citep{russakovsky2015imagenet}, which
contains 1.28 million training images. We report the top-1  accuracy on the 50,000 validation
images. Following existing literature, we conduct the ImageNet experiments in the mobile setting,
where the model is constrained to be under 600M FLOPS. We directly transfer the best architecture
block found in the CIFAR-10 experiments, using an initial convolution layer of stride 2 before
stacking 14 blocks with scale reduction at blocks 1, 2, 6 and 10. The total number of flops is
constrained by choosing the initial number of channels. We follow existing NAS methods on the
training procedure of the final architecture; details can be found in the Appendix. As shown in
Table \ref{table:Results3} the transferred block is competitive with other NAS methods which require
a far greater search cost.% !TEX root = top.tex\subsection{Anytime Prediction}
In the real-time setting, the computational budget available can vary for each test case and cannot
be known ahead of time. This is formalized in anytime prediction, \citep{grubb2012speedboost}  the
setting in which for each test example $\rvx$, there is non-deterministic computational budget $B$
drawn from the joint distribution $P(\rvx, B)$. The goal is then to minimize the expected loss $L(f)
= \E\left[ L\left( f(\rvx), B \right)\right]_{P(\rvx, B)}$, where $f(\cdot)$ is the model and
$L(\cdot)$ is the loss for an $f(\cdot)$ that must produce a prediction within the budget $B$.

We conduct experiments on CIFAR-10. Our anytime search space consists of networks with 3 cells
containing 24, 16, and 8 nodes. Each node is given the additional properties: 1) the spatial size it
operates at and 2) if an early-exit classifier is attached to it. A node enforces its spatial size
by pooling or upsampling any input feature maps inputs that are of different scale. Note that while
a naive one-shot model would triple its size to include three different parameter sets at three
different scales, the GHN is negligibly affected by such a change. The GHN uses the area under the
predicted accuracy-FLOPS curve as its selection criteria. The search space, contains various
convolution and pooling operators. Training methodology of the final architectures are chosen to
match \cite{huang2017multi} and can be found in the Appendix.

Figure \ref{fig:test1} shows a comparison with the various methods presented by
\cite{huang2017multi}. Our experiments show that the best searched architectures can outperform the
current state-of-the-art human designed networks. We see the GHN is amenable to the changes proposed
above, and can find efficient architectures with a random search when used with a strong search
space.

\begin{figure}[t]
\vspace{-0.5cm}
\centering
\begin{minipage}{.48\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{figures/anytime_compare.pdf}
  \captionof{figure}{Comparison with state-of-the-art\\ human-designed networks on CIFAR-10.}
\label{table:Results4}
  \label{fig:test1}
\end{minipage}%
\begin{minipage}{.48\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{figures/anytime_randoms.pdf}
  \captionof{figure}{Comparison between random 10 and\\ top 10 networks on CIFAR-10.}
  \label{fig:test2}
\end{minipage}
\end{figure}% !TEX root = top.tex\subsection{Predicted performance correlation (CIFAR-10)}\begin{table}[t]
\caption{Benchmarking the correlation between the predicted and true performance of the GHN against SGD and a one-shot model baselines. Results are on CIFAR-10.}
\vspace{-0.2cm}
\label{table:correlation}
\small
\begin{center}
\begin{tabular}{ c c c c c} 
Method & \multicolumn{2}{c}{Computation cost}   & \multicolumn{2}{c}{Correlation}    \\ 
%\hline
 & Initial (GPU hours) & Per arch. (GPU seconds)  & Random-100 & Top-50   \\ 
\hline
SGD 10 Steps & - & 0.9 & 0.26 & -0.05\\
SGD 100 Steps & - & 9 & 0.59 & 0.06\\
SGD 200 Steps & - & 18 & 0.62 & 0.20 \\
SGD 1000 Steps & - & 90 & 0.77 & 0.26 \\
One-Shot & 9.8 & 0.06 & 0.58 & 0.31\\
\hline
\hline
GHN & 6.1 & 0.08 & 0.68 & 0.48
\end{tabular}
\end{center}
\end{table}

In this section, we evaluate  whether the parameters generated from GHN can be indicative of the
final performance. Our metric is the correlation between the accuracy of a model with trained
weights vs. GHN generated weights. We use a fixed set of 100 random architectures that have not been
seen by the GHN during training, and we train them for 50 epochs to obtain our ``ground-truth''
accuracy, and finally compare with the accuracy obtained from GHN generated weights. We report the
Pearson's R score on all 100 random architectures and the top 50 performing architectures (i.e.\above average architectures). Since we are interested in searching for the best architecture,
obtaining a higher correlation on top performing architectures is more meaningful.

To evaluate the effectiveness of GHN, we further consider two baselines: 1) training a network with
SGD from scratch for a varying number of steps, and 2) our own implementation of the one-shot model
proposed by \citet{pham2018efficient}, where nodes store a set of shared parameters for each
possible operation. Unlike GHN, which is compatible with varying number of nodes, the one-shot model
must be trained with $N=17$ nodes to match the evaluation. The GHN is trained with $N=7$, $T=5$
using forward-backward propagation. These GHN parameters are selected based on the results found in
Section~\ref{section:ablations}.

Table \ref{table:correlation} shows performance correlation and search cost of SGD, the one-shot
model, and our GHN. Note that GHN clearly outperforms the one-shot model, showing the effectiveness
of dynamically predicting parameters based on graph topology. While it takes 1000 SGD steps to
surpasses GHN in the ``Random-100'' setting, GHN is still the strongest in the ``Top-50'' setting,
which is more important for architecture search. Moreover, compared to GHN, running 1000 SGD steps
for every random architecture is over 1000 times more computationally expensive. In contrast, GHN
only requires a pre-training stage of 6 hours, and afterwards, the trained GHN can be used to
efficiently evaluate a massive number of random architectures of different sizes.% !TEX root = top.tex\subsection{Ablation Studies (CIFAR-10)}\label{section:ablations}\begin{figure}[t]
\vspace{-1.0cm}
 \begin{center}
\begin{subfigure}{.48\textwidth}
  \includegraphics[width=0.8\linewidth]{figures/nodes.pdf}
  \caption{Vary number of nodes; $T=5$ , forward-backward}
  \label{fig:sfig2}
\end{subfigure}
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{figures/tsteps.pdf}
  \caption{Vary propagation schemes, $N=7$ }
  \label{fig:sfig1}
\end{subfigure}%
\caption{ GHN when varying the number of nodes and propagation scheme}
\label{fig:ghn_hyps}
\end{center}
\vspace{-0.5cm}
\end{figure}\vspace{-0.25cm}\paragraph{Number of graph nodes:}
The GHN is compatible with varying number of nodes - graphs used in training need not be the same
size as the graphs used for evaluation. Figure~\ref{fig:sfig2} shows how GHN performance varies as a
function of the number of nodes employed during training - fewer nodes generally produces better
performance. While the GHN has difficulty learning on larger graphs, likely due to the vanishing
gradient problem, it can generalize well from just learning on smaller graphs. Note that all GHNs
are  tested with the full graph size ($N=17$ nodes).

\vspace{-0.25cm}\paragraph{Number of propagation steps:}
We now compare  the forward-backward propagation scheme with the regular synchronous propagation
scheme. Note that $T=1$ synchronous step corresponds to one full forward-backward phase. As shown in
Figure~\ref{fig:sfig1},  the forward-backward scheme consistently outperforms the synchronous
scheme. More propagation steps also help improving the performance, with a diminishing return. While
the forward-backward scheme is less amenable to acceleration from parallelization due to its
sequential nature, it is possible to parallelize the evaluation phase across multiple GHNs when
testing the fitness of candidate architectures.

\begin{wraptable}[8]{r}{5.5cm}
\footnotesize
\vspace{-0.4cm}
\begin{center}
\begin{tabular}{ c c c c} 
SP & PE & \multicolumn{2}{c}{Correlation}    \\ 
 &&  Random-100 & Top-50   \\ 
\hline
\xmark & \xmark & 0.24 & 0.15\\
\xmark & \cmark  &  0.44 & 0.37\\
\cmark & \cmark  & 0.68 & 0.48 
\end{tabular}
\end{center}
\vspace{-0.1in}
\caption{Stacked GHN Correlation. SP denotes sharing parameters and PE denotes passing embeddings}
\label{table:stacked}
\end{wraptable}\vspace{-0.25cm}\paragraph{Stacked GHN for architectural motifs:}
We also evaluate different design choices of GHNs on representing architectural motifs. We compare
1) individual GHNs, each predicting one block independently, 
2) a stacked GHN where individual GHN's
   pass on their graph embedding without sharing parameters, 
3) a stacked GHN with shared parameters (our proposed approach). 
As shown in Table~\ref{table:stacked},  passing messages between GHN's is crucial, and sharing parameters produces better performance.
% !TEX root = top.tex\vspace{-0.1cm}\section{Conclusion}
In this work, we propose the Graph HyperNetwork (GHN), a composition of graph neural networks and
hypernetworks that generates the weights of any architecture by operating directly on their
computation graph representation. We demonstrate a strong correlation between the performance with
the generated weights and the fully-trained weights. Using our GHN to form a surrogate search
signal, we achieve competitive results on CIFAR-10 and ImageNet mobile with nearly 10$\times$ faster
speed compared to other random search methods. Furthermore, we show that our proposed method can be
extended to outperform the best human-designed architectures in setting of anytime prediction,
greatly reducing the computation cost of real-time neural networks.

\bibliography{iclr2019_conference}
\bibliographystyle{iclr2019_conference}

\newpage
% !TEX root = top.tex\section{appendix}\subsection{Search space}\paragraph{Standard image classification on CIFAR-10 and ImageNet}
The search space for CIFAR-10 and ImageNet classification experiments includes the following operations:
\begin{itemize}
\item identity
\item $1\times1$ convolution
\item $3\times3$ separable convolution
\item $5\times5$ separable convolution
\item $3\times3$ dilated separable convolution
\item $5\times5$ dilated separable convolution
\item $1\times7$ convolution followed $7\times1$ convolution
\item $3\times3$ max pooling
\item $3\times3$ average pooling
\end{itemize}
A block forms an output by concatenating all leaf nodes in the graph. Blocks have 2 input nodes which ingest the output of block $i-1$ and block $i-2$ respectively. The input nodes are bottleneck layers, and can reduce the spatial size by using stride 2. 

Note that while ENAS supports only 5 operators due to memory constraints, GHNs can search for more operators. 
This is because ENAS (and other methods which use one-shot models) must store all the parameters in memory because it finds paths in a larger model.
Thus the memory requirements are $O(KN)$ where $K$ is the number of operations and $N$ is the number of nodes in the candidate architecture. 
In contrast, the memory requirement for GHNs is $O(N) + O(K)$ for the candidate architecture and GHN respectively. 

\paragraph{Anytime prediction on CIFAR-10}
The search space for the CIFAR-10 anytime prediction experiments includes the following operations:
\begin{itemize}
\item $1\times1$ convolution
\item $3\times3$  convolution
\item $5\times5$  convolution
\item $3\times3$ max pooling
\item $3\times3$ average pooling
\end{itemize}
In the anytime setting, nodes concatenate their inputs rather than sum. Thus, the identity operator was removed as it would be redundant. The search space does not include separable convolutions so that it is comparable with our baselines \citep{huang2017multi}. Block 1 contains nodes which may operate on any of the 3 scales ($32\times32, 16\times16, 8\times8$). Block 2 contains nodes which can only operate on scales $16\times16$ and $8\times 8$. Block 3 only contains nodes which operate on the scale $8\times 8$. We fix the number of exit nodes. These choices are inspired by  \cite{huang2017multi}\subsection{Graph HyperNetwork details}\paragraph{Standard image classification on CIFAR-10 and ImageNet}
While node embeddings are initialized to a one-hot vector representing computational operator of the node, we found it helpful to pass the sparse vector through a learned embedding matrix  prior to graph propagation. The GHN is trained for 200 epochs  with batch size 64 using the ADAM optimizer with an initial learning rate 1e-3 that is divided by 2 at epoch 100 and 150. A naive hypernet would have a separate output branch for each possible node type, and simply ignore branches that aren't applicable to the specific node. In this manner, the number of parameters of the hypernetwork scale according to the number of possible node computations. In contrast, the number of parameters for a one-shot model scale according to the number of nodes in the graph. We further reduce number of parameters by obtaining smaller sized convolutions kernels through the slicing of larger sized kernels. 

\paragraph{Anytime prediction }
In the anytime prediction setting, two one-hot vectors representing the node's scale and presence of an early exit classifier are additionally concatenated to the first initialized node embedding. We found it helpful to train the GHN with a random number of nodes per block, with maximum number of allowed nodes being the evaluation block size. Because nodes concatenate their inputs, a bottleneck layer is required. The hypernetwork can predict bottleneck parameters for a varying number of input nodes by generating weights based on edge activations rather than node activations. We form edge activations by concatenating the node activations of the parent and child. Edge weights generated this way can be concatenated, allowing the dimensionality of the predicted bottleneck weights the be proportional to the number of incoming edges. 

\subsection{Final architecture training details}\paragraph{CIFAR-10} Following existing NAS methods \citep{zoph2017learning,real2018regularized}, the final candidates are trained for 600 epochs  using  SGD with momentum 0.9,  a single period cosine schedule with $l_{max}=0.025$, and batch size 64. For regularization, we use scheduled drop-path with a final dropout probability of 0.4. We use an auxiliary head located at 2/3 of the network weighted by 0.5. We accelerate training by performing distributed training across 32 GPUs; the learning rate is multiplied by 32 with an initial linear warmup of 5 epochs. 

\paragraph{ImageNet Mobile} For ImageNet mobile experiments, we use an image size of $224\times224$. Following existing NAS methods \citep{zoph2017learning,real2018regularized}, the final candidates are trained for 250 epochs using SGD with momentum 0.9, initial learning rate 0.1 multiplied by 0.97 every epoch. We use an auxiliary head located at 2/3 of the network weighted by 0.5. We use the same regularization techniques, and similarly accelerate training in a distributed fashion. 

\paragraph{Anytime}Following \cite{huang2017multi}, the final candidates are trained using SGD with momentum 0.9. We train the models for 300 epochs use an initial learning rate of 0.1, which is divided by 10 after 150 and 225 epochs using a batch size of 64. We accelerate training with distributed training in a similar fashion as the CIFAR-10 classification and ImageNet mobile experiments. 
The number of filters for the final architecture is chosen such that the number of FLOPS is comparable to existing baselines.

\subsection{Investigating Accuracy Drop off}
Figure  \ref{fig:sample_correlation} shows a plot comparing the accuracy of an architecture that is trained for 50 epochs and the accuracy of the same architecture using GHN generated weights.
\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{figures/sample_correlation.pdf}
\caption{Comparison for 100 randomly sampled architectures.}
\label{fig:sample_correlation}
\end{figure}\subsection{Visualization of Final architectures}\subsubsection{CIFAR-10 and ImageNet Classification}
Figure \ref{fig:best_block} shows the best found block in the CIFAR-10 Experiments.

\begin{figure}[h]
\includegraphics[width=\linewidth]{figures/graph_-2132791772218152156.pdf}
\caption{Best block found for classification}
\label{fig:best_block}
\end{figure}\subsubsection{Anytime Prediction}
Figures \ref{fig:best_anytime1}, \ref{fig:best_anytime2} and \ref{fig:best_anytime3} show blocks 1 2 and 3 of the best architecture found in the anytime experiments. The color red denotes that an early exit is attached to the output of the node.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.4\linewidth]{figures/graph_642261663860860599_cell_0.pdf}
\end{center}
\caption{Block 1 for anytime network. Red color denotes early exit.}
\label{fig:best_anytime1}
\end{figure}\begin{figure}[h]
\begin{center}
\includegraphics[width=0.6\linewidth]{figures/graph_642261663860860599_cell_1.pdf}
\caption{Block 2 for anytime network. Red color denotes early exit.}
\label{fig:best_anytime2}
\end{center}
\end{figure}\begin{figure}[h]
\begin{center}
\includegraphics[width=\linewidth]{figures/graph_642261663860860599_cell_2.pdf}
\caption{Block 3 for anytime network. Red color denotes early exit.}
\label{fig:best_anytime3}
\end{center}
\end{figure}%\subsection{GHN Settings}%%%the recurrent cell function $U$ is  with latent dimension 32, %and the message function $M$ is a 2 layer MLP with a latent dimension of 32 and ReLU non-linearity. % Existin Nas methods \citep{zoph2017learning,real2018regularized} %
\end{document}
