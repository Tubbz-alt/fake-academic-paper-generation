\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage{comment}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{tabulary}
%\usepackage{bibunits}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
\newcommand\todo[1]{\textcolor{red}{#1}}

% If you comment hyperref and then uncomment it, you should delete
% main.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\newcommand{\model}{\text{ImGAN}}\newcommand{\zz}{z}\newcommand{\D}{D}%\newcommand{\ZZ}{Z}\newcommand{\ZZ}{K}\newcommand{\B}{B}\newcommand{\R}{R}\newcommand{\xxpred}{\hat{\mathbf{x}}}\newcommand{\xxgt}{\mathbf{x}^*}\newcommand{\mdots}{..}\DeclareMathOperator{\E}{\mathbb{E}}
\newcolumntype{K}[1]{>{\centering\arraybackslash}p{#1}}
\def\iccvPaperID{1967} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% extra commands
\graphicspath{{figures/}}

\pagenumbering{gobble}
% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi
%\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{Supplementary Material for \\
``Adversarial Inverse Graphics Networks: Learning 2D-to-3D Lifting and Image-to-Image Translation from Unpaired Supervision"}
% Learning Inverse Graphics with Adversarial Priors and Parameter-Free Rendering

\author{Hsiao-Yu Fish Tung \thanks{equal contribution} 
\qquad
Adam W. Harley \samethanks[1] 
\qquad
William Seto \samethanks[1] 
\qquad
Katerina Fragkiadaki
\\
%Machine Learning Department \\ 
Carnegie Mellon University \\
{\tt\small \{htung,aharley,wseto,katef\}@cs.cmu.edu}
}

\maketitle

\section{Parametric vs. non-parametric decoders}
Here we discuss the benefits of using non-parametric and domain-specific renderers, over learned decoders.
Both the proposed model and CycleGAN \cite{CycleGAN2017} can be viewed as autoencoders: the input is first transformed 
into a target domain, and then transformed back to its original space.
A parametric decoder could be more desirable, for the reason that we do not need to hand-engineer a mapping function from the target domain back to the inputs.
However, simply using reconstruction loss and adversarial loss does not
guarantee that the predictions look spatially similar to the inputs. In tasks such as image-to-image translation, spatial precision can be of critical importance.
%We explain this in terms of information theory. 
With a parametric decoder, the transformed input can be viewed as a information bottleneck, and as long as the decoder can correctly ``guess'' the final output from the transformed input (\ie, the code), the code is valid and the solution is optimal.

To support this point, we conduct an experiment on image inpainting using the MNIST dataset. %as a sanity check. 
Similar to the parametric encoder-decoder described in the main text, the network has two main parts:
(1) an encoder that transforms the input (a partially obscured image of a digit) into prediction (a hallucinated digit), 
and (2) a decoder
that transforms the prediction back into the input. 
Instead of using convolutional layers, which have an architectural bias on 
preserving spatial relationships, we use fully-connected layers in both the 
encoder and the decoder. This is important, because such architectural conveniences are unavailable in less-structured tasks, such as 3D pose prediction and SfM. 
We train the model with a reconstruction loss on the decoder, and adversarial loss on the encoder.
%In this task, the encoder is provided with a a partially obscured image of a digit.

The results are shown in Figure \ref{fig:mnist}.
While inpainting, the encoder (incorrectly) transforms many of the digits into other digits. For instance, several obscured ``1'' images are inpainted as ``4''. In the parametric decoding process, however, these errors are \textit{undone}, and the original input is recovered successfully. In other words, the decoder takes the burden of the reconstruction loss, allowing the encoder to learn an inaccurate latent space. Parameter-free rendering avoids this problem.

% shown in Figure \ref{fig:mnist}, digit (\eg, 1) is transformed into another digit (\eg, 4). However, since the decoder is learnable, the prediction can still be transformed back to its input. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{supp_images/mnist}
     \centering
    \caption{Digit inpainting using an encoder-decoder architecture with fully-connected layers. Many predictions are incorrect, while the recovered inputs are accurate. Orange squares highlight instances of the digit ``1'' transformed into other digits; purple squares highlight instances of the digit ``2'' transformed into other digits.}
    \label{fig:mnist}
\end{figure}

%In order to obtain the desired outputs, more constraints and domain-knowledge are needed. In image-to-image translation, convolutional layers impose good constraints on preserving spatial information. Such architectural conveniences are unavailable in less-structured tasks, such as 3D pose prediction and SfM. 
%Designing a domain-specific and non-parametric renderer is a way to add domain knowledge and impose such constraints on the predictions. 
%, necessitating non-parametric renderers.
%Designing a domain-specific and non-parametric renderer is a way to add domain knowledge and impose such constraints on the predictions.%, and in some scenarios, it is in fact necessary.








 %we explore different neural network structure and replace convolutional layers with fully-connected layers. We conduct the experiment on image-inpainting using MNIST dataset as a sanity check. In Figure \ref{fig:mnist}(b), 
%we show the results of using encoder and decoder with fully-connected layers. The results show that a specific digit (ex. 1) is transformed into another digit (ex. 4). However, since the decoder is learnable, the prediction still can be transformed back to its input. 
%If we replace the decoder with domain-specific renderer, we are able to recover the digits correctly. The reason why in the second case the model fails to recover the correct 
%prediction is because simply using reconstruction loss and adversarial loss does not
%guaranteed that the predictions look spatially similar to the inputs. The prediction
%can be viewed as a information bottle neck, and, as long as the decoder can correctly ''guess'' the final output, the prediction is optimal. In the first case, since 
%convolutional layers can preserve spatial information, the model still works even without
%domain-specific renderer. 

%\begin{figure} [h!]
%    \centering
%    \includegraphics[width=\linewidth]{figures/william_results_mnist}
%     \centering
%    \caption{Results using parametric decoder. (a) gender transformation using decoder with convolutional layers. (b)digits inpainting using encoder/decoder with fully-connected layers.}
%    \label{fig:mnist}
%\end{figure}

\section{Additional experiments and details}
In the sections to follow,  we provide implementation details, including architecture descriptions for the generator and discriminator in each task, and training details. Additionally, we provide more experimental results.


%In the sections to follow,  we provide implementation details, including architecture descriptions for the generator and discriminator in each task, and training details, as well as more experimental results.
%\section{Experiments} \label{sec:exp}

\begin{comment}


\begin{table*}
\centering
\begin{tabular}{ c c c c c c c c c}
 \hline
  & Direction & Discussion & Eating & Greeting & Phoning & Photo & Posing & Purchases\\
 \hline
  Forward2Dto3D & 75.2 & 118.4 & 165.7 & 95.9 & 149.1 & 154.1 & 77.7 & 176.9 \\
  3D interpreter \cite{Wu2016} & 56.3 & 77.5 & 96.2 & 71.6 & 96.3 & 106.7 & 59.1 & 109.2 \\
  Monocap \cite{DBLP:journals/corr/ZhouZPLDD17}&  78.0 & 78.9 & 88.1 & 93.9 & 102.1 & 115.7 & 71.0 & {\bf 90.6} \\
  Adversarial Inv (ours) & {\bf 53.7} & {\bf 71.5} & {\bf 82.3} & {\bf 58.6} & {\bf 86.9} & {\bf 98.4} & {\bf 57.6} &  104.2 \\
  
 \hline
 & Sitting & SittingDown & Smoking & Waiting & WalkDog & Walking & WalkTogether & Average \\
 \hline
 Forward2Dto3D & 186.5 & 193.7 & 142.7 & 99.8 & 143.7 & 74.7 & 79.9 & 128.9\\
 3D interpreter \cite{Wu2016} & 111.9 & {\bf 111.9} & 124.2 & 93.3 & 93.6 & 58.0 & 63.3 & 88.6\\
  Monocap \cite{DBLP:journals/corr/ZhouZPLDD17} &  121.0 & 118.2 & 102.5 & 82.6 & 88.8 & 75.62 & 76.9 & 92.3  \\
 Adversarial Inv (ours) & {\bf 100.0} & 112.5 & {\bf 83.3} & {\bf 68.9} & {\bf 86.8} & {\bf 57.0} & {\bf 62.8} & {\bf 79.0}\\
  \hline
\end{tabular}
\vspace{1mm}
\caption{\textbf{3D reconstruction error in H3.6M using ground-truth 2D keypoints} as input.}\label{tab:monocap}
\end{table*}
\end{comment}



\subsection{3D human pose estimation from static images}
Figure~\ref{fig:architectures1} shows the architecture of our generator network for 3D human pose estimation from a single  RGB image. Our generator predicts 
weights over the shape bases $\alpha$, rotation $R,$ translation $T$ and focal length $f$, as described in our paper. 
The generator takes as input a set of 2D body joint heatmaps. 
We use convolutional pose machines \cite{wei2016cpm} 
to estimate 2D keypoints, and convert them into heatmaps  by creating a Gaussian distribution  
%$N(0, \sigma)$ 
centered 
around each 2D keypoint. 
The network consists of 8 convolutional layers with leaky ReLU activations and batch normalization and two fully connected layers at the end that map to the desired outputs. The width, height and number of channels for each layer is specified in Figure~\ref{fig:architectures}. 
The discriminator for this task consists of five fully connected layers with featuremap depth 512, 512, 256, 256 and 1, with a leaky ReLU and batch normalization after each layer. The discriminator takes all values output from the generator (\ie, $\alpha$, $R,$ $T$,  $f$) as input. 

In all experiments, we set the variance for the Gaussian heatmap $\sigma$ to 0.25, 
and the dimensionality of our PCA shape basis to 60 (out of 96 total bases). The dimensionality reduction is small, and indeed, we only use basis weights for ease of prediction, relying on our adversarial priors (rather than PCA) to regularize the 3D shape prediction. We use gradient descent for both generator and discriminator training. Learning rate for reconstruction loss is set to 0.00001  and learning rate for the adversarial loss is set to 0.0001. All parameters are initialized with random sampling from zero mean normal distributions with variance of 0.02.

In Figure \ref{fig:mpii}, we show predicted 3D human poses on  images from the MPII dataset \cite{andriluka14cvpr} using the ground-truth 2D keypoints available. Our model generalizes well \textit{on unseen  images without any further self-supervised finetuning}, though we would expect  additional self-supervised finetuning to further improve performance. 
\begin{figure}[t]
    \begin{tabular}{l}
    %\centering
    \includegraphics[width=0.9\linewidth]{supp_images/structure1} \\
    \end{tabular}
\centering
    \label{fig:architectures1}
    \caption{\textbf{Generators and discriminators' architectures for the \\task of 3D human pose estimation from a single image.}}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/MPII3d.pdf}
     \centering
    \caption{\textbf{Predicted 3D human poses in MPII dataset using the supplied ground-truth 2D keypoints as input.} }
    \label{fig:mpii}
\end{figure}


\begin{figure}[t]
    \begin{tabular}{l}
    %\centering
    \includegraphics[width=0.9\linewidth]{supp_images/adam_arch} \\
    \end{tabular}
\centering
    \label{fig:architectures2}
    \caption{\textbf{Generator and discriminator architectures for  \\Structure from Motion.} Dashed lines indicate skip  \\connections. \\}
\end{figure}



\subsection{Structure from Motion}
Our generator networks for the task of structure from motion is illustrated in Figure~\ref{fig:architectures2}. It includes three encoder-decoder convolutional networks with skip connections, which solve for optical flow, depth, and camera motion. The egomotion network uses RGB, optical flow and an angle field as input, and estimates the camera motion in SE(3). %^ \cite{SE3-Nets}.
The depth network takes an RGB image as input and predicts logdepth. 
%We combine the camera motion with the depth to transform the pointcloud from one camera viewpoint to the next. We then project the difference between the two pointclouds to a 2D image, producing a prediction of the optical flow of the static part of scene. 
%The flow has a geometric interpretation, as the motion of the static 3D scene under camera motion. 
%It accounts only for the static part of the scene and neglects the moving objects, so there is no way to recover their depth values. 
The depth discriminator consists of four convolution layers with batch normalization on the second and the third layers, and leaky ReLU activation after each layer. The depth  discriminator is fully convolutional as we are interested in the realism of every depth patch, as opposed to the depthmap as a whole. 

The egomotion discriminator is a 3-layer fully-connected network that takes $\{R,T\}$ matrices as input. The hidden layers have 128, 128, and 64 neurons, respectively, with batch normalization and a leaky ReLU after each layer.

%That is, it has one probabilistic output at each image sub-region, similar to Isola \etal. \cite{DBLP:journals/corr/IsolaZZE16}. 
%We observed a convolutional discriminator was less stable than a single-output (fully-connected) one. 
\textbf{Stabilizing training.} 
In order to  make sure that generators and discriminators progress together during training, we update the generator  only when the discriminator has low enough loss. We add an updating heuristic such that if the likelihood loss of the discriminator is above a threshold $\theta_d,$ we do not update the generator. While discriminator is strong enough (below this threshold) and the generator is relatively weak (below a different threshold $\theta_g$), we update the generator twice %for another time
in the iteration. 
%\todo{i commented out a phrase that made it sound like we update the generator twice. (we don't, right?)}. 
In the experiments, we set $\theta_d$ to 0.695 and $\theta_g$ to 0.75.



\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{supp_images/male_merge}
     \centering
    \caption{AIGN on \textbf{gender transformation (female to male, male to female)} and \textbf{age transformation (young to old).}}
    \label{fig:more-male-merge}
\end{figure*}

\begin{figure}[t]
    \begin{tabular}{l}
    %\centering
    %\includegraphics[width=0.9\linewidth]{supp_images/adam_arch} \\
    %(a) Generator and discriminator architectures for  \\Structure from Motion. Dashed lines indicate skip  \\connections. \\
    \includegraphics[width=0.9\linewidth]{supp_images/structure3} \\
    (a) Generator's and discriminator's architectures for  \\image super-resolution. \\
    \includegraphics[width=0.9\linewidth]{supp_images/structure4} \\
    (b) Generator and discriminator architectures for image  \\inpainting. \\
    \end{tabular}
\centering
    \caption{\textbf{Architectures for AIGN.}}
    \label{fig:architectures}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{supp_images/young2}
     \centering
    \caption{AIGN on \textbf{age transformation (old to young).}}
    \label{fig:more-young}
\end{figure}



\subsection{Image Super-Resolution}

In Figure~\ref{fig:architectures}, we show the architecture of the generator and discriminator for image super-resolution.
The input image is first passed through a convolutional layer with 64 channels, then $n$ ``residual blocks''. Each residual block consists of two convolutional 
layers, with a batch normalization after each convolution layer and ReLU activation
after the first batch normalization. The output from the last block is further passed to two deconvolution layers
and generates the final image. 
The discriminator for this task consists of five convolution layers that use leaky ReLU activations and batch normalization, and one fully-connected layer that outputs one final value.
In all experiments, we use Adam optimizer with learning rate 0.0001.%$0.0001.$
All parameters are initialized with truncated normal distribution with variance 0.02. 



In Figures~\ref{fig:baseline-male}, \ref{fig:baseline-female} and \ref{fig:baseline-old}, we compare our model with Attribute2Image  \cite{DBLP:journals/corr/YanYSL15} and with Unsupervised Image Translation \cite{DBLP:journals/corr/DongNWG17} for gender and age transformations. 
%Attribute2Image is a model that uses a large labelled attribute dataset of faces builds a generative network, that conditioned on an input image and a set of attributes, generates the transformed image. The authors test their model on age and gender transformations. 
We use the code provided by the authors for our comparisons. 
%the attempts to disentangle foreground- and background-related latent variables with a conditional autoencoder. 
%To our best knowledge, this is currently the state-of-the-art model in  age and gender transformation on face 
%images. 
%Image Translation model is another work that does high level transformation on face images. The results illustrate that our approach is able to generate images with more detail than the baselines, while also accurately maintaining the visual characteristics of the original images.
In Figures~\ref{fig:more-male-merge} and \ref{fig:more-young},
we show additional results of our model on gender and age transformation.



%\subsubsection{More results on age and gender transformations}
\subsection{Inpainting}

Figure~\ref{fig:architectures} illustrates the architecture of our generator and discriminator for image inpainting.
The occluded input image and the corresponding mask are separately passed through two convolution 
layers, and then concatenated. The concatenated outputs are then passed to three deconvolutional layers  
to generate the inpainted image.  
The discriminator  consists of four convolutional layers  with leaky ReLU and batch normalization layers, and one fully connected layer that outputs one final value.  
In all experiments, we use the Adam optimizer, with a learning rate $1\mathrm{e}{-4}$%$0.0001.$
All parameters are initialized from the truncated Normal distribution, with variance 0.02. 




In Figure~\ref{fig:more-lips}, 
we show additional results on biased inpainting for making bigger lips.



\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{supp_images/baseline_male2}
     \centering
    \caption{Comparison with Attribute2Image \cite{DBLP:journals/corr/YanYSL15} and Unsupervised Image to Image Translation \cite{DBLP:journals/corr/DongNWG17} on \textbf{Gender transformation (female to male)}. Input to our model is a tight crop around the face, tighter than the crop used by \cite{DBLP:journals/corr/YanYSL15}. The proposed AIGN (\textit{Column 2}) provides more realistic results that better preserves the ``identity" of the subject while changing its gender, in comparison to previous work \cite{DBLP:journals/corr/YanYSL15} (\textit{Column 4}). We further show gender transformations from the model of \cite{DBLP:journals/corr/DongNWG17} (\textit{Columns 5,6}) where as we see the identity preservation is much weaker. Code is not available so we just paste some results from their paper. }
    \label{fig:baseline-male}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9 \linewidth]{supp_images/baseline_female2}
     \centering
    \caption{Comparison of AIGN with Attribute2Image \cite{DBLP:journals/corr/YanYSL15}  on \textbf{gender transformation (male to female)}.}
    \label{fig:baseline-female}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\linewidth]{supp_images/baseline_old2}
    \includegraphics[width=0.45\linewidth]{supp_images/baseline_young2}
     \centering
    \caption{Comparison of AIGN with Attribute2Image \cite{DBLP:journals/corr/YanYSL15} on \textbf{age transformation (left: young to old; right: old to young).}}
    \label{fig:baseline-old}
\end{figure}









\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{supp_images/lip2}
     \centering
    \caption{Additional results  of AIGN on \textbf{biased image inpainting (big lips).}}
    \label{fig:more-lips}
\end{figure}


%For our bias experiments, we have only used the gender, 'Old/Young', and 'Big Lips' attributes.








{\small
\bibliographystyle{ieee}
\bibliography{bibref_definitions_short,refs}
}





\end{document}