\documentclass[../main.tex]{subfiles}
\begin{document}

\section{Related Work}
\label{rel_work}

\subsection*{Machine Teaching}
The goal of machine teaching is the design of algorithms and systems that can teach humans efficiently and automatically. 
To date, a variety of different approaches have been explored for modeling the teaching of students from assuming perfect learners \cite{goldman1995complexity, zhu2013machine, liu2017iterative}, heuristic-based approaches \cite{basu2013teaching}, Bayesian models \cite{corbett1994knowledge,eaves2015tractable}, recurrent neural networks \cite{piech2015deep}, and reinforcement learning based methods \cite{rafferty2011faster,bak2016adaptive,whitehill2017approximately}.

While machine teaching has been successfully deployed in online tutoring systems that feature highly structured knowledge \eg mathematics, the teaching of challenging \emph{visual} concepts to human learners is less explored. 
\cite{singla2014near} teach binary visual classification tasks by modeling the student as stochastically switching between a set of different hypotheses during learning. 
Their model attempts to select the set of teaching examples offline that will best discount the incorrect hypotheses and guide the student towards the ground truth classification function.  
\cite{johns2015} propose an interactive approach, where the choice of future images to show is based on the individualâ€™s past responses. 
However, they must update the model's parameters online for each user making it computationally difficult to scale to large numbers of simultaneous users in real-world settings.

The major limitation of these existing approaches is that the feedback they provide to the student is not fully informative. 
In both \cite{singla2014near} and \cite{johns2015} a student is shown a sequence of images and asked to estimate what object category from a finite list they believe to be present in each image. 
After they respond they are simply told what the correct answer is. 
They are not informed of the parts in the image that are discriminative for identifying that particular object, thus making the learning problem artificially hard for the student. 
To overcome this limitation, we propose a novel teaching algorithm that selects both images and provides interpretable explanations resulting in more understandable and efficient learning for the student. 
Complementary to our work, \cite{chenAiStats2018} recently introduced an explanation based teaching algorithm for binary tasks. Explanations are provided via pre-existing semantically meaningful features, but the interpretability of a given explanation is not modeled.  

\subsection*{Interpretable Models}
Using clear and understandable instructional material can dramatically improve a student's ability  to learn a new concept.  
It has been shown that highlighting informative regions on an image can help improve novice classification performance by guiding the student's attention \cite{grant2003,roads2016}.

In another example, when a human teacher is unavailable, the most common way novices learn species identification is by consulting expertly curated field guides. 
These field guides are typically books or apps that contain descriptive text and example images highlighting important features for classifying different species \eg \cite{peterson1980field}.
Attempts have been made to automate the creation of these guides using highlighted part annotations \cite{berg2013you}, automatic generation of image specific text descriptions \cite{hendricks2016generating}, or through gamification \cite{deng2016}. 
However, in addition to image level class labels, the majority of these approaches require the collection of \emph{expert} annotations in the form of text descriptions, anatomical part locations, or visual attributes which can be expensive and time consuming to obtain for very large image collections \cite{branson2010}. 
Furthermore, the efficacy of these annotations for teaching visual identification skills has not yet been evaluated on real human subjects. 
An alternative approach that requires less additional annotations is to learn human interpretable models from the raw data \eg \cite{ribeiro2016should, lakkaraju2016interpretable}.
In the context of computer vision, there is some evidence to suggest that the deep models commonly used for large-scale image classification tasks can be adapted to generate features that are semantically meaningful to humans \cite{zhou2016learning, zhang2017interpretable}.

Recently, \cite{pmlr-v54-poulis17a} outlined an approach for incorporating additional supervised data from users which they call `feature feedback'.
In addition to class level labels that are typically provided by human annotators when training supervised classifiers, they allow their annotators to provide information about the value of specific feature dimensions. 
In contrast, our approach instead gives explanations to the \emph{learner} about the importance of different image regions and models how they incorporate this information when updating their belief. 

We are concerned with selecting the set of teaching examples with associated interpretable explanations to best teach noisy human learners. 
While in practice these explanations can be generated with additional time-consuming human annotation, we show that it is possible to extract meaningful explanations using existing image level labels.

\end{document}
