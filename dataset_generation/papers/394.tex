%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal,transmag]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.




% *** CITATION PACKAGES ***
%
\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
% 
\usepackage{graphicx}
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage{amsmath}
\usepackage{amssymb}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath
\usepackage{gensymb}
\usepackage{textcomp}





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{multirow}




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
\usepackage{caption}
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig
\usepackage{subcaption}




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks=false,bookmarks=false]{hyperref}
\renewcommand{\UrlFont}{\footnotesize}

\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand\InsertFailure[1]{
    \hspace{-12pt}
    \begin{subfigure}[c]{0.19\linewidth}
        \centering
        \includegraphics[trim={1.2cm 0 0 0},clip,width=\textwidth]{./Figures/supp/failures/#1-size.pdf}
    \end{subfigure}
    ~
    \begin{subfigure}[c]{0.19\linewidth}
        \centering
        \includegraphics[width=\textwidth]{./Figures/supp/failures/#1-pred-1-enc.jpg}
    \end{subfigure}
    \begin{subfigure}[c]{0.19\linewidth}
        \centering
        \includegraphics[width=\textwidth]{./Figures/supp/failures/#1-pred-2-enc.jpg}
    \end{subfigure}
    ~~~
    \begin{subfigure}[c]{0.19\linewidth}
        \centering
        \includegraphics[width=\textwidth]{./Figures/supp/failures/#1-min-1-enc.jpg}
    \end{subfigure}
    \begin{subfigure}[c]{0.19\linewidth}
        \centering
        \includegraphics[width=\textwidth]{./Figures/supp/failures/#1-min-2-enc.jpg}
    \end{subfigure}
    \hspace{-12pt}
}

\newcommand\InsertQualitative[1]{
    \hspace{-12pt}
    \begin{subfigure}[c]{0.19\linewidth}
        \centering
        \includegraphics[trim={1.2cm 0 0 0},clip,width=\textwidth]{./Figures/supp/qualitative/#1-size.pdf}
    \end{subfigure}
    ~
    \begin{subfigure}[c]{0.19\linewidth}
        \centering
        \includegraphics[width=\textwidth]{./Figures/supp/qualitative/#1-pred-1-enc.jpg}
    \end{subfigure}
    \begin{subfigure}[c]{0.19\linewidth}
        \centering
        \includegraphics[width=\textwidth]{./Figures/supp/qualitative/#1-pred-2-enc.jpg}
    \end{subfigure}
    ~~~
    \begin{subfigure}[c]{0.19\linewidth}
        \centering
        \includegraphics[width=\textwidth]{./Figures/supp/qualitative/#1-center-1-enc.jpg}
    \end{subfigure}
    \begin{subfigure}[c]{0.19\linewidth}
        \centering
        \includegraphics[width=\textwidth]{./Figures/supp/qualitative/#1-center-2-enc.jpg}
    \end{subfigure}
    \hspace{-12pt}
}



% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Learning Compressible 360\textdegree~Video Isomers}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{
\IEEEauthorblockN{Yu-Chuan~Su and
Kristen~Grauman}
\IEEEauthorblockA{Department of Computer Science, The University of Texas at Austin}
}


% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
Standard video encoders developed for conventional narrow field-of-view video are widely applied to $360$\textdegree~video as well, with reasonable results.
However, while this approach commits arbitrarily to a projection of the spherical frames, we observe that some orientations of a $360$\textdegree~video, once projected, are more compressible than others.
We introduce an approach to predict the sphere rotation that will yield the maximal compression rate.
Given video clips in their original encoding, a convolutional neural network learns the association between a clip's visual content and its compressibility at different rotations of a cubemap projection.
Given a novel video,
our learning-based approach efficiently infers the most compressible direction in one shot,
without repeated rendering and compression of the source video.
We validate our idea on thousands of video clips and multiple popular video codecs.
The results show that this untapped dimension of $360$\textdegree~compression has substantial potential---``good'' rotations are typically $8{-}10\%$ more compressible than bad ones,
and our learning approach can predict them reliably $82\%$ of the time.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
%\begin{IEEEkeywords}
%IEEE, IEEEtran, journal, \LaTeX, paper, template.
%\end{IEEEkeywords}


\section{Introduction}


Both the technology and popularity of
$360\degree$ video has grown rapidly in recent years, for emerging Virtual Reality (VR) applications and others.
Sales of $360\degree$ cameras are expected to grow by $1500\%$ from 2016 to 2022~\cite{360camera}.
Foreseeing the tremendous opportunities in $360\degree$ video,
many companies are investing in it.  
For example, Facebook and YouTube have offered $360\degree$ content support since 2015.
Facebook users have since uploaded more than one million $360\degree$ videos~\cite{fb360videostatistics},
and YouTube plans to bring $360\degree$ videos to even broader platforms (TV, gaming consoles).
$360\degree$ editing tools are now available in popular video editors such as PowerDirector and Premiere Pro.
Meanwhile, on the research side, there is strong interest in improving $360\degree$ video display~\cite{kasahara2015first,kopf2016tog,kamali2011stabilizing,su2016accv,su2017cvpr,hu2017deep,lai2017semantic},
and performing visual processing efficiently on the new format~\cite{khasanova2017graph,cohen2017convolutional,su2017nips}.
All together, these efforts make $360\degree$ video production and distribution easier and more prevalent than ever.

At the core of all video technologies is the data format.
In particular, a compressed video bit-stream format is the basis for all video related applications,
ranging from video capture, storage, processing to distribution.
Without adequate compression,
all of the above suffer.  
$360\degree$ video is no exception.
Thus far, the focus for $360\degree$ video compression is to find a proper projection that transforms a $360\degree$ frame into a rectangular planar image that will have a high compression rate.
A current favorite is to project the sphere to a \emph{cubemap} and unwrap the cube into a planar image~\cite{fb2015cubemap,google2017eac,mpeg120} (see Fig.~\ref{fig:faces}).
Cubemaps can improve the compression rate by up to $25\%$ compared to the previously popular equirectangular projection~\cite{fb2016compressionrate}.

\begin{figure}[t]
    \center
    \includegraphics[width=1.\linewidth]{./Figures/approach.pdf}
    \caption{
        Our approach learns to automatically rotate the $360\degree$ video axis before storing the video in cubemap format.
        While the $360\degree$ videos are equivalent under rotation (``isomers''),
        the bit-streams are not because of the video compression procedures.  Our approach analyzes the video's visual content to predict its most compressible isomer.
        \label{fig:approach}
    }
\end{figure}

 
One unique property of $360\degree$ video is that each spherical video has an \emph{infinite number of equivalents related by a rotation.}
Therefore,
each $360\degree$ video could be transformed into multiple possible cubemaps by changing the orientation of the cube, yet all of them represent the very same video content.
We refer to these content-equivalent rotations as \emph{$360\degree$ isomers}.\footnote{Strictly speaking isomers are equivalent only theoretically, because pixels are discretely sampled and rotating a cubemap requires interpolating the pixels.  Nevertheless, as long as the pixel density, i.e.~video resolution, is high enough, the information delta is negligible.}
The isomers, however, are \emph{not} equivalents in terms of compression.  Different isomers interact differently with a given compression algorithm and so yield different compression rates (See Fig.~\ref{fig:approach}).
This is because the unwrapped cubemap is not a homogenous perspective image.
Therefore, some of the properties that current compression algorithms exploit in perspective images do not hold.
For example, while the content is smooth and continuous in perspective images,
this need not be true along an inter-face boundary in an unwrapped cubemap.
The discontinuity can introduce artificial high frequency signals and large abrupt motions,
both of which harm the compression rate (cf.~Sec.~\ref{sub:data_analysis} and Fig.~\ref{fig:concept}).
In short, our key insight is that the compression rate of a $360\degree$ video will depend on the orientation of the cubemap it is projected on.

We propose a learning-based approach to predict---from the video's visual content itself---the cubemap orientation that will minimize the video size.
First we demonstrate empirically that the orientation of a cubemap does influence the compression rate,
and the difference is not an artifact of a specific encoder but a general property over a variety of popular video formats.
Based on that observation,
we propose to automatically re-orient the cubemap for every group of pictures (GOP).\footnote{a collection of successive pictures within a coded video stream.}
A naive solution would enumerate each possible orientation, compress the GOP, and pick the one with the lowest encoded bit-stream size.
However, doing so would incur substantial overhead during compression, prohibitively costly for many settings.
Instead, our approach renders the GOP for a \emph{single} orientation after predicting the optimal orientation from the video clip rendered in its canonical orientation.
Given encoded videos in a fixed orientation,
we train a Convolutional Neural Network (CNN) that takes both the 
segmentation contours and motion vectors
in the encoded bit-stream and predicts the orientation that will yield the minimum video size.
By avoiding rendering and encoding the video clip in all possible orientations,
our approach greatly reduces the computational cost and strikes a balance between speed and compression rate.


The key benefit of our approach is a higher compression rate for $360\degree$ video that requires only to re-render the cubemap.  In particular, our idea does not require changing the video format nor the compression algorithm,
which makes it fully compatible with any existing video codec.
This is especially important in the realm of video compression,
because a new video format often takes years to standardize and deploy,
and so changing the bit-stream format would incur very high overhead.
The only additional information that our method needs to encode is the selected orientation of each GOP,
which can easily be encoded as meta data (and may become part of the standard in the future~\cite{omaf2017wd}).


We evaluate our approach on 7,436 clips containing varying content.
We demonstrate our idea has consistent impact across three popular encoders,
with video size reductions up to 76\% and typical reductions of about 8\%.
Across all videos, our learning approach achieves on average 82\% of the best potential compression rate available for all feasible isomers.



\section{Related Work}

\begin{figure*}[t]
    \center
    \includegraphics[width=\linewidth]{./Figures/cubemap_illustration.pdf}
    \caption{
        Cubemap format transformation.
        The $360\degree$ video is first projected to a cube enclosing the unit sphere and then unwrapped into 6 faces.
        The 6 faces are re-arranged to form a rectangular picture to fit video compression standards ($2{\times}3$ frame on the right).
    }
    \label{fig:faces}
\end{figure*}\paragraph{360$\degree$ video analysis}\label{par:_360degree_video}
Recent research explores ways to improve the user experience of watching $360\degree$ videos,
including stabilizing the videos~\cite{kasahara2015first,kopf2016tog,kamali2011stabilizing} or directing the field-of-view (FOV) automatically~\cite{su2016accv,su2017cvpr,hu2017deep,lai2017semantic}.
Other works study visual features in $360\degree$ images such as detecting SIFT~\cite{hansen2007scale} or learning a CNN either from scratch~\cite{khasanova2017graph,cohen2017convolutional} or from an existing model trained on ordinary perspective images~\cite{su2017nips}.
All of these methods offer new applications of $360\degree$ videos, and they assume the inputs are in some given form, e.g., equirectangular projection.
In contrast, we address learning to optimize the data format of $360\degree$ video, which can benefit many applications.

\paragraph{360$\degree$ video compression}$360\degree$ video has sparked initial interest in new video compression techniques.
A Call for Evidence this year for a meeting on video standards~\cite{cfe2017JVET} calls attention to the need for compression techniques specific to $360\degree$ video,
and responses indicate that substantial improvement can be achieved in test cases~\cite{JVET-G0023,JVET-G0024,JVET-G0025,JVET-G0026}.
Whereas these efforts aim for the next generation in video compression standards,
 our method is compatible with existing video formats and can be applied directly without any modification of existing video codecs.
For video streaming, some work studies the value in devoting more bits to the region of $360\degree$ content currently viewed by the user~\cite{sanchez2015panohevc,sreedhar2016adaptive}.

\paragraph{Projection of spherical images}\label{par:video_format}$360\degree$ image projection has long been studied in the field of map projection.
As famously proven by Gauss, no single projection can project a sphere to a plane without introducing some kind of distortion.
Therefore, many different projections are proposed,
each designed to preserve certain properties such as distance, area, direction, etc.~\cite{snyder1987map}.
For example, the popular equirectangular projection preserves the distance along longitude circles.
Various projection models have been developed to improve perceived quality for $360\degree$ images.
Prior work~\cite{zelnik2005squaring} studies how to select or combine the projections for a better display, and others develop new projection methods to minimize visual artifacts~\cite{kim-iccv2017,chang-iccv2013}.
Our work is not about the human-perceived quality of a projected $360\degree$ image; rather, the mode of projection is relevant to our problem only in regards to how well the resulting stack of 2D frames can be compressed.


Cubemap is adopted as one of the two presentations for $360\degree$ video in the MPEG Omnidirectional MediA Format (OMAF)~\cite{mpeg120},
which is likely to become part of future standards,
and major $360\degree$ video sharing sites such as YouTube and Facebook have turned to the new format~\cite{fb2015cubemap,google2017eac}.
Cubemaps can improve the compression rate by $25\%$ compared to equirectangular projection, which suffers from redundant pixels and distorted motions~\cite{fb2016compressionrate}.
The Rotated Sphere Projection is an alternative to cubemap projection with fewer discontinuous boundaries~\cite{adeel2017rsp}.
Motivated by the compression findings~\cite{fb2016compressionrate}, our approach is built upon the standard cubemap format.
Our method is compatible with existing data formats and can offer a further reduction of video size at almost zero cost.

\paragraph{Deep learning for image compression}\label{par:deep_learning_for_compression}

Recent work investigates ways to improve image compression using deep neural networks.
One common approach is to improve predictive coding using either a feed-forward CNN~\cite{santurkar2017generative,rippel2017real} or recurrent neural network (RNN)~\cite{toderici2015variable,toderici2016full,johnston2017improved}.
The concept can also be extended to video compression~\cite{santurkar2017generative}.
Another approach is to allocate the bit rate dynamically using a CNN~\cite{li2017learning}.
While we also study video compression using a CNN,
we are the first to study $360\degree$ video compression, and---CNN or otherwise---the first to exploit spherical video orientation to improve compression rates.
Our idea is orthogonal to existing video compression algorithms, which could be combined with our approach without any modification to further improve performance.



\section{Cubemap Orientation Analysis}
\label{sec:analysis}

Our goal is to develop a computationally efficient method that exploits a cubemap's orientation for better compression rates.
In this section, we perform a detailed analysis on the correlation between encoded video size and cubemap orientation.
The intent is to verify that orientation is indeed important for $360\degree$ video compression.
We then introduce our method to utilize this correlation in Sec.~\ref{sec:approach}.


First we briefly review fundamental video compression concepts, which will help in understanding where our idea has leverage.  
Modern video compression standards divide a video into a series of groups of pictures (GOPs), 
which can be decoded independently to allow fast seeking and error recovery.
Each GOP starts with an \emph{I-frame},
or intra-coded picture,
which is encoded independently of other frames like a static image.
Other frames are encoded as inter-coded pictures,
and are divided into rectangular blocks.
The encoder finds a reference block in previous frames for each block that minimizes their difference.
Instead of encoding the pixels directly,
the encoder encodes the relative location of the reference block,
i.e.,~the \emph{motion vector},
and the residual between the current and reference block.
This inter-frame prediction allows encoders to exploit temporal redundancy in the video.
Note that the encoder has the freedom to fall back to intra-coding mode for blocks in an inter-coded frame if no reference block is found.

Just like static image compression, 
the encoder performs transform coding by transforming the pixels in I-frames and residuals in inter-coded frames into the frequency domain and encoding the coefficients.
The transformation improves the compression rate because high frequency signals are usually few in natural images,
and many coefficients will be zero.
To further reduce the video size,
video compression formats also exploit spatial redundancy through intra-prediction,
which predicts values to be encoded using adjacent values that are previously encoded.
The encoder will encode only the residual between the prediction and real value.
This applies to both the motion vector and transformed coefficients encoding.
Most of the residuals will be small and can be encoded efficiently using entropy coding.
For a more complete survey, see~\cite{videocompression}.


\subsection{Data Preparation}
\label{sub:data_collection}


To study the correlation between cubemap orientation and compression rate,
we collect a $360\degree$ video dataset from YouTube.
Existing datasets~\cite{su2016accv,hu2017deep} contain videos with arbitrary quality, many with 
compression artifacts that could bias the result.
Instead, we collect only high quality videos using the 4K filter in YouTube search.
We use the keyword ``360 video'' together with the $360\degree$ filter to search for videos and manually filter out those consisting of static images or CG videos.
The dataset covers a variety of video content and recording situations,
including but not limited to aerial, underwater, sports, animal, news, and event videos,
and the camera can be either static or moving.
We download the videos in equirectangular projection with 3,840 pixels width encoded in H264 high profile.

\begin{table}[t]
    \small
    \tabcolsep=0.12cm
    \center
    \begin{tabular}{llccc}
    \toprule
                                   &        & H264            & HEVC           & VP9\\
    \midrule
        \multirow{2}{*}{Video $r$ (\%)} & Avg.   & $8.43 \pm 2.43$ & $8.11 \pm 2.03$ & $7.83 \pm 2.34$\\
                                   & Range  & [4.34, 15.18]   & [4.58, 13.67]   & [3.80, 14.72]\\
    \midrule
        \multirow{2}{*}{Clip $r$ (\%)}  & Avg.   & $10.37 \pm 8.79$& $8.88 \pm 8.23$ & $9.78 \pm 8.62$\\
                                   & Range  & [1.08, 76.93]   & [1.40, 74.95]   & [1.70, 75.84]\\
    \bottomrule
    \end{tabular}
    \caption{
        Achievable video size reduction through rotation for each of three encoders.
        We can reduce the video size by up to $76\%$ by optimally changing the cubemap orientation.
    }
    \label{tab:encoders}
\end{table}

We next transcode the video into cubemap format and extract the video size in different orientations.
Because it is impossible to enumerate all possible cubemap orientations over time,
we discretize the problem by dividing the video into 2 second clips and encode each clip independently.
This is compliant with the closed GOP structure,
except that video codecs usually have the flexibility to adjust the GOP length within a given range.
For example, the default x264 encoder limits the GOP length between 25-250 frames,
i.e.~roughly 1-10 seconds,
and a common constraint for Blu-ray videos is 1-2 seconds~\cite{bluray}.
This results in a dataset consisting of 7,436 video clips from 80 videos with 4.2 hours total length.

For each clip, we sample the cubemap orientation
\begin{equation}
    \Omega = (\phi, \theta) \in \Phi \times \Theta
\end{equation}
with different yaw ($\phi$) and pitch ($\theta$) in
 $\Theta = \Phi = \{-45\degree, -40\degree, \cdots, 45\degree\}$,
i.e.,~every $5\degree$ between $[-45\degree,~45\degree]$.
This yields $|\Phi \times \Theta| = 361$ different orientations.
We restrict the orientation within $90\degree$ because of the rotational symmetry along each axis.

For each orientation,
we transform the video into cubemap format using the transform360 filter\footnote{\url{https://github.com/facebook/transform360}} in FFMPEG released by Facebook with 960 pixels resolution for each face.
Fig.~\ref{fig:faces} illustrates the transformation.
The video is then encoded using off-the-shelf encoders.
We encode the video into three popular formats---H264 using x264\footnote{\url{https://www.videolan.org/developers/x264.html}},
HEVC using x265\footnote{\url{http://x265.org}},
and VP9 using libvpx\footnote{\url{https://chromium.googlesource.com/webm/libvpx/}}.
Among them, H264 is currently the most common video format.
HEVC, also known as H265, is the successor of H264 and is the latest video compression standard.
VP9 is a competitor of HEVC developed by Google and is most popular in web applications.
We use lossless compression for all three formats to ensure rotational symmetry and extract the size of the final encoded bit-stream.
See supp.~for the exact encoding parameters.
Note that we use popular open source tools for both cubemap rendering and video compression to ensure that they are well optimized and tested.  This way any size changes we observe can be taken as common in $360\degree$ video production instead of an artifact of our implementation.



\subsection{Data Analysis}
\label{sub:data_analysis}

\begin{figure}[t]
    \center
    \includegraphics[width=1.\linewidth]{./Figures/size_distribution.pdf}
    \caption{
        \label{fig:size_distribution}
        Relative clip size distribution w.r.t. $\Omega$.
        We cluster the distribution into 16 clusters and show 8 of them.
    }
\end{figure}\begin{figure}[t]
    \centering
    \includegraphics[trim={1cm 0 0 0},clip,width=\linewidth]{./Figures/cubemap_distribution.pdf}
    \caption{
        Clip size distribution of a single clip.
        We also show the cubemaps corresponding to $\Omega_{max}/\Omega_{min}$.
    }
    \label{fig:example_cube}
\end{figure}

Next we investigate how much and why the orientation of an isomer matters for compressibility.
If not mentioned specifically, all the results are obtained from H264 fromat.

\paragraph{Achievable video size reduction}
We first examine the size reduction we can achieve by changing the cubemap orientation.
In particular, we compute the \emph{reduction}\begin{equation}
    r = 100 \times \frac{S_{\Omega^{max}}-S_{\Omega^{min}}}{S_{\Omega^{max}}},
\end{equation}
where $S_{\Omega}$ is the encoded bit-stream size with orientation $\Omega$ and $\Omega^{max}$/$\Omega^{min}$ corresponds to the orientation with maximum/minimum bit-stream size.

\begin{figure*}[t]
    \center
    \begin{subfigure}[c]{0.305\linewidth}
        \centering
        \includegraphics[width=\textwidth]{./Figures/static_concept.pdf}
        \caption{
            Content discontinuity.
        }
    \end{subfigure}
    ~~~
    \begin{subfigure}[c]{0.65\linewidth}
        \centering
        \includegraphics[width=\textwidth]{./Figures/dynamic_concept.pdf}
        \caption{
            Motion discontinuity.
        }
    \end{subfigure}
    \caption{
        \label{fig:concept}
        Explanations for why different $\Omega$ have different compression rate, shown for good ($\Omega_{min}$) and bad ($\Omega_{max}$) rotations.
        (a) From a static picture perspective,
        some $\Omega$ introduce content discontinuity and reduce spatial redundancy.
        (b) From a dynamic picture perspective,
        some $\Omega$ make the motion more disordered and break the temporal redundancy.
    }
\end{figure*}\begin{figure*}[t]
    \centering
    \includegraphics[trim={1cm 0 0 0},clip,width=\linewidth]{./Figures/concept_example.pdf}
    \caption{
        \label{fig:concept_example}
        Real examples for the explanations in Fig.~\ref{fig:concept}.
        Example (A) shows content discontinuity introduced by rotation.
        Example (B) shows motion discontinuity.
        The encoder fails to find reference blocks in this example,
        and the number of intra-coded blocks increases.
    }
\end{figure*}

Table~\ref{tab:encoders} shows the results.  
For example, the average video size reduction $\overline{r}$ is $8.43\%$ for H264,
which means that we can reduce the overall $360\degree$ video size by more than $8\%$ through rotating the video axis.
This corresponds to a 2GB reduction in our 80 video database and would scale to 25.3TB for a million video database.
The range of $r$ for each clip is $[1.08, 76.93]$,
which indicates that the compression rate is strongly content dependent,
and the size reduction can be up to $77\%$ for a single video if we allow the encoder to re-orient the $360\degree$ video.
If we restrict the rotation to $\phi$ and fix $\theta=0\degree$,
$\overline{r}$ will drop to $2.35\%$.
This result suggests that it is important to allow rotation along both axes.  
Finally we see that the average and range of reductions is quite similar across encoders, indicating that compressibility of isomers is not unique to a particular codec.

\paragraph{Video size distribution w.r.t. $\Omega$}

We next show the video size distribution with respect to $\Omega$.
We compute the \emph{normalized clip size}\begin{equation}
    \tilde{S}_{\Omega} = 100 \times \frac{S_{\Omega} - S_{\Omega^{min}}}{S_{\Omega^{max}}-S_{\Omega^{min}}}
    \label{eq:normalized_size}
\end{equation}
for every $\Omega$ and cluster the size distribution of each clip using K-Means.
Each cluster is represented by the nearest neighbor to the center.

Fig.~\ref{fig:size_distribution} shows the results.
We can see $\Omega^{min}$ lies on or near $\theta{=}0\degree$ in half the clusters.
In general, this corresponds to orienting the cubemap perpendicular to the ground such that the top face captures the sky and the bottom face captures the camera and ground.
See Fig.~\ref{fig:faces} for example.
The top and bottom faces tend to have smaller motion within the faces in these orientations,
and the compression rate is higher because the problem reduces from compressing six dynamic pictures to four dynamic pictures plus two near static pictures.  
However, $\theta{=}0\degree$ is not best for every clip, and there are multiple modes visible in Fig.~\ref{fig:size_distribution}.
For example, the minimum size occurs at $\theta{=}\phi{=}45\degree$ in Fig.~\ref{fig:example_cube}.
Therefore, again we see it is important to allow two-dimensional rotations.

\paragraph{Reasons for the compression rate difference}

Why does the video size depend on $\Omega$?  
The fundamental reason is that all the video compression formats are designed for perspective images and heavily exploit the image properties.
The unwrapped cubemap format is a perspective image only locally within each of the six faces.
The cubemap projection introduces perspective distortion near the face boundaries and artificial discontinuities across face boundaries,
both of which make the cubemap significantly different from perspective images and can degrade the compression rate.
Because the degradation is content dependent,
different orientations result in different compression rates.

More specifically,
the reasons for the compression rate difference can be divided into two parts.
From the static image perspective,
artificial edges may be introduced if continuous patterns fall on the face boundary.
See Fig.~\ref{fig:concept} (a) and Fig.~\ref{fig:concept_example} for examples.
The edges introduce additional high frequency signals and reduce the efficiency of transform coding.
Furthermore,
the single continuous patch is divided into multiple patches that are dispersed to multiple locations in the image.
This reduces the spatial redundancy and breaks the intra-prediction.

From the dynamic video perspective,
the face boundaries can introduce abrupt jumps in the motion.
If an object moves across the boundary,
it may be teleported to a distant location on the image.
See Fig.~\ref{fig:concept} (b) and Fig.~\ref{fig:concept_example} for examples.
The abrupt motion makes it difficult to find the reference block during encoding,
and the encoder may fall back to intra-coding mode which is much less efficient.
Even if the encoder successfully finds the reference block,
the motion vectors would have very different magnitude and direction compared to those within the faces,
which breaks intra-prediction.
Finally,
because the perspective distortion is location dependent,
the same pattern will be distorted differently when it falls on different faces,
and the residual of inter-frame prediction may increase.
The analysis applies similarly across the three formats, which makes sense, since their compression strategies are broadly similar.

\paragraph{Video size correlation across formats}

Next we verify the correlation between video size and orientation is not an artifact of the specific video format or encoder.
We first compare the size reduction that can be achieved through rotation using different encoders (Table \ref{tab:encoders}).  
We can clearly see that the dependency between the compression rate and $\Omega$ is not the consequence of a specific video encoder.
Instead, it is a common property across current video compression formats.
Although different encoders have different compression rate improvements,
the differences are relatively minor,
which indicates that the problem cannot be solved by simply using a more advanced video compression standard designed for ordinary perspective images.

We further test the correlation between video size of different encoders.
We first compute the relative size of every orientation as
\begin{equation}
    S^{\prime}_{\Omega} = S_{\Omega} - S_{0,0},
    \label{eq:relative_size}
\end{equation}
where $S_{0,0}$ denotes the non-rotated source video, and then compute the correlation between encoders for each $\Omega$.
We report the average correlation across all $\Omega$ in Table~\ref{tab:correlations}.
The high correlation again verifies that the correlation between $\Omega$ and video size is common across video formats.

\begin{table}[t]
    \small
    \center
    \begin{tabular}{lccc}
    \toprule
    Encoders & H264 / H265 & H264 / VP9 & H265 / VP9\\
    \midrule
        Avg. $\rho$ & 0.8757  & 0.9533 & 0.8423 \\
    \bottomrule
    \end{tabular}
    \caption{
        The correlation of relative video sizes across video formats.
        The high correlation indicates that the dependency between video size and $\Omega$ is common across formats.
    }
    \label{tab:correlations}
\end{table}

\section{Approach}
\label{sec:approach}

\begin{figure*}[t]
    \center
    \includegraphics[width=1.\linewidth]{./Figures/context_model.pdf}
    \caption{
        Our model takes a video clip as input and predicts $\Omega^{min}$ as output.
        (A) It first divides the video into 4 segments temporally and (B) extracts appearance and motion features from each segment.
        (C) It then concatenates the appearance and motion feature maps and feeds them into a CNN.
        (D) The model concatenates the outputs of each segment together and joins the output with the input feature map using skip connections to form the video feature.
        (F) It then learns a regression model that predicts the relative video size $S^{\prime}_{\Omega}$ for all $\Omega$ and takes the minimum one as the predicted optimally compressible isomer.
    }
    \label{fig:model}
\end{figure*}

In this section,
we introduce our approach for improving $360\degree$ video compression rates by predicting the most compressible isomer.
Given a $360\degree$ video clip,
our goal is to identify $\Omega^{min}$ to minimize the video size.
A naive solution is to render and compress the video for all possible $\Omega$ and compare their sizes.
While this guarantees the optimal solution,
it introduces a significant computational overhead, i.e.,~360 times more computation than encoding the video with a fixed $\Omega$.
For example,
it takes more than 15 seconds to encode one single clip using the default x264 encoder on a 48 core machine with Intel Xeon E5-2697 processor,
which corresponds to $15s \times 360 \approx 1.5$ hours for one clip if we try to enumerate $\Omega$.
Moreover, the computational cost will grow quadratically if we allow more fine-grained control.
Therefore, enumerating $\Omega$ is not practical.

Instead, we propose to predict $\Omega^{min}$ from the raw input without rerendering the video.
Given the input video in cubemap format,
we extract both motion and appearance features (details below) and feed them into a CNN that predicts the video size $S_{\Omega}$ for each $\Omega$,
and the final prediction of the model is
\begin{equation}
    \Omega^{min} = \argmin_{\Omega} S_{\Omega}.
\end{equation}
See Fig.~\ref{fig:model}.
The computational cost remains roughly the same as transcoding the video because the prediction takes less than a second,
which is orders of magnitude shorter than encoding the video and thus negligible.  
Since no predictor will generalize perfectly, there is a chance of decreasing the compression rate in some cases.
However, experimental results show that it yields very good results and strikes a balance between computation time and video size.

Because our goal is to find $\Omega^{min}$ for a given video clip,
exact prediction of $S_{\Omega}$ is not necessary.
Instead,
the model predicts the relative video size $S^{\prime}_{\Omega}$ from Eq.~\ref{eq:relative_size}.
The value $S^{\prime}_{\Omega}$ is scaled to $[0, 100]$ over the entire dataset to facilitate training.
We treat it as a regression problem and learn a model that predicts $361$ real values using L2 loss as the objective function.
Note that we do not predict $S_{\Omega}$ in Eq.~\ref{eq:normalized_size} because it would amplify the loss for clips with smaller size,
which may be harmful for the absolute size reduction.

We first divide the input video into 4 equal length segments.
For each segment,
we extract the appearance and motion features for each frame and average them over the segment.
For appearance features,
we segment the frame into regions using SLIC~\cite{achanta2012slic} and take the segmentation contour map as feature.
The segmentation contour represents edges in the frame,
which imply object boundaries and high frequency signals that take more bits in video compression.

For motion features,
we take the motion vectors directly from the input video stream encoding, as opposed to computing optical flow. 
The motion vectors are readily available in the input and thus this saves computation.
Furthermore, motion vectors provide more direct information about the encoder.
Specifically,
we sample one motion vector every 8 pixels and take both the forward and backward motion vectors as the feature.
Because each motion vector consists of both spatial and temporal displacement,
this results in a 6-dimensional feature.
For regions without a motion vector, we simply pad 0 for the input regardless of the encoding mode.
We concatenate the appearance and motion feature to construct a feature map with depth 7.
Because the motion feature map has lower resolution than the video frame,
we downscale the appearance feature map by 8 to match the spatial resolution.
The input resolution of each face of the cube map is therefore $960/8=160$ pixels.

The feature maps for each segment are then fed into a CNN and concatenated together as the video feature.
We use the VGG architecture~\cite{simonyan2014very} except that we increase the number of input channels in the first convolution layer.
Because fine details are important in video compression,
we use skip connections to combine low level information with high level features, following models for image segmentation~\cite{long2015fully}.
In particular,
we combine the input feature map and final convolution output as the segment feature after performing 1x1 convolution to reduce the dimension to 4 and 64 respectively.
The video feature is then fed into a fully-connected layer with $361$ outputs as the regression model.
Note that we remove the fully-connected layers in the VGG architecture to keep the spatial resolution for the regression model and reduce model size.

Aside from predicting $S_{\Omega}$, in preliminary research 
we also tried other objective functions such as regression for $\Omega^{min}$ directly or predicting $\Omega^{min}$ from the 361 possible $\Omega$ with $361$-way classification,
but none of them perform as well as the proposed approach.
Regressing $\Omega^{min}$ often falls back to predicting $(\theta, \phi) = (0, 0)$ because the distribution is symmetric.
Treating the problem as $361$-way classification has very poor accuracy because the number of training data is small and imbalanced.
We also examined 3D convolution instead of explicitly feeding the motion information as input,
but we find that 3D convolution is hard to train and performs worse than 2D convolution.



\section{Experiments}
\label{sec:experiments}


To evaluate our method,
we compute the size reduction it achieves on the $360\degree$ video dataset introduced in Sec.~\ref{sec:analysis}.

\paragraph{Baselines}\label{par:baselines}

Because we are the first to study how to predict the cubemap orientation for better compression,
we compare our method with the following two heuristics:
\begin{itemize}[leftmargin=*,label=$\bullet$]
    \item \textsc{Random} --- Randomly rotate the cubemap to one of the 361 orientations.
        This represents the compression rate when we have no knowledge about the video orientation.
    \item \textsc{Center} --- Use the orientation provided by the videographer.
        This is a strong prior, usually corresponding to the direction of the videographer's gaze or movement and lying on the horizon of the world coordinate.
\end{itemize}\paragraph{Evaluation metrics}\label{par:evaluation_metrics}

We compare each method using the normalized size reduction $\tilde{r} = 1 - \tilde{S}$ for each video.
Specifically, we compute the largest full-video size by choosing $\Omega^{max}$ for every clip and sum the clip sizes.
Similarly, we compute the minimum video size.
Given the predicted orientation for each clip,
we compute the video size by rotating the cubemap by the predicted orientation.
The result indicates the fraction of reduction the method achieves compared to the optimal result.

To train and test the model,
we divide the dataset into 4 folds, each containing 20 videos.
Three are used for training, and the other is used for testing.
We report the average result over 4 folds as the final performance.

\paragraph{Implementation details}\label{par:implementation_details}

We initialize the weights using an ImageNet pre-trained VGG model provided by the authors~\cite{simonyan2014very}.
For the first layer, we replicate the weights of the original network to increase the number of input channels.
Weights that are not in the original model are randomly initialized using Xavier initialization~\cite{glorot2010initialization}.
We train the model using ADAM~\cite{kingma2014adam} for 4,000 iterations with batch size 64 parallelized to 16 GPUs.
The base learning rate is initialized to $1.0 \times 10^{-3}$ and is decreased by a factor of 10 after 2,000 iterations.
We also apply $L_{2}$ regularization with the weight set to $5.0 \times 10^{-4}$ and use dropout for the fully-connected layers with ratio 0.5.
For SLIC,
we segment each face of the cubemap independently into 256 superpixels with compactness $m{=}1$.
The low compactness value leads to more emphasis on the color proximity in the superpixels.



\subsection{Results}
\label{sub:results}


We first examine the size reduction our method achieves.
Table~\ref{tab:size_reduction} shows the results.  
Our method performs better than the baselines in all video compression formats by $7\%-16\%$.
The improvement over the baseline is largest in HEVC,
which indicates that the advantage of our approach will become more significant as HEVC gradually replaces H264.
Interestingly,
the \textsc{Center} baseline performs particularly worse in HEVC.
The reason is that HEVC allows the encoder to achieve good compression rates in more diversed situations,
so the distribution of $\Omega^{min}$ becomes more dispersed.
The result further shows the value in considering cubemap orientation during compression as more advanced video codecs are used.
While there remains a $20\%$ room for improvement compared to the optimal result (as ascertained by enumerating $\Omega$),
our approach is significantly faster and takes less than $0.3\%$ the computation.

\begin{table}[t]
    \small
    \center
    \begin{tabular}{lccc}
    \toprule
    & H264 & HEVC & VP9\\
    \midrule
    \textsc{Random} & 50.75 & 51.62 & 51.20 \\
    \textsc{Center} & 74.35 & 63.34 & 72.92 \\
    \midrule
    \textsc{Ours}   & 82.10 & 79.10 & 81.55\\
    \bottomrule
    \end{tabular}
    \caption{
        Size reduction of each method.
        The range is $[0, 100]$, the higher the better.
    }
    \label{tab:size_reduction}
\end{table}\begin{figure}[t]
    \center
    \begin{subfigure}[c]{0.31\linewidth}
        \centering
        \includegraphics[trim={1.2cm 0 0 0},clip,width=\textwidth]{./Figures/qualitative/2OzlksZBTiA-seg065-size.pdf}
    \end{subfigure}
    \begin{subfigure}[c]{0.33\linewidth}
        \centering
        \includegraphics[width=\textwidth]{./Figures/qualitative/2OzlksZBTiA-seg065-min-1-enc.jpg}
    \end{subfigure}
    \begin{subfigure}[c]{0.33\linewidth}
        \centering
        \includegraphics[width=\textwidth]{./Figures/qualitative/2OzlksZBTiA-seg065-min-2-enc.jpg}
    \end{subfigure}
    \\
    \begin{subfigure}[c]{0.31\linewidth}
        \centering
        \includegraphics[trim={1.2cm 0 0 0},clip,width=\textwidth]{./Figures/qualitative/VI4sPuwvOVk-seg101-size.pdf}
    \end{subfigure}
    \begin{subfigure}[c]{0.33\linewidth}
        \centering
        \includegraphics[width=\textwidth]{./Figures/qualitative/VI4sPuwvOVk-seg101-min-1-enc.jpg}
    \end{subfigure}
    \begin{subfigure}[c]{0.33\linewidth}
        \centering
        \includegraphics[width=\textwidth]{./Figures/qualitative/VI4sPuwvOVk-seg101-min-1-enc.jpg}
    \end{subfigure}
    \\
    \begin{subfigure}[c]{0.31\linewidth}
        \centering
        \includegraphics[trim={1.2cm 0 0 0},clip,width=\textwidth]{./Figures/qualitative/ftJlHsqSr9A-seg021-size.pdf}
    \end{subfigure}
    \begin{subfigure}[c]{0.33\linewidth}
        \centering
        \includegraphics[width=\textwidth]{./Figures/qualitative/ftJlHsqSr9A-seg021-min-1-enc.jpg}
    \end{subfigure}
    \begin{subfigure}[c]{0.33\linewidth}
        \centering
        \includegraphics[width=\textwidth]{./Figures/qualitative/ftJlHsqSr9A-seg021-min-2-enc.jpg}
    \end{subfigure}
    \\
    \begin{subfigure}[c]{0.31\linewidth}
        \centering
        \includegraphics[trim={1.2cm 0 0 0},clip,width=\textwidth]{./Figures/qualitative/sPyAQQklc1s-seg041-size.pdf}
    \end{subfigure}
    \begin{subfigure}[c]{0.33\linewidth}
        \centering
        \includegraphics[width=\textwidth]{./Figures/qualitative/sPyAQQklc1s-seg041-min-1-enc.jpg}
    \end{subfigure}
    \begin{subfigure}[c]{0.33\linewidth}
        \centering
        \includegraphics[width=\textwidth]{./Figures/qualitative/sPyAQQklc1s-seg041-min-2-enc.jpg}
    \end{subfigure}
    \\
    \caption{
        Qualitative examples.
        The heatmap shows the normalized reduction, and the overlaid circle shows our predicted result.
        The two images are the first and last frame of the clip rendered in the predicted orientation.
        Last row shows a failure example.
        Best viewed in color.
    }
    \label{fig:qualitative}
\end{figure}

Fig.~\ref{fig:qualitative} shows example prediction results.
Our approach performs well despite the diversity in the video content and recording situation.
The complexity in the content would make it hard to design a simple rule-based method to predict $\Omega^{min}$
(such as analyzing the continuity in Fig.~\ref{fig:concept_example}); a learning based method is necessary.
The last row shows a failure case of our method,
where the distribution of video size is multimodal,
and the model selects the suboptimal mode.


We next examine whether the model can be transferred across video formats,
e.g.~can the model trained on H264 videos improve the compression rate of HEVC videos?
Table~\ref{tab:transcode} shows the results.  
Overall, the results show our approach is capable of generalizing across video formats given common features.  We find that the model trained on H264 is less transferable,
while the models trained on HEVC and VP9 perform fairly well on H264.
In particular, the model trained on HEVC performs the best across all formats.
The reasons are twofold.
First, the models trained on HEVC and VP9 focus on the appearance feature which is common across all formats.
Second, the models trained on H264 suffer more from overfitting because the distribution of $\Omega^{min}$ is more concentrated.

\begin{table}[t]
    \small
    \center
    \begin{tabular}{cccccc}
    \toprule
        \multicolumn{2}{c}{H264} & \multicolumn{2}{c}{HEVC} & \multicolumn{2}{c}{VP9}\\
        \cmidrule(lr){1-2} \cmidrule(lr){3-4} \cmidrule(lr){5-6}
        HEVC & VP9 & H264 & VP9 & H264 & HEVC\\
        \midrule
        70.82 & 78.17 & 85.79 & 84.61 & 83.19 & 75.16 \\
    \bottomrule
    \end{tabular}
    \caption{
        Size reduction of our approach. Top row indicates training source, second row is test sources.
    }
    \label{tab:transcode}
\end{table}\begin{figure}[t]
    \center
    \begin{subfigure}[c]{0.48\linewidth}
        \centering
        \includegraphics[trim={0.4cm 0 0 0},clip,width=.98\textwidth]{./Figures/prediction_distribution.pdf}
        \caption{
            Predicted $\Omega^{min}$ (H264).
        }
    \end{subfigure}
    ~
    \begin{subfigure}[c]{0.48\linewidth}
        \centering
        \includegraphics[trim={0.4cm 0 0 0},clip,width=.98\textwidth]{./Figures/min_distribution.pdf}
        \caption{
            Real $\Omega^{min}$ of H264.
        }
    \end{subfigure}
    \begin{subfigure}[c]{0.48\linewidth}
        \centering
        \includegraphics[trim={0.4cm 0 0 0},clip,width=.98\textwidth]{./Figures/prediction_distribution_hevc.pdf}
        \caption{
            Predicted $\Omega^{min}$ (HEVC).
        }
    \end{subfigure}
    ~
    \begin{subfigure}[c]{0.48\linewidth}
        \centering
        \includegraphics[trim={0.4cm 0 0 0},clip,width=.98\textwidth]{./Figures/min_distribution_hevc.pdf}
        \caption{
            Real $\Omega^{min}$ of HEVC.
        }
    \end{subfigure}
    \caption{
        Distribution of $\Omega^{min}$ (\%).
        Predictions are on H264 videos with different training data.
    }
    \label{fig:prediction_distribution}
\end{figure}

The distribution of $\Omega^{min}$ provides further insight into the advantage of the model trained on HEVC.
See Fig.~\ref{fig:prediction_distribution}.
The predicted $\Omega^{min}$ tend to be more concentrated around $\theta{=}0$ than the real $\Omega^{min}$.
Because the distribution of $\Omega^{min}$ is more dispersed in HEVC,
so is the prediction of $\Omega^{min}$ by the model trained on HEVC.



\section{Conclusion}


This work studies how to improve $360\degree$ video compression by selecting a proper orientation for cubemap projection.
Our analysis across 3 popular codecs shows scope for reducing video sizes by up to $76\%$ through rotation,
with an average of more than $8\%$ over all videos.
We propose an approach that predicts the optimal orientation given the video in a single orientation.
It achieves $82\%$ the compression rate of the optimal orientation while requiring less than $0.3\%$ of the computation of a non-learned solution (fraction of a second vs.~1.5 hours per GOP).
Future work will explore how to combine orientation search and prediction to reach better compression rates under a computational budget.






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle




\appendices
\section{Compression Parameters}

We encode the videos using x264/x265/libvpx through FFMPEG.
The compression parameters of FFMPEG for each encoder are as follows.
\begin{itemize}[leftmargin=*,label=$\bullet$]
    \item x264 --- ``-preset medium -crf 0 -an''
    \item x265 --- ``-preset medium -x265-params lossless=1 -crf 0 -an''
    \item libvpx --- ``-speed 4 -cpu-used 4 -lossless 1 -qmin 0 -qmax 0 -an''
\end{itemize}
The ``-an'' option disables audio in the output bit-stream.
The ``-preset medium'' in x264/x264 and ``-speed 4 -cpu-used 4'' controls the encoding speed.
We use the default setting for x264/x265 which provides a reasonable balance between speed and compression rate.
Other options specify lossless compression for each encoder.
For the transform360 filter,
we use bicubic interpolation for pixel values.

\section{Rotational Symmetry}

We justify our design of restricting the rotation within $90\degree$.
We compute the correlation between cubemap size related by $90\degree$ rotation along either $\theta$ or $\phi$.
In order to do so,
we compute the size of cubemaps with $\{\pm60\degree, \pm75\degree, \pm90\degree\}$ rotation along either pitch or yaw and compare them with those with rotation within $[-45\degree, 45\degree]$.
The correlations are in Table~\ref{tab:rotation}.
We also show the correlations for $45\degree$ rotation for comparison.
The strong correlation clearly shows that the cubemap sizes are indeed symmetric to $90\degree$ rotation.

\begin{table}[h]
    \small
    \center
    \begin{tabular}{cccc}
    \toprule
    Encoders & H264 & HEVC & VP9\\
    \midrule
        $90\degree$ & 1.00  & 0.95 & 0.99\\
        $45\degree$ & 0.25  & 0.23 & 0.23 \\
    \bottomrule
    \end{tabular}
    \caption{
        Correlation between cubemap sizes related by $90\degree$ and $45\degree$ rotation.
    }
    \label{tab:rotation}
\end{table}\section{Qualitative Examples}

In this section, we show more qualitative examples similar to Fig.~\ref{fig:qualitative} in the main paper.
See Fig.~\ref{fig:qualitative2} and \ref{fig:qualitative3}.
We can see that even small objects can affect the compression rate,
such as the rhinos in the first example and the diver in the second example.
The pattern doesn't even have to correspond to a real object like the blank region in the fourth example or the logo in the fifth example.
The fifth example also shows how the file size is affected by multiple factors jointly.
The distribution would be symmetric with respect to $\theta{=}0$ if the file size only depended on the logo,
but the sky and cloud lead to the additional mode at the top middle.
We also see that the continuity of foreground objects is not the only factor that matters from the sixth and seventh example;
the person in the sixth example and the pilot in the seventh example lie on the face boundary in the optimal orientation.
The result suggests that heuristics based on object location, either automatic or manual, do not solve the problem.
The eighth example shows that the compression would be more efficient if the motions fall in the same face even if it does not introduce discontinuity in motion.

\section{Failure Cases}

In this section, we show failure examples similar to Fig.~\ref{fig:qualitative} in the main paper.
See Fig.~\ref{fig:failures}.
In the first example,
we can see the best compression rate occurs when the coral is continuous,
while our method fails because it decides to keep the sun light (round white pattern) continuous.
In the second and third example,
the video size tends to be smaller when the horizon falls on the face diagonal,
possibly because it is more friendly for intra-prediction in compression.
Our method doesn't learn this tendency,
so it fails to predict the optimal $\theta$ and only predicts the correct $\phi$.

\begin{figure*}[t]
    \vspace{-4pt}
    \center
    \hspace{0.19\linewidth}
    ~
    \begin{subfigure}[c]{0.38\linewidth}
        \centering
        \caption{Ours}
    \end{subfigure}
    ~~~
    \begin{subfigure}[c]{0.38\linewidth}
        \centering
        \caption{\textsc{Center} baseline}
    \end{subfigure}
    \\
    \vspace{-12pt}
    \InsertQualitative{7IWp875pCxQ-seg042}\\
    \vspace{-1pt}
    \InsertQualitative{BbT_e8lWWdo-seg052}\\
    \vspace{-1pt}
    \InsertQualitative{sPyAQQklc1s-seg040}\\
    \vspace{-1pt}
    \InsertQualitative{2OzlksZBTiA-seg195}\\
    \vspace{-1pt}
    \InsertQualitative{fORNZZkQVFc-seg033}\\
    \vspace{-1pt}
    \InsertQualitative{yarcdW91djQ-seg117}\\
    \vspace{-1pt}
    \InsertQualitative{NdZ02-Qenso-seg102}\\
    \vspace{-1pt}
    \InsertQualitative{ftJlHsqSr9A-seg049}
    \\
    \vspace{-9pt}
    \caption{
        Qualitative results.
        Each row shows a clip.
        The first figure per row shows the size distribution.
        Black circle shows the predicted result, which is rendered in the second and third figures.
        The fourth and fifth figures show the \textsc{Center} baseline.
        The second and fourth figures show the first frame of the clip, and the third and fifth figures are the last frame.
    }
    \label{fig:qualitative2}
\end{figure*}\begin{figure*}[t]
    \vspace{-8pt}
    \center
    \hspace{0.19\linewidth}
    ~
    \begin{subfigure}[c]{0.38\linewidth}
        \centering
        \caption{Ours}
    \end{subfigure}
    ~~~
    \begin{subfigure}[c]{0.38\linewidth}
        \centering
        \caption{\textsc{Center} baseline}
    \end{subfigure}
    \\
    \vspace{-12pt}
    \InsertQualitative{Gvf3jbxdkug-seg009}
    \vspace{-4pt}
    \InsertQualitative{LbQJ1p6eaqA-seg082}
    \vspace{-4pt}
    \InsertQualitative{pwivE6bvD8w-seg019}
    \vspace{-4pt}
    \InsertQualitative{Wvm2h6iIHHM-seg024}
    \\
    \vspace{-8pt}
    \caption{
        Qualitative results (cont.).
    }
    \label{fig:qualitative3}
    \vspace{-6pt}
\end{figure*}\begin{figure*}[t]
    \center
    \hspace{0.19\linewidth}
    ~
    \begin{subfigure}[c]{0.38\linewidth}
        \centering
        \caption{Ours}
    \end{subfigure}
    ~~~
    \begin{subfigure}[c]{0.38\linewidth}
        \centering
        \caption{\textsc{Center} baseline}
    \end{subfigure}
    \\
    \vspace{-12pt}
    \InsertFailure{2OzlksZBTiA-seg063}
    \vspace{-4pt}
    \InsertFailure{ILOiq7gNnBo-seg037}
    \vspace{-4pt}
    \InsertFailure{mlOiXMvMaZo-seg014}
    \vspace{-4pt}
    \InsertFailure{ypnspTcgw3A-seg038}
    \\
    \vspace{-6pt}
    \caption{
        Failure cases.
        Blue circle shows true minimum and is rendered in the fourth and fifth figures.
        The two figures are the first and last frame of the clip.
    }
    \label{fig:failures}
    \vspace{-9pt}
\end{figure*}\clearpage

% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

\bibliographystyle{IEEEtran}
\bibliography{isomer360}



% that's all folks
\end{document}


