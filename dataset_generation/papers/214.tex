
\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

% \usepackage[nonatbib]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
%\usepackage[preprint, nonatbib]{nips_2018}
\usepackage{iclr2019_conference,times}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsmath}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[pdftex]{graphicx}
\usepackage{placeins}
\usepackage{color}



\newcommand{\dd}{{\mathrm{d}}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pdn}[3]{\frac{\partial^#1 #2}{\partial #3^#1}}
\newcommand{\od}[2]{\frac{\dd #1}{\dd #2}}
\newcommand{\odn}[3]{\frac{\dd^#1 #2}{\dd #3^#1}}
\newcommand{\avg}[1]{\left< #1 \right>}
\newcommand{\pp}[1]{\left( #1 \right)}
\newcommand{\mb}{\mathbf}
\newcommand{\mx}{\mathbf x}
\newcommand{\mc}{\mathcal}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\median}{\operatornamewithlimits{median}}
\newcommand{\norm}[1]{\left|\left| #1 \right|\right|}
\newcommand{\expect}[2]{\mathbbm{E}_{#1}\left[ #2 \right]}

%\usepackage[
%backend=biber,
%style=numeric-comp,
%sorting=none
%]{biblatex}
%\addbibresource{adv_reprog_2018.bib}

% New added packages
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{hyperref}

% New defined symbols

%\usepackage[colorinlistoftodos]{todonotes}
%\newcommand{\gamal}[1]{\todo[color=green!30]{GE: #1}}
%\newcommand{\ian}[1]{\todo[color=blue!30]{IG: #1}}
%\newcommand{\jcom}[1]{\todo[color=orange!30]{JS: #1}}

\usepackage{xcolor}
\definecolor{darkgreen}{rgb}{0,0.6,0}
\definecolor{darkred}{rgb}{0.7,0.0,0}
\definecolor{darkblue}{rgb}{0,0.0,0.6}
\newcommand{\gamal}[1]{{\textcolor{darkblue}{[Gamal: #1]}}}
\newcommand{\jcom}[1]{{\textcolor{darkgreen}{[Jascha: #1]}}}
\newcommand{\ian}[1]{{\textcolor{darkred}{[Ian: #1]}}}

\newcommand{\defeq}{\stackrel{\text{def}}{=}}

\title{Adversarial Reprogramming of \\ Neural Networks}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{Gamaleldin F. Elsayed\thanks{Work done as a member of the Google AI Residency program (g.co/airesidency).} \\
  Google Brain\\
  \texttt{gamaleldin.elsayed@gmail.com}
  %% examples of more authors
  \And
Ian Goodfellow \\
Google Brain \\
\texttt{goodfellow@google.com}
\And
Jascha Sohl-Dickstein\\
Google Brain\\
\texttt{jaschasd@google.com}
%% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle


\begin{abstract}
Deep neural networks are susceptible to \emph{adversarial} attacks. 
In computer vision, well-crafted perturbations to images can cause
neural networks to make mistakes such as confusing a cat with a computer.
Previous adversarial attacks have been designed to degrade performance
of models or cause machine learning models to produce specific outputs chosen
ahead of time by the attacker.
We introduce attacks that instead {\em reprogram} the target model
to perform a task chosen by the attacker---without the attacker needing
to specify or compute the desired output for each test-time input.
This attack finds a single adversarial perturbation, that can be added to all test-time inputs to
a machine learning model in order to cause the model to perform a task chosen by
the adversary---even if the model was not
trained to do this task.
These perturbations can thus be considered a program for the new task. 
We demonstrate adversarial reprogramming on six ImageNet classification models,
repurposing these models to perform a counting task, as well as classification
tasks: classification of MNIST and CIFAR-10 examples presented as inputs
to the ImageNet model.

\end{abstract}

\section{Introduction}

The study of adversarial examples is often motivated in terms of the danger posed by an attacker whose goal is to cause model prediction errors with a small change to the model's input. 
Such an attacker could make a self-driving car react to a phantom stop sign \citep{evtimov2017robust} by means of a sticker (a small $L_{0}$ perturbation), or cause an insurance company's damage model to overestimate the claim value from the resulting accident by subtly doctoring photos of the damage (a small $L_\infty$ perturbation). 
With this context, 
various methods have been proposed both to construct \citep{szegedy2013intriguing, papernot2015limitations, papernot2017practical, papernot2016transferability, brown2017adversarial, liu2016delving} and defend against
 \citep{goodfellow2014explaining,kurakin2016mlatscale,madry2017towards,ensemble_training, kolter2017provable, kannan2018adversarial} this style of adversarial attack. 
Thus far, the majority of adversarial attacks have consisted of {\em untargeted}
 attacks that aim to degrade the performance of a model without necessarily
 requiring it to produce a specific output, or {\em targeted} attacks
 in which the attacker designs an adversarial perturbation to produce
 a specific output for that input.
 For example, an attack against a classifier might target a specific desired output class for
 each input image, or an attack against a reinforcement learning agent might induce
 that agent to enter a specific state \citep{lin2017tactics}.
 % gamaleldin: I added the following sentences.
 
 
In practice, there is no constraint that adversarial attacks should adhere to this framework. Thus, it is crucial to proactively anticipate other unexplored adversarial goals in order to make machine learning systems more secure. 
In this work, we consider a novel and more challenging adversarial goal:
reprogramming the model to perform a task chosen by the attacker,
without the attacker needing to compute the specific desired output.
Consider a model trained to perform some {\em original task}:
for inputs $x$ it produces outputs $f(x)$.
Consider an adversary who wishes to perform an {\em adversarial task}:
for inputs $\tilde{x}$ (not necessarily in the same domain as $x$)
the adversary wishes to compute a function $g(\tilde{x})$.
We show that an adversary can accomplish this by learning 
{\em adversarial reprogramming functions}$h_f(\cdot ; \theta)$ and $h_g(\cdot ; \theta)$ that map 
between the two tasks.
Here, $h_f$ converts 
inputs from the domain of $\tilde{x}$ 
into the domain of $x$ 
(i.e.,
$h_f(\tilde{x}; \theta)$ is a valid input to the function $f$), 
while $h_g$ maps output of $f(h(\tilde{x}; \theta))$ back to outputs of $g(\tilde{x})$. 
%This adversarial reprogramming function is paired with a second readout function $\bar{h}\left( y; \theta\right)$ %that 
The parameters $\theta$ of the adversarial program are then adjusted to achieve
$h_g\left(
	f\left(
		h_f\left(\tilde{x}\right)
	\right)
\right) = g\left(
	\tilde{x}
\right)$.
%$\bar{h}: f(h(\tilde{x})) \mapsto g(\tilde{x})$.%%and%there is a%map from %$f(h(\tilde{x}))$ to $g(\tilde{x})$, %$f(h(\tilde{x})) \mapsto g(\tilde{x})$.%%We show that an adversary can accomplish this by learning an%{\em adversarial reprogramming function}%$h(\cdot ; \theta)$ such that $h$ converts %inputs %between the two %%domains %tasks (i.e.,%$h(\tilde{x}; \theta)$ is a valid input to the function $f$)%and%there is a%map from %$f(h(\tilde{x}))$ to $g(\tilde{x})$, %$f(h(\tilde{x})) \mapsto g(\tilde{x})$.%n equivalence between $f(h(\tilde{x}))$ and $g(\tilde{x})$, %$f(h(\tilde{x})) \sim g(\tilde{x})$.% \sim is the symbol indicating an equivalence relationship. it allows the outputs to be in different domains.% This paragraph is not needed as now we do other adversarial settings.
In our work, for simplicity, 
%and to obtain highly interpretable results,
we define $\tilde{x}$ to be a small image,
$g$ a function that processes small images,
$x$ a large image,
and $f$ a function that processes large images.
Our function $h_f$ then just consists of drawing $x$ in the center
of the large image and $\theta$ in the borders (though we explore other schemes as well), and $h_g$ is simply a hard coded mapping between output class labels. 
However, the idea is more general; $h_f$ ($h_g$) could be any consistent
transformation that converts between the input (output) formats for the two tasks
and causes the model to perform the adversarial task.

We refer to the class of attacks where a model is repurposed to
perform a new task as {\em adversarial reprogramming}.
We refer to $\theta$
as an {\em adversarial program}.
In contrast to most previous adversarial work, the magnitude of this perturbation need not be constrained for adversarial reprogramming to work. Though, we note that it is still possible to construct reprogramming attacks that are imperceptible.
%The attack does not need to be imperceptible to humans, or even subtle, in order to be considered a success. 
Potential consequences of adversarial reprogramming include theft of computational resources from public facing services, repurposing of AI-driven assistants into spies or spam bots, and abusing machine learning services for tasks violating the ethical principles of system providers. 
Risks stemming from this type of attack are discussed in more detail in Section 
%\ref{sec risks}. \ref{sec beyond images}.
% perturbation can be of any magnitude is required to be a small perturbation, typically as measured by an $\ell_0$, $\ell_2$, or $\ell_\infty$ norm. %This contrasts with most current research in adversarial examples, in which attacks aim only to cause a neural network to produce (or fail to produce) a target output, rather than performing a different task. %This also contrasts with all previous work in adversarial examples, where the attack is required to be a small perturbation, typically as measured by an $\ell_0$, $\ell_2$, or $\ell_\infty$ norm. %attack can be %That is reprogramming a machine learning system to perform an adversarial task instead of forcing a neural network to make wrong predictions. %This new class of attacks could for example be used to steal computational resources, or turn an AI-driven assistant into a spy or a spam bot. %Risks stemming from this type of attack are discussed in more detail in Section \ref{sec risks}.% and use them to perform nefarious tasks. 

It may seem unlikely that an additive offset to a neural network's input would be sufficient on its own to repurpose the network to a new task. 
However, this flexibility stemming 
only from changes to a network's inputs is consistent with results on the expressive power of deep neural networks. 
For instance, in \citet{raghu2016expressive} it is shown that, depending on network hyperparameters, the number of unique output patterns achievable by moving along a one-dimensional trajectory in input space increases exponentially with network depth. 
% gamaleldin: I think the below statement is problematic and contradicts now the message of the paper.%Further, \citet{li2018measuring} shows that networks can be trained to high accuracy on common tasks even if parameter updates are restricted to occur only in a low dimensional subspace. %An additive offset to a neural network's input is equivalent to a modification of its first layer biases (for a convolutional network with biases%shared across space, this operation effectively introduces new%parameters because the additive input is not %shared across space), % subject to the sharing constraint), %and therefore an adversarial program corresponds to an update in a low dimensional parameter subspace. % gamaleldin: i am removing these two sentences as our new results is contradicting them.% gamaleldin: moved this to related work the below sentence.%Finally, successes in transfer learning have shown that representations in neural networks can generalize to surprisingly disparate tasks. %\jcom{last sentence doesn't make sense without this sentence}%The task of reprograming a trained network may therefore be easier than training a network from scratch --- a hypothesis we explore experimentally.%% -- and we show suggesting the possibility of a successful attackM%Additionally, we find some evidence of feature sharing between the original and adversarial tasks, in that trained networks are more susceptible to adversarial reprogramming than untrained networks in our experiments.

In this paper, we present the first instances of adversarial reprogramming. 
In Section \ref{sec related}, we discuss related work. 
In Section \ref{sec method}, we present a training procedure for crafting adversarial programs, which cause a neural network to perform a new task. 
% We propose a training procedure to craft inputs, such that they reprogram neural networks to perform a task other than the one they were originally trained to do. %Specifically, we develop a fixed additive contribution to a network's input, which changes the function computed by the network to a desired adversarial task. 
In Section \ref{sec results}, we experimentally demonstrate adversarial programs that target several convolutional neural networks designed to classify ImageNet data. 
%In this work, we target several convolutional neural networks designed to classify ImageNet data. 
These adversarial programs alter the network function from ImageNet classification to: counting squares in an image, classifying MNIST digits, and classifying CIFAR-10 images. 
Next, we examine the susceptibility of trained and untrained networks to adversarial reprogramming. We then demonstrate the possibility of reprograming adversarial tasks with adversarial data that has no resemblance to original data, demonstrating that results from transfer learning do not fully explain adversarial reprogramming. Further, we demonstrate the possibility of concealing adversarial programs and data.
Finally, we end in Sections \ref{sec discuss} and \ref{sec conclusion} by discussing and summarizing our results.
%risks, connections to other work,  and potential mechanisms of action.%These experiments thus demonstrate the possibility of adversarially reprogramming neural networks.\begin{figure}
\centering
\includegraphics[width=\columnwidth]{figures/counting_examples.pdf}
\caption{{\bf Illustration of adversarial reprogramming.}
(a) Mapping of ImageNet labels to adversarial task labels (squares count in an image). (b) Two examples of images from the adversarial task (left) are embedded at the center of an adversarial program (middle), yielding adversarial images (right). The adversarial program shown repurposes an Inception V3 network to count squares in images.
(c) Illustration of inference with adversarial images. The network when presented with adversarial images will predict ImageNet labels that map to the adversarial task labels.}
\label{fig: counting}
\end{figure}

\section{Background and Related Work}\label{sec related}\subsection{Adversarial examples}

One definition of adversarial examples is that they are
``inputs to machine learning models that an attacker has intentionally designed to cause the model to make a mistake''
\citep{goodfellow2017}.
They are often formed by starting with a naturally occuring image
and using a gradient-based optimizer to search for a nearby
image that causes a mistake
\citep{Biggio13,szegedy2013intriguing,7958570}.
These attacks can be either {\em untargeted} (the adversary succeeds
when causing any mistake at all) or {\em targeted} (the adversary
succeeds when causing the model to predict a specific incorrect class).
Adversarial attacks have been also proposed for other domains like malware detection \citep{grosse17}, generative models \citep{kos2017adversarial}, network policies for reinforcement learning tasks \citep{huang2017adversarial}, and network interpretations \citep{ghorbani2017interpretation}. 
In these domains, the attack remains either untargeted (generally degrading the performance)
or targeted (producing a specific output).
We extend this line of work by developing reprogramming methods that aim to
produce specific {\em functionality} rather than a specific hardcoded output.

% Do we need this info?% Studies also have demonstrated the possibility of targeting models that are not used in the optimization procedure with different architecture \cite{szegedy2013intriguing, papernot2017practical}, or trained with a different algorithm \cite{papernot2016transferability}. % Do we need this info?% \textcite{kurakin17physical} found that it is possible to transfer attacks from the digital to the physical world.
Several authors have observed that the same modification can be
applied to many different inputs in order to form adversarial
examples \citep{goodfellow2014explaining,moosavi2017universal}.
For example,  \citet{brown2017adversarial} designed an
``adversarial patch'' that can switch the prediction of many models
to one specific class (e.g. toaster) when it is placed physically
in their field of view.
% \textcite{elsayed2018adversarial} found that even humans can be susceptible to adversarial examples in limited-time viewing conditions. 
We continue this line of work by finding a single adversarial
program that can be presented with many input images to cause
the model to process each image according to the adversarial
program.

%Another adversarial goal has also been proposed for model training that aims to \emph{poison} machine learning models, and affect systems that learn online \cite{joseph2013machine}. %The main focus of these attacks is to introduce data during training to bias the training process towards bad models. For example, an adversary may design spam emails including words commonly found in normal emails, thus raising the false positive rate of spam classifier \cite{nelson2008exploiting}. %Some work has explored adversarial attacks with goals other than misclassification, and which are closer to the general reprogramming we propose. %\textcite{edwards2015censoring} used an adversary critic to learning flexible representations while censoring information that can lead to stereotypes. %One positive use of adversarial research is machine learning fairness. It is known that models can develop biases based on gender or race when trained on data that includes collected with these stereotypes. Adversarial research has proposed ways to make models more fair. For example, % gamaleldin: we may add BriarPatch if it is already out%\textcite{} designed an adversarial patch that influenced models to forget sensitive information such as gender. %Most similar to our work, [discussion of briar patch / fairness / invisibility of protected status work? to other examples Ian left in Google doc?]\subsection{Parasitic Computing and Weird Machines}

Parasitic computing involves 
forcing a target system to solve a complex computational task it wasn't originally designed to perform, 
by taking advantage of peculiarities in network communication protocols \citep{barabasi2001parasitic,peresini2013network}. 
Weird machines, on the other hand, are a class of computational exploits where carefully crafted inputs can be used to run arbitrary code on a targeted computer \citep{bratus2011exploit}. 
Adversarial reprogramming can be seen as a form of parasitic computing, though without the focus on leveraging the communication protocol itself to perform the computation. 
Similarly, adversarial reprogramming can be seen as an example of neural networks behaving like weird machines, 
though adversarial reprogramming functions only within the neural network paradigm -- we do not gain 
%more privileged 
access to the host computer.

\subsection{Transfer Learning}

Transfer learning \citep{raina2007self, mesnil2011unsupervised} and adversarial reprogramming share the goal of repurposing networks to perform a new task. 
Transfer learning methods use the knowledge obtained from one task as a base to learn how to perform another. Neural networks possess properties that can be useful for many tasks \citep{yosinski2014transferable}. For example, neural networks when trained on images develop features that resemble Gabor filters in early layers even if they are trained with different datasets or different training objectives such as supervised image classification \citep{krizhevsky2012imagenet}, unsupervised density learning \citep{lee2009convolutional}, or unsupervised learning of sparse representations \citep{le2011ica}. Empirical work has demonstrated that it is possible to take a convolutional neural network trained to perform one task, and simply train a linear SVM classifier to make the network work for other tasks \citep{razavian2014cnn, donahue2014decaf}. 
%These findings suggest that the task of repurposing neural networks may not require retraining all the weights of neural network. Instead, the adversary task may be simplified to only design a perturbation that effectively realign the output layer of the network for the new task. 
However, transfer learning is very different from the adversarial reprogramming task in that it allows model parameters to be changed for the new task. In typical adversarial settings, an attacker is unable to alter the model, and instead must achieve their goals solely through manipulation of the input. Further, one may wish to adversarially reprogram across tasks with very different datasets. This makes the task of adversarial reprogramming much more challenging than transfer learning. 
%That said, we demonstrate in this work for the first time the possibility of adversarially reprogramming neural networks by a universal manipulation of input data.

\section{Methods}\label{sec method}

In this work, we consider an adversary with access to the parameters of a neural network that is performing a specific task. The objective of the adversary is to reprogram the model to perform a new task by crafting an adversarial program to be included within the network input. Here, the network was originally designed to perform ImageNet classification, but the methods discussed here can be directly extended to other settings.

Our adversarial program is formulated as an additive contribution to network input. 
% such that the network would change its function to perform a desired adversarial task.
Note that unlike most adversarial perturbations, the adversarial program is not specific to a single image. 
The same adversarial program will be applied to all images.
%Adversarial program defines an adversarial task and thus does not change across different input images. In particular, 
We define the adversarial program as:
\begin{align}
P = \tanh\left(W \odot M\right)
\end{align}
where $W \in \mathbb{R}^{n \times n \times 3}$ is the adversarial program parameters to be learned, $n$ is the ImageNet image width, and $M$ is a masking matrix that is 0 for image locations that corresponds to the adversarial data for the new task, otherwise 1. 
Note that the mask $M$ is not required -- we mask out the central region of the adversarial program purely to improve visualization of the action of the adversarial program. 
Also, note that we use $\tanh\left(\cdot\right)$ to bound the adversarial perturbation to be in $(-1, 1)$ -- the same range as the (rescaled) ImageNet images the target networks are trained to classify. 
%. These two choices can safely be dropped if one is interested in more flexible adversarial program. % gamaleldin: I addressed the comment below%\jcom{should make it clear in the text that the mask is a convenience for easy visualization, but is not required}

Let, $\tilde{x} \in \mathbb{R}^{k \times k \times 3}$ be a sample from the dataset to which we wish to apply the adversarial task, where $k < n$. $\tilde{X} \in \mathbb{R}^{n \times n \times 3}$ is the equivalent ImageNet size image with $\tilde{x}$ placed in the proper area, defined by the mask $M$.
% (here we place $\text{x}_i$ in the center, but one can place it elsewhere). 
The corresponding adversarial image is then:
\begin{align}
X_{adv} = h_f\left(\tilde{x}; W\right) = 
%\operatorname{clip}(\tilde{X} + P \,, \left[-1, 1\right])
\tilde{X} + P
\label{eqn: adv prog}
\end{align}%\jcom{I think the $\operatorname{clip}$ was redundant?}

Let $P(y | X)$ be the probability that an ImageNet classifier gives to ImageNet label $y \in \{1, \hdots, 1000\}$, given an input image $X$. We define a hard-coded mapping function $h_g(y_{adv})$ that maps a label from an adversarial task $y_{adv}$ to a set of ImageNet labels. For example, if an adversarial task has 10 different classes ($y_{adv} \in \{1, \hdots, 10\}$), $h_g\left(\cdot\right)$ may be defined to assign the first 10 classes of ImageNet, any other 10 classes, or multiple ImageNet classes to the adversarial labels. Our adversarial goal is thus to maximize the probability $P(h_g(y_{adv}) | X_{adv})$. We set up our optimization problem as
\begin{align}
\label{eq adv obj}
\hat{W} = 
\argmin_{W} \left( 
	- \log P(h_g(y_{adv}) | X_{adv}) + \lambda ||W||_F^2
	\right)
,
\end{align}
where $\lambda$ is the coefficient for a weight norm penalty, to reduce overfitting.
%to the adversarial task train set. 
We optimize this loss with Adam while exponentially decaying the learning rate. Hyperparameters 
are given in Appendix \ref{sec: supp tables}.
%\jcom{give hyperparameters somewhere}
Note that after the optimization the adversarial program has a minimal computation cost from the adversary's side as it only requires computing $X_{adv}$ (Equation \ref{eqn: adv prog}), and mapping the resulting ImageNet label to the correct class. 
In other words, during inference the adversary needs only to store the program and add it to the data, thus leaving the majority of computation to the target network. 
%Of course an adversary may be willing to afford doing more computation, but we present here the minimal computation cost possible.

One interesting property of adversarial reprogramming is that it must
exploit nonlinear behavior of the target model.
This is in contrast to traditional adversarial examples, where attack
algorithms based on linear approximations of deep neural networks are
sufficient to cause high error rate \citep{goodfellow2014explaining}.
Consider a linear model that receives an input $\tilde{x}$ and a program
$\theta$ concatenated into a single vector: $x = [\tilde{x}, \theta]^{\top}$.
Suppose that the weights of the linear model are partitioned into two
sets, $v = [v_{\tilde{x}}, v_\theta]^{\top}$.
The output of the model is $v^{\top} x = v_{\tilde{x}}^{\top} \tilde{x} + v_\theta^{\top} \theta$.
The adversarial program $\theta$ adapts the effective biases
$v_\theta^{\top} \theta$ but cannot adapt the weights applied to the input $\tilde{x}$.
The adversarial program $\theta$ can thus bias the model toward consistently outputting one class or
the other but cannot change the way the input is processed.
For adversarial reprogramming to work, the model must include nonlinear interactions of $\tilde{x}$ and $\theta$.
A nonlinear deep network satisfies this requirement.



\begin{figure}
\centering
\includegraphics[width=\columnwidth]{figures/examples.pdf}
\caption{{\bf Examples of adversarial programs.}
Adversarial program which cause Inception V3 ImageNet model to function as (a) MNIST classifier. (b) CIFAR-10 classifier}
\label{fig: ex}
\end{figure}\section{Results}\label{sec results}
To demonstrate the feasibility of adversarial reprogramming, 
we conducted experiments on six architectures trained on ImageNet. 
In each case, we reprogrammed the network to perform three different adversarial tasks: counting squares, MNIST classification, and CIFAR-10 classification. 
The weights of all trained models were obtained from \citet{tfslim}, and top-1 ImageNet precisions are shown in Table \ref{table: ImageNet precision}. 
We additionally examined whether adversarial training conferred resistance to adversarial reprogramming, and compared the susceptibility of trained networks to random networks. Further, we investigated the possibility of reprogramming the networks when the adversarial data has no resemblance to the original data. Finally, we demonstrated the possibility of concealing the adversarial program and the adversarial data.

\subsection{Counting squares}

To illustrate the adversarial reprogramming procedure, we start with a simple adversarial task. That is counting the number of squares in an image. 
We generated images ($\tilde x$) of size $36 \times 36 \times 3$ that include $9 \times 9$ white squares with black frames. Each square could appear in 16 different position in the image, and the number of squares ranged from $1$ to $10$. The squares were placed randomly on gridpoints (Figure \ref{fig: counting}b left). 
We embedded these images in an adversarial program (Figure \ref{fig: counting}b middle). 
The resulting images ($X_{adv}$) are of size $299 \times 299 \times 3$ with the $36 \times 36 \times 3$ images of the squares at the center (Figure \ref{fig: counting}b right). Thus, the adversarial program is simply a frame around the counting task images. 
We trained one adversarial program per ImageNet model, such that the first 10 ImageNet labels represent the number of squares in each image (Figure \ref{fig: counting}c). 
Note that the labels we used from ImageNet have no relation to the labels of the new adversarial task. For example, a `White Shark' has nothing to do with counting 3 squares in an image, and an `Ostrich' does not at all resemble 10 squares. 
We then evaluated the accuracy in the task by sampling 100,000 images and comparing the network prediction to the number of squares in the image. 

%We redefine ImageNet's first 10 labels to represent the square count (Figure \ref{fig: counting}a). Note that the labels we used from ImageNet have no relation to the labels of the new adversarial task. For example, a 'White Shark' has nothing to do with counting 3 squares of the images, and an 'Ostrich' does not at all resemble 10 squares. Thus, it may seem unlikely that these ImageNet labels can be repurposed to reflect the new adversarial task. 
Despite the dissimilarity of ImageNet labels and adversarial labels, and that the adversarial program is equivalent simply to a first layer bias, the adversarial program masters this counting task for all networks (Table \ref{table: reprogramming results}). 
These results demonstrate the vulnerability of neural networks to reprogramming on this simple task using only additive contributions to the input.

%This task has only 58,650 possible inputs. While it might be possible here for an adversarial program to effectively memorize all inputs, this will not be true of later tasks. \subsection{MNIST classification}\label{sec mnist}\begin{table}
  \caption{{\bf Neural networks adversarially reprogrammed to perform a variety of tasks.}
  Table gives accuracy of reprogrammed networks to perform a counting task, MNIST classification task, and CIFAR-10 classification task, and Shuffled MNIST pixels classification task.}
  % gamaleldin: done
  %\jcom{specify test or train. better, should give both, e.g. as [test accuracy] / [train accuracy].}
  \label{table: reprogramming results}
  \centering
  \begin{tabular}{llllllll}
    \toprule
       Model &  \multicolumn{6}{c}{Pretrained on ImageNet}   & \multicolumn{1}{c}{Untrained}       \\
        \cmidrule(r){2-7}
        \cmidrule(r){8-8}

   &Counting &  \multicolumn{2}{c}{MNIST}     &       \multicolumn{2}{c}{CIFAR-10}     & \multicolumn{1}{c}{Shuffled MNIST}  & \multicolumn{1}{c}{MNIST}       \\
        \cmidrule(r){3-4}
        \cmidrule(r){5-6}
        \cmidrule(r){7-7}
        \cmidrule(r){8-8}

           &  & train &  test & train & test  &test  &test  \\
    \midrule
    Incep. V3                 &  $0.9993$  &   $0.9781$    & $0.9753$    & $0.7311$  & $0.6911$  & $0.9709$ & $0.4539$  \\
    Incep. V4                 & $0.9999$   &    $0.9638$  & $0.9646$   &  $0.6948$   & $0.6683$& $0.9715$ & $0.1861$  \\
    Incep. Res. V2    & $0.9994$   &   $0.9773$  & $0.9744$    &  $0.6985$ & $0.6719$ & $0.9683$ & $0.1135$  \\
    Res. V2 152             &  $0.9763$  &    $0.9478$ &$0.9534$   &  $0.6410$ & $0.6210$ & $0.9691$ & $0.1032$ \\
    Res. V2 101             &  $0.9843$  &  $0.9650$  & $0.9664$  & $0.6435$  & $0.6301$ & $0.9678$ & $0.1756$ \\
    Res. V2 50             &  $0.9966$  &  $0.9506$  & $0.9496$  & $0.6$  & $0.5858$ & $0.9717$ &  $0.9325$ \\
    Incep. V3 adv.          &     &   $0.9761$    & $0.9752$    &   &  & & \\
    \bottomrule
  \end{tabular}
\end{table}%\begin{table}%  \caption{{\bf Investigation of the effect of the trained model details and original data.} %  Table gives accuracy of reprogrammed networks on an MNIST classification task. %  Target networks have been randomly initialized, and have not been trained.}%  \label{table: random reprogramming results}%  \centering%  \begin{tabular}{lllll}%    \toprule%       &  \multicolumn{2}{c}{Untrained network} &  \multicolumn{2}{c}{ImageNet trained network}  \\%\cmidrule(r){2-3} \cmidrule(r){4-5}%   &  \multicolumn{2}{c}{MNIST} &  \multicolumn{2}{c}{Shuffled MNIST}  \\%\cmidrule(r){2-3} \cmidrule(r){4-5}%    Model      & train set & test set & train set & test set\\%    \midrule%     Inception V3  & $0.4530$ & $0.4539$   & $0.$ & $0.$   \\%     Inception V4  & $0.1876$ &$0.1861$    & $0.$ & $0.$  \\%     Inception Resnet V2  & $0.1125$  & $0.1135$     & $0.$ & $0.$ \\%    Resnet V2 152  & $0.0986$  & $0.1032$     & $0.$ & $0.$ \\%    Resnet V2 101  & $0.1688$ & $0.1756$    & $0.$ & $0.$ \\%    Resnet V2 50  & $0.9342$ & $0.9325$     & $0.$ & $0.$\\%    \bottomrule%  \end{tabular}%\end{table}%The previous task is easy in the sense that the network need only to know the finite patterns of squares to figure out the count. 
In this section, we demonstrate adversarial reprogramming on somewhat more complex task of classifying MNIST digits. We measure {\em test} and train accuracy, so it is impossible for the adversarial program to have simply memorized all training examples.
%with unbounded possibilities, which requires generalization. \jcom{it's not unbounded, just larger}
Similar to the counting task, we embedded MNIST digits of size $28 \times 28 \times 3$ inside a frame representing the adversarial program, we assigned the first 10 ImageNet labels to the MNIST digits, and trained an adversarial program for each ImageNet model. 
Figure \ref{fig: ex}a shows example of the adversarial program for Inception V3 being applied. 

Our results show that ImageNet networks can be successfully reprogramed to function as an MNIST classifier by presenting an additive adversarial program. 
The adversarial program additionally generalized well from the training to test set, suggesting that the reprogramming does not function purely by memorizing train examples, and is not brittle to small changes in the input. 
% gamaleldin: removing this because there is no rigorous result here%One interesting observation is that the adversarial programs targeted at Inception architectures are qualitatively different from those targeted at Resnet architectures (Figure \ref{fig: ex}). %This suggests that the method of action of the adversarial program is in some sense architecture-specific.%some architectural influences on the properties of adversarial programs that will be effective.%That is, adversarial program that target Resnet architectures seem to have lower spatial frequency than those targeting Inception architectures. This may suggest some architecture influence on the possible forms of adversarial programs that can be generated.\subsection{CIFAR-10 classification}

Here we implement a more challenging adversarial task. That is, crafting adversarial programs to repurpose ImageNet models to instead classify CIFAR-10 images. 
An example of the resulting adversarial images are given in Figure \ref{fig: ex}b.
Our results show that our adversarial program was able to increase the accuracy on CIFAR-10 from chance to a moderate accuracy (Table \ref{table: reprogramming results}). This accuracy is near what is expected from typical fully connected networks \citep{lin2015far} but with minimal computation cost from the adversary side at inference time. 
% gamaleldin: we already mention something like this in mnist no need to say this statement again%The small train-test gap further suggests that the adversarial program is effectively learning the class boundary, rather than memorizing training examples. 
One observation is that although adversarial programs trained to classify CIFAR-10 are different from those that classify MNIST or perform the counting task, the programs show some visual similarities, e.g. ResNet architecture adversarial programs seem to possess some low spatial frequency texture (Figure \ref{fig: programs}a).

%We embedded CIFAR images inside the adversarial program frame. Then, used the first 10 ImageNet labels for CIFAR10 labels. We trained an adversarial program to repurpose ImageNet networks to classify CIFAR10 images (Figure \ref{fig: cifar}). \subsection{Investigation of the effect of the trained model details and original data}One important question is what is the degree to which susceptibility to adversarial reprogramming depends on the details of the model being attacked. 
To address this question, we examined attack success on an Inception V3 model that was trained on ImageNet data using adversarial training \citep{ensemble_training}. Adversarial training augments data with adversarial examples during training, and is one of the most common methods for guarding against adversarial examples. 
As in Section \ref{sec mnist}, we adversarially reprogrammed this network to classify MNIST digits.
Our results (Table \ref{table: reprogramming results}) indicate that the model trained with adversarial training is still vulnerable to reprogramming, with only a slight reduction in attack success. 
This finding shows that standard approaches to adversarial defense has little efficacy against adversarial reprogramming.
%as they are specific to the original task and data.  \begin{figure}
\centering
\includegraphics[width=\columnwidth]{figures/concealing.pdf}
\caption{{\bf Adversarial programs may be limited in size or concealed.}
In all panels, an Inception V3 model pretrained on ImageNet is reprogrammed to classify MNIST digits. Example images
(a) with adversarial programs of different sizes.
(b) with adversarial programs of different perturbation scales.
(c) Here adversarial data + program (right) are hidden inside a normal image from ImageNet (left), yielding an adversarial image (center) that is able to reprogram the network to function as an MNIST classifier. The pixels of the adversarial data are shuffled to conceal its structure.
}
\label{fig: concealing programs}
\end{figure}
This is likely explained by the differences between adversarial reprogramming and standard adversarial attacks. First, that the goal is to repurpose the network rather than cause it to make a specific mistake, second that the magnitude of adversarial programs can be large, while traditional adversarial attacks are of a 
%restricted 
small perturbations magnitude, and third adversarial defense methods may be specific to original data and may not generalize to data from the adversarial task.

%expected because adversarial reprogram differ from adversarial examples. First the goal of adversarial reprogramming is to repurpose the network not force it to make a mistake, also adversarial reprogram posses large perturbation unlike adversarial examples.%One important question is whether typical methods for guarding against adversarial examples can help defend against adversarial reprogramming. To address this question, we started from an Inception V3 model that is trained on ImageNet data using adversarial training \cite{ensemble_training}, which is one of the most common methods for guarding against adversarial examples. Adversarial training guards against adversarial examples by augmenting the training data with adversarial examples generated for each training batch. %We used our method to generate an adversarial program to repurpose the network to classify MNIST digits. Our results indicate that the model trained with adversarial training is still vulnerable to reprogramming (MNIST classification accuracy is $0.9761$ and $0.9752$ for train and test sets, respectively). 

To further explore dependence on the details of the model, we performed adversarial reprogramming attacks on models with random weights.
We used the same experiment set up and MNIST reprogramming task as in Section \ref{sec mnist} -- we simply used the ImageNet models with randomly initialized rather than trained weights.
The MNIST classification task was easy for networks pretrained on ImageNet (Table \ref{table: reprogramming results}). However, for random networks, training was very challenging and generally converged to a much lower accuracy (only ResNet V2 50 could train to a similar accuracy as trained ImageNet models; see Table \ref{table: reprogramming results}). Moreover, the appearance of the adversarial programs was qualitatively distinct from the adversarial programs obtained with networks pretrained on ImageNet (see Figure \ref{fig: programs}b). This finding demonstrates that the original task the neural networks perform is important for adversarial reprogramming.   
 This result may seem surprising, as random networks have rich structure adversarial programs might be expected to take advantage of. 
 For example, theoretical results have shown that wide neural networks become identical to Gaussian processes, where training specific weights in intermediate layers is not necessary to perform tasks \citep{matthews2018gaussian,lee2017deep}. Other work has demonstrated that it is possible to use random networks as generative models for images \citep{ustyuzhaninov2016texture, he2016powerful}, further supporting their potential richness. One explanation may be that randomly initialized networks perform poorly for simple reasons, such as poor scaling of network weights at initialization, whereas the trained weights are better conditioned. 
 %gamaleldin: I modified the below sentence following.%On the other hand, ideas from transfer learning suggest that networks generalize best to tasks with similar structure. %gamaleldin: I suggest removing the following.%Our experimental results suggest that the structure in our three adversarial tasks is similar enough to that in ImageNet that the adversarial program can benefit from training of the target model on ImageNet. %They also suggest that it is possible for changes to the input of the network to take advantage of that similarity, rather than changes to the output layer as is more typical in transfer learning. 
One explanation of adversarial reprogramming that is motivated by transfer learning  \citep{yosinski2014transferable} is that the network may be relying on some similarities between original and adversarial data. To address this hypothesis, we randomized the pixels on MNIST digits such that any resemblance between the adversarial data (MNIST) and images in the original data (ImageNet) is removed (see Figure \ref{fig: shuffled}). We then attempted to reprogram pretrained ImageNet networks to classify the shuffled MNIST digits. Despite shuffled MNIST not sharing any spatial structure with images, we managed to reprogram the ImageNet network for this task (Table \ref{table: reprogramming results}) with almost equal accuracy to standard MNIST (in some cases shuffled MNIST even achieved higher accuracy). These results thus suggest that transferring knowledge between the original and adversarial data does not explain the susceptibility to adversarial reprogramming. Even more interestingly, these results suggest the possibility of reprogramming across tasks with unrelated datasets and across domains.

%, which may make it possible to reuse the network for a different task.% This can be due to some common properties between original and adversarial task, where transfer learning can help, or the pretrained networks provide a better initialization that makes adversarial reprograming optimization better behaved.%On one hand, ideas from transfer learning suggest that tasks may have some common structure, which may make it possible to reuse the network for a different task. On the other hand, the class of functions that neural networks generally represent contains rich basis for the adversarial program to utilize rather than transferring knowledge from the original task. For example, theoretical ideas demonstrates that neural networks can be modeled as Gaussian processes, where training specific weights is not necessary to perform tasks \cite{matthews2018gaussian, lee2017deep, neal1996priors}. Others have demonstrated that it is possible to use random networks as generative models for images \cite{ustyuzhaninov2016texture, he2016powerful}.%One important question is whether the original task is important for adversarial reprograming of networks to perform a different task. %\begin{table}%  \caption{{\bf Investigation of the effect of the trained model details and original data.} %  Table gives accuracy of reprogrammed networks on an MNIST classification task. %  Target networks have been randomly initialized, and have not been trained.}%  \label{table: random reprogramming results}%  \centering%  \begin{tabular}{lllll}%    \toprule%       &  \multicolumn{2}{c}{Untrained network} &  \multicolumn{2}{c}{ImageNet trained network}  \\%\cmidrule(r){2-3} \cmidrule(r){4-5}%   &  \multicolumn{2}{c}{MNIST} &  \multicolumn{2}{c}{Shuffled MNIST}  \\%\cmidrule(r){2-3} \cmidrule(r){4-5}%    Model      & train set & test set & train set & test set\\%    \midrule%     Inception V3  & $0.4530$ & $0.4539$   & $0.$ & $0.$   \\%     Inception V4  & $0.1876$ &$0.1861$    & $0.$ & $0.$  \\%     Inception Resnet V2  & $0.1125$  & $0.1135$     & $0.$ & $0.$ \\%    Resnet V2 152  & $0.0986$  & $0.1032$     & $0.$ & $0.$ \\%    Resnet V2 101  & $0.1688$ & $0.1756$    & $0.$ & $0.$ \\%    Resnet V2 50  & $0.9342$ & $0.9325$     & $0.$ & $0.$\\%    \bottomrule%  \end{tabular}%\end{table}\subsection{Concealing adversarial programs}
In our previous experiment, there were no constraints on the size (number of program pixels) and scale (magnitude of perturbations) of the program and adversarial data. Here, we demonstrate the possibility of limiting the visibility of the adversarial perturbations by limiting the program size, scale, or even concealing the whole adversarial task. In these experiments, we used Inception V3 model pretrained to classify ImageNet. In our first experiment, we adversarially reprogrammed the network to classify MNIST digits while limiting the size of the program (see Figure \ref{fig: concealing programs}a). Our results show that adversarial reprogramming is still successful, yet with lower accuracy, even if we used a very small adversarial program. In our next experiment, we made the adversarial program nearly imperceptible by limiting the $L_{\inf}$ norm of the adversarial perturbation to a small percentage of the pixel values. Our results show that adversarial reprogramming is still successful (see Figure \ref{fig: concealing programs}b) even with nearly imperceptible programs. Further, we tested the possibility of concealing the whole adversarial task by hiding both the adversarial data and program within a normal image from ImageNet. To do that, we shuffled the pixels of the adversarial data (here MNIST), so that the adversarial data structure is hidden. Then, we limited the scale of both the adversarial program and data to a small fraction of the possible pixel values. We added the resulting image to a random image from ImageNet. Formally, we extended our reprogramming methods as follows:
\begin{align}
P_X &= \alpha \tanh\left(\text{shuffle}_{ix}(\tilde X) + (W \odot \text{shuffle}_{ix}(M)) \right) \\
X_{adv} &= \text{clip}\left(X_{ImageNet} +P_X, ~~[0, 1] \right)
,
\end{align}
where $\tilde X$, $M$ and $W$ are as described in Section \ref{sec method}, $P_X$ is the adversarial data combined with the adversarial program, $ix$ is the shuffling sequence (same for $M$ and $\forall X$), $\alpha$ is a scalar used to limit the perturbation scale, and $X_{ImageNet}$ is an image chosen randomly from ImageNet, which is the same for all MNIST examples. We then optimized the adversarial program for the network to classify MNIST digits (see Equation \ref{eq adv obj}). The resulting adversarial images are very similar to normal images from ImageNet (see Figure \ref{fig: concealing programs}c), yet the network is successfully reprogrammed to classify MNIST digits, though with lower accuracy (see Figure \ref{fig: concealing programs}c). This result demonstrate the possibility of hiding the adversarial task. Here, we used a simple shuffling technique and picked an image from ImageNet to hide the adversarial task, but one could go further and use more complex schemes for hiding the adversarial task and optimize the choice of the image from ImageNet, which may make adversarial reprogramming even harder to detect.



\section{Discussion}\label{sec discuss}\subsection{Flexibility of trained neural networks}%%%%Mixtut%%%\subsection{Greater attack success on trained networks}%Our results indicate that that original task was important for the susceptibility of networks to reprogramming. 
We found that trained neural networks were more susceptible to adversarial reprogramming than random networks. Further, we found that reprogramming is still successful even when data structure is very different from the structure of the data in the original task.
This demonstrates a large flexibility of repurposing trained weights for a new task. 
% gamaleldin: removing the below sentence%This can be seen as a novel form of transfer learning, where the inputs to the network (equivalent to first layer biases) are modified, rather than the readout weights as is more typical. %Reuse of neural circuits for new tasks which are flexibly defined based on input is a phenomenon which is ubiquitous in the brain \cite{}. 
Our results suggest that dynamical reuse of neural circuits should 
%similarly 
be practical in modern artificial neural networks. 
This holds the promise of enabling machine learning systems which are easier to repurpose, more flexible, and more efficient due to shared compute. 
Indeed, recent work in machine learning has focused on building large dynamically connected networks with reusable components \citep{shazeer2017outrageously}.

It is unclear whether the reduced performance when targeting random networks, and when reprogramming to perform CIFAR-10 classification, 
was due to limitations in the expressivity of the adversarial perturbation,
or due to the optimization task in Equation \ref{eq adv obj} being more difficult in these situations. 
Disentangling limitations in expressivity and trainability will be an interesting future direction.

%This is a , where the same neural circuits are reused for many tasks. %It is also a phenomenon which has begun to be explored in the context of aritifical neural networks \cite{shazeer2017outrageously}.%%This is promising for computation to be shared between multiple tasks in deep neural networks.%%%This finding is consistent with work in the field about transfer learning with networks having features relevant to many tasks. %In fact, the idea of adversarial reprogramming can be developed to help generate systems that can perform multiple tasks. %For example, one can build systems that are designed specifically to perform multiple tasks with the adversarial program acting as a context signal that switches the function of network. %Definitely, human brain is able to do that and with the  cortical circuits performing different functions based on conditions of the environment relayed through sensory input.%%Another important question is whether adversarial reprogramming can work if the task is very different. For example, is it possible to reprogram a network performing an image classification task to classify audio data, or perform translation?  Our intuition is that this may be possible yet less likely than between image tasks. Another idea is that if an adversary is willing to perform some of the computation then we expect that very different tasks can become more likely. For example, an adversary maybe willing instead of only designing an additive input to append a shallow network to design a new input of the network the adversary desire to attack.%Suggests that there is transfer of useful features from imagenet to the target tasks. %Would be interesting to explore non-image based tasks.%\subsection{Possible methods of action}%%One idea to guard against adversarial reprogramming is to reduce the nonlinearity of the network. However, this may make the network less effective on the original task, or even make it more susceptible to adversarial examples. Another idea is to design filters that can sense manipulations of inputs to the networks. To do that, one can leverage the expected statistics of the data in the original tasks, and design a filter that can detect anomalies on these statistics.%comments about linearity from above?%call back to discussion about expressivity and intrinsic dimensionality of tasks from intro?\subsection{Adversarial goals beyond the image domain}\label{sec beyond images}

We demonstrated adversarial reprogramming on classification tasks in the image domain. 
It is an interesting area for future research whether similar attacks might succeed for audio, video, text, or other domains and tasks. Our finding that trained networks can be reprogram to classify shuffled MNIST examples, which do not have any resemblance to images, suggest that the reprogramming across domains is likely. 

Adversarial reprogramming of recurrent neural networks (RNNs) would be particularly interesting, since RNNs (especially those with attention or memory) can be Turing complete \citep{neelakantan2015neural}. 
An attacker would thus only need to find inputs which induced the RNN to perform a number of simple operations, such as increment counter, decrement counter, and change input attention location if counter is zero \citep{minsky1961recursive}. 
If adversarial programs can be found for these simple operations, then they could be composed to reprogram the RNN to perform various tasks.

% gamaleldin: I propose to merge this with previous section.%\subsection{Potential goals of an adversarial reprogramming attack}\label{sec risks}%\subsection{Risks stemming from adversarial reprogramming}\label{sec risks}

A variety of nefarious ends may be achievable if machine learning systems can be reprogrammed by a specially crafted input. 
The most direct of these is the theft of computational resources. 
For instance, an attacker might develop an adversarial program which causes the computer vision classifier in a cloud hosted photos service
%Google photos
 to solve image captchas and enable creation of spam accounts. 
If RNNs can be flexibly reprogrammed as 
%described in Section \ref{sec beyond images},
mentioned above, this computational theft might extend to more arbitrary tasks.
%gamaleldin: many people and reviewers did not like this cryptocurrency and phone examples. I suggest removing them.% such as mining cryptocurrency. 
A major danger beyond the computational theft is that an adversary may repurpose computational resources to perform a task which violates the code of ethics of system providers. This is particularly important as ML service providers are largely interested in protecting the ethical principles and guidelines that governs the use of their services. 
%Although we only demonstrate reprogramming from one%%In this paper we demonstrated a proof of concept that neural networks can be reprogrammed using only a simple manipulation of their input. %%The results in this paper were a %%Alternatively they might develop an input that causes a machine learning system to mine cryptocurrency. %%public facing machine learning system -- especially one with a dynamic or recurrent computation graph -- to mine cryptocurrency rather than e.g. identifying and translating text in an image. %%might design an adversarial photo which when uploaded to Google photos would cause the backend to mine Bitcoin, rather than identify faces and objects in the image. They might more directly repurpose the object classifier to solve captchas and create spam accounts.%gamaleldin: many people and reviewers did not like these cryptocurrency and phone examples. I suggest removing them.%Adversarial programs could also be used as a novel way to achieve more traditional computer hacks. For instance, as phones increasingly act as AI-driven digital assistants, the plausibility of reprogramming someone's phone by exposing it to an adversarial image or audio file increases. As these digital assistants have access to a user's email, calendar, social media accounts, and credit cards the consequences of this type of attack also grow larger. %This might even enable computer worms which do not depend on a traditional data connection between devices. For instance, if viewing an adversarial image can cause a device to present the same image on its display.%Our work here demonstrates the possibility of reprogramming networks when having access to its parameters. An even more concerning possibility that would make this security vulnerability a reality is if a network that is unknown to the adversary can be reprogrammed. There are many ideas that an adversary can leverage to do that. One is to use ensembling to increase the probability of transferring adversarial programs. This idea has proved to be successfulling in transferring adversarial examples. So, it would be very interesting to investigate if ensembling would help transfer adversarial programs \cite{}. Another approach is to probe the unknown models through interactions with inputs and use ideas from reinforcement learning in order to construct an adversarial program \cite{}. One more approach is to use a third model and modify it to approximate the unknown model, then construct an adversarial program from the approximate model, then use it later on the unknown model. All these ideas can be leveraged by an adversary to make adversarial program a reality.%If an input can be designed that causes a machine learning system to reproduce that same input on one of its outputs, then this could  %Hacking into a phone or computer which uses machine learning to automatically process audio or images, by showing a carefully crafted image, or playing carefully crafted audio.%If the phone is a full on AI-driven digital assistant, which has complete control over your email and calendar and social media accounts and credit cards, then the consequences of being able to reprogram that assistant are much larger%Computer viruses that can spread through images on screens, rather than requiring a traditional data connection between devices\section{Conclusion}\label{sec conclusion}

In this work, we proposed a new class of adversarial attacks that aim to reprogram neural networks to perform novel adversarial tasks. 
Our results demonstrate for the first time the possibility of such attacks. 
They are also illustrative of 
%These results demonstrate 
both surprising flexibility and surprising vulnerability in deep neural networks. 
%This result is consistent with both the ideas on flexibility of neural networks and the vulnerability of neural networks. 
Future investigation should address the properties and limitations of adversarial programming and possible ways to defend against it.



\subsubsection*{Acknowledgments}
We are grateful to Jaehoon Lee, Sara Hooker, Simon Kornblith, Supasorn Suwajanakorn for useful comments on the manuscript. We thank Alexey Kurakin for help reviewing the code. We thank Justin Gilmer and Luke Metz for discussion surrounding the original idea.


%\printbibliography

\bibliography{adv_reprog_2018}
\bibliographystyle{iclr2019_conference}

\clearpage
\appendix
\normalsize
\onecolumn\part*{Supplemental material}\setcounter{figure}{0}\renewcommand{\thefigure}{Supp. \arabic{figure}}\setcounter{table}{0}\renewcommand{\thetable}{Supp. \arabic{table}}\section{Supplementary Tables}\label{sec: supp tables}\begin{table}[!hbt]
  \caption{Top-1 precision of models on ImageNet data}
  %gamaleldin: I grouped the other tables, but this one is very different table and in suppmatts
  %\jcom{to save space we will probably eventually want to combine all three (four?) tables, with one column for accuracy on each task.}
  \label{table: ImageNet precision}
  \centering
  \begin{tabular}{ll}
    \toprule
    Model      & Accuracy \\
    \midrule
    Inception V3  &  $0.78$     \\
    Inception V4  & $0.802$     \\
    Inception Resnet V2  & $0.804$     \\
    Resnet V2 152  &  $0.778$     \\
    Resnet V2 101  &  $0.77$    \\
    Resnet V2 50 & $0.756$ \\
    Inception V3  adv. &  $0.776$     \\
    \bottomrule
  \end{tabular}
\end{table}\begin{table}
  \caption{Hyper-parameters for adversarial program training for the square counting adversarial task. For all models, we used the Adam optimizer with its default parameters while decaying the learning rate exponentially during training. We distributed training data across a number of GPUs (each GPU receive `batch' data samples ). We then performed synchronized updates of the adversarial program parameters.}
  % gamaleldin: done
  %\jcom{specify test or train. better, should give both, e.g. as [test accuracy] / [train accuracy].}
  \label{table: hyperparams count}
  \centering
  \begin{tabular}{llllllll}
    \toprule
     ImageNet Model &  $\lambda$ & batch & GPUS & learn rate & decay & epochs/decay & steps \\
     \midrule
    Inception V3                 & $0.01$ & $50$  & $4$  &  $0.05$ & $0.96$ & $2$ & $100000$  \\
    Inception V4                & $0.01$ & $50$  & $4$  &  $0.05$ & $0.96$ & $2$ & $100000$  \\
    Inception Resnet V2    & $0.01$ & $50$  & $4$  &  $0.05$ & $0.96$ & $2$ & $100000$  \\
    Resnet V2 152            & $0.01$ & $20$  & $4$  &  $0.05$ & $0.96$ & $2$ & $100000$  \\
    Resnet V2 101           & $0.01$ & $20$  & $4$  &  $0.05$ & $0.96$ & $2$ & $60000$  \\
    Resnet V2 50           & $0.01$ & $20$  & $4$  &  $0.05$ & $0.96$ & $2$ & $100000$  \\
    \bottomrule
  \end{tabular}
\end{table}\begin{table}
  \caption{Hyper-parameters for adversarial program training for MNIST classification adversarial task. For all models, we used the Adam optimizer with its default parameters while decaying the learning rate exponentially during training.  We distributed training data across a number of GPUs (each GPU receive `batch' data samples ). We then performed synchronized updates of the adversarial program parameters. (The Model Inception V3 adv is pretrained on ImageNet data using adversarial training method.}
  \label{table: hyperparams mnist}
  \centering
  \begin{tabular}{llllllll}
    \toprule
     ImageNet Model &  $\lambda$ & batch & GPUS & learn rate & decay & epochs/decay & steps \\
     \midrule
    Inception V3                 & $0.05$ & $100$  & $4$  &  $0.05$ & $0.96$ & $2$ & $60000$  \\
    Inception V4                & $0.05$ & $100$  & $4$  &  $0.05$ & $0.96$ & $2$ & $60000$  \\
    Inception Resnet V2    & $0.05$ & $50$  & $8$  &  $0.05$ & $0.96$ & $2$ & $60000$  \\
    Resnet V2 152            & $0.05$ & $50$  & $8$  &  $0.05$ & $0.96$ & $2$ & $60000$  \\
    Resnet V2 101           & $0.05$ & $50$  & $8$  &  $0.05$ & $0.96$ & $2$ & $60000$  \\
    Resnet V2 50           & $0.05$ & $100$  & $4$  &  $0.05$ & $0.96$ & $2$ & $60000$  \\
    Inception V3 adv.        & $0.01$ & $50$  & $6$  &  $0.05$ & $0.98$ & $4$ & $100000$  \\
    \bottomrule
  \end{tabular}
\end{table}\begin{table}
  \caption{Hyper-parameters for adversarial program training for CIFAR-10 classification adversarial task. For all models, we used ADAM optimizer with its default parameters while decaying the learning rate exponentially during training.  We distributed training data on number of GPUS (each GPU receive `batch' data samples ). We then performed synchronized updates of the adversarial program parameters.}
  \label{table: hyperparams cifar}
  \centering
  \begin{tabular}{llllllll}
    \toprule
     ImageNet Model &  $\lambda$ & batch & GPUS & learn rate & decay & epochs/decay & steps \\
     \midrule
    Inception V3                 & $0.01$ & $50$  & $6$  &  $0.05$ & $0.99$ & $4$ & $300000$  \\
    Inception V4                & $0.01$ & $50$  & $6$  &  $0.05$ & $0.99$ & $4$ & $300000$  \\
    Inception Resnet V2    & $0.01$ & $50$  & $6$  &  $0.05$ & $0.99$ & $4$ & $300000$  \\
    Resnet V2 152            & $0.01$ & $30$  & $6$  &  $0.05$ & $0.99$ & $4$ & $300000$  \\
    Resnet V2 101           & $0.01$ & $30$  & $6$  &  $0.05$ & $0.99$ & $4$ & $300000$  \\
    Resnet V2 50           & $0.01$ & $30$  & $6$  &  $0.05$ & $0.99$ & $4$ & $300000$  \\
    \bottomrule
  \end{tabular}
\end{table}\begin{table}
  \caption{Hyper-parameters for adversarial program training for MNIST classification adversarial task. For all models, we used the Adam optimizer with its default parameters while decaying the learning rate exponentially during training.  We distributed training data across a number of GPUs (each GPU receive `batch' data samples ). We then performed synchronized updates of the adversarial program parameters.}
  \label{table: hyperparams mnist random}
  \centering
  \begin{tabular}{llllllll}
    \toprule
     Random Model &  $\lambda$ & batch & GPUS & learn rate & decay & epochs/decay & steps \\
     \midrule
    Inception V3                 & $0.01$ & $50$  & $4$  &  $0.05$ & $0.96$ & $2$ & $100000$  \\
    Inception V4                & $0.01$ & $50$  & $4$  &  $0.05$ & $0.96$ & $2$ & $100000$  \\
    Inception Resnet V2    & $0.01$ & $50$  & $4$  &  $0.05$ & $0.96$ & $2$ & $60000$  \\
    Resnet V2 152            & $0.01$ & $20$  & $4$  &  $0.05$ & $0.96$ & $2$ & $60000$  \\
    Resnet V2 101           & $0.01$ & $20$  & $4$  &  $0.05$ & $0.96$ & $2$ & $60000$  \\
    Resnet V2 50           & $0.01$ & $50$  & $4$  &  $0.05$ & $0.96$ & $2$ & $60000$  \\
    \bottomrule
  \end{tabular}
\end{table}\newpage%\begin{figure}%\centering%\includegraphics[width=\columnwidth]{figures/cifar_examples.pdf}%\caption{{\bf Examples of adversarial images for CIFAR-10 classification.}%An adversarial program repurposing an Inception V3 model to classify CIFAR-10 images, applied to four CIFAR-10 images.%%Example of adversarial images which reprogram Inception V3 model to function as an CIFAR-10 classifier.%}%\label{fig: cifar}%\end{figure}\begin{figure}
\centering
\includegraphics[width=\columnwidth]{figures/all_programs.pdf}
\caption{{\bf Adversarial programs exhibit qualitative similarities and differences across both network and task.}
%Comparison of adversarial programs.
(a) Top: adversarial programs targeted to repurpose networks pre-trained on ImageNet to count squares in images.
Middle: adversarial programs targeted to repurpose networks pre-trained on ImageNet to function as MNIST classifiers.
Bottom: adversarial programs to cause the same networks to function as CIFAR-10 classifiers.
(b) Adversarial programs targeted to repurpose networks with randomly initialized parameters to function as MNIST classifiers.
}
\label{fig: programs}
\end{figure}\begin{figure}
\centering
\includegraphics[width=\columnwidth]{figures/shuffled.pdf}
\caption{{\bf Neural networks are susceptible to adversarial reprogramming even in cases when adversarial data and original task data are unrelated.}
%Comparison of adversarial programs.
The pixels in MNIST digits are shuffled. So, that the resulting image has no resemblance to any image. Then, the shuffled image is combined with the adversarial program to create a reprogramming image. This image successfully reprogram Inception V3 model to classify the shuffled digits, despite that the adversarial data (i.e., shuffled MNIST digits) being unrelated to the original data (i.e., ImageNet).
}
\label{fig: shuffled}
\end{figure}%\begin{table}%  \caption{Accuracy of reprogrammed ImageNet networks on CIFAR-10 (using random labels assignment). 100 random labels are assigned to each CIFAR10 label.}%  % gamaleldin: done%  %\jcom{specify test or train. better, should give both, e.g. as [test accuracy] / [train accuracy].}%  \label{table: reprogramming results}%  \centering%  \begin{tabular}{llllll}%    \toprule%   & \multicolumn{2}{c}{CIFAR10}         \\%        \cmidrule(r){2-3}%    ImageNet Model   &   train set & test  set \\%    \midrule%    Inception V3                 &  $0.7155$  & $0.6804$ \\%    Inception V4                 &   $0.7100$   & $0.6884$ \\%    Inception Resnet V2    &  $0.6973$ & $0.6616$  \\%    Resnet V2 152           &  $0.6515$ & $0.6329$ \\%    Resnet V2 101          & $0.6562$  & $0.6413$ \\%    \bottomrule%  \end{tabular}%\end{table}

\end{document}
