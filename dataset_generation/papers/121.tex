\documentclass{article}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[pdftex]{graphics}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

% to use Korean
\usepackage{kotex}
% to include images
\usepackage{graphbox}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{xcolor}
\graphicspath{./figure/} 

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
%\usepackage{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
\usepackage[numbers]{natbib}
%\usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\makeatletter
\newcommand{\printfnsymbol}[1]{%
  \textsuperscript{\@fnsymbol{#1}}%
}
\makeatother


\title{Auto-Meta: \\
Automated Gradient Based Meta Learner Search}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Jaehong Kim\textsuperscript{1}
  \And
  Sangyeul Lee\textsuperscript{1}
  \And
  Sungwan Kim\textsuperscript{1}
  \And
  Moonsu Cha\textsuperscript{1}
  \And
  Jung Kwon Lee\textsuperscript{1}
  \AND
  Youngduck Choi\textsuperscript{1,2}
  \And
  Yongseok Choi\textsuperscript{1}
  \And
  Dong-Yeon Cho\textsuperscript{1}
  \And
  Jiwon Kim\textsuperscript{1} 
  \AND
  \normalfont{SK T-Brain}\textsuperscript{1} \\
  Yale University\textsuperscript{2} \\
  \{ \texttt{xhark, sylee0335, sw0726.kim, ckanstnzja, jklee,} \\ 
  \texttt{yschoi, dycho24, jk}\} \texttt{@sktbrain.com} \\
  \texttt{youngduck.choi@yale.edu}
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle
\begin{abstract}
Fully automating machine learning pipelines is one of the key challenges of current artificial intelligence research, since practical machine learning often requires costly and time-consuming human-powered processes such as model design, algorithm development, and hyperparameter tuning. In this paper, we verify that automated architecture search synergizes with the effect of gradient-based meta learning. We adopt the progressive neural architecture search \cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559} to find optimal architectures for meta-learners. 
%Specifically, a good convolutional cell instead of a full convolutional neural network for few-shot image classification tasks was searched.
%Through the recent theoretical result on universality of gradient 
%based meta-learner, which states that sufficiently large depth of proto-architecture is a sufficient condition
%for the universality of the corresponding meta-learner, our search  
%introduces the issue of universality---a new dimension to neural architecture literature---of pre-defined search space.
%A posteriori, our search algorithm, given appropriately designed search spaces, finds gradient based meta learners with non-intuitive proto-architectures that are narrowly deep, unlike the inception-like structures previously observed in the resulting architectures of traditional NAS algorithms.
The gradient based meta-learner whose architecture was automatically found achieved state-of-the-art results on the 5-shot 5-way Mini-ImageNet classification problem with $74.65\%$ accuracy, which is $11.54\%$ improvement over the result obtained by the first gradient-based meta-learner called MAML \cite{finn:maml:DBLP:conf/icml/FinnAL17}.
To our best knowledge, this work is the first successful neural architecture search implementation in the context of meta learning.
 \end{abstract}
\section{Introduction}
Despite the lack of full knowledge of human learning mechanisms, many researchers tried to develop learning systems to mimic our ability to quickly adapt to new environments based on previous experiences. In \cite{DBLP:journals/corr/DuanSCBSA16:fast_rl,mishra:attentive_meta:DBLP:journals/corr/MishraRCA17,santoro:memoty_meta:DBLP:conf/icml/SantoroBBWL16,DBLP:journals/corr/WangKTSLMBKB16:learn_to_rl}, for example, recurrent neural networks (RNNs) were adopted to recognize the input and output mappings represented by training data and rapidly predict outputs for the test data using the internal states of learned RNN models. In contrast to these methods that rely on expert hand-crafted architectures, model-agnostic meta-learning (MAML) \cite{finn:maml:DBLP:conf/icml/FinnAL17,nichol:reptile:DBLP:journals/corr/abs-1803-02999} estimated a good initialization of model parameters for the fast adaptation to new tasks purely by a gradient-based search. Although we can apply model-agnostic meta-learning techniques to a variety of learning tasks without deeply contemplating the model architectures due to its model-agnostic properties, it is natural to expect that well-designed models for given tasks have better performances than conventional architectures.

In recent years, automation of machine learning has rapidly progressed. For instance, neural network architecture search
for image classification tasks has been successful. The selected architectures, when followed up with appropriate fine tuning, outperform models that are manually selected and trained by deep learning experts, a slow process that requires a large amount of trial and error guided by intuition.
Progressive neural architecture  search (PNAS) \cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559} 
is a particular form of the automated search that progressively expands the search candidate neural network architectures, supported by an RNN model for predicting candidates' performances without fully training the candidate architectures. Progressive neural architecture search achieves outstanding results without the significant computational expense required by reinforcement learning or evolutionary algorithm based searches.

In this paper, we propose a new approach to automatic design of neural network architectures for gradient-based meta learners. 
% Specifically, a good convolutional cell instead of a full convolutional neural network like in  \cite{zoph:nasnet_google:DBLP:journals/corr/ZophVSL17} for few-shot image classification tasks was searched by PNAS. 
As the gradient-based meta-learning algorithm for the task, we considered Reptile \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999} which is an first-order approximation of MAML \cite{finn:maml:DBLP:conf/icml/FinnAL17}. To our best knowledge, this combination is the first instance of successful
scalable neural architecture search work within the meta learning literature. For 5-shot 5-way Mini-ImageNet classification problem, we obtained $74.65\%$ accuracy, which is $11.54\%$ improvement over the result from MAML \cite{finn:maml:DBLP:conf/icml/FinnAL17}.


\section{Related Works}\paragraph{Meta-learning}%%%%%%%%%%%% YSCHOI added 180514_1547 %%%%%%%%%%%%%%%%%Meta learning as a theme is quite general, and extends well beyond the gradient based meta learners
for few shot classification tasks. In fact, much of the meta learning literature focuses on the general reinforcement
learning tasks \cite{DBLP:journals/corr/DuanSCBSA16:fast_rl, DBLP:journals/corr/WangKTSLMBKB16:learn_to_rl}.
One of the most common approaches to meta-learning is to build a recurrent neural network as a meta-learner. 
In particular, RNN based methods, augmented with
memory-augment network \cite{santoro:memoty_meta:DBLP:conf/icml/SantoroBBWL16} or simple attention mechanism \cite{mishra:attentive_meta:DBLP:journals/corr/MishraRCA17} have been
applied to few-shot image classification tasks. 

% \cite{mishra:attentive_meta:DBLP:journals/corr/MishraRCA17}.% \cite{DBLP:journals/corr/DuanSCBSA16:fast_rl}% \cite{santoro:memoty_meta:DBLP:conf/icml/SantoroBBWL16}% One of the most common approaches to meta-learning is to build a recurrent neural network as a meta-learner which takes a training dataset as input and produces the parameters of a learner as output \cite{s_bengio:1992}\cite{y_bengio:1990}\cite{Andrychowicz:nips2016:DBLP:conf/nips/AndrychowiczDCH16}\cite{Li:2016:DBLP:journals/corr/LiM16b}. Similarly, there have been some studies about training recurrent memory-augmented networks to take the labeled dataset and a test example as input and output a prediction of that example \cite{santoro:memoty_meta:DBLP:conf/icml/SantoroBBWL16}\cite{DBLP:journals/corr/DuanSCBSA16:fast_rl}\cite{DBLP:journals/corr/WangKTSLMBKB16:learn_to_rl}\cite{mishra:attentive_meta:DBLP:journals/corr/MishraRCA17}.
Metric learning is another popular approach to address meta-learning problems. The meta learner attempts to
learn a metric which can be used to compare two different examples effectively and performs tasks in the learned metric space \cite{Vinyals:nips2016:DBLP:conf/nips/VinyalsBLKW16}. Some studies train a Siamese network to achieve the same objective \cite{Koch:icmlw2015:DBLP:conf/icml/ShyamGD17}. The metric-based meta-learning has been known to perform well for few-shot image classification tasks \cite{Snell:nips2017:DBLP:conf/nips/SnellSZ17, Shyam:icml2017:DBLP:conf/icml/ShyamGD17}.

% Another popular approach is to learn a metric space research direction to the meta-learning has focused on few-shot learning for specific tasks such as image recognition. \cite{Edwards:2016:DBLP:journals/corr/EdwardsS16}\cite{Rezende:2016:DBLP:conf/icml/RezendeMDGW16} % Few-shot classification has been one of the most successful applications by learn a metric space in this direction \cite{Shyam:icml2017:DBLP:conf/icml/ShyamGD17}\cite{Snell:nips2017:DBLP:conf/nips/SnellSZ17}\cite{Koch:icmlw2015:DBLP:conf/icml/ShyamGD17}.% 또 다른 방향의 연구로는 특정한 task에 대한 few-shot learning을 수행하기 위한 몇 가지 연구들이 이루어졌으며\cite{Edwards:2016:DBLP:journals/corr/EdwardsS16}; \cite{Rezende:2016:DBLP:conf/icml/RezendeMDGW16}, 이 중 metric space를 학습하여 few-shot image classification에 성공적으로 적용한 사례들이 있다.\cite{Vinyals:nips2016:DBLP:conf/nips/VinyalsBLKW16}\cite{Shyam:icml2017:DBLP:conf/icml/ShyamGD17}\cite{Snell:nips2017:DBLP:conf/nips/SnellSZ17}\cite{Koch:icmlw2015:DBLP:conf/icml/ShyamGD17}% % Few-shot learning methods have also been developed for specific tasks such as generative modeling (\cite{Edwards:2016:DBLP:journals/corr/EdwardsS16}; \cite{Rezende:2016:DBLP:conf/icml/RezendeMDGW16}) and image recognition (Vinyals et al., 2016). One successful approach for few-shot classification is to learn to compare new examples in a learned metric space using e.g. Siamese networks (Koch, 2015) or recurrence with attention mechanisms (Vinyals et al., 2016; Shyam et al., 2017; Snell et al., 2017). These approaches have generated some of the most successful results, but are difficult to directly extend to other problems, such as reinforcement learning. Our method, in contrast, is agnostic to the form of the model and to the particular learning task.%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% * Model-agnostic meta learning%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The other major category of meta learning is to learn an optimizer 
as the meta-learner which enables the learner to learn a new task more effectively \cite{Hochreiter:2001:DBLP:conf/icann/HochreiterYC01, Andrychowicz:nips2016:DBLP:conf/nips/AndrychowiczDCH16}. 
This approach has been applied to few-shot learning successfully \cite{ravi:opt_fewshot:DBLP:conf/iclr/RaviL17}. 
Rather than using the learned optimizer, a new meta-learning scheme applicable 
to all gradient-based learning algorithms, model-agnostic meta-learning (MAML), has recently been proposed \cite{finn:maml:DBLP:conf/icml/FinnAL17}. MAML attempts to find a set of parameters which initializes 
a learner for any specific task to be trained quickly only with small amount of data. 
Although this technique has shown the effectiveness for various few-shot learning problems including few-shot image classification and reinforcement learning, Hessian vector product calculation during training requires a 
large amount of computation. The first-order approximation algorithm has been proposed to avoid the Hessian computations in \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999}. Some advantages of gradient based meta learners have also
been discussed in \cite{finn:universality_maml:DBLP:/journals/corr/abs-1710-11622}.

% so that it trains% they could be trained within a few gradient steps and with a few amount of data for a specific task % \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999}% 최근에는 gradient descent 기반 모든 model에 적용할 수 있는 "model-agnostic meta learning (MAML)"이라는 meta-learning 방안을 제안되었다.\cite{finn:maml:DBLP:conf/icml/FinnAL17} 구체적으로 MAML에서는 적은 수의 data를 가지고 a few gradient step 내에 특정 task에 효과적으로 generalization할 수 있는 parameter의 initial set을 찾는 것을 목적으로 하고 있으며 image classification, reinforcement learning을 포함하는 few-shot learning 문제에서 효과적으로 동작함을 보였다. \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999}에서는 MAML에서의 meta-parameter update 시 발생하는 Hessian-vector product 계산의 복잡도를 줄이기 위해 이에 대한 first-order approximation algorithm을 제안하였다. \cite{finn:universality_maml:DBLP:/journals/corr/abs-1710-11622}에서는 이와 같은 gradient descent 기반 meta learning 방안이 어떠한 learning algorithm도 approximation 할 수 있다는 universality를 가진다는 점을 주장하였다.% % * Model-agnostic meta learning% \cite{finn:maml:DBLP:conf/icml/FinnAL17}에서는 model-agnostic meta learning (MAML)이라고 부르는 gradient descent 기반 모든 model에 적용할 수 있는 meta learning 방안을 제안했다. 구체적으로 MAML에서는 적은 수의 data를 가지고 a few gradient step 내에 특정 task에 효과적으로 generalization할 수 있는 parameter의 initial set을 찾는 방법을 제안하였으며 image classification, reinforcement learning을 포함하는 few-shot learning 문제에서 효과적으로 동작함을 보였다. \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999}에서는 MAML에서의 meta-parameter update 시 발생하는 Hessian-vector product 계산의 복잡도를 줄이기 위해 이에 대한 first-order approximation algorithm을 제안하였다. \cite{finn:universality_maml:DBLP:/journals/corr/abs-1710-11622}에서는 이와 같은 gradient descent 기반 meta learning 방안의 universality 문제를 다루고 있다.% % \cite{al-shedivat:cont_adapt_meta:DBLP:journals/corr/abs-1710-03641} % ICLR 2018 best paper% We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. % Model-Agnostic Meta-Learning (MAML) is a method that proposes to learn an initial set of parameters θ such that one or a few gradient steps on θ computed using a small amount of data for one task leads to effective generalization on that task (Finn et al., 2017a).%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Before 180514_1547 %%%%%%%%%%%%%%%%%% % The method that we propose in this paper addresses the general problem of meta-learning (Thrun & Pratt, 1998; Schmidhuber, 1987; Naik & Mammone, 1992), which includes few-shot learning. % For meta-learning, one historically popular appraoch has been to find % learner model'sparameter update % * 전통적으로 popular한 meta-learning 방안은 learner's model의 parameter를 update하는 방법을 배우는 meta-learner를 학습하는 방안들이며. (???)% Recently, there has been deep learning approaches, including RNNs.% \cite{s_bengio:1992}\cite{schmidhuber:1987:srl}\cite{y_bengio:1990}% \cite{Ha:2016:DBLP:journals/corr/HaDL16}\cite{Hochreiter:2001:DBLP:conf/icann/HochreiterYC01}\cite{Andrychowicz:nips2016:DBLP:conf/nips/AndrychowiczDCH16}; % \cite{Li:2016:DBLP:journals/corr/LiM16b}% % A popular approach for meta-learning is to train a meta-learner that learns how to update the parameters of the learner’s model (\cite{s_bengio:1992}\cite{schmidhuber:1987:srl}\cite{y_bengio:1990}). This approach has been applied to learning to optimize deep networks (\cite{Hochreiter:2001:DBLP:conf/icann/HochreiterYC01}; \cite{Andrychowicz:nips2016:DBLP:conf/nips/AndrychowiczDCH16}; \cite{Li:2016:DBLP:journals/corr/LiM16b}), as well as for learning dynamically changing recurrent networks (\cite{Ha:2016:DBLP:journals/corr/HaDL16}). % Another approach has been to learn weight initialization and optimizer% at the same time(\cite{ravi:opt_fewshot:DBLP:conf/iclr/RaviL17}),% which makes a stark contrast with MAML, which fixes the gradient descent step.% Another approach that has gathered much attention is to learn the metric space% directly, which has succesfully been applied to few shot image classification.% \cite{Vinyals:nips2016:DBLP:conf/nips/VinyalsBLKW16}\cite{Shyam:icml2017:DBLP:conf/icml/ShyamGD17}\cite{Snell:nips2017:DBLP:conf/nips/SnellSZ17}\cite{Koch:icmlw2015:DBLP:conf/icml/ShyamGD17}.% % One recent approach learns both the weight initialization and the optimizer, for few-shot image recognition (\cite{ravi:opt_fewshot:DBLP:conf/iclr/RaviL17}). Unlike these methods, the MAML learner’s weights are updated using the gradient, rather than a learned update; our method does not introduce additional parameters for meta-learning nor require a particular learner architecture. % For applications of meta-learning, tasks, such as Image classification, % and generative modeling with few shot have been widly explored. % \cite{Edwards:2016:DBLP:journals/corr/EdwardsS16}; \cite{Rezende:2016:DBLP:conf/icml/RezendeMDGW16}, % % Few-shot learning methods have also been developed for specific tasks such as generative modeling (\cite{Edwards:2016:DBLP:journals/corr/EdwardsS16}; \cite{Rezende:2016:DBLP:conf/icml/RezendeMDGW16}) and image recognition (Vinyals et al., 2016). One successful approach for few-shot classification is to learn to compare new examples in a learned metric space using e.g. Siamese networks (Koch, 2015) or recurrence with attention mechanisms (Vinyals et al., 2016; Shyam et al., 2017; Snell et al., 2017). These approaches have generated some of the most successful results, but are difficult to directly extend to other problems, such as reinforcement learning. Our method, in contrast, is agnostic to the form of the model and to the particular learning task.% * Memory-augmented network을 학습하여 meta-learning을 수행하는 방안들도 제안되었으며, 이들은 실제 deploy 시 새로운 task에 adapt할 수 있는 recurrent learner를 train하게 된다. Image recognition \cite{santoro:memoty_meta:DBLP:conf/icml/SantoroBBWL16}, \cite{DBLP:conf/icml/MunkhdalaiY17:meta_net}, “fast” reinforcement learning agents \cite{DBLP:journals/corr/DuanSCBSA16:fast_rl}\cite{DBLP:journals/corr/WangKTSLMBKB16:learn_to_rl} 등에 이러한 방법이 적용되었다.% % Another approach to meta-learning is to train memory augmented models on many tasks, where the recurrent learner is trained to adapt to new tasks as it is rolled out. Such networks have been applied to few-shot image recognition (Santoro et al., 2016; Munkhdalai & Yu, 2017) and learning “fast” reinforcement learning agents (Duan et al., 2016b; Wang et al., 2016). Our experiments show that our method outperforms the recurrent approach on fewshot classification. Furthermore, unlike these methods, our approach simply provides a good weight initialization and uses the same gradient descent update for both the learner and meta-update. As a result, it is straightforward to finetune the learner for additional gradient steps.% * Model-agnostic meta learning% \cite{finn:maml:DBLP:conf/icml/FinnAL17}에서는 model-agnostic meta learning (MAML)이라고 부르는 gradient descent 기반 모든 model에 적용할 수 있는 meta learning 방안을 제안했다. 구체적으로 MAML에서는 적은 수의 data를 가지고 a few gradient step 내에 특정 task에 효과적으로 generalization할 수 있는 parameter의 initial set을 찾는 방법을 제안하였으며 image classification, reinforcement learning을 포함하는 few-shot learning 문제에서 효과적으로 동작함을 보였다. \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999}에서는 MAML에서의 meta-parameter update 시 발생하는 Hessian-vector product 계산의 복잡도를 줄이기 위해 이에 대한 first-order approximation algorithm을 제안하였다. \cite{finn:universality_maml:DBLP:/journals/corr/abs-1710-11622}에서는 이와 같은 gradient descent 기반 meta learning 방안의 universality 문제를 다루고 있다.% % \cite{al-shedivat:cont_adapt_meta:DBLP:journals/corr/abs-1710-03641} % ICLR 2018 best paper% We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. % Model-Agnostic Meta-Learning (MAML) is a method that proposes to learn an initial set of parameters θ such that one or a few gradient steps on θ computed using a small amount of data for one task leads to effective generalization on that task (Finn et al., 2017a).% \paragraph{Datasets}% Omniglot few-shot classification% Mini-ImageNet few-shot classification% 우리의 연구는 두 가지 다른 방향에서 meta-learning을 동시에 수행하는 연구로서 이들이 효과적으로 성능을 높이는데 기여함을 보여준다.% 우리의 연구는 \cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559}와 \cite{finn:maml:DBLP:conf/icml/FinnAL17}, \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999} 방안에 기반을 두고 있다. % Literature on Neural Architecture Search% From http://www.ml4aad.org/literature-on-neural-architecture-search/% The following list considers papers related to neural architecture search. It is by no means a complete list. If you miss a paper on this list, please let us know.% Architecture Search (and Hyperparameter Optimization): % Neural Architecture Search with Bayesian Optimisation and Optimal Transport (Kandasamy et al. 2018) % https://arxiv.org/abs/1802.07191% Efficient Neural Architecture Search via Parameter Sharing (Pham et al. 2018)% https://arxiv.org/abs/1802.03268% Neural Architecture Construction using EnvelopeNets (Kamath et al. 2018)% https://arxiv.org/abs/1803.06744% Regularized Evolution for Image Classifier Architecture Search (Real et al. 2018)% https://arxiv.org/abs/1802.01548% Large-Scale Evolution of Image Classifiers (Real et al. 2017)% https://arxiv.org/abs/1703.01041% Hierarchical Representations for Efficient Architecture Search (Liu et al. 2017)% https://arxiv.org/abs/1711.00436% Neural Optimizer Search with Reinforcement Learning (Bello et al. 2017)% https://arxiv.org/abs/1709.07417% Progressive Neural Architecture Search (Liu et al. 2017)% https://arxiv.org/abs/1712.00559% Learning Transferable Architectures for Scalable Image Recognition (Zoph et al. 2017)% https://arxiv.org/abs/1707.07012% Simple And Efficient Architecture Search for Convolutional Neural Networks (Elsken et al. 2017) % https://arxiv.org/abs/1711.04528% Finding Competitive Network Architectures Within a Day Using UCT (Wistuba 2017) % https://arxiv.org/abs/1712.07420% Hyperparameter Optimization: A Spectral Approach (Hazan et al. 2017)% https://arxiv.org/abs/1706.00764% Population Based Training of Neural Networks (Jaderberg et al. 2017)% https://arxiv.org/abs/1711.09846% SMASH: One-Shot Model Architecture Search through HyperNetworks (Brock et al. 2017) % https://arxiv.org/abs/1708.05344% Efficient Architecture Search by Network Transformation (Cai et al. 2017)% https://arxiv.org/abs/1707.04873% Towards Automatically-Tuned Neural Networks (Mendoza et al. 2016)% http://proceedings.mlr.press/v64/mendoza_towards_2016.html% Convolutional Neural Fabrics (Saxena and Verbeek 2016)% https://arxiv.org/abs/1606.02492% CMA-ES for Hyperparameter Optimization of Deep Neural Networks (Loshchilov and Hutter 2016)% https://arxiv.org/abs/1604.07269% Designing Neural Network Architectures using Reinforcement Learning (Baker et al. 2016) % https://arxiv.org/abs/1611.02167% Neural Architecture Search with Reinforcement Learning (Zoph and Le. 2016)% https://arxiv.org/abs/1611.01578% Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization (Li et al. 2016) % https://arxiv.org/abs/1603.06560% Optimizing deep learning hyper-parameters through an evolutionary algorithm (Young et al. 2015)% https://dl.acm.org/citation.cfm?id=2834896% Practical Bayesian Optimization of Machine Learning Algorithms (Snoek et al. 2012)% https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf  % Make it more efficient % Gitgraph – From Computational Subgraphs to Smaller Architecture search spaces (Bennani-Smires et al. 2018)% https://openreview.net/pdf?id=rkiO1_1Pz% Transfer Automatic Machine Learning (Wong et al. 2018)% https://arxiv.org/abs/1803.02780% Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets (Klein et al. 2017)% http://proceedings.mlr.press/v54/klein17a.html% Learning curve prediction with Bayesian Neural Networks (Klein et al. 2017)% http://ml.informatik.uni-freiburg.de/papers/17-ICLR-LCNet.pdf% Multi-Objective NAS % PPP-Net: Platform-aware progressive search for pareto-optimal neural architectures (Dong et al. 2018)% https://openreview.net/pdf?id=B1NT3TAIM% http://www.ml4aad.org/literature-on-neural-architecture-search/% http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/% http://metalearning-symposium.ml/% Meta-learning은 "learn to learn"에 대한 광범위한 개념을 포함한다. % % 정의:% % The framework of using one learning system to modify or optimize certain aspects of another learning system% % 중요성:% % Machine Learning systems become increasingly complex: Many hyperparameters, Special design \& architecture components% % How can the complexity, i.e. the design, components, and hyperparameters, be configured automatically so that these systems perform as well as possible?% 이 중 우리의 연구와 밀접하게 관련된 분야는 다음과 같다.\paragraph{Neural network architecture search}
Neural network architecture search (NAS) is a methodology to 
automatically find optimal neural network architectures for a given task. There are various 
types of NAS, such as reinforcement learning based NAS and evolutionary algorithm based NAS. 
Reinforcement learning based NAS includes REINFORCE \cite{zoph:rl_nas_google:DBLP:journals/corr/ZophL16}, 
Q-learning \cite{zhong:q_learning_nas:DBLP:journals/corr/abs-1708-05552, baker:rl_nas:DBLP:journals/corr/BakerGNR16}, 
and PPO-type algorithms \cite{zoph:nasnet_google:DBLP:journals/corr/ZophVSL17}. Evolutionary algorithm based NAS has extensively been explored in \cite{real:large_scale_nas_google:DBLP:conf/icml/RealMSSSTLK17,
miikkulainen:evolv_nas:DBLP:journals/corr/MiikkulainenLMR17, xie:genetic_cnn:DBLP:conf/iccv/XieY17, 
liu:hier_nas:DBLP:journals/corr/abs-1711-00436,
real:amoebanet_reg_nas:DBLP:journals/corr/abs-1802-01548}.
For example, AmoebaNet \cite{real:large_scale_nas_google:DBLP:conf/icml/RealMSSSTLK17} applies an evolutionary
algorithm to the same search space of NASNet and achieves state-of-the-art results on image classification tasks.
%On the other hand, evolutionary algorithms %Second type is that of evolutionary algorithm%Different tpyes of NAS%Search space를 convolution layer에 대한 각각의 design parameter를 선택하는 대신 pre-defined convolution operation들 중 하나를 선택하도록 바꾸어 search 효율성 향상 \cite{zoph:nasnet_google:DBLP:journals/corr/ZophVSL17} ("NASNet"), 탐색 대상 models 간 learned parameter sharing을 통한 search 효율성 향상 \cite{pham:enas_google:DBLP:journals/corr/abs-1802-03268} ("ENAS")
Other methods deploy various types of reasonable heuristics that attempt to reduce computational cost. 
This line of thinking is present in hypernetworks \cite{brock:smash:DBLP:journals/corr/abs-1708-05344}, co-evolving NEAT \cite{miikkulainen:evolv_nas:DBLP:journals/corr/MiikkulainenLMR17}, boosting \cite{cortes:adanet:DBLP:conf/icml/CortesGKMY17,  huang:boost_nas:DBLP:journals/corr/HuangALS17}, MCTS \cite{negrinho:deeparch:DBLP:journals/corr/NegrinhoG17}, early stopping of unpromising models \cite{baker:nas_perf_pred:DBLP:journals/corr/BakerGRN17}, and progressive neural architecture 
search (PNAS) \cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559}.
In particular, PNAS expands the search space incrementally from simple to complex so that it can search architectures in an efficient way without limiting the search to the space of fully-specified architectures.
%\textcolor{red}{PNAS 특별 코멘트 추가?}%greedy type alogrithms, which we call%progressive heuristic search. %\cite{negrinho:deeparch:DBLP:journals/corr/NegrinhoG17,%mendoza:auto_tune_net:DBLP:conf/icml/MendozaKFSH16,%cortes:adanet:DBLP:conf/icml/CortesGKMY17,%huang:boost_nas:DBLP:journals/corr/HuangALS17,%liu:pnas_google:DBLP:journals/corr/abs-1712-00559} ("PNAS")%Rather than searching within the set of fully-specified architectures,%one can incrementally expand the search space from simple to complex. %Negrinho et al. introduced a MCTS method. Liu et al. %introduced a method for surrograte predictor network that %seearchs within Zoph. %\cite{negrinho:deeparch:DBLP:journals/corr/NegrinhoG17}이 제안되었다. \cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559} ("PNAS")에서는 \cite{zoph:nasnet_google:DBLP:journals/corr/ZophVSL17}에서 제안된 structured search space 내에서 progressively search 하는 방안을 제안하였고, performanc predictor로 동작하는 surrogate network을 함께 training하면서 이 결과를 이용하여 각 단계에서의 search space를 합리적인 수준으로 줄이는 데 활용하고 있다. %In Adanet, one uses boosting to expend CNN sequentially. \cite{cortes:adanet:DBLP:conf/icml/CortesGKMY17}, \cite{huang:boost_nas:DBLP:journals/corr/HuangALS17}%Other heuristics include early stopping of unpromising models%\cite{baker:nas_perf_pred:DBLP:journals/corr/BakerGRN17} for improving search speed.% 성능 prediction 위한 surrogate function% \cite{brock:smash:DBLP:journals/corr/abs-1708-05344}% \cite{cai:transform_nas:DBLP:conf/aaai/CaiCZYW18} % \cite{domhan:extrapol_nas:DBLP:conf/ijcai/DomhanSH15}% \cite{baker:nas_perf_pred:DBLP:journals/corr/BakerGRN17}% \cite{mendoza:auto_tune_net:DBLP:conf/icml/MendozaKFSH16}% 더 자세히% \cite{negrinho:deeparch:DBLP:journals/corr/NegrinhoG17}% \cite{mendoza:auto_tune_net:DBLP:conf/icml/MendozaKFSH16}% \cite{cortes:adanet:DBLP:conf/icml/CortesGKMY17}% \cite{huang:boost_nas:DBLP:journals/corr/HuangALS17}% \cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559} ("PNAS")%* Others% 성능 prediction 위한 surrogate function% \cite{brock:smash:DBLP:journals/corr/abs-1708-05344}% \cite{cai:transform_nas:DBLP:conf/aaai/CaiCZYW18} % \cite{domhan:extrapol_nas:DBLP:conf/ijcai/DomhanSH15}% \cite{baker:nas_perf_pred:DBLP:journals/corr/BakerGRN17}%Unpromising model에 대한 training을 early stopping하여 search 속도를 향상하는 방안을 제안하였다. \cite{baker:nas_perf_pred:DBLP:journals/corr/BakerGRN17} %\cite{dong:ppp_net:DBLP:journals/corr/abs-1804-06964}에서는 speed와 accuracy (performance)의 multiple objective를 가지고 이들의 tradeoff에 대한 탐색을 수행하는 search 방안을 제안했다.%\cite{huang:gnas:DBLP:journals/corr/abs-1804-06964} ("GNAS")에서는 multi-attribute (multi-task?) learning을 위한 neural architecture search 방안을 제안하였다. 
\section{Auto-Meta}
Our goal is to automatically find the optimal network architecture for gradient-based meta-learners. Considering the loss function $\mathcal{L}$ for given tasks $j$ represented by training and test data sets $(D_{j}^{Tr},D_{j}^{Te})$, this can be formulated as follows:
\begin{eqnarray}
\min_{A,\theta} \sum_{j} \mathcal{L} \big (D_{j}^{Te}, U(D_{j}^{Tr},\theta;A) \big ),
\label{eq1}
\end{eqnarray}
where $A$ and $\theta$ are the neural network architecture and its parameters, respectively. $U$ denotes the computation of parameter updates using one or more gradient descent steps.
A natural and simply way to solve this problem is to minimize the loss $\mathcal{L}$ over parameters keeping the candidate architectures fixed. Then, based on some promising architectures, more complicated architectures are searched progressively. By repeating these two steps, we can obtain a good approximate solution to Equation (\ref{eq1}).

As the gradient-based meta-learning algorithm, we adopt Reptile \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999} which showed comparable performance with MAML \cite{finn:maml:DBLP:conf/icml/FinnAL17} on few-shot classification problems using only first-order gradient information. This allows us to avoid the time-consuming calculation of second-order derivatives.
As the network architecture search method, we consider the PNAS algorithm \cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559} where three layers (i.e., block, cell, and network) of abstraction for representing a neural network topology were defined. At most $B$ blocks which represent a combination operators applied to two inputs are included in a cell. This cell is then stacked a certain number of times to create a full CNN. During the architecture search, the cells ``progressively'' get more complicated by adding a block to themselves. Without expensive training procedure, the performance of each cell is evaluated with a surrogate predictor, such as LSTM to rank all expanded candidate cells. Then, CNNs with the top $K$ cells are trained and evaluated. We continue in this way until each cell has the maximum number of blocks.

%\begin{algorithm}[h!]%\caption{PNAS on Reptile}\label{euclid}% \hspace*{\algorithmicindent} \textbf{Input: } \textit{B} (max num block)%\begin{algorithmic}[1]%\Procedure{Architecture Search}{}%\State $\textit{S}_{1} \gets $ Set of architectures with one block%\State $\textit{C}_{1} \gets train\_Reptile(\textit{S}_{1}, trainSet)$%\State $\textit{A}_{1} \gets eval\_Reptile(\textit{C}_{1}, valSet)$%\State $\pi \gets fit (\textit{S}_{1}, \textit{A}_{1}) $%\For{$\textit{b} = 2:\textit{B}$}%\State $\textit{S}'_{b} \gets expand\_block(S_{b-1})$%\State $\hat{\textit{A}}'_{b} \gets \pi(\textit{S}'_{b})$%\State $\textit{S}_{b} \gets top\_K(\textit{S}'_{b}, \hat{\textit{A}}'_{b}, \textit{K})$%\State $\textit{C}_{b} \gets train\_Reptile(\textit{S}_{b}, trainSet)$%\State $\textit{A}_{b} \gets eval\_Reptile(\textit{C}_{b}, valSet)$%\State $\pi \gets update\_predictor (\textit{S}_{b}, \textit{A}_{b}, \pi) $%\EndFor%\Return $top\_K(\textit{S}_B, \textit{A}_B, 1)$%\EndProcedure%\end{algorithmic}%\end{algorithm}
%\input{text/Main2.tex}
\section{Experiments and Results}
To implement our gradient based meta-learner search, 
a distributed system that deploys and trains many neural networks
in a stable manner is required. We apply a Kubernetes system \cite{Kubernetes} that supports utilities to deploy over clusters of GPUs, in order to achieve the parallelization of training and testing top models in each iteration of block progression. 
We used the system with $112$ P$40$ GPUs for experiments described in this section. 
Our architecture search procedure on Mini-ImageNet dataset takes $24$ hours on average after parallelization.

%새로운 neural network topology 를 찾기 위해서는 다량의 Neural Architecture들을 안정적으로 deploy 하고 빠르게 학습할 수 있는 분산 환경이 필요하다.  우리는 이러한 workload 를 처리하기 위해 100개 이상의 GPU 클러스터 상에서 배포, 유지, 확장 할 수 있는 Kubernetes System 을 적용하였으며,  LSTM Predictor 가 찾은 수백개의 새로운 Neural Architecture 들을 안정적으로 배포할 수 있었다. % Gradient Based Meta-learner 를 찾기 위한 구현에는 두 파트가 있다. 첫 번째는 상기 알고리즘에 정의된 LSTM Predictor 를 update 하기 위한 방식으로,  하나의 Search Process 가 Progressive 하게 B개의 Iteration 을 통해  Search Space 에서 Cell 을 생산한다.  생산된 Cell 은 100개의 GPU Node 상에 deploy 되고, 각각의 Cell 은 MiniImagenet Dataset 을 기반으로 Reptile 을 학습한다.  학습이 끝난 각각의 Cell 은 LSTM Predictor 에 Accuracy 를 전달하고,  LSTM 은 다음 Block 의 Iteration 을 수행할 Top-K Candidate cell 을 예측한다. Architecture Search 단계에서는 100개의 GPU 를 이용하여 12시간 동안 search 하였다.   \subsection{Few Shot Image Classification}\label{experiment_details}
To evaluate the automated architecture search algorithm for the gradient-based meta-learning, we applied our method to few-shot image classification problems. We used two benchmark datasets Omniglot \cite{Lake1332} and Mini-ImageNet \cite{Vinyals:nips2016:DBLP:conf/nips/VinyalsBLKW16}\cite{ravi:opt_fewshot:DBLP:conf/iclr/RaviL17}. In the Omniglot dataset, there are 1623 handwritten characters from 50 different alphabets. The Mini-ImageNet dataset consists of $60,000$ images ($84\times 84$  pixels) with $100$ classes.
%Among them, we use $82,099$ images with $64$ classes for training, $25,964$ images with $20$ classes for testing, and $20,716$ images with $16$ classes as a validation set.
In particular, we conducted the 5-way, 1-shot and 5-shot classification tasks through the proposed gradient based meta learner search.
Meta-test accuracy of meta learner was used as score for the LSTM predictor. Based on predicted scores from surrogate the LSTM, we trained 100 most promising cell candidates.
After search completed, we chose the cell structure with best score for final training. We use $3\times3$ convolution, $5\times5$ factorized convolution, identity, $3\times3$ average pooling, and $3\times3$ max pooling as operations for blocks. For training parameters of the network and surrogate LSTM predictor, we used Adam optimizer with learning rate $0.01$. Other hyperparameters used in our experiments are given in Table \ref{Tab:hyperparameters}.
%When searching, $N = 0,1, F = 4,32, B = 5$ are used for the progressive neural architecture search part. Meta batch size is $5$, and meta iterations is $1000$. Meta step is primarily set to be $2$  and then decays linearly to $0$ during training.\begin{table}[h!]
 \centering
 \begin{tabular}{l|l|l}
 \hline
  Algorithm& \#Params &Accuracy\\
  \hline
  Ours ($F=10$) & 25.9k & \bf{97.44} $\pm$ \bf{0.07}\%\\
  Ours$^{Transduction}$ ($F=10$) & 25.9k & \bf{98.94} $\pm$ \bf{0.05}\%\\
  MAML$^{Transduction}$ \cite{finn:maml:DBLP:conf/icml/FinnAL17}& 112.0k & 98.7 $\pm$ 0.4\%\\
  1st-order MAML$^{Transduction}$ \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999}& 112.0k &98.3 $\pm$ 0.5\%\\
  Reptile \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999}& 113.2k & 95.39 $\pm$ 0.09\%\\
  Reptile$^{Transduction}$ \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999}& 113.2k & 97.68 $\pm$ 0.04\%\\
  \hline
  Matching-Nets \cite{Vinyals:nips2016:DBLP:conf/nips/VinyalsBLKW16}&-& 98.1\%\\
 Prototypical-Nets \cite{Snell:nips2017:DBLP:conf/nips/SnellSZ17}&111.9k&  98.8\%\\
 \hline
 \end{tabular}
 \\ \smallskip
 \caption{Results on Omniglot dataset for the 1-shot 5-way classification\\
 (\textit{F}: The number of filters in the first convolution layer, 
 \textit{Transduction}: Prediction at test time using batch normalization with batch mean and variance \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999})}
 \label{Tab:Omniglot}
 \end{table}
As shown in Table \ref{Tab:Omniglot}, our architecture search algorithm successfully found an excellent cell in 1-shot 5-way image classification for the Omniglot dataset so that the full convolution neural network (CNN) constructed with the cell outperformed the human-designed CNN trained with the Reptile \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999}. It is also worth pointing out that our CNN model has much smaller number of parameters than other ones.

\begin{table}[h]
\centering
\begin{tabular}{r|l|l|l|l|l}
\hline
\multicolumn{2}{c|}{ } & \multicolumn{2}{c|}{1-shot 5-way} & \multicolumn{2}{c}{5-shot 5-way} \\
 &Algorithm&\#Params&Accuracy&\#Params&Accuracy\\
 \hline
  \textit{small}&Ours ($F=10,10$)& 28k & \bf{49.58} $\pm$ \bf{0.20}\% &28k& \bf{65.09} $\pm$ \bf{0.24}\%\\
  setting&Ours$^{Transduction} (F=10,10)$& 28k & \bf{54.02} $\pm$ \bf{0.13}\%&28k&\bf{69.77} $\pm$ \bf{0.31\%}\\

  & MAML$^{Transduction}$ \cite{finn:maml:DBLP:conf/icml/FinnAL17}& 32.8k &  $48.70 \pm 1.84 \%$&32.8k& 63.11 $\pm$ 0.92\%\\
%  & 1st-order MAML + Transduction \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999}& 35k &$47.07 \pm 0.26\%$ \\
 & Reptile \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999}& 34.7k & $47.07 \pm 0.26\%$&34.7k& 62.74 $\pm$ 0.37\%\\
 & Reptile$^{Transduction}$ \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999}& 34.7k & 49.96 $\pm$ 0.32\%&34.7k& 65.99 $\pm$ 0.58\% \\

 \hline\hline
 \textit{large}& Ours ($F=12,12$) & 98.7k & \bf{51.16} $\pm$ \bf{0.17}\% &94.0k& \bf{69.18} $\pm$ \bf{0.14}\%\\
 setting & Ours$^{Transduction} (F=12,12)$& 98.7k & \bf{57.58} $\pm$ \bf{0.20}\% &94.0k& \bf{74.65} $\pm$ \bf{0.19}\%\\
 & Matching-Nets \cite{Vinyals:nips2016:DBLP:conf/nips/VinyalsBLKW16} & - & 43.60\% &-& 55.30\%\\
 & Ravi and Laroche \cite{ravi:opt_fewshot:DBLP:conf/iclr/RaviL17} & - & $43.40 \pm 0.77\%$ &-& 60.20 $\pm$ 0.71\%\\
 & Prototypical-Nets \cite{Snell:nips2017:DBLP:conf/nips/SnellSZ17} & 113.1k & $49.42 \pm 0.78\%$ &113.1k& 68.20 $\pm$ 0.66\%\\
 \hline
\end{tabular}
\\ \smallskip
\caption{Number of parameters and accuracy on Mini-ImageNet for 5-way, 1-shot and 5-shot classification tasks}
\label{Tab:MiniImage}
\end{table}

Results for Mini-ImageNet dataset are shown in Table \ref{Tab:MiniImage}. For fair comparison, we tried to make full CNN models with the searched cell have similar capacity to models adopted by previous work in \textit{small} setting. For the 1-shot and 5-way task, our method showed the comparable performance with others. However, the results clearly showed the superiority of the automated search for the 5-shot and 5-way task. Without restricting the model size in terms of the number of parameters, we constructed the best performing CNN with the best cell by appropriately choosing the value of \textit{F} in \textit{large} setting. For both 1-shot and 5-shot tasks, our model had the best accuracy. More specifically, our gradient based meta learner which have the automatically searched cell achieved $74.65\%$ accuracy on the 5-shot 5-way task. This implies that our approach not only drastically improved performance of other meta-learners but also can compete with state-of-the-art techniques such as \cite{Rusu2018} for few shot classification tasks. The best cell architectures found in these tasks were shown in Figure \ref{fig:best_cells}.

% 우리는 Mini-ImageNet의 Few Shot classification task에 대한 Meta-Learner로 Reptile Algorithm 사용하였다. 84x84로 리사이즈된 100개 class 총 128,779개 Mini-ImageNet 이미지 리소스를 사용하였다. 이 중 64개 class 82,099개를 train set, 20개 class 25,964개를 test set, 16개 class 20,716개를 validation set으로 나누어 사용하였다.% Reptile Algorithm에 적합한 최적 cell을 찾기 위하여 5-shot, 5-way classification task에 대한 Reptile Algorithm의 test accuracy를 score로 하는 B=5 blocks, F=32 filters, N=0 times의 PNAS Algorithm을 사용하였다. % PNAS Algorithm에서 매 Stage마다 Predictor에서 예측한 score를 토대로 상위 K=100 networks를 이용하여 Predictor를 학습시키고, 모든 stage가 종료된 후 최종적으로 가장 score가 높은 cell을 Reptile Algorithm에 적합한 최적 cell로 선택하였다. PNAS의 Operation은 3x3 conv, 5x5 factorized conv, identity, 3x3 average pooling, 3x3 max pooling 5가지를 사용하였고, 각 cell과 predictor는 learning rate lr=0.01의 AdamOptimizer를 사용하였다. Reptile의 meta batch size는 5, meta iterations는 1000이며, meta step은 초기 2, 최종 0으로 선형 감소한다.% 그후 선택된 최적 cell에서 원래 Reptile의 Parameter 수와 유사한 Parameter 수를 갖는 PNAS Parameter의 F=10 or 12, N=0으로 5-shot, 5-way classification task와 1-shot, 5-way classification task 대해 실험하였다. 또한 같은 task에 대해 여러 F, N으로 가장 좋은 성능을 갖는 값을 찾는 실험을 하였다.  % \begin{table}[h!]% \centering% \label{my-label}% \begin{tabular}{l |l |l |l |}%  Algorithm& Params &  1-shot 5-way& 5-shot 5-way   \\%  \hline%  MAML + Transduction \cite{finn:maml:DBLP:conf/icml/FinnAL17}& 35k &  $48.70 \pm 1.84 \%$ & $63.11 \pm 0.92\% $  \\%  1st-order MAML + Transduction \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999}& 35k &$47.07 \pm 0.26\%$ & $63.15 \pm 0.91 \%$\\%  Reptile + Transduction \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999}& 35k & $49.96 \pm 0.32 \% $ & $65.99 \pm 0.58\%$ \\%  Ours + Transduction ($F=10$) & 28k &  - & $\bf{69.77 \pm 0.31\%}$ \\%  Ours + Transduction ($F=10$) & 28k & $48.35 \pm 0.35\%$ & - \\%  \hline%  Reptile \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999}& 35k & $47.07 \pm 0.26\%$ & $62.74 \pm 0.37\%$  \\%  Ours ($F=10$) & 28k &  $45.92 \pm 0.27\%$ &  $\bf{65.09 \pm 0.24\%}$  \\% \end{tabular}% \\ \smallskip% \caption{Results on Mini-ImageNet in \textit{small} settings.}% \end{table}%\begin{table}[h!]%\centering%\label{my-label}%\begin{tabular}{l |l |l |}% Algorithm & Params & 5-shot 5-way   \\% \hline% MAML + Transduction \cite{finn:maml:DBLP:conf/icml/FinnAL17}& 35k & $63.11 \pm 0.92\% $  \\% 1st-order MAML + Transduction \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999}& 35k & $63.15 \pm 0.91 \%$\\% Reptile + Transduction \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999}& 35k & $65.99 \pm 0.58\%$ \\% Ours + Transduction ($F=10$) & 28k & $\bf{69.77 \pm 0.31\%}$ \\%\hline% Reptile \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999}& 35k & $62.74 \pm 0.37\%$  \\% Ours ($F=10$) & 28k &  $\bf{65.09 \pm 0.24\%}$  \\%\end{tabular}%\\ \smallskip%\caption{Results on Mini-ImageNet in \textit{small} setting. For \textit{small} setting, we use $F = 10$, feature scale rate $= 1.0$, $N = 0$, $B = 5$, inner learning rate $= 0.01$, meta iteration $=35.2k$, and meta learning rate $=1.0$.}%\end{table}% \begin{table}[h!]% \centering% \label{my-label}% \begin{tabular}{l |l |l |l |}%  Algorithm& Params &  1-shot 5-way& 5-shot 5-way   \\%  \hline%  MAML + Transduction \cite{finn:maml:DBLP:conf/icml/FinnAL17} & 35k &  $48.70 \pm 1.84 \%$ & $63.11 \pm 0.92\% $  \\%  Reptile + Transduction \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999}& 35k & $49.96 \pm 0.32 \% $ & $65.99 \pm 0.58\%$ \\%  Ours + Transduction ($F=64$) & 1,094k &  - & $\bf{76.29 \pm 0.38\%}$ \\%  Ours + Transduction ($F=24$) & 157k & $51.76 \pm 0.30\%$ & - \\%  \hline%  Reptile \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999} & 35k & $47.07 \pm 0.26\%$ & $62.74 \pm 0.37\%$  \\ %  Matching-Nets \cite{Vinyals:nips2016:DBLP:conf/nips/VinyalsBLKW16} & - & 43.60\% & 55.30\% \\%  Ravi and Laroche \cite{ravi:opt_fewshot:DBLP:conf/iclr/RaviL17} & - & $43.40 \pm 0.77\%$ & $60.20 \pm 0.71\%$ \\%  Prototypical-Nets \cite{Snell:nips2017:DBLP:conf/nips/SnellSZ17} & - & $49.42 \pm 0.78\%$ & $68.20 \pm 0.66\%$ \\%  Ours (F=64) & 1,094k & - & $\bf{70.87 \pm 0.23\%}$  \\%  Ours (F=24) & 157k & $49.09 \pm 0.26\% $ & -  \\% \end{tabular}% \\ \smallskip% \caption{Results on Mini-ImageNet in \textit{large} settings.}% \end{table}%\begin{table}[h!]%\centering%\label{my-label}%\begin{tabular}{l |l |l |}% Algorithm& Params & 5-shot 5-way   \\% \hline% MAML + Transduction \cite{finn:maml:DBLP:conf/icml/FinnAL17} & 35k & $63.11 \pm 0.92\% $  \\% Reptile + Transduction \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999}& 35k & $65.99 \pm 0.58\%$ \\% Ours + Transduction ($F=64$) & 1,094k & $\bf{76.29 \pm 0.38\%}$ \\%\hline% Reptile \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999} & 35k & $62.74 \pm 0.37\%$  \\ % Matching-Nets \cite{Vinyals:nips2016:DBLP:conf/nips/VinyalsBLKW16} & - & 55.30\% \\% Ravi and Laroche \cite{ravi:opt_fewshot:DBLP:conf/iclr/RaviL17} & - & $60.20 \pm 0.71\%$ \\% Prototypical-Nets \cite{Snell:nips2017:DBLP:conf/nips/SnellSZ17} & - & $68.20 \pm 0.66\%$ \\% Ours ($F=64$) & 1,094k & $\bf{70.87 \pm 0.23\%}$  \\%\end{tabular}%\\ \smallskip%\caption{Results on Mini-ImageNet in \textit{large} setting. For \textit{large} setting, we use $F = 64$, feature scale rate $=1.0$, $N = 0$, $B = 5$, inner learning rate $=0.01$, meta iteration $=11.2k$, and meta learning rate $=1.0$.}%\end{table}%\begin{table}[h!]%\centering%\label{my-label}%\begin{tabular}{l |l |l |l |}% & Algorithm& Params &  1-shot 5-way   \\% \hline%  \textit{small}& Ours + Transduction ($F=10$) & 28k & $48.35 \pm 0.35\%$ \\%  settings& Ours ($F=10$) & 28k &  $45.92 \pm 0.27\%$ \\%  & MAML + Transduction \cite{finn:maml:DBLP:conf/icml/FinnAL17}& 35k &  $48.70 \pm 1.84 \%$ \\%  & 1st-order MAML + Transduction \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999}& 35k &$47.07 \pm 0.26\%$ \\% & Reptile + Transduction \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999}& 35k & $\bf{49.96 \pm 0.32} \% $ \\%  & Reptile \cite{nichol:reptile:DBLP:journals/corr/abs-1803-02999}& 35k & $47.07 \pm 0.26\%$ \\% \hline\hline% \textit{large}& Ours + Transduction ($F=24$) & 157k & $\bf{51.76 \pm 0.30}\%$ \\% settings & Ours ($F=24$) & 157k & $49.09 \pm 0.26\% $ \\% & Matching-Nets \cite{Vinyals:nips2016:DBLP:conf/nips/VinyalsBLKW16} & - & 43.60\%  \\% & Ravi and Laroche \cite{ravi:opt_fewshot:DBLP:conf/iclr/RaviL17} & - & $43.40 \pm 0.77\%$ \\% & Prototypical-Nets \cite{Snell:nips2017:DBLP:conf/nips/SnellSZ17} & - & $49.42 \pm 0.78\%$ \\%\end{tabular}%\\ \smallskip%\caption{Results on $1-$shot $5-$way Mini-ImageNet. For \textit{small} settings, $F=10$, feature scale rate $=1.0$, $N=0$, $B=5$, inner learning rate$=0.01$, meta iteration$=32k$, meta learning rate$=0.3$. For \textit{large} settings, $F=24$, feature scale rate $=1.0$, $N=0$, $B=5$, inner learning rate$=0.01$, meta iteration$=33k$, and meta learning rate$=0.235$.}%\end{table}

To investigate how our progressive network architecture search algorithm can find the best cells for few-shot image classification tasks, we observed the distribution of depths of the promising cells as the search progresses. As shown in Figure \ref{fig:depth_stats}, the distributions are moving toward deeper architectures in both settings. Surely, further empirical studies would be required to have a more conclusive remark on this phenomenon.
\section{Conclusion and Future Work}
Our gradient based meta learner search was motivated by the current trend in machine learning communities. There have been many attempts to automate the machine learning pipelines in different manner. We tried a simple, but natural combination of two automation techniques: neural architecture search and gradient-based meta-learning. To our best knowledge, this implementation is the first successful AutoML execution in the context of meta learning.

Our results indeed show that automated architecture search is beneficial for improvement of the performance of meta-learning algorithms which tried to find good initialization for different tasks. Our gradient based meta learners with automatically search architectures have much better results than other meta-learners with human-crafted models on some few shot image classification tasks and compatible results with state-of-the-art techniques which employed more sophisticated auxiliary components such as encoder and decoder networks for the tasks.

Future work includes exploring the effect of operators in our search method, and further empirical studies on the shape of cells in terms of depth and width for good performance. Also, various kinds of neural architecture search methods should be explored in the context of meta learning. For example, it would be quite interesting to see if the architecture and parameters can be jointly optimized by formulating the model search in a differentiable manner. The applicability of our method can be tested to other domains which require the fast-adaptation property.


% \input{text/Acknowledgements.tex} % Do not include acknowledgments in the anonymized submission
% \input{text/References.tex}

\bibliographystyle{abbrv}
\bibliography{References}

\newpage% \vspace{-0.1in}\section*{Appendix}\vspace{-0.1in}\subsection*{Architecture Search Hyperparameters}\vspace{-0.1in}\begin{table}[h!]
\renewcommand\thetable{A1}
\centering
\begin{tabular}{l | r}
\hline
\multicolumn{2}{c}{Progressive neural architecture search} \\
\hline
Max num blocks (B) &  5 \\
Num filters in first layer (F) & 4, 10, 12, 32 \\
Beam size (K) & 100 \\
Num times to unroll cell (N) & 0, 1 \\
Feature Scale Rate & 1, 2 \\
Surrogate predictor \cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559} & Cell size $= 100$, Number of layers $= 1$\\
\hline
\multicolumn{2}{c}{Gradient-based meta-learning} \\
\hline
Inner iterations & 8 \\
Inner-batch size & 10 \\
Inner Adam learning rate & 0.001, 0.01 \\
Meta-batch size & 5\\
Outer step size & 0.3, 1.0\\
Outer iterations & 1000  \\
\hline
\end{tabular}
%\caption{Hyperparameters used in our search.}
\caption{Hyperparameters}
\label{Tab:hyperparameters}
\end{table}\vspace{-0.1in}\subsection*{Further Empirical Analysis}% The below figure not only shows the top$-1$ cell at $b=5$, but also shows the progression of how the cell% has been established, starting from $b=3$. One can see that upto $b=4$, the progression has only chosen% to build itself vertically without any horizontal expansion. At $b=5$, we finally see the last block addition% to happen vertically at the top part of the cell. 
Figure \ref{fig:best_cells} shows the cell architectures that have the highest classification accuracies for 1-shot 5-way Mini-Imagenet classification tasks in the small and large settings respectively. The depths of both are equivalent to three vertically stacked blocks while their constituent operations and connections are different from each other.

\begin{figure}[h!]
\centering
% \includegraphics[width=0.95\textwidth]{figures/cell_results.png}
% \includegraphics[width=0.95\textwidth]{figures/best_cells_m5w1s.png}
\includegraphics[width=0.95\textwidth]{figures/best_cells_m5w1s.pdf}
% 	\begin{subfigure}[t]{0.40\textwidth}
% 	\includegraphics[width=0.8\textwidth]{figures/best_cell_m5w1s_small.png}
% 	\end{subfigure}
% 	\begin{subfigure}[t]{0.55\textwidth}
% 	\includegraphics[width=0.8\textwidth]{figures/best_cell_m5w1s_large.png}
% 	\end{subfigure}
% \includegraphics[width=0.8\textwidth]{figures/cell_results.png}
% \caption{The best cell architectures for 1-shot 5way MiniImagenet classification tasks in the small (left) and large (right) settings
\caption{The best cell architectures for 1-shot 5-way Mini-Imagenet tasks in the small (left) and large (right) settings (some pre-/post-processing operations are omitted for the sake of simplicity)}
\label{fig:best_cells}
\end{figure}% The Figure 3 shows the depth statistics($x-$axis is depth) of two executed searches with block progression from $b=3$ to $b=5$,% which includes total of $600$ cells($K=100$), thus corresponding $600$ CNN statistics. % This substantially extends the depth-universality analysis of Finn to a much larger scale.% One can see that the depths of top % $100$ CNNs in the search are consistently on the higher end of the $x$-axis. The depth phenomenon that has been suggested% by the diagram of the top-$1$ cell's progression is also present in many other top $100$ cells. % Interestingly enough, the depth histograms shift to the left, % as the CNN unrolling parameter increase from $0$ to $1$. This might suggest that one only needs% sufficient amount of depth as a function of number of parameters in the proto-architecture. Of course, further% empirical studies would be required to have a more conclusive remark on this phenomenon. 
Figure \ref{fig:depth_stats} shows the distribution of depths of the best cells at each stage in terms of the number of blocks as the search progresses from $b=3$ to $b=5$. We can see the distributions are moving toward deeper architectures in both settings while the degree of the change seems much larger in the large setting.

\begin{figure}[h!]
% \includegraphics[width=0.95\textwidth]{figures/depth_statistics.png}
	\begin{subfigure}[t]{0.49\textwidth}
	\includegraphics[width=\textwidth]{figures/depth_stat_small.png}
	\end{subfigure}
	\begin{subfigure}[t]{0.49\textwidth}
	\includegraphics[width=\textwidth]{figures/depth_stat_large.png}
	\end{subfigure}
        
\caption{Evolution of the cell depth distribution during the progressive network architecture search\\
         ($Dx$ refers to cells whose depth is $x$ in terms of blocks.)}
\label{fig:depth_stats}
\end{figure}
\end{document}
