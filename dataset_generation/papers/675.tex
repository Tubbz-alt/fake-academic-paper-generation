\documentclass[journal]{IEEEtran}
%\documentclass[12pt,draftclsnofoot,onecolumn,journal]{IEEEtran}
\newcommand{\ignore}[1]{}

%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:

% \documentclass[journal]{../sty/IEEEtran}
% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,epsfig}
\usepackage{graphicx} %use graph format
\usepackage{epstopdf}
\usepackage{url}
\usepackage{multirow}
\usepackage{color}
%\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,linkcolor=blue,citecolor=blue,bookmarks=false]{hyperref}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{cite}
\usepackage{array}
\usepackage[colorlinks,
            linkcolor=red,
            anchorcolor=blue,
            citecolor=green
            ]{hyperref}
\newcommand*{\rd}{\textcolor{red}}
%\usepackage[cmex10]{amsmath}


% correct bad hyphenation here
%\hyphenation{op-tical net-works semi-conduc-tor}



%important parameter for spacing
%\setlength{\parsep}{0pt}
%important parameter for spacing

%\setlength{\partopsep}{0pt}

%reduce spacing below figure/table and text
%\renewcommand{\textfloatsep}{8pt}
%\renewcommand{\dbltextfloatsep}{10pt}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Boundary-guided Feature Aggregation Network for Salient Object Detection}
\author{Yunzhi Zhuge, Pingping Zhang, Huchuan Lu}
% make the title area
\maketitle


%MH: consider low-level and mid-level structural cues
% The paper headers
\markboth{IEEE Signal Processing Letter}{}


\begin{abstract}
Fully convolutional networks (FCN) has significantly improved the performance of many pixel-labeling tasks, such as semantic segmentation and depth estimation.
%
However, it still remains non-trivial to thoroughly utilize the multi-level convolutional feature maps and boundary information for salient object detection.
%
In this paper, we propose a novel FCN framework to integrate multi-level convolutional features recurrently with the guidance of object boundary information.
%
First, a deep convolutional network is used to extract multi-level feature maps and separately aggregate them into multiple resolutions, which can be used to generate coarse saliency maps.
%
Meanwhile, another boundary information extraction branch is proposed to generate boundary features.
%
Finally, an attention-based feature fusion module is designed to fuse boundary information into salient regions to achieve accurate boundary inference and semantic enhancement.
%
The final saliency maps are the combination of the predicted boundary maps and integrated saliency maps, which are more closer to the ground truths.
%
Experiments and analysis on four large-scale benchmarks verify that our framework achieves new state-of-the-art results.
\end{abstract}
% IEEEtran.cls defaults to using nonbold math in the Abstract.
% This preserves the distinction between vectors and scalars. However,
% if the journal you are submitting to favors bold math in the abstract,
% then you can use LaTeX's standard command \boldmath at the very start
% of the abstract to achieve this. Many IEEE journals frown on math
% in the abstract anyway.



% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Salient object detection, Boundary information extraction, Attention, feature fusion.
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

%==================================================================
%\section{Introduction}
\section{Introduction}
\label{sec:intro}
%\IEEEPARstart{H}{uman} vision system tends to find the most
%informative region in the scenes and then apply further processing
Saliency object detection is a fundamental computer vision task which aims to identify the most eye-catching objects and areas in an image~\cite{Itti1998A}\cite{Achanta2009Frequency}~\cite{Tong2015Salient}~\cite{Wang2015Deep}.
%
In the past two decades, great success has been made in this pixel-labeling task.
%
However, due to several inevitable factors such as cluttered backgrounds or blurred boundaries, it still remains a difficult task to combine all hand-tuned cues in an appropriate way.

Recently, deep convolutional neural networks (CNNs) have greatly improved the performances of many computer vision tasks, such as image classification~\cite{Krizhevsky2012ImageNet}, semantic segmentation~\cite{FCN} and visual tracking~\cite{Wang2016STCT,zhang2018non}.
%
%Inspired by these achievements, several attempts~\cite{Wang2015Deep,Li2015Visual} to handle saliency detection as a dense labeling problem, have delivered superior performance.
%
With the advantages of fully convolutional networks (FCNs)~\cite{FCN}, several FCNs-based attempts have been performed and delivered state-of-the-art performance in predicting saliency maps~\cite{Wang2016Saliency,Lee2016Deep,Liu2016DHSNet}.
%
Nonetheless, existing models mainly focus on utilizing high-level features extracted from last convolutional layers.
%
As a result, they are lack of low-level visual information such as object boundary.
%
Thus, these models tend to predict imperfect results with poorly localized object boundaries.

In this paper, we propose a novel saliency detection method based on multi-level features and boundary cues.
%
To make full use of the multi-level convolutional features and boundary information, we present a boundary-guided feature aggregating architecture, which simultaneously generates and merges multi-level saliency maps and boundary prediction maps to obtain accurate saliency maps.

Our framework has two streams for saliency prediction.
%
In the main stream, we predict saliency maps with the incorporated features maps at different resolutions, and these predicted saliency maps are recursively sent to the refinement stage as the inputs.
%
In another stream, boundary features are extracted through a boundary extraction structure.
%
To utilize the boundary information, we propose an attention-based feature fusion module to integrate two-stream features.

Our main contributions are as follows:
\begin{itemize}
\item 
We propose a boundary-guided feature aggregation network, termed as BFANet, to utilize multi-level convolutional features and boundary cues for salient object detection.
%
The BFANet first extracts multi-level features, then integrates them into multiple resolutions.
%
The boundary information fusion is performed to enhance these features to generate finer saliency maps.
\item 
We propose a boundary extraction network as a branch of the BFANet, which generates boundary feature maps under the supervision of boundary ground truth.
%
Besides, we also introduce an attention-based feature fusion module to produce saliency maps with robust boundary.
\item 
Extensive experiments on four large-scale benchmarks have shown that our approach performs favorably against other state-of-the-art methods.
%Extensive experiments on four large-scale benchmarks have validated the feasibility of our proposed modules, and show that our approach performs favorably against other state-of-the-art methods.
\end{itemize}
\section{Our Proposed Method}
\label{sec:previous work}
%We propose a novel method to efficiently detect and refine the boundary details of salient objects.
%
Our method is mainly motivated by the following facts.
%
First, previous methods usually focus on precisely localizing salient objects, which more or less neglect the sharpness of boundary areas.
%
To enhance the boundary, we propose a two-steam structure to generate saliency maps with accurate object boundaries.
%
Secondly, features of variant scales contribute differently to detection.
%
However, there still exist questions in how to effectively utilize these features.
%
Therefore, we make a feasible attempt and propose an effective attention-based structure to perform the multi-level feature fusion.

As shown in Fig.~\ref{diagram}, our proposed framework is composed of three components: Aggregating Feature Extraction Network (AFEN), Boundary Prediction Network (BPN) and Attention-based Feature Fusion Module (AFFM).
%
%Our model is trained end-to-end on the DUTS-TR~\cite{Lijun2017Learning} dataset under the supervision of both saliency and boundary labels.
In the following subsections, we will elaborate these components in detail.
\vspace{-2mm}
\subsection{Aggregating Feature Extraction Network}
\begin{figure*}[!t]
\centering
\begin{tabular}{@{}c@{}c}
\includegraphics[width=0.85\linewidth]{liuchengtu.pdf} \&
\includegraphics[width=0.16\linewidth]{liuchengtu2.pdf} \\\
\end{tabular}
\caption{
The overall architecture of our proposed BFANet. The upper stream represents the reproduced Amulet~\cite{Zhang2017Amulet}, which is used to extract multi-level convolutional features.
%
The details of Amulet are described in Section~\ref{sec:AmuletNet for initial prediction}.
%
In another stream, Boundary Prediction Network (BPN) can progressively enlarge the resolution and refine the details of boundary predictions through a cascaded of residual convolution unit (RCU).
%
Attention-based feature fusion module (AFFM) is employed to integrate multi-level features and boundary cues.
%
After that, we obtain multi-level fused feature maps by parallel fused prediction modules (FPM).
%
The final saliency map is produced with the fused feature maps.
\label{diagram}}
\vspace{-5mm}
\end{figure*}
\label{sec:AmuletNet for initial prediction}
The residual networks (ResNet)~\cite{He2016Deep} have shown excellent performances in many computer vision tasks.
%
Our feature extraction network is based on the ResNet-101, which extracts multi-level feature maps from raw RGB images for feature integration and saliency prediction.
%
We modify the ResNet-101 and reduce the resolution of features by a factor of 16 comparing to the input image, as shown in the Fig.~\ref{diagram}.
%
The feature maps are extracted from specified convolutional layers.
%
For the balance of resolution, we adopt the resolution-based feature combination structure (RFC)~\cite{Zhang2017Amulet} to integrate multi-level convolutional features.
%
The RFC structure unifies convolutional features through shrink and extend operations.
%
More specifically, given an input image $\bf{I}$, the integrated feature maps of scale $\tau\in[1,..,5]$ are computed by
\begin{equation}
\bf{F}^{\tau} = \it{C}_{m=1}^{\text{4}}( \it{R_{m}^{\tau}}(\bf{F}_{\it{m}}(I);\psi_{\it{m}})),
%
\label{eq:original-bayes}
\end{equation}
where $\it{R_{m}^{\tau}}({\cdot};\psi_{m})$ represents the reshape operator that expands or shrinks the feature maps by a factor of $\psi_{m}$.
%
$\bf{F}_{\it{m}}$ denotes the $m$-level feature maps.
%dimension reduction operation, which is a $1\times1$ convolutional layer with 64 kernels.
$\it{C}$ is the concatenation operation in channel-wise.
%
The resolution of generated feature maps $\bf{F^{\tau}}$ is $[\frac{W}{2^{\tau}},\frac{H}{2^{\tau}}]$.
%
Besides, to enhance the feature interaction, we adopt the bidirectional information streams~\cite{Zhang2017Amulet}, which integrate multi-level features in both bottom-up and top-down directions.
%
Although multi-level features have been extracted and fused in this effective way, there still exists a large gap between predicted saliency maps and ground truth.
%
Due to the defects of down-sampling operations, the predicted saliency maps are blurred or occluded on boundary areas.
\vspace{-2mm}
\subsection{Boundary Prediction Network}
\label{Boundary Prediction Network}
To resolve the blurry boundary problem, we introduce the BPNet, which generates boundary predictions to guide saliency prediction.
%
To generate the boundary labels, we apply the open-source ``Canny” algorithm~\cite{Canny} to the binary saliency labels,  which usually provide in public saliency benchmarks. An example is shown in Fig.~\ref{boundary}.
%
The detailed structure of our BPNet is shown at the bottom of Fig.~\ref{diagram}.
%
We adopt five convolutional blocks of the VGG-16 model~\cite{simonyan2014very} to extract multi-scale boundary features.
%
Given an input image, our BPNet first extracts five scale feature maps $\bf{B}^{\tau}_{\it{f}}$.
%
%
To progressively merge multi-scale features and enlarge the boundary prediction map, we cascade several Residual Convolution Units (RCU)~\cite{Lin2017RefineNet} on the side-output feature maps.
%
%The RCUs first unit the resolutions and dimensions of the neighboring feature maps, then an element-wise sum operation is performed to integrate different feature maps.
In the scale $\tau$, BPNet generates boundary feature maps $\bf{B}^{\tau}$ by
\begin{equation}
\begin{aligned}
\bf{B}^{\tau}=
\begin{cases}
((\bf{W}^{\tau}{\star}_{s}\bf{B}^{{\tau}+1})\oplus \it{RCU}(\bf{B}_{\it{f}}^{\tau})),\rm{1}\le\it{\tau}<\rm{5}\\
\it{RCU}(\bf{B}_{\it{f}}^{\tau}),\it{\tau}=\rm{5}
\end{cases}
\label{boundary feature map}
\end{aligned}
\end{equation}
where $\bf{B}^{\it{\tau}}$ and $\bf{B}_{\it{f}}^{\it{\tau}}$ represent boundary feature maps and corresponding convolutional feature maps respectively.
%
${\star}_{s}$ denotes the deconvolution with a stride $s$ to ensure the same resolution.
%
$\oplus$ is the element-wise addition.
%
Then we apply a $1\times1$ convolution operation on the boundary feature maps of each scale to generate the boundary prediction map $\bf{B}^{\tau}_{\it{p}}$.
%
The comparison of boundary prediction maps is shown in Fig.~\ref{boundary}.
%
\begin{figure}[!t]
\centering
\begin{tabular}{@{}c}
\includegraphics[width=0.9\linewidth]{boundary.pdf} \\\
\end{tabular}
\caption{Illustration of boundary ground truth and boundary prediction maps. $\bf{B}^{\tau}_{\it{p}} (\tau=1,2,...,5)$ represents the prediction results of level $\tau$. Note that we rescale the predictions to the same size for better visualization.}
\vspace{-6mm}
\label{boundary}
\end{figure}
\vspace{-2mm}
\subsection{Attention-based Feature Fusion Module}
\begin{table*}[thp]
\begin{center}
\caption{Quantitative comparison on four large-scale datasets. The best two results are shown in \textcolor[rgb]{1,0,0}{red} and \textcolor[rgb]{0,0,1}{blue}, respectively.}
\begin{tabular}{|p{1.8cm}<{\centering}|p{1.3cm}<{\centering}|p{0.6cm}<{\centering} p{0.6cm}<{\centering} p{0.6cm}<{\centering} p{0.6cm}<{\centering}  p{0.6cm}<{\centering} p{0.6cm}<{\centering} p{0.6cm}<{\centering} p{0.6cm}<{\centering} p{0.6cm}<{\centering} p{0.6cm}<{\centering} p{0.6cm}<{\centering} p{0.6cm}<{\centering} p{0.6cm}<{\centering}|}
\hline
Dataset&Metric&Ours&Amulet&UCF&DHS&NLDF&RFCN&DS&DCL&ELD&LEGS&MDF&DRFI&BSCA\\
\hline
\multirow{3}{*}{ECSSD}&$\tt{F_{\beta}\uparrow}$&\textcolor[rgb]{1,0,0}{0.882}&0.867&0.839&0.872&\textcolor[rgb]{0,0,1}{0.878}&0.834&0.825&0.829&0.810&0.785&0.807&0.733&0.705\\
&$\tt{MAE}\downarrow$&\textcolor[rgb]{1,0,0}{0.051}&\textcolor[rgb]{0,0,1}{0.059}&0.078&0.059&0.063&0.107&0.122&0.088&0.079&0.118&0.105&0.164&0.182\\
\hline
\multirow{3}{*}{DUT-OMRON}&$\tt{F_{\beta}\uparrow}$&\textcolor[rgb]{1,0,0}{0.721}&0.669&0.613&/&\textcolor[rgb]{0,0,1}{0.683}&0.626&0.603&0.684&0.611&0.591&0.644&0.550&0.509\\
&$\tt{MAE}\downarrow$&\textcolor[rgb]{1,0,0}{0.060}&0.090&0.132&/&\textcolor[rgb]{0,0,1}{0.079}&0.111&0.120&0.097&0.092&0.133&0.092&0.139&0.190\\
\hline
\multirow{3}{*}{DUTS-TE}&$\tt{F_{\beta}\uparrow}$&\textcolor[rgb]{1,0,0}{0.763}&0.709&0.629&0.724&\textcolor[rgb]{0,0,1}{0.743}&0.712&0.632&0.714&0.628&0.585&0.673&0.541&0.499\\
&$\tt{MAE}\downarrow$&\textcolor[rgb]{1,0,0}{0.050}&0.080&0.117&0.067&\textcolor[rgb]{0,0,1}{0.065}&0.091&0.090&0.088&0.093&0.138&0.094&0.175&0.197\\
\hline
\multirow{3}{*}{HKU-IS}&$\tt{F_{\beta}\uparrow}$&\textcolor[rgb]{1,0,0}{0.887}&0.861&0.808&0.855&\textcolor[rgb]{0,0,1}{0.873}&0.835&0.785&0.853&0.769&0.723&0.801&0.722&0.654\\
&$\tt{MAE}\downarrow$&\textcolor[rgb]{1,0,0}{0.043}&0.053&0.074&0.053&\textcolor[rgb]{0,0,1}{0.048}&0.089&0.078&0.072&0.074&0.119&0.089&0.144&0.175\\
\hline
%\multirow{3}{*}{PASCAL-S}&$\tt{F_{\beta}\uparrow}$&\textcolor[rgb]{1,0,0}{0.798}&\textcolor[rgb]{0,0,1}{0.779}&0.706&0.779&0.778&0.751&0.659&0.714&0.718&0.699&0.709&0.618&0.601\\
%&$\tt{MAE}\downarrow$&\textcolor[rgb]{1,0,0}{0.079}&\textcolor[rgb]{0,0,1}{0.090}&0.126&0.094&0.099&0.132&0.176&0.125&0.123&0.157&0.146&0.206&0.223\\
%\hline
\end{tabular}
\label{table}
\vspace{-4mm}
\end{center}
\end{table*}
%As described in previous sections, we have proposed two branches including an initial saliency branch and its corresponding boundary branch. However, there is no such algorithm efficiently utilising boundary information to refine saliency maps.
To efficiently utilize the boundary information and refine saliency maps, we introduce the attention-based feature fusion module (AFFM), which exploits multiple attention cues~\cite{Chen2017SCA}.
%, \emph{e.g.}, spatial and channel-wise attention~\cite{Chen2016SCA}.
%
More specifically, we first reduce the channels of saliency features to the same size of boundary features by shrink dimension.
%, as shown in Fig.~\ref{diagram}.
%
Then we perform a global average pooling on the saliency and boundary features to obtain a feature vector $\textbf{v}=[v_{1},v_{2},...,v_{n}]$ ($n$ is the channel dimension of layers).
%
A spatial softmax operator is applied on the feature vector to generate a normalized weight vector of the feature maps:
\begin{equation}
\begin{aligned}
{w_{i}}=\frac{e^{v_{i}}}{\sum_{i=1}^{n}e^{v_{i}}},
\label{boundary feature map}
\end{aligned}
\end{equation}
where $w_{i}$ is the weight of channel $i$ and $\sum_{i=1}^{n}w_{i}=1$. Subsequently, the fused feature maps $\bf{F}_{\text{fused}}^{\tau}$ is generated by
\begin{equation}
\begin{aligned}
\bf{F}_{\it{fused}}^{\tau}=(\bf{w}_{\it{F}}^{\tau}\otimes \bf{F}^{\tau}) \oplus (\bf{w}_{\it{B}}\otimes \bf{B}),
\label{equation8}
\end{aligned}
\end{equation}
where $\otimes$ is channel-wise product.
%
$\bf{B}$ denotes the boundary feature maps with the resolution of $256\times256$, which are in accordance with the aggregated saliency feature maps.
%
$\bf{w}_{\it{F}}^{\tau}$ and $\bf{w}_{\it{B}}$ represents the attention weights for saliency features and boundary features respectively.
%
With the fused feature, five paralleled fused prediction modules (FPM) (each of which is composed of a $3\times 3$ convolutional layer and an upsampling layer) are used to predict stage-wise prediction maps.
%
With the stage-wise prediction maps, we add another convolutional layer with a $1\times1$ kernel to predict the final prediction map.
\section{Experiments}
\label{sec:Experiments}
%To verify the effectiveness of our proposed algorithm, we conduct experiments on five widely used saliency benchmarks.
%
%The detail of which is shown in section\ref{Dataset}. We adopt three measure methods to evaluate the performance of our method.
\subsection{Training and Testing Datasets}
\label{Dataset}
In the training process, the \textbf{DUTS-TR}~\cite{Lijun2017Learning} dataset is chosen as our training dataset, which includes 10, 553 images with accurate pixel-wise annotations.
%
We implement our proposed model based on the Caffe toolbox~\cite{jia2014caffe}.
%
We train and test our method with an NVIDIA 1080 GPU (with 8G memory).
%
%To train our model, we augment the training data by random cropping and mirror reflection, resulting 12 times larger than the original training data.
%
The input image is uniformly resized into $256\times256\times3$ pixels and subtracted the ImageNet mean~\cite{deng2009imagenet}.
%
We find our model with this resolution achieves both effectiveness and efficiency.
%
We adopt the sigmoid cross entropy as the loss function for both saliency and boundary prediction.
%
Following previous works~\cite{Zhang2017Amulet,Zhang2017Learning,Zhang2018Salient}, we train the model until its training loss converges.
%such that it has
%~\cite{simonyan2014very}
The weights of FCN backbones are initialized from the VGG-16~\cite{simonyan2014very} and ResNet-101~\cite{He2016Deep} models.
%, and ResNet-101
%~\cite{glorot2010understanding}
For other layers, we initialize the weights by the ``msra'' method.
%
We follow the parameters in~\cite{Zhang2017Amulet} and use the standard SGD method with a batch size 8, momentum 0.9 and weight decay 0.0005.
%
We set the learning rate to 1$\times$e$^{-8}$ and decrease it by 10\% after every 10 epoch.
%
%The training process converges after 25 epoches.
%
Our model has a size of 410 MB and runs around 10 \emph{fps} for saliency inference, which is comparable even faster than most of methods.
%

We evaluate the performance of our method on four large-scale datasets described as follows.
%
\textbf{ECSSD}~\cite{Yang2013Saliency} is composed of 1000 images with random objects of different scales.
%This dataset contains many semantically meaningful and complex structures contends.
%
%\textbf{PASCAL-S}~\cite{Yin2014The} is derived from PASCAL VOC2010 segmentation dataset~\cite{Everingham2008The} and contains 850 natural images.
%
\textbf{HKU-IS}~\cite{Li2015Visual} includes 4447 images with fine pixel-wise annotations. Images of this dataset are
well chosen to include multiple disconnected salient objects or objects touching the image boundary.
%
\textbf{DUTS-TE}~\cite{Lijun2017Learning} has 5019 images with accurate pixel-wise annotations. All images are picked from the ImageNet DET test set and the SUN dataset~\cite{Xiao2010SUN}.
%
\textbf{DUT-OMRON}~\cite{Yang2013Saliency} has a total of 5168 high-quality images.
\begin{figure}[!t]
\centering
\begin{tabular}{@{}c@{}c@{}c@{}c}
%\vspace{-0.5mm}
\includegraphics[width=0.5\linewidth,height=2.6cm]{ecssd-pr}
\includegraphics[width=0.5\linewidth,height=2.6cm]{omron-pr}\\
\includegraphics[width=0.5\linewidth,height=2.6cm]{duts-pr}
\includegraphics[width=0.5\linewidth,height=2.6cm]{hkuis-pr}\\
%\includegraphics[width=0.45\linewidth,height=4cm]{pascals-pr}
\end{tabular}
 \caption{The PR curves of different state-of-the-art methods.
}
\vspace{-4mm}
\label{Fig pr}
\end{figure}
\vspace{-2mm}
\subsection{Evaluation Metrics.}
To evaluate the performance, we adopt three main metrics, \emph{i.e.}, the PR curves, mean F-measure score and mean absolute error (MAE)~\cite{Borji2015Salient}.
%
Precision-recall (PR) curves can be computed by binarizing the saliency map with a threshold in [0, 255] and then comparing the binary maps with the ground truth.
%
In many occasions, both precision and recall are important to measure methods.
%
Therefore, F-measure, which is averaged with precision and recall, is proposed to achieve the overall performance evaluation,
\begin{align}
F_{\beta} =\frac{(1+\beta^2)\times Precision\times Recall}{\beta^2\times Precision \times Recall}.
\end{align}
We set $\beta ^2$ to 0.3 to weigh precision more than recall as suggested in~\cite{Achanta2009Frequency,Borji2015What,Wang2015Deep}.
%
%We report the result when the saliency map is binarized with an image-dependent threshold. The threshold is determined to be twice the mean saliency of the image.

The above evaluations usually assign high saliency scores to salient pixels, which can be unfair especially for the methods which successfully detect non-salient regions, but miss the detection of salient regions.
%
Therefore, we also adopt the MAE metric~\cite{Borji2015Salient} to measure the average difference between the saliency prediction and the ground truth.
\begin{equation}
MAE =\frac{1}{W\times H}\sum_{x=1}^{W}\sum_{y=1}^{H}|S(x,y) - G(x,y)|,
\end{equation}
where $S$ is the predicted saliency map and $G$ is the binary ground truth mask.
%
It indicates how similar a saliency map is compared to the ground truth.
\vspace{-2mm}
\subsection{Comparison with Other Methods}
\begin{figure*}[!t]
\begin{center}
%\centering
\begin{tabular}{@{}c}
%\vspace{-5mm}
\includegraphics[width=0.94\linewidth]{tupian.pdf}
%\vspace{-0.5mm}
\end{tabular}
%\vspace{-2mm}
{\caption{Comparisons of saliency maps with state-of-the-art methods. Due to the limitation of space, we do not show results of DRFI and BSCA methods.}
\label{Fig saliency}}
\vspace{-6mm}
\end{center}
\end{figure*}
We compared our method with other 12 algorithms, including 10 deep learning based algorithms
%
(Amulet\cite{Zhang2017Amulet}, UCF\cite{Zhang2017Learning}, NLDF\cite{Luo2017Non}, DHS\cite{Liu2016DHSNet}, DS\cite{Li2015DeepSaliency}, DCL\cite{Li2016Deep}, ELD\cite{Lee2016Deep}, RFCN\cite{Wang2016Saliency}, LEGS\cite{Wang2015Deep}, MDF\cite{Li2015Visual}) and 2 conventional  algorithms (DRFI\cite{Jiang2013Salient}, BSCA\cite{Qin2015Saliency}).
%
We compute saliency maps with the original implementations or use them provided by the authors.

\textbf{Quantitative Results}
As shown in Tab.~\ref{table} and Fig.~\ref{Fig pr}, our model consistently outperforms other methods across all the datasets in terms of all evaluation metrics, which convincingly demonstrates the effectiveness of the proposed method.
%
%From the results, we have observed: (1) Our model improves the F-measure with a considerable margin on most of datasets, especially on large-scale datasets, such as DUTS-TE, ECSSD, HKU-IS.
%%
%And at the same time, our model generally decreases the MAE.
%%
%This indicates that our model is more convinced of the predicted regions and provides more accurate saliency maps.
%%
%(2) Although only trained on the MSRA10K dataset, our model significantly outperforms other algorithms that pre-trained on specific saliency datasets, such as LEGS and RFCN on PASCAL-S, MDF on HKU-IS.
%%
%The superior performance confirms that our model have good generalization abilities on other large-scale datasets.
%(3) Our method is inferior to DHS on several datasets. However, these datasets are relatively small compared to the era of deep learning.

\textbf{Qualitative Results.}
Fig.~\ref{Fig saliency} shows visual comparison between our method and other algorithms.
%
As shown in the 1st row, the foreground is very complex, while our method successfully captures the main components against the competing algorithms.
%
The object in 2nd row is an unusual roof. Many algorithms fail to capture the semantic structure, while our method successfully highlights it. Our proposed method also perform better on images with similar color distribution between foreground and background (3rd row). For disconnected objects (last two rows), our method still performs well, while other algorithms misjudge the interferences.
%---------------------------------
\vspace{-2mm}
\subsection{Ablation Studies}{\bf{Effects of the boundary branch.}} We verify the effectiveness of boundary branch in our framework.
%
The compared models include:
(1) With the same ResNet-101, we construct a baseline network, which is composed of encoder-decoder and feature combination structure~\cite{Zhang2017Amulet}.
%
The multi-level features are used to generate stage-wise prediction maps. The prediction map is generated by merging stage-wise prediction maps.
%
(2) We add boundary-stream to the baseline network and use concatenation operations to combine features of both branches for saliency detection.
%
The prediction scheme is similar with baseline network.
%
Different from (1), features of each level incorporate both saliency and boundary information. We name this setting as $Boundary^{+}$.
%
(3) To verify the effectiveness of cascaded RCUs, we implement the boundary-stream approach without RCUs, named $Boundary^{-}$
%
(4) Finally, AFFM is added to fuse saliency and boundary features to generate attentive features, resulting in our final model $AFFM^{+}$.

We perform detailed experiments on two datasets,~\emph{i.e.}, ECSSD and DUT-OMRON.
%
The results are shown in Tab.~\ref{table1}.
%
From quantitative evaluations and qualitative comparisons, one can observe that our proposed components effectively enhance saliency detection performance.
%
Especially, the boundary prediction module improves the MAE with 4\%.
%
Qualitative comparisons with different model settings are shown in Fig.~\ref{Fig_com}.
%
Compared with the baseline, our boundary-guided prediction is much better in predicting the boundary details.
%
The proposed method indeed produces saliency map with sharp boundaries. The low MAE metric also verifies this fact.
\begin{figure}[!t]
\begin{center}
%\centering
\begin{tabular}{@{}c}
%\vspace{-5mm}
\includegraphics[width=0.9\linewidth,height=3cm]{duibi.pdf}
%\vspace{-0.5mm}
\end{tabular}
%\vspace{-2mm}
{\caption{Qualitative comparisons with different model settings.}
\label{Fig_com}}
\vspace{-6mm}
\end{center}
\end{figure}
\begin{table}[thp]
\vspace{-2mm}
\begin{center}
\caption{Quantitative comparison of different settings. The best results are in bold. The VGG-16\cite{simonyan2014very} version is AFFM$^{+}\bullet$.}
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{|p{1.8cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|}
\hline
\multirow{2}{*}{*}&\multicolumn{2}{c|}{ECSSD}&\multicolumn{2}{c|}{DUT-OMRON}\\
\cline{2-5}
&$\tt{F_{\beta}}\uparrow$&$\tt{MAE}\downarrow$&$\tt{F_{\beta}}\uparrow$&$\tt{MAE}\downarrow$\\
\hline
$Baseline$&0.8753&0.0568&0.7008&0.0761\\
$Boundary^{-}$&0.8712&0.0576&0.6944&0.0779\\
$Boundary^{+}$&$\normalsize{0.8797}$&0.0521&0.7083&0.0712\\
$AFFM^{+}$&$\normalsize{\bf{0.8821}}$&$\bf{0.0509}$&$\bf{0.7213}$&$\bf{0.0601}$\\
\hline
$AFFM^{+}\bullet$&0.8756&0.0558&0.7141&0.0669\\
\hline
\end{tabular}
}
\label{table1}
\vspace{-5mm}
\end{center}
\end{table}

\textbf{Effects of merging multi-scale predictions.} To verify the effects of the multi-scale FPM, we also perform a series experiments on ECSSD dataset.
%
Firstly, we evaluate the results of a single FPM.
%, which denotes as ``$1$“, ``$2$“, ``$3$“, ``$4$“,``$5$“.in Table.~\ref{table3}.
%
Then we progressively add more FPMs to obtain the merged multi-scale results.
%
The quantitative results are listed in Tab.~\ref{table3}.
%
The ``$12345$” is our final model. From the results, we can observe that adding more FPMs can integrate more information, thus improving the performances.
%\begin{table}[thp]
%\begin{center}
%\caption{Quantitative results of merging multi-scale predictions.}
%\begin{tabular}{|p{0.56cm}<{\centering}|p{0.56cm}<{\centering}|p{0.56cm}<{\centering}|p{0.56cm}<{\centering}|p{0.56cm}<{\centering}|p{0.56cm}<{\centering}| p{0.56cm}<{\centering}|p{0.56cm}<{\centering}|p{0.56cm}<{\centering}|}
%\hline
%Metric&1&2&3&4&5&45&345&12345\\
%\hline
%$\tt{F_{\beta}}\uparrow$&0.8385&0.8629&0.8721&0.8763&0.8790&0.8765&0.8788&0.8821\\
%\hline
%$\tt{MAE}\uparrow$&0.0595&0.0566&0.0541&0.0529&0.0520&0.0533&0.0524&0.0509\\
%\hline
%%\hline
%\end{tabular}
%\label{table3}
%\vspace{-6mm}
%\end{center}
%\end{table}
\begin{table}[thp]
\begin{center}
\caption{Quantitative results of merging multi-scale predictions.}
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{|p{0.56cm}<{\centering}|p{0.56cm}<{\centering}|p{0.56cm}<{\centering}|p{0.56cm}<{\centering}|p{0.56cm}<{\centering}|p{0.56cm}<{\centering}| p{0.56cm}<{\centering}|p{0.56cm}<{\centering}|p{0.56cm}<{\centering}|p{0.56cm}<{\centering}|}
\hline
Metric&1&2&3&4&5&45&345&2345&12345\\
\hline
$\tt{F_{\beta}}\uparrow$&0.839&0.863&0.872&0.876&0.879&0.877&0.879&0.881&0.882\\
\hline
$\tt{MAE}\downarrow$&0.060&0.057&0.054&0.053&0.052&0.053&0.052&0.051&0.051\\
\hline
%\hline
\end{tabular}
}
\label{table3}
\vspace{-6mm}
\end{center}
\end{table}
\section{Conclusion}
\label{sec:Conclusion}
In this paper, we propose a boundary-guided aggregating feature fusion network for salient object detection.
%
Different from the methods directly introduce high-level features into shallow layers, our method integrates feature maps into multiple resolutions.
%
The proposed attention-based feature fusion module can effectively refine the results with clear boundaries.


% use section* for acknowledgement
%\section*{Acknowledgment}
%The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section


\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,refs}
%\begin{thebibliography}{1}
%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.
%\end{thebibliography}

% biography section
%
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{biography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\ignore{
\begin{IEEEbiography}{}

\end{IEEEbiography}



% if you will not have a photo at all:
\begin{IEEEbiographynophoto}{John Doe}

\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

\begin{IEEEbiographynophoto}{Jane Doe}
Biography text here.
\end{IEEEbiographynophoto}

}
% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


