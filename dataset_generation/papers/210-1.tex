\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{color}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{multirow}
\usepackage{floatrow}
\usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{rotating}

% \captionsetup{compatibility=false}
\floatsetup{style=plain, framestyle=plain, heightadjust=object, framearound=object, framefit=yes, capposition=bottom, valign=t}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}


\newcommand\minisection[1]{\vspace{2mm}\noindent \textbf{#1}}

\newcommand\subfix[1]{\mathtt{#1}}
\newcommand\mf[1]{\mathbf{#1}}
\newcommand\vf[1]{\mathbf{#1}}

\newcommand{\fisher}[1]{\textcolor{blue}{[Fisher: #1]}}

\newcommand{\model}{TAFE-Net\xspace}
\newcommand{\modelplural}{TAFE-Nets\xspace}



% \cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{440} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE %%%%%%%%%%
\title{TAFE-Net: Task-Aware Feature Embeddings for \\ Efficient Learning and Inference \\
\textcolor{red}{\large Supplementary Material}}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% Input %%%%%%%%
\section*{Appendix}
In the appendix, we are going to investigate that how critical the proposed task-aware feature embedding is for the improved
performance of \model as it adjusts the image feature based on the task 
specification. We present more analysis over the learned embeddings (task-aware feature embeddings and latent task embedding of the meta learner) in \model. Moreover, 
we show experimental results  and visualization on the latent task embedding to demonstrate that the 
embedding network in the task-aware meta learner is able to capture the visual/semantic
information and thus embeddings of semantically similar tasks are closer 
to those of semantically different tasks even when the task encoding itself does 
not contain any semantic information (e.g., one-hot encoding). 

% \textbf{Erratum}. There is a typo in the legend of the Figure.5 in the submission. The % blue bars are for \textit{\model} and the green bars are for the runtime neural pruning% (RNP)~\cite{lin2017runtime}. \section*{A.1 Task-aware Embedding Visualization}\label{sec:tafe}
We now provide more visualization for the task-aware feature embedding (TAFE). 
The main characteristics of TAFE is the same image input may have different feature 
embeddings under different tasks/contexts. For example, consider the task: 
\textit{is this a building or statue?} which can be further decomposed to binary tasks:
\textit{is this a building} and \textit{is this a statue?} The image of a 
building
will be projected to a \texttt{building} cluster and a \texttt{non-statue} cluster. 

To this end, we visualize the trained \model on the aPY dataset under different 
binary task descriptions. We use t-SNE~\cite{tsne} to visualize the TAFEs for 
all test images in the aPY dataset. The blue points stand for the positive examples
of the task and the green points for the negative examples of the task. As shown in Figure~\ref{fig:tsne}, we randomly select three pairs of tasks and demonstrate that 
the same image can be projected to different clusters under different task 
specifications. The learned task-aware image feature is adjusted to the task specification. 

\section*{A.2 Image Shallow Embeddings as Task Desc.}\label{sec:efficient}
As presented in Section 4.4 in the main paper, the input image itself can be used
as the task description. In this section, we conduct an additional experiment to
analyze the behavior of the meta learner. 
 
\begin{figure*}[h!]
\begin{tabular}{cc}
\includegraphics[width=0.45\textwidth]{figs/tsne_supp/building.pdf} &
\includegraphics[width=0.45\textwidth]{figs/tsne_supp/statue.pdf} \\
% (a) first & (b) second \\[6pt]
\includegraphics[width=0.45\textwidth]{figs/tsne_supp/carriage.pdf} &
\includegraphics[width=0.45\textwidth]{figs/tsne_supp/jetski.pdf} \\
% (c) third & (d) fourth \\[6pt]
\includegraphics[width=0.45\textwidth]{figs/tsne_supp/monkey.pdf} &
\includegraphics[width=0.45\textwidth]{figs/tsne_supp/goat.pdf} \\
\end{tabular}
\caption{Task-aware Image Feature Embedding projected into two dimensions using t-SNE\cite{tsne} for six different tasks. Note that changing the task produces different embeddings for the same data.}
\label{fig:tsne}
\end{figure*}% \begin{figure*}[h!]% \begin{tabular}{cc}% \includegraphics[width=0.48\textwidth]{figs/apy_tsne/cow.pdf} &% \includegraphics[width=0.48\textwidth]{figs/apy_tsne/sheep.pdf} \\% (a) first & (b) second \\[6pt]% \includegraphics[width=0.48\textwidth]{figs/apy_tsne/tv.pdf} &% \includegraphics[width=0.48\textwidth]{figs/apy_tsne/statue.pdf} \\% (c) third & (d) fourth \\[6pt]% \includegraphics[width=0.48\textwidth]{figs/apy_tsne/train.pdf} &% \includegraphics[width=0.48\textwidth]{figs/apy_tsne/jetsky.pdf} \\% \end{tabular}%  \caption{Task-aware Image Feature Embedding projected into two dimensions using t-SNE\cite{tsne} for six different tasks. Note that changing the task produces different embeddings for the same data.}% \label{fig:tsne}% \end{figure*}\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figs/gate_vis.pdf}
    \caption{Weight shuffling. The in-group shuffling has much higher accuracy than the out-of-group shuffling.}
    \label{fig:vis_shuffle}
\end{figure*}\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/retrievals_supp.pdf}
    \caption{Top retrievals on the unseen pairs of the MITStates dataset}
    \label{fig:vis_retrievals_supp}
\end{figure*}\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{figs/comp_vis2.png}
    \caption{Task Embedding Visualization with t-SNE on MITState Dataset. The
    figure shows the nearest seen compositions (e.g. \texttt{crushed garlic}, \texttt{duced garlic}) of the unseen composition \texttt{peeled garlic}. }
    \label{fig:comp_vis2}
\end{figure*}\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{figs/comp_vis3.png}
    \caption{Task Embedding Visualization with t-SNE on MITState Dataset. The
    figure shows the nearest seen compositions (e.g. \texttt{broken city}, \texttt{torn city}) of the unseen composition \texttt{shattered city}.}
    \label{fig:comp_vis3}
\end{figure*}

We evaluate the trained \model with VGG-16 used in Section 4.4 as follows: for a given fine-grained class A 
(e.g., dolphin), we re-assign the task embedding for each input in class A with a
randomly chosen task embedding from other classes either within the same coarse category
(referred to as in-group shuffling) or different categories (referred to as out-of-group
shuffling).  In Figure~\ref{fig:vis_shuffle}, we plot the test accuracy of class
\texttt{dolphin}, belonging to the coarse category \texttt{aquatic mammals} with randomly selected task
embeddings (repeated 20 times for each input) from classes in the same coarse category
(in red) and 5 classes for other coarse categories (in blue). The test accuracy with
in-group shuffling is much higher than that that with out-of-group shuffling. Especially, when
applying the task embeddings from the tulip category, the test accuracy drops to 1\%
while the accuracy with in-group shuffling is mostly above 50\%. 
% This indicates that the latent task embeddings are similar for semantically related image categories. \section*{A.3 Visual Attribute Composition}\label{sec:composition}
In the visual attribute composition task, the attribute-object pairs are used as 
\textit{task descriptions}. The goal is to predict unseen compositions at test time. In Figure~\ref{fig:vis_retrievals_supp}, we present the top retrievals 
on the unseen attribute-object pairs of the MITStates dataset. 

% \textbf{How are the unseen compositions related to the seen compositions?}
In our main paper, we use either the word2vec~\cite{mikolov2013efficient} or one-hot encoding to encode
each attribute and object separately and the concatenation is fed into the 
\textit{task-aware meta learner} producing a latent task embedding $\mathcal{T}(t)$. 
The latent task embedding is then used as the input to 
the layer-specific weight generators. 

To study the relation of the unseen and seen compositions, we visualize the latent task embeddings for all compositions with t-SNE~\cite{tsne}. In Figure~\ref{fig:comp_vis2} and Figure~\ref{fig:comp_vis3}, we randomly pick up a 
unseen composition and show its nearest neighbors in the embedding space.  
We find that the unseen compositions are close to the seen compositions that are
semantically related even when one-hot encoding is used as the task encoding.
This implies that through the joint visual-task embedding learning, the task
embedding network is able to capture the visual semantic information effectively.

\clearpage
\clearpage{\small
\bibliographystyle{ieee}
\bibliography{references}
}

\end{document}
