\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{style/cvpr}
\usepackage{epsfig}
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}

\graphicspath{{./images/}}
% \graphicspath{{./scratch/}} % FOR EDITING: run ./gen_scratch_images.sh and then enable this line to compile pdf faster (low-res images).

\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{times}

\usepackage{style/ownstyles}
% Include other packages here, before hyperref.
%%
%\definecolor{ballblue}{rgb}{0.01, 0.30, 0.68}
%\definecolor{ao}{rgb}{0.0, 0.5, 0.0}
%\definecolor{darkpastelgreen}{rgb}{0.01, 0.75, 0.24}
\definecolor{neural_renderer}{rgb}{0.4, 0.6, 0.4}
\definecolor{classifier}{rgb}{0.58, 0.43, 0.38}
\definecolor{red_dashed_line}{rgb}{0.75, 0.0, 0.0}
\definecolor{correct}{rgb}{0.07, 0.54, 0.31}
\definecolor{incorrect}{rgb}{0.72, 0.1, 0.08}

% Custom subsection (not CVPR style)
\newcommand{\subsec}[1]{\noindent{\textbf{#1.}}}
%\newcommand{\class}[1]{\ensuremath{\mathsf{#1}\xspace}}
\newcommand{\class}[1]{{\small\texttt{#1}}}

%%%%%%%%%%%%%%%%%%%%%%
% Commenting highlights
%%%%%%%%%%%%%%%%%%%%%%

% Paste the contained lines in the document preamble to enable commenting

% Commenting highlights
\newif\ifcomments%

%Uncomment one of the two lines below to turn todos on/off
%\commentsfalse
\commentstrue%

\ifcomments%
\newcommand{\comments}[1]{#1}
\else
\newcommand{\comments}[1]{}
\fi
% Commenting highlights

%%% Math
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\newcommand{\x}{\mathbf{x}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\xs}{\mathbf{x^*}}
\newcommand{\xn}{\mathbf{x_0}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\h}{\mathbf{h}}
\newcommand{\ys}{\mathbf{y^*}}
\newcommand{\hs}{\mathbf{h^*}}
\newcommand{\yn}{\mathbf{y_0}}

\newcommand{\LL}{\mathcal{L}}


\newcommand{\R}{\mathbb{R}}

%%% Lighting names %%%
\newcommand{\bright}{\ensuremath{\mathsf{bright}}\xspace}
\newcommand{\medium}{\ensuremath{\mathsf{medium}}\xspace}
\newcommand{\dark}{\ensuremath{\mathsf{dark}}\xspace}

%%% Layer and Unit %%%
\newcommand{\layer}[1]{\ensuremath{\mathsf{#1}\xspace}}
\newcommand{\unit}[1]{\ensuremath{\mathsf{#1}\xspace}}
\newcommand{\layerunit}[2]{\ensuremath{\mathsf{#1_{#2}}\xspace}}
%\newcommand{\thinplus}{\hspace*{-.3ex}+\hspace*{-.3ex}}
%\newcommand{\thinminus}{\hspace*{-.3ex}-\hspace*{-.3ex}}
%\newcommand{\thineq}{\hspace*{-.3ex}=\hspace*{-.3ex}}

% Footnote formatting
%\newcommand{\ddag}{\ensuremath{\mathsf{\ddag}}\xspace}


% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\newcommand{\overbar}[1]{\mkern1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern1.5mu}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{2951} % *** Enter the CVPR Paper ID here % AN: Added this!
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\newcommand{\papertitle}{Strike (with) a Pose: Neural Networks Are Easily Fooled \\by Strange Poses of Familiar Objects}
\title{\papertitle}


\author{Michael A. Alcorn\\
%Auburn University\\
%Institution1 address\\
{\tt\small alcorma@auburn.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Qi Li\\
%Auburn University\\
%First line of institution2 address\\
{\tt\small qzl0019@auburn.edu}
\and
Zhitao Gong\\
%Auburn University\\
%First line of institution2 address\\
{\tt\small gong@auburn.edu}
\and
Chengfei Wang\\
%Auburn University\\
%First line of institution2 address\\
{\tt\small czw0078@auburn.edu}
\and
Long Mai\\
%Institution2\\
%First line of institution2 address\\
{\tt\small malong@adobe.com}
%%
\and
Wei-Shinn Ku\\
%Institution2\\
%First line of institution2 address\\
{\tt\small weishinn@auburn.edu}
\and
Anh Nguyen\\
%Institution2\\
%First line of institution2 address\\
{\tt\small anhnguyen@auburn.edu}
\and
{Auburn University}\hspace{1cm}{Adobe Inc.~~~~~~~}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}

Despite excellent performance on stationary test sets, deep neural networks (DNNs) can fail to generalize to out-of-distribution (OoD) inputs, including natural, non-adversarial ones, which are common in real-world settings.
In this paper, we present a framework for discovering DNN failures that harnesses 3D renderers and 3D models.
That is, we estimate the parameters of a 3D renderer that cause a target DNN to misbehave in response to the rendered image.
Using our framework and a self-assembled dataset of 3D objects, we investigate the vulnerability of DNNs to OoD poses of well-known objects in ImageNet.
For objects that are readily recognized by DNNs in their canonical poses, DNNs incorrectly classify 97\% of their pose space.
In addition, DNNs are highly sensitive to slight pose perturbations.
Importantly, adversarial poses transfer across models and datasets.
We find that 99.9\% and 99.4\% of the poses misclassified by Inception-v3 also transfer to the AlexNet and ResNet-50 image classifiers trained on the same ImageNet dataset, respectively, and 75.5\% transfer to the YOLOv3 object detector trained on MS COCO.
%we found misclassified poses transfer to dif
%are easily fooled by OoD poses.
%For objects that are readily recognized by DNNs in their canonical poses, DNNs incorrectly classify 97\% of their pose space.
%Further, using optimization methods, we can generate adversarial poses corresponding to a variety of targeted classes.
%Our work suggests state-of-the-art DNNs do not yet perform true object recognition.

%In this paper, we show that state-of-the-art DNNs perform 
%Deep neural networks (DNNs) are increasingly common components of modern real-world computer vision systems.
%However, recent events suggest DNNs can behave in unexpected ways when encountering natural out-of-distribution (OoD) scenes.
%In this paper, we present a framework for discovering OoD scenes in which we backpropagate the image classification error of a DNN to the parameters of a 3D renderer.
%Using our framework and a self-assembled dataset of 3D objects, we investigate the vulnerability of DNNs to OoD poses of well-known classes from ImageNet.
%We find that DNNs are easily fooled by OoD poses.
%For objects that are readily recognized by DNNs in their canonical poses, DNNs incorrectly classify 97\% of their pose space.
%Further, using optimization methods, we can generate adversarial poses corresponding to a variety of targeted classes.
%Our work (along with concurrent work of others) suggests today's state-of-the-art DNNs do not yet perform true object recognition.	
	
%Deep neural networks (DNNs) are increasingly common components of modern real-world computer vision systems.
%However, recent events suggest DNNs can behave in unexpected ways when encountering natural out-of-distribution (OoD) scenes.
%In this paper, we present a framework for discovering OoD scenes in which we backpropagate the image classification error of a DNN to the parameters of a 3D renderer.
%Using our framework and a self-assembled dataset of 3D objects, we investigate the vulnerability of DNNs to OoD poses of well-known classes from ImageNet.
%We find that DNNs are easily fooled by OoD poses.
%For objects that are readily recognized by DNNs in their canonical poses, DNNs incorrectly classify 97\% of their pose space.
%Further, using optimization methods, we can generate adversarial poses corresponding to a variety of targeted classes.
%Our work (along with concurrent work of others) suggests today's state-of-the-art DNNs do not yet perform true object recognition.
\end{abstract}


%%%%%%%%% BODY TEXT
\section{Introduction}

%Covariate shift---where test examples are out-of-distribution (OoD), \ie, not drawn from the training distribution---is a long-standing challenge in machine learning (ML)~\cite{sugiyama2017dataset}.
For real-world technologies, such as self-driving cars~\cite{chen2015deepdriving}, autonomous drones~\cite{gandhi2017learning}, and search-and-rescue robots~\cite{sampedro2018fully}, the test distribution may be non-stationary, and new observations will often be out-of-distribution (OoD), \ie, not 
%drawn %%% removed to get 1 line
from the training distribution \cite{sugiyama2017dataset}.
%not be independent from one another, nor identically distributed to the training set.
%That is, a self-driving car should expect inputs that are out-of-distribution (OoD) \ie, not drawn from the training distribution. \ma{Redundant.}
However, machine learning (ML) models frequently assign wrong labels with high confidence to OoD examples, such as adversarial examples~\cite{szegedy2013intriguing,nguyen2015deep}---inputs specially crafted by an adversary to cause a target model to misbehave.
But ML models are also vulnerable to \emph{natural} OoD examples~\cite{lambert2016understanding,uber2017killed,tian2017deeptest,tesla2016killed}.
For example, when a Tesla autopilot car failed to recognize a white truck against a bright-lit sky---an unusual view that might be OoD---it crashed into the truck, killing the driver~\cite{tesla2016killed}.
%\ma{This paragraph feels pretty awkward to me.}
%\an{is it better?}
%\ma{Yep!}

\begin{figure}[t]
	\centering{
		(a)
		\hspace{1.5cm} (b)
		\hspace{1.5cm} (c)
		\hspace{1.7cm} (d)
	}
	\includegraphics[width=1.0\columnwidth]{teaser.jpg}
	
	\caption{
		The Google Inception-v3 classifier~\cite{szegedy2016rethinking} correctly labels the canonical poses of objects (a), but fails to recognize out-of-distribution images of objects in unusual poses (b--d), including real photographs retrieved from the Internet (d).
		The left $3 \times 3$ images (a--c) are found by our framework and rendered via a 3D renderer.
		Below each image are its top-1 predicted label and confidence score.
	}\label{fig:teaser}
	%	\vspace*{-0.9em}
\end{figure}

To understand such natural Type II classification errors, we searched for 6D poses (\ie, 3D translations and 3D rotations) of 3D objects that caused DNNs to misclassify.
%\ma{Are we mitigating anything in this paper? Seems like we're just describing the problem.}
%\an{Correct. Our only mitigation attempt is E6. We mainly try to understand the problem in this paper. We need to edit to reflect this.}
%such failures by studying a specific kind of Type II classification error, .
Our results reveal that state-of-the-art image classifiers and object detectors trained on large-scale image datasets \cite{russakovsky2015imagenet,lin2014microsoft} misclassify most poses for many familiar training-set objects.
For example, DNNs predict the front view of a school bus---an object in the ImageNet dataset \cite{russakovsky2015imagenet}---extremely well (Fig.~\ref{fig:teaser}a) but fail to recognize the same object when it is too close or flipped over, \ie, in poses that are OoD yet exist in the real world (Fig.~\ref{fig:teaser}d).




%%%%% WHY IT IS IMPORTANT TO STUDY THIS TYPE OF ERROR
Addressing this type of OoD error is a non-trivial challenge.
First, objects on roads may appear in an infinite variety of poses~\cite{tesla2016killed,uber2017killed}.
Second, these OoD poses come from known objects and should be assigned known labels rather than being rejected as unknown objects~\cite{hendrycks2016baseline,scheirer2013toward}.
Moreover, a self-driving car needs to correctly estimate at least some attributes of an incoming, unknown object (instead of simply rejecting it) to handle the situation gracefully and minimize damage.
%Third, it is infeasible to obtain many of such examples in reality (\eg, by crashing a school bus into a tree) to collect training data for a discriminative model.
%$p_\theta(y|\x)$ model.

%However, the phenomenon also poses a great challenge to computer vision researchers.
%First, while such poses might be OoD, rejecting to

%These errors present an important challenge for computer vision researchers.
%First, there is an infinite variety of such OoD examples on real driving roads~\cite{tesla2016killed,uber2017killed}.
%Second, it is infeasible to obtain many of such examples in reality (\eg, by running a car into a tree) to collect training data for a discriminative $p_\theta(y|\x)$ models.
%Third, in real world settings, these OoD poses should be assigned the correct labels rather than simply being rejected
%Third, even if we had a perfect density model, $p_\theta(\x)$, which assigns low likelihoods to every OoD sample that are not from the true $p(\x)$ distribution, simply rejecting to classify an OoD input may yield undesired consequences.
%\lm{The second and third points are a bit strong to state here. Both are viable solutions in theory. In the end it gets down to better modeling the generative process p(x|y). I think we may want to move this to the discussion part as suggestions for future research: using synthetic data to expand the data and/or augmenting recognition networks with viewpoint reasoning --- both are attempts to enlarge the space for p(x|y)}
%For example, a self-driving car needs to correctly estimate at least some attributes of an incoming, unknown object in order to handle the situation gracefully and minimize damage.
%Our study suggests harnessing synthetic images generated from 3D objects is a viable, but incomplete solution.
% AN: Wait for the adversarial training result to confirm on the previous sentence.

%%% AN: This list of contributions is to be updated as we have results.

%\an{Editing here}

%In this paper, we generate unrestricted adversarial examples by searching for a 6D pose (3D translations and 3D rotations) of a 3D object to cause DNNs to misclassify.

%Our main contributions in this paper are:

In this paper, we propose a framework for finding OoD errors in computer vision models in which iterative optimization in the parameter space of a 3D renderer is used to estimate changes (\eg, in object geometry and appearance, lighting, background, or camera settings) that cause a target DNN to misbehave (Fig.~\ref{fig:concept}).
With our framework, we generated unrestricted 6D poses of 3D objects and studied how DNNs respond to 3D translations and 3D rotations of objects.
For our study, we built a dataset of 3D objects corresponding to 30 ImageNet classes relevant to the self-driving car application.
All code and data for our experiments will be available at 
\url{https://github.com/airalcorn2/strike-with-a-pose}.
%\url{http://anonymizedForReview}.
In addition, we will release a simple GUI tool that allows users to generate their own adversarial poses of an object.

Our main findings are:

\begin{itemize}
%    \item We propose a framework\footnote{with code available on GitHub: \url{https://www.github.com/<anonymous>/<anonymous>}} for finding OoD errors in computer vision models: perform iterative optimization in the parameter space of a 3D renderer to estimate the changes (\eg, in object geometry and appearances, lighting, background or camera settings) that cause a target DNN to misbehave (Fig.~\ref{fig:concept}).
%    We compare zero- v.s.\ first-order optimization methods via differentiable and non-differentiable renderers.
%    \item We build a dataset of 30 3D objects (corresponding to 30 ImageNet classes relevant to the self-driving car application) for studying how image classifiers are robust to translations and rotations of 3D objects.
    \item ImageNet classifiers only correctly label $3.09\%$ of the entire 6D pose space of a 3D object, and misclassify many generated adversarial examples (AXs) that are human-recognizable (Fig.~\ref{fig:teaser}b--c).
    A misclassification can be found via a change as small as $10.31\degree$, $8.02\degree$, and $9.17\degree$ to the yaw, pitch, and roll, respectively.
    \item 99.9\% and 99.4\% of AXs generated against Inception-v3 transfer to the AlexNet and ResNet-50 image classifiers, respectively, and 75.5\% transfer to the YOLOv3 object detector.

    \item Training on adversarial poses generated by the 30 objects (in addition to the original ImageNet data) did not help DNNs generalize well to held-out objects in the same class.
\end{itemize}

In sum, our work shows that state-of-the-art DNNs perform \emph{image classification} well but are still far from true \emph{object recognition}.
While it might be possible to improve DNN robustness through adversarial training with many more 3D objects, we hypothesize that future ML models capable of visual reasoning may instead benefit from strong 3D geometry priors.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.90\linewidth]{concept.pdf}
    \caption{To test a target DNN, we build a 3D scene (a) that consists of 3D objects (here, a school bus and a pedestrian), lighting, a background scene, and camera parameters.
    Our 3D renderer renders the scene into a 2D image, which the image classifier labels \class{school bus}.
    We can estimate the pose changes of the school bus that cause the classifier to misclassify by (1) approximating gradients via finite differences; or (2) backpropagating (\textcolor{red_dashed_line}{red} dashed line) through a differentiable renderer.
%     By backpropagating through both the classifier and the renderer (\textcolor{red_dashed_line}{red} dashed line), we can find a change in the pose of the school bus that causes the classifier to misclassify.
    }\label{fig:concept}
\end{figure*}

%------------------------------------------------------------------------
\section{Framework}

\subsection{Problem formulation}

Let $f$ be an image classifier  that maps an image $\x \in \R^{H\times W\times C}$ onto a softmax probability distribution over 1,000 output classes~\cite{szegedy2016rethinking}.
%The predicted label for $\x$ would be the index of the maximum probability.
%
%The predicted label for an image $\x$ is given by $y = \argmax_i{f(\x)_i}$:
%
Let $R$ be a 3D renderer that takes as input a set of parameters $\phi$ and outputs a render, \ie, a 2D image $R(\phi) \in \R^{H\times W\times C}$ (see Fig.~\ref{fig:concept}).

Typically, $\phi$ is factored into mesh vertices $V$, texture images $T$, a background image $B$, camera parameters $C$, and lighting parameters $L$, \ie, $\phi = \{V, T, B, C, L\}$~\cite{kato2018neural}.
To change the 6D pose of a given 3D object, we apply a set of 3D rotations and 3D translations, parameterized by $\w \in \R^6$, to the original vertices $V$, yielding a new set of vertices $V^*$.

Here, we wish to estimate only the pose transformation parameters $\w$ (while keeping all parameters in $\phi$ fixed) such that the rendered image $R(\w;\phi)$ causes the classifier $f$ to assign the highest probability (among all outputs) to an incorrect target output at index $t$.
Formally, we attempt to solve the below optimization problem:

%\vspace*{-0.1cm}
\begin{equation}
\label{eq:min}
\w^* = \argmax_{\w}(f_t(R(\w; \phi)))
\end{equation}

In practice, we minimize the cross-entropy loss $\LL$ for the target class.
Eq.~\ref{eq:min} may be solved efficiently via backpropagation if both $f$ and $R$ are differentiable, \ie, we are able to compute $\partial \LL/\partial\w$.
However, standard 3D renderers, \eg, OpenGL~\cite{woo1999opengl}, typically include many non-differentiable operations and cannot be inverted~\cite{marschner2015fundamentals}.
Therefore, we attempted two approaches: (1) harnessing a recently proposed differentiable renderer and performing gradient descent using its analytical gradients; and (2) harnessing a non-differentiable renderer and approximating the gradient via finite differences.

We will next describe the target classifier (Sec.~\ref{sec:target_dnn}), the renderers (Sec.~\ref{sec:renderers}), and our dataset of 3D objects (Sec.~\ref{sec:3d_object_dataset}) before discussing the optimization methods (Sec.~\ref{sec:methods}).

%\subsec{Code} All our code and data will be available on \url{http://anonymizedForReview}. 
%We will also release a GUI tool that allows users to interact with a 3D scene and observe DNN responses.

%Recent work has shown great benefits of simulators \cite{shrivastava2017learning} and 3D renderers \cite{unreal2018renderer} in generating training and test data for ML models \cite{dosovitskiy2017carla}.

%We wish to backpropagate through all the parameters $\phi$ that a renderer $R(.)$ uses to compute an output image $\x = R(\phi)$.
%Assume we factor $\phi$ into mesh vertex locations $V$, textures $T$, camera parameters $C$, and lighting parameters $L$ i.e. $\phi = \{V, T, C, L\}$.
%We wish to compute the following partial derivatives $\frac{\partial R}{\partial V}$,
%$\frac{\partial R}{\partial T}$,
%$\frac{\partial R}{\partial C}$, and $\frac{\partial R}{\partial L}$.
%

\subsection{Classification networks}
\label{sec:target_dnn}

We chose the well-known, pre-trained Google Inception-v3 \cite{Szegedy2015} DNN from the PyTorch model zoo \cite{torch2018vision} as the main image classifier for our study (the default DNN if not otherwise stated).
The DNN has a 77.45\% top-1 accuracy on the ImageNet ILSVRC 2012 dataset~\cite{russakovsky2015imagenet} of 1.2 million images corresponding to 1,000 categories.

%\anremoved{
%We used Inception-v3~\cite{Szegedy2015}, a model that is frequently used in computer vision applications, as our primary neural network of interest. Inception-v3 is a convolutional neural network that was trained on the 2012 ImageNet Large Scale Visual Recognition Challenge dataset~\cite{ILSVRC15}, a labeled dataset of 1.2 million images corresponding to 1,000 different categories.
%}

\subsection{3D renderers}
\label{sec:renderers}

\subsec{Non-differentiable renderer}
We chose ModernGL~\cite{modernGL}
%\footnote{\url{https://moderngl.readthedocs.io}}
as our non-differentiable renderer.
ModernGL is a simple Python interface for the widely used OpenGL graphics engine.
ModernGL supports fast, GPU-accelerated rendering.
%The module follows the OpenGL API---an industry standard in computer graphics first released in 1992---and, as a result, is extremely flexible in terms of the scenes it can depict.
%\an{We assume CVPR community knows OpenGL}

\subsec{Differentiable renderer}
To enable backpropagation through the non-differentiable rasterization process, Kato et al.~\cite{kato2018neural} replaced the discrete pixel color sampling step with a linear interpolation sampling scheme that admits non-zero gradients.
While the approximation enables gradients to flow from the output image back to the renderer parameters $\phi$, the render quality is lower than that of our non-differentiable renderer (see Fig.~\ref{fig:compare_tessellation} for a 
%side-by-side 
comparison).
Hereafter, we refer to the two renderers as NR and DR.
%We wish to compute the following partial derivatives $\frac{\partial R}{\partial V}$,
%$\frac{\partial R}{\partial T}$,
%$\frac{\partial R}{\partial C}$, and $\frac{\partial R}{\partial L}$.

%Recent work~\cite{kato2018neural} provides a method to approximate the gradient of the rasterization process.
%We refer to this renderer as the Kato Renderer (KR).

\subsection{3D object dataset}
\label{sec:3d_object_dataset}

\subsec{Construction}
Our main dataset consists of 30 unique 3D object models (purchased from many 3D model marketplaces) corresponding to 30 ImageNet classes relevant to a traffic environment (Fig.~\ref{fig:dataset_A}).
The 30 classes include 20 vehicles (\eg, \class{school bus} and \class{cab}) and 10 street-related items (\eg, \class{traffic light}). See Sec.~\ref{sec:SI_3d_object_dataset} for more details.
% For each 3D object, we make sure there is a real-world counterpart in the ImageNet dataset. \ma{We already say the 30 objects correspond to 30 classes.}

Each 3D object is represented as a mesh, \ie, a list of triangular faces, each defined by three vertices \cite{marschner2015fundamentals}.
The 30 meshes have on average 9,908 triangles (Table~\ref{tab:num_triangles}).
To maximize the realism of the rendered images, we used only 3D models that have high-quality 2D image textures.
% WHY WE HAVE TO BUILD OUR OWN 3D OBJECT DATASET
We did not choose 3D models from public datasets, \eg, ObjectNet3D \cite{xiang2016objectnet3d}, because most of them do not have high-quality image textures.
That is, the renders of such models may be correctly classified by DNNs but still have poor realism.

\subsec{Evaluation}
We recognize that a reality gap will often exist between a render and a real photo.
% \ma{I think a lot of modern movie CGI is indistinguishable from a picture.}
%We recognize there exists an inevitable reality gap between rendered images vs. ImageNet photos.
Therefore, we rigorously evaluated our renders to make sure the reality gap was acceptable for our study.
From $\sim$100 initially-purchased 3D models, we selected the 30 highest-quality models using the evaluation method below.

First, we quantitatively evaluated DNN predictions on the renders.
For each object, we sampled 36 unique views (common in ImageNet) evenly divided into three sets.
For each set, we set the object at the origin, the up direction to $(0,1,0)$, and the camera position to $(0,0,-z)$ where $z = \{4, 6, 8\}$.
%Starting the object at a $10^\circ{}$ azimuth, we uniformly sampled 12 views from 0 to $2\pi$ azimuthal angle.
We sampled 12 views per set by starting the object at a $10^\circ{}$ yaw and generating a render at every $30^\circ{}$ yaw-rotation.
Across all objects and all renders, the Inception-v3 top-1 accuracy was $83.23\%$ (compared to $77.45\%$ on ImageNet images \cite{szegedy2016rethinking}) with a mean top-1 confidence score of $0.78$ (Table~\ref{tab:avg_accuracy_30obj}).
See Sec.~\ref{sec:SI_3d_object_dataset} for more details.

Second, we qualitatively evaluated the renders by comparing them to real photos.
We produced 56 (real photo, render) pairs via three steps: (1) we retrieved real photos of an object (\eg, a car) from the Internet; (2) we replaced the object with matching background content in Adobe Photoshop; and (3) we manually rendered the 3D object on the background such that its pose closely matched that in the reference photo.
Fig.~\ref{fig:dataset_B} shows example (real photo, render) pairs.
While discrepancies can be spotted in our side-by-side comparisons, we found that most of the renders passed our human visual Turing test if presented alone.

%\anremoved{
%3D object models corresponding to 30 different ImageNet classes were obtained either by purchase or through free offerings.
%The 30 object models include 20 vehicles and 10 traffic-related items (with our primary focus being traffic environments).
%A typical 3D model consists of a list of triangular faces, each defined by three vertices, which themselves have three coordinates each.
%3D models also typically include a texture map, which maps areas of a 2D texture image to each face.
%
%To ensure the object models were adequate reproductions of their real world counterparts, we scrutinized the predictions made by Inception-v3 on renders of the objects in poses resembling those found in the ImageNet dataset.
%For all experiments, the camera was placed at $(0, 0, 2)$ in world coordinates, pointed towards the origin, and oriented such that the up direction was $(0, 1, 0)$.
%The perspective projection matrix was generated using an angle of view of 16.426\degree, an aspect ratio of 1, a near clipping plane of 0.1, and a far clipping plane of 1000.
%}

\subsection{Background images}

Previous studies have shown that image classifiers may be able to correctly label an image when foreground objects are removed (\ie, based on only the background content) \cite{zhu2016object}.
%``classify'' no need quotes because this is literally still an "image classification task".
%an image given only the background content (\ie, no foreground objects) \cite{zhu2016object}.
Because the purpose of our study was to understand how DNNs recognize an object itself, a non-empty background would have hindered our interpretation of the results.
Therefore, we rendered all images against a plain background with RGB values of $(0.485, 0.456, 0.406)$, \ie, the mean pixel of ImageNet images.
Note that the presence of a non-empty background should not alter our main qualitative findings in this paper---adversarial poses can be easily found against real background photos (Fig.~\ref{fig:teaser}).
%\todo{Refer to the teaser Fig.~\ref{fig:teaser} and Fig.~\ref{fig:real_ax}. Emphasize that adversarial poses happen with background as well.}.

%That is, the background image should contain no signal (after mean subtraction) to minimize the contribution to a DNN recognition of the object in the image.

%The objects were rendered on a plain background with RGB values of $(0.485, 0.456, 0.406)$, the mean pixel color of the ImageNet dataset.
%Previous research has shown that neural networks can frequently predict the correct class of an image even after the object has been removed, suggesting the model is learning.

\section{Methods}
\label{sec:methods}

%We attempt optimization problem (Eq.~\ref{eq:min}) can be solved via both

We will describe the common pose transformations  (Sec.~\ref{sec:transformations}) used in the main experiments.
We were able to experiment with non-gradient methods because: (1) the pose transformation space $\R^6$ that we optimize in is fairly low-dimensional; and (2) although the NR is non-differentiable, its rendering speed is several orders of magnitude faster than that of DR.
In addition, our preliminary results showed that the objective function considered in Eq.~\ref{eq:min} is highly non-convex (see Fig.~\ref{fig:landscape}), therefore, it is interesting to compare (1) random search vs. (2) gradient descent using finite-difference (FD) approximated gradients vs. (3) gradient descent using the DR gradients.

\subsection{Pose transformations}
\label{sec:transformations}

We used standard computer graphics transformation matrices to change the pose of 3D objects \cite{marschner2015fundamentals}.
Specifically, to rotate an object with geometry defined by a set of vertices $V = \{v_i\}$, we applied the linear transformations in Eq.~\ref{eq:rot} to each vertex $v_{i} \in \R^3$:

%\vspace*{-0.3cm}
\begin{equation}
\label{eq:rot}
v_{i}^{R} = R_{y}R_{p}R_{r}v_{i}
\end{equation}

\noindent
where $R_{y}$, $R_{p}$, and $R_{r}$ are the $3\times 3$ rotation matrices for yaw, pitch, and roll, respectively (the matrices can be found in Sec.~\ref{sec:trans-mat}).
% We then translated the rotated object by adding a vector $T \in \R^3$ to each vertex:
%\ma{I think it makes more sense to introduce $T$ this way because we refer to $x_{\delta}$, $y_{\delta}$, and $z_{\delta}$ later.}
%\an{Agreed!}
We then translated the rotated object by adding a vector $T = \begin{bmatrix} x_{\delta} & y_{\delta} & z_{\delta} \end{bmatrix}^\top$ to each vertex:

\begin{equation}
v_{i}^{R,T} = T + v_{i}^{R}
\end{equation}

In all experiments, the center $c \in \R^3$ of the object was constrained to be inside a sub-volume of the camera viewing frustum.
That is, the $x$-, $y$-, and $z$-coordinates of $c$ were within $[-s,s]$, $[-s,s]$, and $[-28,0]$, respectively, with $s$ being the maximum value that would keep $c$ within the camera frame.
Specifically, $s$ is defined as:

\begin{equation} \label{eq:max_trans}
s = d \cdot\tan(\theta_{v})
\end{equation}

\noindent
where $\theta_{v}$ is one half the camera's angle of view (\ie, $8.213\degree$ in our experiments) and $d$ is the absolute value of the difference between the camera's $z$-coordinate and $z_{\delta}$.

%\noindent
%where $t$ is the vector $(x_{\delta}, y_{\delta}, z_{\delta})$. $z_{\delta}$ was drawn from $\mathcal{U}(-28, 0)$ and $x_{\delta}$ and $y_{\delta}$ were drawn from $\mathcal{U}(-s, s)$ where $s$ was the maximum possible translation in the $x$ and $y$ directions that would keep the center of the object within the frame of the image.



\subsection{Random search}
\label{sec:random_search}

In reinforcement learning problems, random search (RS) can be surprisingly effective compared to more sophisticated methods \cite{such2017deep}.
For our RS procedure, instead of iteratively following some approximated gradient to solve the optimization problem in Eq.~\ref{eq:min}, we simply randomly selected a new pose in each iteration.
The rotation angles for the matrices in Eq.~\ref{eq:rot} were uniformly sampled from $(0, 2\pi)$.
$x_{\delta}$, $y_{\delta}$, and $z_{\delta}$ were also uniformly sampled from the ranges defined in Sec.~\ref{sec:transformations}.

\subsection{$z_{\delta}$-constrained random search}

%\subsec{$z_{\delta}$-constrained random search}
Our preliminary RS results suggest the value of $z_{\delta}$ (which is a proxy for the object's size in the rendered image) has a large influence on a DNN's predictions.
Based on this observation, we used a $z_{\delta}$-constrained random search (ZRS) procedure both as an initializer for our gradient-based methods and as a naive performance baseline (for comparisons in Sec.~\ref{sec:comparing_methods}).
The ZRS procedure consisted of generating 10 random samples of $(x_{\delta}, y_{\delta}, \theta_{y}, \theta_{p}, \theta_{r})$ at each of 30 evenly spaced $z_{\delta}$ from $-28$ to $0$.

When using ZRS for initialization, the parameter set with the maximum target probability was selected as the starting point.
When using the procedure as an attack method, we first gathered the maximum target probabilities for each $z_{\delta}$, and then selected the best two $z_{\delta}$ to serve as the new range for RS.

\subsection{Gradient descent with finite-difference}
\label{sec:fd}

We calculated the first-order derivatives via finite central differences and performed vanilla gradient descent to iteratively minimize the cross-entropy loss $\LL$ for a target class.
That is, for each parameter $\w_{i}$, the partial derivative is approximated by:
%partial derivative of the cross-entropy loss $\LL$ for a target class with respect to $\theta_{i}$ was approximated by:

\begin{equation} \label{eq:fd}
\frac{\partial \LL}{\partial \w_{i}} = \frac{\LL(\w_{i} + \frac{h}{2}) - \LL(\w_{i} - \frac{h}{2})}{h}
\end{equation}

\noindent
Although we used an $h$ of 0.001 for all parameters, a different step size can be used per parameter.
Because radians have a circular topology (\ie, a rotation of 0 radians is the same as a rotation of $2\pi$ radians, $4\pi$ radians, etc.), we parameterized each rotation angle $\theta_i$ as $(\cos(\theta_i), \sin(\theta_i))$---a technique commonly used for pose estimation~\cite{Osadchy2005} and inverse kinematics~\cite{Choi1992}---which maps the Cartesian plane to angles via the $atan2$ function.
Therefore, we optimized in a space of $3 + 2 \times 3 = 9$ parameters.

The approximate gradient $\nabla \LL$ obtained from Equation~\eqref{eq:fd} served as the gradient in our gradient descent.
We used the vanilla gradient descent update rule:

\begin{equation}
\w \coloneqq \w - \gamma{\nabla \LL}(\w)
\end{equation}

\noindent
with a learning rate $\gamma$ of 0.001 for all parameters and optimized for $100$ steps (no other stopping criteria).
%Both FD-G and ZRS were run for 100 steps following the initialization procedure.
%Moved this number of steps to Sec.4.4


%\subsec{$z_{\delta}$-constrained random search}
%Our random search results suggest the value of $z_{\delta}$ (which is a proxy for the object's size in the rendered image) has a large influence on the predictions made by a neural network.
%Based on this observation, we used a $z_{\delta}$-constrained random (ZRS) search procedure both as an initializer for our gradient-based optimization methods and as a naive performance baseline.
%The $z_{\delta}$-constrained random search procedure consisted of generating 10 random samples of $(x_{\delta}, y_{\delta}, \theta_{y}, \theta_{p}, \theta_{r})$ at each of 30 evenly spaced $z_{\delta}$ from $0$ to $-28$.
%
%When using ZRS search for initialization, the parameter set with the maximum target probability was selected as the starting point.
%When using the procedure as an attack method, we first gathered the maximum target probabilities for each $z_{\delta}$, and then selected the best two $z_{\delta}$ to serve as the new range for random search.

%\subsection{Optimization}
%\label{sec:optimization}

%\anremoved{
%For simplicity, the renderer and classifier shown in Fig.~\ref{fig:concept} can be considered a single function, denoted by \(f\).
%The input to \(f\) is all the information defining the 3D scene and the output is the prediction.
%Concretely, given an object \(obj\) under lighting condition \(light\) with a background image \(bg\) and parameters \(x\) that control the transformation matrix of the object, the prediction \(\overline{y}\) is given by:
%%
%\[
%  \overline{y} = f(x; \text{obj}, \text{light}, \text{bg})
%\]
%%
%Our goal is to find a configuration of \(x\) such that the rendered image, which can be correctly recognized by a human, is wrongly classified by the pipeline.
%In a targeted attack where \(y_t\) denotes the target class and \(J\) denotes the loss, we want to solve the following optimization problem:
%%
%\begin{equation}
%  \label{eq:optim}
%  \argmin_x J(y, y_t)
%\end{equation}
%}

%------------------------------------------------------------------------



%------------------------------------------------------------------------
\section{Experiments and results}


\subsection{Neural networks are easily confused by object rotations and translations}
\label{sec:easily_confused}

%\subsection{Generating Adversarial Examples}

\begin{figure}[h]
\begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=\linewidth]{high_conf_params.pdf}
    \caption{Incorrect classifications}\label{fig:high_conf_params}
\end{subfigure}
\begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=\linewidth]{high_conf_correct_params.pdf}
    \caption{Correct classifications}\label{fig:high_conf_correct_params}
\end{subfigure}
\caption{The distributions of individual pose parameters for (a) high-confidence ($p \geq 0.7$) incorrect classifications and (b) correct classifications obtained from the random sampling procedure described in Sec.~\ref{sec:random_search}.
$x_{\delta}$ and $y_{\delta}$ have been normalized w.r.t. their corresponding $s$ from Eq.~\ref{eq:max_trans}.}
\label{fig:param_distributions}
\end{figure}

%\subsec{Neural networks are easily confused by object rotations and translations}

\subsec{Experiment}
To test DNN robustness to object rotations and translations, we used RS to generate samples for every 3D object in our dataset.
In addition, to explore the impact of lighting on DNN performance, we considered three different lighting settings: \bright, \medium, and \dark (example renders in Fig.~\ref{fig:light_intensity}).
In all three settings, both the directional light and the ambient light were white in color, \ie, had RGB values of $(1.0, 1.0, 1.0)$, and the directional light was oriented at $(0, -1, 0)$ (\ie, pointing straight down).
The directional light intensities and ambient light intensities were $(1.2, 1.6)$, $(0.4, 1.0)$, and $(0.2, 0.5)$ for the \bright, \medium, and \dark settings, respectively.
All other experiments used the \medium lighting setting.

\subsec{Misclassifications uniformly cover the pose space}
For each object, we calculated the DNN accuracy (\ie, percent of correctly classified samples) across all three lighting settings (Table~\ref{tab:sampling_stats}).
%
%%% Most important findings first %%%
The DNN was wrong for the vast majority of samples, \ie, the median percent of correct classifications for all 30 objects was only 3.09\%.
Moreover, high-confidence misclassifications ($p \geq 0.7$) are largely uniformly distributed across every pose parameter (Fig.~\ref{fig:high_conf_params}), \ie, AXs can be found throughout the parameter landscape (see Fig.~\ref{fig:30_ax} for examples).
In contrast, correctly classified examples are highly multimodal w.r.t. the rotation axis angles and heavily biased towards $z_{\delta}$ values that are closer to the camera (Fig.~\ref{fig:high_conf_correct_params}; also compare Fig.~\ref{fig:tsne_img_correct} vs. Fig.~\ref{fig:tsne_img_ax}).

%%% 2nd-order important results %%%

\subsec{An object can be misclassified as many different labels}
%\subsec{Pose misclassifications are diverse}
% First, it is easy to produce an AX that is misclassified as any target class when optimizing input images~\cite{szegedy2013intriguing} or 3D object's textures~\cite{Athalye2017}, which are very high-dimensional.
Previous research has shown that it is relatively easy to produce AXs corresponding to many different classes when optimizing input images~\cite{szegedy2013intriguing} or 3D object textures~\cite{Athalye2017}, which are very high-dimensional.
When finding adversarial poses, one might expect---because all renderer parameters, including the original object geometry and textures, are held constant---the success rate to depend largely on the similarities between a given 3D object and examples of the target in ImageNet.
Interestingly, across our 30 objects, RS discovered $990/1000$ different ImageNet classes (132 of which were shared between all objects).
When only considering high-confidence ($p \geq 0.7$) misclassifications, our 30 objects were still misclassified into $797$ different classes with a median number of 240 incorrect labels found per object (see Fig.~\ref{fig:common_failures_per_label} and Fig.~\ref{fig:tsne_img_ax} for examples).
%\ma{I don't think we don't need to say ``large''.}
Across all adversarial poses and objects, DNNs tend to be more confident when correct than when wrong (the median of median probabilities were 0.41 vs. 0.21, respectively).
%We also calculated the median probability for both correct and incorrect classifications for each object.
%The median of median correct probabilities was 0.41 while the median of median incorrect probabilities was 0.21.

%
%(only three of which---\class{dirigible}, \class{mousetrap}, and \class{syringe}---were shared between all objects).
% \an{No need to mention unless we show images or discuss something deeper with these classes}

%%% Class-specific results %%%

%\anremoved{
%The median number of incorrect classes hit per object was 600.
%When only considering high confidence (\ie, $p \geq 0.7$) examples, random search discovered 797 different classes (only three of which---\class{dirigible}, \class{mousetrap}, and \class{syringe}---were shared between all objects) with a median number of incorrect classes hit per object of 240.
%708 different classes were represented among the five objects with the largest number of high confidence misclassified labels (\ie, \class{dog}, \class{backpack}, \class{traffic light}, \class{cat}, and \class{fire truck}).
%}

%The median correct percent for all 30 objects was 3.09\%, \ie, the neural network was wrong for the vast majority of samples.
%We also calculated the median probability for both correct and incorrect classifications for each object.
%The median of median correct probabilities was 0.41 while the median of median incorrect probabilities was 0.21.
%As can be seen in Fig.~\ref{fig:high_conf_params}, adversarial examples are frequently found throughout the pose parameter landscape.
%Correctly classified examples (Fig.~\ref{fig:high_conf_correct_params}), in contrast, are highly multimodal with regards to the rotation axis angles and heavily biased towards $z_{\delta}$ values that are closer to the camera.

\begin{figure}[h]
\begin{subfigure}{\linewidth}
    % \centering
    % \includegraphics[width=\linewidth]{firetruck01_49_roll_pitch.png}
    \includegraphics[width=0.92\linewidth]{firetruck01_49_roll_pitch.pdf}
    \caption{}\label{fig:fire_truck_roll_pitch}
\end{subfigure}
\begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=0.85\linewidth]{firetruck01_49_collage.pdf}
    \caption{}\label{fig:fire_truck_pitch_roll_collage}
\end{subfigure}
\caption{
Inception-v3's ability to correctly classify images is highly localized in the rotation and translation parameter space.
% (a) The probability landscape for a fire truck object when altering $\theta_{r}$ and $\theta_{p}$ and holding $(x_{\delta}, y_{\delta}, z_{\delta}, \theta_{y})$ at $(0, 0, -3, \frac{\pi}{4})$.
(a) The classification landscape for a fire truck object when altering $\theta_{r}$ and $\theta_{p}$ and holding $(x_{\delta}, y_{\delta}, z_{\delta}, \theta_{y})$ at $(0, 0, -3, \frac{\pi}{4})$.
Light regions correspond to correct classifications while dark regions correspond to incorrect classifications.
Green and red circles indicate correct and incorrect classifications, respectively, corresponding to the fire truck object poses found in (b).
}\label{fig:landscape}
\end{figure}


\subsection{Common object classifications are shared across different lighting settings}

%\subsec{Experiment}
%\subsec{Random search with altered lighting}

%In addition, to explore the impact of lighting conditions on DNN performance, we replicated the random search procedure under three different lighting settings---``bright'', ``medium'', and ``dark''.
%In all three settings, both the directional light and the ambient light were white in color, \ie, had RGB values of $(1.0, 1.0, 1.0)$, and the directional light was oriented at $(0, -1, 0)$ (\ie, pointing straight down).
%The directional light intensities and ambient light intensities were $(1.2, 1.6)$, $(0.4, 1.0)$, and $(0.2, 0.5)$ for the ``bright'', ``medium'', and ``dark'' settings, respectively.
%All other experiments used the ``medium'' lighting setting.

%\subsec{Results}
Here, we analyze how our results generalize across different lighting conditions.
From the data produced in Sec.~\ref{sec:easily_confused}, for each object, we calculated the
%percent of random samples that were correctly classified
DNN accuracy under each lighting setting.
Then, for each object, we took the absolute difference of the accuracies for all three lighting combinations (\ie, \bright vs.~\medium, \bright vs.~\dark, and \medium vs.~\dark) and recorded the maximum of those values.
The median ``maximum absolute difference'' of accuracies for all objects was 2.29\% (compared to the median accuracy of $3.09\%$ across all lighting settings).
That is, DNN accuracy is consistently low across all lighting conditions.
Lighting changes would not alter the fact that DNNs are vulnerable to adversarial poses.

We also recorded the 50 most frequent classes for each object under the different lighting settings ($S_{b}$, $S_{m}$, and $S_{d}$).
Then, for each object, we computed the intersection over union score $o_{S}$ for these sets:
%\ma{We don't use IoU again elsewhere.}

\vspace*{-0.3cm}
\begin{equation}
o_{S} = 100 \cdot \frac{|S_{b} \cap S_{m} \cap S_{d}|}{|  S_{b} \cup S_{m} \cup S_{d} |}
\end{equation}

%\begin{equation}
%    S_{I} = S_{b} \cap S_{m} \cap S_{d}
%\end{equation}
%
%\noindent
%and the union:
%
%\begin{equation}
%    S_{U} = S_{b} \cup S_{m} \cup S_{d}
%\end{equation}
%
%\noindent
%and calculated the percent overlap of $S_{I}$ and $S_{U}$:
%
%\begin{equation}
%    o_{S} = 100 \cdot \frac{|S_{I}|}{|S_{U}|}
%\end{equation}

\noindent
The median $o_{S}$ for all objects was 47.10\%.
That is, for 15 out of 30 objects, 47.10\% of the 50 most frequent classes were shared across lighting settings.
While lighting does have an impact on DNN misclassifications (as expected), the large number of shared labels across lighting settings suggests ImageNet classes are strongly associated with certain adversarial poses regardless of lighting.

%\subsec{Correct classifications are highly localized in the rotation and translation parameter landscape}
\subsection{Correct classifications are highly localized in the rotation and translation landscape}
%parameter: cut because of space
\label{sec:landscape}

To gain some intuition for how Inception-v3 responds to rotations and translations of an object, we plotted the probability and classification landscapes for paired parameters (\eg, Fig.~\ref{fig:landscape}; pitch vs. roll) while holding the other parameters constant.
We qualitatively observed that the DNN's ability to recognize an object (\eg, a fire truck) in an image varies radically as the object is rotated in the world (Fig.~\ref{fig:landscape}).

%As can be seen in Fig.~\ref{fig:landscape}, Inception-v3's ability to recognize a fire truck in an image rapidly declines as the fire truck object is rotated in the world.

\subsec{Experiment}
To quantitatively evaluate the DNN's sensitivity to rotations and translations, we tested how it responded to single parameter disturbances.
For each object, we randomly selected 100 distinct starting poses that the DNN had correctly classified in our random sampling runs.
Then, for each parameter (\eg, yaw rotation angle), we randomly sampled 100 new values\footnote{\label{note:sampling}using the random sampling procedure described in Sec.~\ref{sec:random_search}} while holding the others constant.
For each sample, we recorded whether or not the object remained correctly classified, and then computed the failure (\ie, misclassification) rate for a given (object,~parameter) pair.
% as the percent of misclassified samples.
Plots of the failure rates for all (object,~parameter) combinations can be found in Fig.~\ref{fig:sensitivity}.

Additionally, for each parameter, we calculated the median of the median failure rates.
That is, for each parameter, we first calculated the median failure rate for all objects, and then calculated the median of those medians for each parameter.
Further, for each (object,~starting pose,~parameter) triple, we recorded the magnitude of the smallest parameter change that resulted in a misclassification.
Then, for each (object,~parameter) pair, we recorded the median of these minimum values.
Finally, we again calculated the median of these medians across objects (Table~\ref{tab:fail_rates}).

\subsec{Results}
As can be seen in Table~\ref{tab:fail_rates}, the DNN is highly sensitive to all single parameter disturbances, but it is especially sensitive to disturbances along the depth ($z_{\delta}$), pitch ($\theta_{p}$), and roll ($\theta_{r}$).
To aid in the interpretation of these results, we converted the raw disturbance values in Table~\ref{tab:fail_rates} to image units.
For $x_{\delta}$ and $y_{\delta}$, the interpretable units are the number of pixels the object shifted in the $x$ or $y$ directions \textit{of the image} (however, note that 3D translations are \textit{not} equivalent to 2D translations because of the perspective projection).
% \an{Already mentioned in the caption and a take-away sentence below}
%For $z_{\delta}$, the interpretable unit is the percent change in the area of the bounding box containing the object.

% Take-aways
We found that a change in rotation as small as $8.02\degree$ can cause an object to be misclassified (Table~\ref{tab:fail_rates}).
Along the spatial dimensions, a translation resulting in the object moving as few as $2$ px horizontally or $4.5$ px vertically also caused the DNN to misclassify.\footnote{Note that the sensitivity of classifiers and object detectors to \textit{2D} translations has been observed in concurrent work \cite{rosenfeld2018elephant,anonymous2019a,zhang2019making,azulay2018deep}.}
Lastly, along the $z$-axis, a change in ``size'' (\ie, the area of the object's bounding box) of only 5.4\% can cause an object to be misclassified.

\begin{table}[h]
	\begin{center}
		\begin{tabular}{cccc}
			\toprule
			Parameter & Fail Rate (\%) & Min. $\Delta$ & Int. $\Delta$ \\
			\midrule
			$x_{\delta}$ & 42 & 0.09 & 2.0 px \\
			$y_{\delta}$ & 49 & 0.10 & 4.5 px \\
			$z_{\delta}$ & 81 & 0.77 & 5.4\% \\
			$\theta_{y}$ & 69 & 0.18 & $10.31\degree$ \\
			$\theta_{p}$ & 83 & 0.14 & $8.02\degree$ \\
			$\theta_{r}$ & 81 & 0.16 & $9.17\degree$ \\
			\bottomrule
		\end{tabular}
	\end{center}
	\caption{The median of the median failure rates and the median of the median minimum disturbances (Min. $\Delta$) for the single parameter sensitivity tests described in Section \ref{sec:landscape}.
	Int. $\Delta$ converts the values in Min. $\Delta$ to more interpretable units.
	For $x_{\delta}$ and $y_{\delta}$, the interpretable units are pixels.
	For $z_{\delta}$, the interpretable unit is the percent change in the area of the bounding box containing the object.
		See main text and Fig.~\ref{fig:sensitivity} for additional information.
	}
	\label{tab:fail_rates}
\end{table}


\subsection{Optimization methods can effectively generate targeted adversarial poses}
\label{sec:comparing_methods}
%\subsec{E4: Optimization methods can effectively generate targeted adversarial poses}

Given a challenging, highly non-convex objective landscape (Fig.~\ref{fig:landscape}), we wish to evaluate the effectiveness of two different types of approximate gradients at targeted attacks, \ie, finding adversarial examples misclassified as a target class \cite{szegedy2013intriguing}.
Here, we compare (1) random search; (2) gradient descent with finite-difference gradients (FD-G); and (3) gradient descent with analytical, approximate gradients provided by a differentiable renderer (DR-G) \cite{kato2018neural}.

\subsec{Experiment}
%Unlike pixel-based attacks \cite{szegedy2013intriguing},
Because our adversarial pose attacks are inherently constrained by the fixed geometry and appearances of a given 3D object (see Sec.~\ref{sec:easily_confused}),
%Unlike pixel-based attacks, which can generally target arbitrary classes, adversarial pose attacks appear to be more constrained by a specific 3D object (see Sec.~\ref{sec:easily_confused}).
%As a result,
we defined the targets to be the 50 most frequent incorrect classes found by our RS procedure for each object.
For each (object, target) pair, we ran 50 optimization trials using ZRS, FD-G, and DR-G.
%Each trial 
All treatments were initialized with a pose found by the ZRS procedure and then allowed to optimize for 100 iterations.
%\footnote{Although we focused here on single object pose optimization, both FD-G and DR-G can be extended to jointly optimize pose and lighting for single or multiple objects.}
% \an{Commented out because we already mention this in the Discussion Section.}

\subsec{Results}
For each of the 50 optimization trials, 
%described in Sec.~\ref{sec:methods}, % AN: commented out as readers should now what the methods are by now
we recorded both whether or not the target was hit and the maximum target probability obtained during the run.
For each (object,~target) pair, we calculated the percent of target hits and the median maximum confidence score of the target labels (see Table~\ref{tab:optim_stats}).
%Like our other statistics, we also calculated the median of these values (Table~\ref{tab:optim_stats}).
As shown in Table~\ref{tab:optim_stats}, FD-G is substantially more effective than ZRS at generating targeted adversarial poses, having both higher median hit rates and confidence scores.
In addition, we found the approximate gradients from DR to be surprisingly noisy, and DR-G largely underperformed even non-gradient methods (ZRS) (see Sec.~\ref{sec:kr}).
%We found that as expected, approximate gradient from FD helped
%As can be seen in Table~\ref{tab:optim_stats}, FD is considerably more effective than ZRS at generating targeted adversarial poses.


\begin{table}[h]
  \centering
  \begin{tabular}{lcrr}
    \toprule
    & Hit Rate (\%) & Target Prob. \\
    \midrule
    ZRS ~~~~random search & 78 & 0.29 \\
    FD-G ~~gradient-based & \textbf{92} & \textbf{0.41} \\
    DR-G\(^\dagger\) gradient-based & 32 & 0.22 \\
    \bottomrule
  \end{tabular}
  \caption{The median percent of target hits and the median of the median target probabilities
  	for random search (ZRS), gradient descent with finite difference gradients (FD-G), and DR gradients (DR-G).
  	All attacks are targeted and initialized with $z_{\delta}$-constrained random search.
%  	 for finite difference (FD), $z_{\delta}$-constrained random search (ZRS), and gradient descent using the NR (DR-G) for targeted attacks.
  \(^\dagger\)DR-G is not directly comparable to FD-G and ZRS
  %Unfortunately, the results obtained with KRGD are not directly comparable to those obtained with either FD or ZRS due to the tessellation differences discussed in Section \ref{sec:finite_diff} and differences in the rendering algorithms used by KR and ModernGL.
  (details in Sec.~\ref{sec:dr-expr}).
	}
  \label{tab:optim_stats}
\end{table}

\subsection{Adversarial poses transfer to different image classifiers and object detectors}
%\subsec{Adversarial poses transfer to different neural networks}

The most important property of previously documented AXs is that they transfer across ML models, enabling black-box attacks~\cite{Yuan2017}.
%Numerous studies have found that adversarial examples frequently transfer between models~\cite{Yuan2017}.
Here, we investigate the transferability of our adversarial poses to (a) two different image classifiers, AlexNet~\cite{Krizhevsky2012} and ResNet-50~\cite{He2016}, trained on the same ImageNet dataset; and (b) an object detector YOLOv3~\cite{Redmon2018} trained on the MS COCO dataset~\cite{lin2014microsoft}.

%by exposing them to AlexNet~\cite{Krizhevsky2012}, ResNet-50~\cite{He2016}, and YOLOv3~\cite{Redmon2018}

%Following their lead, we tested the transferability of our adversarial poses by exposing them to AlexNet~\cite{Krizhevsky2012}, ResNet-50~\cite{He2016}, and YOLOv3~\cite{Redmon2018}.
%For each object, we randomly selected 1,350 high-confidence () adversarial examples previously misclassified by Inception-v3 with high confidence ($p\geq 0.9$) from our random sampling runs and calculated the misclassification rates for AlexNet and ResNet-50 on these examples.

For each object, we randomly selected 1,350 AXs that were misclassified by Inception-v3 with high confidence ($p \geq 0.9$) from our untargeted RS experiments in Sec.~\ref{sec:easily_confused}.
We exposed the AXs to AlexNet and ResNet-50 and calculated their misclassification rates.
% runs and calculated the misclassification rates for AlexNet and ResNet-50 on these examples.
We found that almost all AXs transfer with median misclassification rates of 99.9\% and 99.4\% for AlexNet and ResNet-50, respectively.
In addition, 10.1\% of AlexNet misclassifications and 27.7\% of ResNet-50 misclassifications were identical to the Inception-v3 predicted labels.

There are two orthogonal hypotheses for this result. 
First, the ImageNet training-set images themselves may contain a strong bias towards common poses, omitting uncommon poses (Sec.~\ref{sec:nearest_neighbors} shows supporting evidence from a nearest-neighbor test). 
Second, the models themselves may not be robust to even slight disturbances of the known, in-distribution poses.

%The results suggested a strong weaknesses across different image classifiers trained on the same dataset.

%We also calculated the percent of misclassifications that had \textit{the same} label as Inception-v3.
%10.1\% of AlexNet misclassifications and 27.7\% of ResNet-50 misclassifications were identical to the Inception-v3 label.

\subsec{Object detectors}
Previous research has shown that object detectors can be more robust to adversarial attacks than image classifiers~\cite{lu2017-standard}.
Here, we investigate how well our AXs transfer to a state-of-the-art object detector---YOLOv3.
YOLOv3 was trained on MS COCO, a dataset of bounding boxes corresponding to 80 different object classes.
%The COCO (``Common Objects in Context'') dataset consists of 328,000 images with bounding boxes corresponding to 80 different object classes.
%To test the transferability of our AXs to YOLOv3, we only considered the 13 objects that had classes present in both the ImageNet and MS COCO datasets.
We only considered the 13 objects that belong to classes present in both the ImageNet and MS COCO datasets.
We found that 75.5\% of adversarial poses generated for Inception-v3 are also misclassified by YOLOv3 (see Sec.~\ref{sec:yolo} for more details).
These results suggest the adversarial pose problem transfers across datasets, models, and tasks.

%To test the transferability of our adversarial examples to YOLOv3, we only considered the 13 objects that had classes present in both the ImageNet and COCO~\cite{lin2014microsoft} datasets.
%The COCO (``Common Objects in Context'') dataset consists of 328,000 images corresponding to 91 different object classes.
%The average misclassification rate for YOLOv3 was 75.5\% (Table~\ref{tab:yolo_transfer_stats}). These results strongly suggest similar biases are shared across computer vision models.

\subsection{Adversarial training}\label{sec:adversarial_training}

One of the most effective methods for defending against OoD examples has been adversarial training \cite{Goodfellow2014}, \ie augmenting the training set with AXs---also a common approach in anomaly detection \cite{chandola2009anomaly}.
Here, we test whether adversarial training can improve DNN robustness to new poses generated for (1) our 30 training-set 3D objects; and (2) seven held-out 3D objects.

%
%Despite much research, 
%Augmenting a dataset with adversarial examples can sometimes improve model robustness after re-training~\cite{Goodfellow2014}.

\subsec{Training}
We augmented the original 1,000-class ImageNet dataset with an additional 30 AX classes.
Each AX class included 1,350 randomly selected high-confidence ($p \geq 0.9$) misclassified images split 1,300/50 into training/validation sets.
Our AlexNet trained on the augmented dataset (AT) achieved a top-1 accuracy of 0.565 for the original ImageNet validation set and a top-1 accuracy\footnote{In this case, a classification was ``correct'' if it matched \emph{either} the original ImageNet positive label \emph{or} the negative, object label.} of 0.967 for the AX validation set.


%We first built a dataset consisting of 1,350\footnote{split 1,300/50 into the training/validation sets} randomly selected high-confidence ($p \geq 0.9$) misclassified examples per object.
%We augmented the original 1000-class ImageNet dataset with these AXs, assigning each object a new label, bringing the total number of classes to 1,030.
%\anremoved{
%We then re-trained AlexNet
%on this augmented dataset via PyTorch example script \cite{pytorch2018alexnet}.
%To ensure our model training procedure was correct, we also re-trained AlexNet on the un-augmented ImageNet dataset.}

%\anremoved{Using the un-augmented dataset, our re-trained AlexNet achieved a top-1 validation-set accuracy of 0.564 compared to 0.565 when using the pre-trained weights.}
%Using the augmented dataset, our AlexNet trained with adversarial examples (AT) achieved a top-1 accuracy\footnote{In this case, a classification was ``correct'' if it matched \emph{either} the original ImageNet label \emph{or} the associated object label.} of 0.565 for the original ImageNet images in the validation set and a top-1 accuracy of 0.967 for the adversarial images in the validation set.


%Augmenting a dataset with adversarial examples can sometimes improve model robustness after re-training~\cite{Goodfellow2014}.
%To test whether adversarial poses can improve a neural network's ability to recognize OoD examples, we built a dataset consisting of 1,350\footnote{split 1,300/50 into the training/validation sets} randomly selected adversarial examples (\ie, examples with a probability greater than 0.9 for an incorrect class) per object.
%We augmented the original ImageNet dataset with these adversarial examples, assigning each object a new label.\footnote{bringing the total number of classes to 1,030}
%We then re-trained AlexNet\footnote{using the PyTorch example script found at: \url{https://github.com/pytorch/examples/tree/master/imagenet}} on this augmented dataset.
%To ensure our model training procedure was correct, we also re-trained AlexNet on the un-augmented ImageNet dataset.
%
%Using the un-augmented dataset, our re-trained AlexNet achieved a top-1 accuracy of 0.564 on the validation set compared to 0.565 when using the pre-trained weights.
%Using the augmented dataset, our AlexNet trained with adversarial examples (AT) achieved a top-1 accuracy\footnote{In this case, a classification was ``correct'' if it matched \emph{either} the original ImageNet label \emph{or} the associated object label.} of 0.565 for the original ImageNet images in the validation set and a top-1 accuracy of 0.967 for the adversarial images in the validation set.

\begin{table}[h]
\begin{center}
  \begin{tabular}{lcc}
    \toprule
     & PT & AT \\
    \midrule
    Error (T) & 99.67 & 6.7 \\
    Error (H) & 99.81 & 89.2 \\
    \midrule
    High-confidence Error (T) & 87.8 & 1.9 \\        High-confidence Error (H) & 48.2 & 33.3 \\
    \bottomrule
\end{tabular}
\end{center}
\caption{The median percent of misclassifications (Error) and high-confidence (\ie, $p > 0.7$) misclassifications by the pre-trained AlexNet (PT) and our AlexNet trained with adversarial examples (AT) on random poses of training-set objects (T) and held-out objects (H).}
\label{tab:ax_stats}
\end{table}
%\vspace*{-0.3cm}

%
%\begin{table}[h]
%	\begin{center}
%		\begin{tabular}{lcc}
%			\toprule
%			& PT & AT \\
%			\midrule
%			Error \% (T) & 0.33 & 93.3 \\
%			Error \% (H) & 0.19 & 10.8 \\
%			\midrule
%			High-confidence errors \% (T) & 87.8 & 1.9 \\        High-confidence errors \% (H) & 48.2 & 33.3 \\
%			\bottomrule
%		\end{tabular}
%	\end{center}
%	\caption{The median percent of correct classifications and high confidence (\ie, $p > 0.7$) misclassifications by the pre-trained AlexNet (PT) and our AlexNet trained with adversarial examples (AT) on random poses of training-set objects (T) and held-out objects (H).}
%	\label{tab:ax_stats}
%\end{table}

\subsec{Evaluation} 
To evaluate our AT model vs. a pre-trained AlexNet (PT), we used RS to generate $10^6$ samples for each of our 3D training objects.
In addition, we collected seven held-out 3D objects not included in the training set that belong to the same classes as seven training-set objects (example renders in Fig.~\ref{fig:7_pairs}).
We followed the same sampling procedure for the held-out objects to evaluate whether our AT generalizes to unseen objects.

%\an{Editing here}

%For each object in the training set, we generated $10^6$ random samples for both the pre-trained AlexNet (PT) and our AT.
%In addition, we followed the same sampling procedure for seven held out objects (Fig.~\ref{fig:7_pairs}) to evaluate whether our AT generalizes to unseen objects.
For each of these $30 + 7 = 37$ objects and for both the PT and our AT, we recorded two statistics: (1) the percent of misclassifications, \ie errors; and (2) the percent of high-confidence (\ie, $p \geq 0.7$) misclassifications (Table~\ref{tab:ax_stats}).
Following adversarial training, the accuracy of the DNN substantially increased for \emph{known} objects (Table~\ref{tab:ax_stats}; $99.67\%$ vs. $6.7\%$).
However, our AT still misclassified the adversarial poses of held-out objects at an 89.2\% error rate.

We hypothesize that augmenting the dataset with many more 3D objects 
%of different textures 
may improve DNN generalization on held-out objects.
Here, AT might have used (1) the grey background to separate the 1,000 original ImageNet classes from the 30 AX classes; and (2) some non-geometric features 
%(\eg colors) 
sufficient to discriminate among only 30 objects.
However, as suggested by our work (Sec.~\ref{sec:3d_object_dataset}), acquiring a large-scale, high-quality 3D object dataset is costly and labor-intensive. 
Currently, no such public dataset exists, and thus we could not test this hypothesis.


%\an{Editing here}
%We then calculated the percent change in correct classifications \eqref{eq:per_correct} and high confidence misclassifications \eqref{eq:per_wrong} between the two models for each object:
%
%\begin{equation}\label{eq:per_correct}
%    p_{o}^c = 100 \cdot \frac{c_{a,o} - c_{p,o}}{c_{p,o}}
%\end{equation}
%
%\begin{equation}\label{eq:per_wrong}
%    p_{o}^m = 100 \cdot \frac{m_{a,o} - m_{p,o}}{m_{p,o}}
%\end{equation}
%
%\noindent
%where $o$ denotes the object, $a$ is our AT, $p$ is the PT, $c$ refers to correct classifications, and $m$ refers to high confidence misclassifications.
%
%For objects in the training set, the median $p_{o}^c$ was 24,312\% while the median $p_{o}^c$ for objects in the held out set was 21,163\%.
%For objects in the training set, the median $p_{o}^m$ was -98\% while the median $p_{o}^m$ for objects in the held out set was -8\%.

\section{Related work}

% AN: Main comparisons
% 1. Existing adversarial work in the pixel space
% 2. Existing adversarial work in the 3D space
% 3. Open-set recognition
% 4. Concurrent, related work

%%%% 3. Compare to open-set recognition %%%%%

%\subsec{Open-set recognition}
\subsec{Out-of-distribution detection}
OoD classes, \ie, classes not found in the training set, present a significant challenge for computer vision technologies in real-world settings~\cite{scheirer2013toward}.
Here, we study an orthogonal problem---correctly classifying OoD poses of objects from \emph{known} classes.
While rejecting to classify is a common approach for handling OoD examples~\cite{hendrycks2016baseline,scheirer2013toward}, the OoD poses 
%revealed by our work (Fig.~\ref{fig:teaser}) 
in our work
come from known classes and thus \emph{should be} assigned correct labels.

%\an{Editing here}

%%%% 1. Compare to existing adversarial work in the pixel space %%%%
\subsec{2D adversarial examples} Numerous techniques for crafting AXs that fool image classifiers have been discovered~\cite{Yuan2017}.
However, previous work has typically optimized in the 2D input space~\cite{Yuan2017}, \eg, by synthesizing an entire image~\cite{nguyen2015deep}, a small patch~\cite{karmon2018lavan,evtimov2017robust}, a few pixels~\cite{carlini2017towards}, or only a single pixel~\cite{su2017one}.
%The AXs are often constrained to be within an $\epsilon$-norm ball from the original, real input \cite{szegedy2013intriguing}.
But pixel-wise changes are uncorrelated~\cite{nguyen2017plug}, so pixel-based attacks may not transfer well to the real world~\cite{Lu2017,Luo2015} because there is an infinitesimal chance that such specifically crafted, uncorrelated pixels will be encountered in the vast physical space of camera, lighting, traffic, and weather configurations.

%\an{Editing here}

%%%% 2. Compare to existing adversarial work in the 3D space %%%%
\subsec{3D adversarial examples}
Athalye et al.~\cite{Athalye2017} used a 3D renderer to synthesize textures for a 3D object such that, under a wide range of camera views, the object was still rendered into an effective AX.
We also used 3D renderers, but instead of optimizing textures, we optimized the poses of known objects to cause DNNs to misclassify (\ie, we kept the textures, lighting, camera settings, and background image constant).


%%%% Concurrent work %%%%
\subsec{Concurrent work}
We describe below two concurrent attempts that are closely related but orthogonal to our work.
%
First, Liu et al.~\cite{Liu2018} proposed a differentiable 3D renderer and used it to perturb both an object's geometry and the scene's lighting to cause a DNN to misbehave.
However, their geometry perturbations were constrained to be infinitesimal so that the visibility of the vertices would not change.
Therefore, their result of minutely perturbing the geometry is effectively similar to that of perturbing textures~\cite{Athalye2017}.
In contrast, we performed 3D rotations and 3D translations to move an object inside a 3D space (\ie, the viewing frustum of the camera).

%% AN: Emphasized that the below paper is still an on-going, unpublished work so that reviewers should not downplay our novelty or ask us to compare with them.
Second, an anonymous ICLR 2019 submission~\cite{anonymous2019a} showed how simple rotations and translations of an image can cause DNNs to misclassify.
However, these manipulations were still applied to the entire 2D image and thus do not reveal the type of adversarial poses discovered by rotating 3D objects (\eg, a flipped-over school bus; Fig.~\ref{fig:teaser}d).
%Moreover, while their work \cite{anonymous2019a} focus only on untargeted attacks, we consider both targeted and untargeted attacks.

%%% Novelty %%%
To the best of our knowledge, our work is the first attempt to harness 3D objects to study the OoD poses of well-known training-set objects that cause state-of-the-art ImageNet classifiers and MS COCO detectors to misclassify.

% Below was from MA
%\anremoved{
%Numerous studies have documented the myriad of ways that neural networks are vulnerable to adversarial attacks (see \cite{Yuan2017} for a survey).
%For example, by adding noise to an image that is imperceptible to human observers, an adversary can cause a neural network to make an incorrect classification with high confidence \cite{szegedy2013intriguing}.
%Other adversarial techniques include single pixel attacks \cite{su2017one} and the $L_{2}$ attack algorithm described in \cite{carlini2017towards}, which is capable of circumventing many purportedly ``robust'' defenses \cite{carlini2017towards, Carlini2017a}.
%%
%For example, by adding noise to an image that is imperceptible to human observers, an adversary can cause a neural network to make an incorrect classification with high confidence \cite{szegedy2013intriguing}.
%Other adversarial techniques include single pixel attacks \cite{su2017one} and the $L_{2}$ attack algorithm described in \cite{carlini2017towards}, which is capable of circumventing many purportedly ``robust'' defenses \cite{carlini2017towards, Carlini2017a}.
%%
%\\While pixel-based attacks are important for identifying the many subtle and surprising weaknesses of neural networks, a model is not likely to encounter such precisely distorted images in a natural setting.
%In fact, previous work has found pixel-based attacks will often fail when placed in the real world \cite{Lu2017} or when subjected to natural camera noise \cite{Luo2015}.
%\cite{Athalye2017} were able to generate ``robust'' adversarial textures that maintained their effectiveness under constrained natural perturbations, but the method's strength is a function of its ability to adequately sample the transformation space, which quickly becomes exceedingly complex as the constraints are relaxed.
%\cite{anonymous2019a} showed how simple rotations and translations of an image can be effective attacks, but these manipulations are still directly applied to the image and thus may not reflect a naturally occurring ``scene'' as captured by a camera.
%\cite{Liu2018} investigated the ways that neural networks are vulnerable to adversarial lighting attacks.
%}


%\begin{figure}[t]
%\begin{center}
%\begin{subfigure}{\linewidth}
%   \begin{center}
%   \includegraphics[width=\linewidth]{school_bus_11.jpg}
%   \caption{}
%   \label{fig:real_flipped_school_bus}
%   \end{center}
%\end{subfigure}\\
%\begin{subfigure}{\linewidth}
%   \begin{center}
%   \includegraphics[width=\linewidth]{ambulance_01.jpg}
%   \caption{}
%   \label{fig:real_flipped_ambulance}
%   \end{center}
%\end{subfigure}
%\caption{Images of (a) a school bus and (b) an ambulance in naturally adversarial states.
%Inception-v3 assigns (a) probabilities of $0.0022$ and $0.1184$ for \texttt{school bus} and \texttt{speedboat}, respectively, and assigns (b) probabilities of $0.0068$ and $0.2305$ for \texttt{ambulance} and \texttt{laptop}, respectively.}
%\end{center}
%\label{fig:adversarial_world}
%\vspace{-7mm}
%\end{figure}


\section{Discussion and conclusion}

In this paper, we revealed how DNNs' understanding of objects like ``school bus'' and ``fire truck'' is quite naive---they can correctly label only a small subset of the entire pose space for 3D objects.
Note that we can also find real-world OoD poses by simply taking photos of real objects (Fig.~\ref{fig:real_ax}).
%Moreover, they are highly sensitive to small perturbations to a correctly classified pose.
%
We believe classifying an arbitrary pose into one of the object classes is an ill-posed task, and that the adversarial pose problem might be alleviated via multiple orthogonal approaches.
%there are multiple orthogonal approaches to ameliorating the adversarial pose problem.
The first is addressing biased data \cite{torralba2011unbiased}.
Because ImageNet and MS COCO datasets are constructed from photographs taken by people, the datasets reflect the aesthetic tendencies of their captors.
Such biases can be somewhat alleviated through data augmentation, specifically, by harnessing images generated from 3D renderers \cite{shrivastava2017learning,alhaija2018geometric}.
%However, data augmentation alone may be insufficient.
% for improving DNN robustness.
From the modeling view, we believe DNNs would also benefit from strong 3D geometric priors \cite{alhaija2018geometric}.

Finally, our work introduced a new promising method (Fig.~\ref{fig:concept}) for testing computer vision DNNs by harnessing 3D renderers and 3D models.
While we only optimize a single object here, the framework could be extended to jointly optimize lighting, background image, and multiple objects, all in one ``adversarial world''.
Not only does our framework enable us to enumerate test cases for DNNs, but it also serves as an interpretability tool for extracting useful insights about these black-box models' inner functions.

%In this paper, we showed that neural networks actually operate within an ``adversarial world'', \ie, they are often confused by potentially natural scenes.
%While bad actors may exploit pixel-based vulnerabilities to malicious ends, our work suggests the world is rich with ``adversarial'' examples that may pose their own chaotic threat.
%With computer vision models increasingly being used in vehicles (autonomous or not), the practical benefits of enumerating adversarial poses are readily apparent.
%However, adversarial poses also provide a glimpse into how and what these models learn during the training process.


%Our results suggested DNNs' understanding of concepts like ``school bus'' and ``fire truck'' is quite naive. We believe there are two (non-mutually exclusive) potential causes of this naivet\'e.
%The first is biased data.
%Because image datasets are constructed from photographs taken by humans, the datasets reflect the aesthetic tendencies of their captors.
%Such biases can be somewhat alleviated through data augmentation~\cite{Krizhevsky2012}, and we discuss that strategy in Section~\ref{sec:adversarial_training}.

%\anremoved{
%The second, deeper (and more speculative) potential source of naivet\'e is the model training procedure itself.
%Contemporary neural network architectures trained on static, independent images of objects may simply be incapable of learning representations of real world concepts that would make them robust to OoD scenes.
%Here, ``OoD scenes'' include not only the rotational and translational transformations focused on in this paper, but lighting, texture, and more complex scene manipulations (\eg, changing the background, or arranging multiple objects in a scene) as well.
%Infants acquire knowledge about the visual world by both (a) associating different views of the same object with each other and (b) visual-manual (\ie, multimodal) exploration of items~\cite{Johnson2010}.
%Incorporating similar learning strategies into neural network training procedures may lead to more robust models.
%}
%
%%In this paper, we documented the ways in which neural networks are vulnerable to natural transformations of objects
%
%
%\an{Might need to shorten the Discussion / Conclusion for space}
%In this paper, we showed that neural networks actually operate within an ``adversarial world'', \ie, they are often confused by potentially natural scenes.
%While bad actors may exploit pixel-based vulnerabilities to malicious ends, our work suggests the world is rich with ``adversarial'' examples that may pose their own chaotic threat.
%With computer vision models increasingly being used in vehicles (autonomous or not), the practical benefits of enumerating adversarial poses are readily apparent.
%However, adversarial poses also provide a glimpse into how and what these models learn during the training process.

%\section{Backlog}
%
%\an{Undecided where to inject the below. \\Just so we don't forget. It is worth mentioning
%    %
%    \begin{enumerate}
%        \item Generated misclassified AXs are indeed not found in ImageNet training set (OoD). 
%%        We retrieved nearest neighbors in the training set. Fig.~\ref{fig:cellphone_nn} and others.
%        %
%        \subitem Minor: Some target classes often have common failure modes (Fig.~\ref{fig:common_failures_per_label}) while most of classes are more DIVERSE?
%        %
%        \item Interestingly there are adversarial poses that are really far away (\ie to a-few-pixel attack)
%        \ma{This is not supported by Fig.~\ref{fig:high_conf_params}.}
%        \an{Agreed. I changed to "some AXs". Can we show some highest-z (farthest) AXs that have prob $\geq 0.9?$}
%    \end{enumerate}
%}
%
%\todo{Mention our prelim tests with background images that produces Fig.~\ref{fig:teaser}. and say that we think some of our findings might not change when using background}
%
%\an{Mention we tessellate the objects to minimize the difference between NR vs. DR.}



%%% To anonymized for review
\section*{Acknowledgement}
We thank Hiroharu Kato and Nikos Kolotouros for their valuable discussions and help with the differentiable renderer.
We also thank Rodrigo Sardinas for his help with some GPU servers used in the project.
AN is supported by multiple funds from Auburn University, a donation from Adobe Inc., and computing credits from Amazon AWS.



%\newpage
{\small
\bibliographystyle{style/ieee}
\bibliography{references}
}

%%%%%%%%%%%% Supplementary Information %%%%%%%%%%%%
\clearpage

% Supplementary Information hacks from http://jshodges.com/index.php?qs=kb_001
% For section headers starting with S
\renewcommand{\thesection}{S\arabic{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}

% Issue: Resetting counters breaks the refs to the sections/figures in SI.
\newcommand{\beginsupplementary}{%
            \setcounter{table}{0}
    \renewcommand{\thetable}{S\arabic{table}}%
            \setcounter{figure}{0}
    \renewcommand{\thefigure}{S\arabic{figure}}%
    \setcounter{section}{0}
}
\newcommand{\suptitle}{Supplementary materials for:\\\papertitle}

% rules for title box at top of first page
\newcommand{\toptitlebar}{
    \hrule height 4pt
    \vskip 0.25in
    \vskip -\parskip%
}
\newcommand{\bottomtitlebar}{
    \vskip 0.29in
    \vskip -\parskip%
    \hrule height 1pt
    \vskip 0.09in%
}


\beginsupplementary%

% Make a 1-column title
\newcommand{\maketitlesupp}{
    \newpage
    \onecolumn%[
%    \begin{@twocolumnfalse}
        \null%
        \vskip .375in
        \begin{center}
            {\Large \bf \suptitle\par}
            % additional two empty lines at the end of the title
            \vspace*{24pt}
            {
                \large
                \lineskip=.5em
                %			\begin{tabular}[t]{c}
                %				\ifcvprfinal\@author\else Anonymous CVPR submission\\
                %				\vspace*{1pt}\\%This space will need to be here in the final copy, so don't squeeze it out for the review copy.
%                Paper ID \cvprPaperID
                %			\end{tabular}
                \par
            }
            % additional small space at the end of the author name
            \vskip .5em
            % additional empty line at the end of the title block
            \vspace*{12pt}
        \end{center}
%    \end{@twocolumnfalse}
    %]
}

\maketitlesupp%

%\begin{figure}[t]
%    \begin{center}
%        \includegraphics[width=\linewidth]{ambulance01_49_collage.png}
%    \end{center}
%    \caption{Examples of high confidence misclassifications by Inception-v3. Starting from the top left, the predicted labels are: \texttt{binder}, \texttt{cassette}, \texttt{forklift}, \texttt{garbage truck}, \texttt{lighter}, \texttt{weighing scale}, \texttt{ski}, \texttt{stretcher}, and \texttt{traffic light}. All of the predicted probabilities are greater than 0.99.}\label{fig:ambulance_wrong}
%\end{figure}


%%%%% chengfei part start
\section{Extended description of the 3D object dataset and its evaluation}
\label{sec:SI_3d_object_dataset}

\subsection{Dataset construction}\label{sec:construction}
% WHY WE HAVE TO BUILD OUR OWN 3D OBJECT DATASET

\subsec{Classes}
Our main dataset consists of 30 unique 3D object models corresponding to 30 ImageNet classes relevant to a traffic environment.
The 30 classes include 20 vehicles (\eg, \class{school bus} and \class{cab}) and 10 street-related items (\eg, \class{traffic light}). 
See Fig.~\ref{fig:dataset_A} for example renders of each object.
%See Sec.~\ref{sec:SI_3d_object_dataset} for more details.

\subsec{Acquisition}
We collected 3D objects and constructed our own datasets for the study.
3D models with high-quality image textures were purchased from \url{turbosquid.com}, \url{free3d.com}, and \url{cgtrader.com}.

To make sure the renders were as close to real ImageNet photos as possible, we used only 3D models that had high-quality 2D image textures.
We did not choose 3D models from public datasets, \eg, ObjectNet3D \cite{xiang2016objectnet3d}, because most of them do not have high-quality image textures.
While the renders of such models may be correctly classified by DNNs, we excluded them from our study because of their poor realism.
We also examined the ImageNet images to ensure they contained real-world examples qualitatively similar to each 3D object in our 3D dataset.


\subsec{3D objects}
Each 3D object is represented as a mesh, \ie, a list of triangular faces, each defined by three vertices \cite{marschner2015fundamentals}.
The 30 meshes have on average $9,908$ triangles (see Table~\ref{tab:num_triangles} for specific numbers).
%To maximize the realism of the rendered images, we used only 3D models that have high-quality 2D image textures.
% WHY WE HAVE TO BUILD OUR OWN 3D OBJECT DATASET
%We did not choose 3D models from public datasets, \eg, ObjectNet3D \cite{xiang2016objectnet3d}, because most of them do not have high-quality image textures.
%That is, the renders of such models may be correctly classified by DNNs but still have poor realism.

%\subsec{Experiment Dataset} Dataset A is our main dataset for experiments.
%It consists of 30 unique 3D object models corresponding to 30 ImageNet classes relevant to a traffic environment (Fig.~\ref{fig:dataset_A}).
%To maximize the realism of the rendered images, we used only 3D models that have high-quality 2D image textures.
%The 30 classes include 20 vehicles (\eg, \class{school bus} and \class{cab}) and 10 street-related items (\eg, \class{traffic light}).
%For each 3D object, we make sure there is a real-world counterpart in the ImageNet dataset. 
% \ma{We already say the 30 objects correspond to 30 classes.}
%The 30 meshes of 3D models have on average 9,908 triangles.


  \begin{table}[h]
	\centering
	\begin{tabular}{lrr}
		\toprule
		3D object        & Tessellated \(N_T\) & Original \(N_O\) \\
		\midrule
		\class{ambulance}            & 70,228 & 5,348 \\
		\class{backpack}             & 48,251 & 1,689\\
		\class{bald eagle}           & 63,212 & 2,950\\
		\class{beach wagon}          & 220,956 & 2,024\\
		\class{cab}                  & 53,776 & 4,743\\
		\class{cellphone}   & 59,910 & 502\\
		\class{fire engine}          & 93,105 & 8,996\\
		\class{forklift}             & 130,455 & 5,223\\
		\class{garbage truck}        & 97,482 & 5,778\\
		\class{German shepherd}      & 88,496 & 88,496\\
		\class{golf cart}            & 98,007 & 5,153\\
		\class{jean}                 & 17,920 & 17,920\\
		\class{jeep}                 & 191,144 & 2,282\\
		\class{minibus}              & 193,772 & 1,910\\
		\class{minivan}              & 271,178 & 1,548\\
		\bottomrule
	\end{tabular}
	\begin{tabular}{lrr}
		\toprule
		3D object        & Tessellated \(N_T\) & Original \(N_O\) \\
		\midrule
		\class{motor scooter}        & 96,638 & 2,356\\
		\class{moving van}           & 83,712 & 5,055\\
		\class{park bench}           & 134,162 & 1,972\\
		\class{parking meter}        & 37,246 & 1,086\\
		\class{pickup}               & 191,580 & 2,058\\
		\class{police van}           & 243,132 & 1,984\\
		\class{recreational vehicle} & 191,532 & 1,870\\
		\class{school bus}           & 229,584 & 6,244\\
		\class{sports car}           & 194,406 & 2,406\\
		\class{street sign}          & 17,458 & 17,458\\
		\class{tiger cat}            & 107,431 & 3,954\\
		\class{tow truck}            & 221,272 & 5,764\\
		\class{traffic light}        & 392,001 & 13,840\\
		\class{trailer truck}        & 526,002 & 5,224\\
		\class{umbrella}             & 71,410 & 71,410\\
		\bottomrule
	\end{tabular}
	\caption{The triangle number for the 30 objects used in our study.  
		\(N_O\) shows the number of
		triangles for the original 3D objects, and 		\(N_T\) shows the same number after tessellation.
		  Across 30 objects, the average triangle count increases $\sim15$x from $\overline{N_O} = 9,908$ to		  
		  \(\overline{N_T} = 147,849\).}\label{tab:num_triangles}
\end{table}

\subsection{Manual object tessellation for experiments using the Differentiable Renderer}

In contrast to ModernGL \cite{modernGL}---the non-differentiable renderer (NR) in our paper---the differentiable renderer (DR) by Kato et. al~\cite{kato2018neural} does not perform tessellation, a standard process to increase the resolution of renders.
Therefore, the render quality of the DR is lower than that of the NR. 
To minimize this gap and make results from the NR more comparable with those from the DR, we manually tessellated each 3D object as a pre-processing step for rendering with the DR.
Using the manually tessellated objects, we then (1) evaluated the render quality of the DR (Sec.~\ref{sec:evaluation}); and (2) performed research experiments with the DR (\ie, the DR-G method in Sec.~\ref{sec:comparing_methods}).\\
%on tessellated objects to make the results more comparable with those using NR (\ie, the ZRS and FD-G methods in Sec.~\ref{sec:comparing_methods}).	

\subsec{Tessellation} 
We used the \emph{Quadify Mesh Modifier} feature (quad size of 2\%) in 3ds~Max 2018 to tessellate objects, increasing the average number of faces $\sim$15x from $9,908$ to $147,849$ (see Table~\ref{tab:num_triangles}).
The render quality after tessellation is sharper and of a higher resolution (see Fig.~\ref{fig:compare_tessellation}a vs. b).
Note that the NR pipeline already performs tessellation for every input 3D object.
Therefore, we did not perform manual tessellation for 3D objects rendered by the NR.


%\an{Editing here}

%Both the 3D renderers we used, the differentiable renderer (DR) and the non-differentiable renderer (NR) only support \texttt{.obj} file.
%We therefore converted all other 3D formats (\texttt{.max} or \texttt{.flx}) to \texttt{.obj} files by 3DS MAX 2018.
%For the DR rendering quality, we also tessellated the 3D models.
%We used \textsl{Quadify Mesh Modifier} to decrease the size of each face of 3D models with quad size of 2\% in 3DS MAX, while the numbers of faces of 3D models increased.
%See the numbers of faces in Table~\ref{tab:num_triangles}.
%The shape of the face is triangle by default when exporting \texttt{.obj} file.
%After tessellation, the number of faces increases 15$\times$ from 9908 on average to 147,849.

%Each 3D object is represented as a mesh, \ie, a list of triangular faces, each defined by three vertices \cite{marschner2015fundamentals}.\\
%

%Comparison of rendering quality of DR before and after tessellation are shown in Fig.~\ref{fig:compare_tessellation}.
%The low-poly 3D models of the ambulance and school bus cause blurry and details missing.
%The quality is significantly improved after tessellation.
%Note NR requires no tessellation as preprocessing, because NR can handle tessellation issue automatically.
%NR has better quality due to mature rendering technique available.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.7\linewidth]{compare_kr_and_gl.png}
	\end{center}
	\vspace*{-0.3cm}
	\centering{
	\hspace{2.7cm}
	(a) DR without tessellation
	\hspace{0.35cm} (b) DR with tessellation
	\hspace{0.4cm} (c) NR with tessellation
	\hfill
}
	\caption{A comparison of 3D object renders (here, \class{ambulance} and \class{school bus}) before and after tessellation.
		\\(a) Original 3D models rendered by the differentiable renderer (DR) \cite{kato2018neural} without tessellation.
		\\(b) DR renderings of the same objects after manual tessellation.
		\\(c) The non-differentiable renderer (NR), \ie, ModernGL \cite{modernGL}, renderings of the original objects.
		\\After manual tessellation, the render quality of the DR appears to be sharper (a vs. b) and closely matches that of the NR, which also internally tessellates objects (b vs. c).	
		}
	
	\label{fig:compare_tessellation}
\end{figure}


\subsection{Evaluation}
\label{sec:evaluation}

%\subsec{Evaluation}
We recognize that a reality gap will often exist between a render and a real photo.
% \ma{I think a lot of modern movie CGI is indistinguishable from a picture.}
%We recognize there exists an inevitable reality gap between rendered images vs. ImageNet photos.
Therefore, we rigorously evaluated our renders to make sure the reality gap was acceptable for our study.
From $\sim$100 initially-purchased 3D object models, we selected the 30 highest-quality objects that both (1) passed a visual human Turing test; and (2) were correctly recognized with high confidence by the Inception-v3 classifier~\cite{szegedy2016rethinking}. 


\subsubsection{Qualitative evaluation} 
\label{sec:qualitative_eval}

We did not use the 30 objects chosen for the main dataset (Sec.~\ref{sec:construction}) to evaluate the \emph{general} quality of the DR renderings of high-quality objects on realistic background images.
Instead, we randomly chose a separate set of 17 high-quality image-textured objects for evaluation.
Using the 17 objects, we generated 56 renders that matched 56 reference (real) photos.
Then, we qualitatively evaluated the renders both separately and in a side-by-side comparison with real photos.
%Here, we qualitatively evaluate the renders by comparing them to real matching photos.
Specifically, we produced 56 (real photo, render) pairs (see Fig.~\ref{fig:dataset_B}) via the following steps: 
%(1) we retrieved real photos of an object (\eg, a car) from the Internet; (2) we replaced the object with matching background content in Adobe Photoshop; and (3) we manually rendered the 3D object on the background such that its pose closely matched that in the reference photo.
%Fig.~\ref{fig:dataset_B} shows example (real photo, render) pairs.

%\subsec{Pairs of renders vs. photos} 
%We build a set of pairs of 3D rendered images and real photos for evaluating the quality of renderers (Fig.~\ref{fig:dataset_B}).
%Rendered images are generated by DR with manual pose alignments.
%Using on the 3D object dataset described above, we did the following steps to complete the set of pairs:

\begin{enumerate}
	\item We retrieved $\sim$3 real photos for each 3D object (\eg, a car) from the Internet (using descriptive information, \eg, a car's make, model, and year).
	\item For each real photo, we replaced the object with matching background content via Adobe Photoshop's Context-Aware Fill-In feature to obtain a background-only photo $B$ (\ie, no foreground objects).
%	\item We manually adjusted the pose of the 3D object via the 3D editing software Blender such that it closely matches the object pose in the reference photo.
%	Our Blender setup for this task is stored in ``scene.blend'' and python script ``tk.py'', see \url{http://anonymizedForReview});
	\item We rendered the 3D object with the differentiable renderer on the background $B$ obtained in Step 2.
	We then manually aligned the pose of the 3D object such that it closely matched that in the reference photo.
	\item We evaluated pairs of (photo, render) in a side-by-side comparison.
\end{enumerate}

%The 56 pairs of (photo, render) are provided in Fig.~\ref{fig:dataset_B}.
%In total, we produced 56 pairs of (renders, real photos).
%This method minimizes affect of background when comparing the quality of rendered and real images.
While discrepancies can be visually spotted in our side-by-side comparisons, we found that most of the renders passed our human visual Turing test if presented alone.
That is, it is not easy for humans to tell whether a render is a real photo or not (if they are not primed with the reference photos).
We only show pairs rendered by the DR because the NR qualitatively has a slightly higher rendering quality (Fig.~\ref{fig:compare_tessellation}b vs. c).

\subsubsection{Quantitative evaluation} 
\label{sec:quantitative_eval}

%\todo{
%	SEPARATE QUANTITATIVE
%	Google Inception v3 predictions of pairs were also provided to evaluate the best possible realism of rendering.
%	The top-1 prediction match among synthesized and real sets of images is 71.43\%
%	and the percentage of intersection of both over top-5 predictions is 76.06\%.
%}

%We quantitatively evaluated DNN predictions on two renderers: Non-differentiable ModernGL renderer (NR), and Differentiable Renderer by \cite{kato2018neural}.

In addition to the qualitative evaluation, we also quantitatively evaluated the Google Inception-v3 \cite{szegedy2016rethinking}'s top-1 accuracy on renders that use either (a) an empty background or (b) real background images.

\subsubsection*{a. Evaluation of the renders of 30 objects on an empty background} 

Because the experiments in the main text used our self-assembled 30-object dataset (Sec.~\ref{sec:construction}), we describe the process and the results of our quantitative evaluation for only those objects.

%We quantitatively evaluated DNN predictions on the renders by NR (Fig.~\ref{fig:dataset_A}).
We rendered the objects on a white background with RGB values of (1.0, 1.0, 1.0), an ambient light intensity of 0.9, and a directional light intensity of 0.5.
For each object, we sampled 36 unique views (common in ImageNet) evenly divided into three sets.
For each set, we set the object at the origin, the up direction to $(0,1,0)$, and the camera position to $(0,0,-z)$ where $z = \{4, 6, 8\}$.
%Starting the object at a $10^\circ{}$ azimuth, we uniformly sampled 12 views from 0 to $2\pi$ azimuthal angle.
We sampled 12 views per set by starting the object at a $10^\circ{}$ yaw and generating a render at every $30^\circ{}$ yaw-rotation.
%\ma{I'm confused about what this actual process was.}
Across all objects and all renders, the Inception-v3 top-1 accuracy is $83.23\%$ (comparable to $77.45\%$ on ImageNet images \cite{szegedy2016rethinking}) with a mean top-1 confidence score of $0.78$.
The top-1 and top-5 average accuracy and confidence scores are shown in
Table~\ref{tab:avg_accuracy_30obj}.
%Actually, we used this evaluation to select the 30 highest-quality models from around 100 initially-purchased 3D models.

\begin{table}[h]
	\centering
	\begin{tabular}{*{5}{lccc}}
		\toprule
		Distance &  4 & 6 & 8 & Average \\ \midrule
		top-1 mean accuracy & 84.2\% & 84.4\%& 81.1\%& 83.2\%\\
		top-5 mean accuracy & 95.3\% & 98.6\%& 96.7\%& 96.9\% \\
		top-1 mean confidence score & 0.77 & 0.80 & 0.76& 0.78 \\ \bottomrule
	\end{tabular}
	\caption{The top-1 and top-5 average accuracy and confidence scores for Inception-v3~\cite{szegedy2016rethinking} on the renders of the 30 objects in our dataset.}\label{tab:avg_accuracy_30obj}
\end{table}

\subsubsection*{b. Evaluation of the renders of test objects on real backgrounds}

In addition to our qualitative side-by-side (real photo, render) comparisons (Fig.~\ref{fig:dataset_B}), we quantitatively compared Inception-v3's predictions for our renders to those for real photos.
%Because our experiments of the gradient descent with DR (DR-G) were performed on DR, we first evaluated the rendering quality of DR before performing experiments.
%Google Inception v3 predictions of pairs were also provided to evaluate the best possible realism of rendering.
%We ran Google Inception v3 on the 56 pairs produced in the previous section, and compared the similarity in predictions between real photos vs. renders.
We found a high similarity between real photos and renders for DNN predictions.
That is, across all 56 pairs (Sec.~\ref{sec:qualitative_eval}), the top-1 predictions match 71.43\% of the time.
Across all pairs, 76.06\% of the top-5 labels for real photos match those for renders.
%Across all pairs, the intersection of top-5 labels for the real photo vs. the top-5 labels for the render
%Considering the top-5 predictions, the average intersection over 5 labels
%over 
%The percentage of intersection of both over top-5 predictions is 76.06\%.

\begin{figure*}
    \centering
    \includegraphics[width=0.9\columnwidth]{30_objects.jpg}
    \caption{    	
    	We tested Inception-v3's predictions on the renders generated by the differentiable renderer (DR).
    	We show here the top-5 predictions for one random pose per object. 
    	However, in total, we generated 36 poses for each object by (1) varying the object distance to the camera; and (2) rotating the object around the yaw axis.
    	See \url{https://goo.gl/7LG3Cy} for all the renders and DNN top-5 predictions.
    	Across all 30 objects, on average, Inception-v3 correctly recognizes 83.2\% of the renders.
    	See Sec.~\ref{sec:quantitative_eval} for more details.
    }\label{fig:dataset_A}
\end{figure*}

 \begin{figure*}[t]
	\begin{center}
		\includegraphics[width=\linewidth]{random_pairs_kr_vs_real.jpg}
	\end{center}
	\caption{
		12 random pairs of renders (left) and real photos (right) among 56 pairs produced in total for our 3D object rendering evaluation (Sec.~\ref{sec:qualitative_eval}).
		The renders are produced by the differentiable renderer by Kato et al.~\cite{kato2018neural}.
		More images are available at \url{https://goo.gl/8z42zL}.
		While discrepancies can be spotted in our side-by-side comparisons, we found that most of the renders passed our human visual Turing test if presented alone.
%		The non-differentiable renderer (NR) \cite{modernGL} (ModernGL based on the standard OpenGL) produces renders of qualitatively higher quality than that of DR (see Fig.~\ref{fig:compare_tessellation}) therefore we did not show similar comparisons between (NR render, real photos).
	}
	\label{fig:dataset_B}
\end{figure*}




%\clearpage
%%% REMOVE BECAUSE THIS INFO CAN BE IN FIGURE CAPTION
%\section{T-SNE plot of images generated from 3D objects}
%\label{sec:tsne}
%
%\subsec{t-SNE plotting} We used OpenFramework t-SNE application \textbf{ofxTSNE}\footnote{\url{https://github.com/genekogan/ofxTSNE}} to plot data.
%The ofxTSNE uses AlexNet fully connected layer to encoding each image into a 4096 dimension vector.
%The version of AlexNet used in ofxTSNE was implemented in \textbf{libccv}\footnote{\url{http://libccv.org/doc/doc-convnet/}} and trained by the author himself.
%We set perplexity = 30 and theta = 0.2 to run t-SNE (Fig.\ref{fig:tsne_img_correct}-Fig.\ref{fig:tsne_color_ax}).
%
%\subsec{Plots of correct classified images} Fig.~\ref{fig:tsne_img_correct} is the t-SNE grid of 900 images generated from 30 objects in Fig.~\ref{fig:dataset_A} by the non-differentiable renderer (NR).
%Each image is classified correctly by inception v3 with confidence $>$90\%.
%Different objects in Fig.~\ref{fig:tsne_img_correct} were labeled by different solid colors in Fig.~\ref{fig:tsne_color_correct}.
%The same color means the images in those positions are generated from the same 3D objects.
%
%\subsec{Plots of misclassified images} 
%Fig.~\ref{fig:tsne_img_ax} is the t-SNE grid of 900 images generated from the same 30 objects in Fig.~\ref{fig:dataset_A} by the NR but uniquely misclassified by Inception v3 with confidence $>$90\%.
%Different objects in Fig.~\ref{fig:tsne_img_ax} were labeled by different solid colors in Fig.~\ref{fig:tsne_color_ax}, which is similar to Fig.~\ref{fig:tsne_color_correct} as mentioned above.

\begin{figure*}
	\centering
	\includegraphics[width=1.0\columnwidth]{tsne_grid_correct.jpg}
	\caption{ 
		For each object, we collected 30 high-confidence ($p \geq 0.9$) correctly classified images by Inception-v3.
		The images were generated via the random search procedure.
		We show here a grid t-SNE of AlexNet \cite{Krizhevsky2012} \layer{fc7} features for all $30$ objects $\times$ $30$ images = $900$ images.
		Correctly classified images for each object tend to be similar and clustered together.
		The original, high-resolution figure is available at \url{https://goo.gl/TGgPgB}.
		%
		\\To better visualize the clusters, we plotted the same t-SNE but used unique colors to denote the different 3D objects in the renders  (Fig.~\ref{fig:tsne_color_correct}).
		Compare and contrast this plot with the t-SNE of only misclassified poses 
		(Figs.~\ref{fig:tsne_img_ax} \&~\ref{fig:tsne_color_ax}).
	}\label{fig:tsne_img_correct}
\end{figure*}

\begin{figure*}
	\centering
	\includegraphics[width=1.0\columnwidth]{tsne_grid_label_correct.png}
	\caption{
		The same t-SNE found in Fig.~\ref{fig:tsne_img_correct} but using a unique color to denote the 3D object found in each rendered image. Here, each color also corresponds to a unique Inception-v3 label.
		Compare and contrast this plot with the t-SNE of only misclassified poses 
		(Fig.~\ref{fig:tsne_color_ax}).
%		3D object labels of grid t-SNE Fig.~\ref{fig:tsne_img_correct}.
%		Images are correctly classified by inception v3. 
%		Squares of the same color represent the poses generated by the same 3D object.
%		The same color means the image in those positions is generated from the same 3D objects. 
		The original, high-resolution figure is available at \url{https://goo.gl/TGgPgB}.
	}\label{fig:tsne_color_correct}

\end{figure*}

\begin{figure*}
	\centering
	\includegraphics[width=1.0\columnwidth]{tsne_grid_incorrect.jpg}
	\caption{
		Following the same process as described in Fig.~\ref{fig:tsne_img_correct}, we show here a grid t-SNE of generated adversarial poses.
		For each object, we assembled 30 high-confidence ($p \geq 0.9$) adversarial examples generated via a random search against Inception-v3 \cite{szegedy2016rethinking}.
		The t-SNE was generated from the AlexNet \cite{Krizhevsky2012} \layer{fc7} features for $30$ objects $\times$ $30$ images = $900$ images.
		The original, high-resolution figure is available at \url{https://goo.gl/TGgPgB}.
		Adversarial poses were found to be both common across different objects (\eg, the top-right corner) and unique to specific objects (\eg, the \class{traffic sign} and \class{umbrella} objects in the middle left).
		%
		\\To better understand how similar misclassified poses can be found across many objects, see Fig.~\ref{fig:tsne_color_ax}.
		Compare and contrast this plot with the t-SNE of correctly classified poses 
		(Figs.~\ref{fig:tsne_img_correct} \&~\ref{fig:tsne_color_correct}).
		%
%		The t-SNE grid of 900 images generated from 30 objects via random search.
%		Each image is misclassified by Inception-v3 (confidence $\geq$0.9). 
%		
%		The t-SNE grid of 900 images generated from 30 objects by NR.\@
%		Each image is uniquely misclassified by inception v3 (confidence $>$90\%). 
%		The original image is available at \url{https://goo.gl/TGgPgB}. 
%		Fig.~\ref{fig:tsne_color_correct} shows the same t-SNE embedding; however, instead of displaying the image, we show an unique color denote the 3D object used to render that image.
%		Also compare the analogous figures of high-confidence correctly labeled examples
%		(see Fig.~\ref{fig:tsne_img_correct} and Fig.~\ref{fig:tsne_color_correct}).
	}\label{fig:tsne_img_ax}
\end{figure*}

\begin{figure*}
	\centering
	\includegraphics[width=1.0\columnwidth]{tsne_grid_label_incorrect.png}
	\caption{
		The same t-SNE as that in Fig.~\ref{fig:tsne_img_ax} but using a unique color to denote the 3D object used to render the adversarial image (\ie, Inception-v3's misclassification labels are not shown here). 
		The original, high-resolution figure is available at \url{https://goo.gl/TGgPgB}.\\
		Compare and contrast this plot with the t-SNE of correctly classified poses 
		(Fig.~\ref{fig:tsne_color_correct}).
		%
%		3D object labels of t-SNE image grid Fig.~\ref{fig:tsne_img_ax}. 
%		Images are uniquely misclassified by inception v3.
%		The same color means the image in those positions is generated from the same 3D objects.
%		
%		Also compare the analogous figures of high confidence correctly labeled examples
%		(see Fig.~\ref{fig:tsne_img_correct} and Fig.~\ref{fig:tsne_color_correct}).
	}\label{fig:tsne_color_ax}
\end{figure*}

\section{Transferability from the Inception-v3 classifier to the YOLO-v3 detector}
\label{sec:yolo}

Previous research has shown that object detectors can be more robust to adversarial attacks than image classifiers~\cite{lu2017-standard}.
Here, we investigate how well our AXs generated for an Inception-v3 classifier trained to perform 1,000-way image classification on ImageNet~\cite{russakovsky2015imagenet} transfer to YOLO-v3, a state-of-the-art object detector trained on MS COCO~\cite{lin2014microsoft}.

%The COCO (``Common Objects in Context'') dataset consists of 328,000 images with bounding boxes corresponding to 80 different object classes.
%To test the transferability of our AXs to YOLOv3, we only considered the 13 objects that had classes present in both the ImageNet and MS COCO datasets.

%YOLO-v3 was trained on MS COCO, a dataset of bounding boxes corresponding to 80 different object classes.

Note that while ImageNet has 1,000 classes, MS COCO has bounding boxes classified into only 80 classes. 
Therefore, among 30 objects, we only selected the 13 objects that (1) belong to classes found in both the ImageNet and MS COCO datasets; and (2) are also well recognized by the YOLO-v3 detector in common poses.


\subsection{Class mappings from ImageNet to MS COCO}
See Table~\ref{tab:yolo_transfer_stats}a for 13 mappings from ImageNet labels to MS COCO labels.

\subsection{Selecting 13 objects for the transferability test}
For the transferability test (Sec.~\ref{sec:transferability}), we identified the 13 objects (out of 30) that are well detected by the YOLO-v3 detector via the two tests described below.

\subsubsection{YOLO-v3 correctly classifies 93.80\% of poses generated via yaw-rotation}
We rendered 36 unique views for each object by generating a render at every $30^\circ{}$ yaw-rotation (see Sec.~\ref{sec:quantitative_eval}).
Note that, across all objects, these yaw-rotation views have an average accuracy of $83.2\%$ by the Inception-v3 classifier.
We tested them against YOLO-v3 to see whether the detector was able to correctly find one single object per image and label it correctly.
Among 30 objects, we removed those that YOLO-v3 had an accuracy $\leq 70\%$, leaving 13 for the transferability test.
Across the remaining 13 objects, YOLO-v3 has an accuracy of 93.80\% on average (with an NMS threshold of $0.4$ and a confidence threshold of $0.5$).
Note that the accuracy was computed as the total number of correct labels over the total number of bounding boxes detected (\ie, we did not measure bounding-box IoU errors).
See class-specific statistics in Table~\ref{tab:yolo_transfer_stats}.
This result shows that YOLO-v3 is substantially more accurate than Inception-v3 on the standard object poses generated by yaw-rotation (93.80\% vs. 83.2\%).

\subsubsection{YOLO-v3 correctly classifies 81.03\% of poses correctly classified by Inception-v3}

Additionally, as a sanity check, we tested whether poses \emph{correctly classified} by Inception-v3 transfer well to YOLO-v3.
For each object, we randomly selected 30 poses that were $100\%$ correctly classified by Inception-v3 with high confidence ($p \geq 0.9$).
The images were generated via the random search procedure in the main text experiment (Sec.~\ref{sec:random_search}).
Across the final 13 objects, YOLO-v3 was able to correctly detect one single object per image and label it correctly at a 81.03\% accuracy (see Table~\ref{tab:yolo_transfer_stats}c).


\subsection{Transferability test: YOLO-v3 fails on 75.5\% of adversarial poses misclassified by Inception-v3}
\label{sec:transferability}

For each object, we collected 1,350 random adversarial poses (\ie, incorrectly classified by Inception-v3) generated via the random search procedure (Sec.~\ref{sec:random_search}).
Across all 13 objects and all adversarial poses, YOLO-v3 obtained an accuracy of only $24.50\%$ (compared to $81.03\%$ when tested on images correctly classified by Inception-v3).
In other words, 75.5\% of adversarial poses generated for Inception-v3 also escaped the detection\footnote{We were not able to check how many misclassification labels by YOLO-v3 were the same as those by Inception-v3 because only a small set of 80 the MS COCO classes overlap with the 1,000 ImageNet classes.} of YOLO-v3 (see Table~\ref{tab:yolo_transfer_stats}d for class-specific statistics).
Our result shows adversarial poses transfer well across tasks (image classification $\to$ object detection), models (Inception-v3 $\to$ YOLO-v3), and datasets (ImageNet $\to$ MS COCO).








%We found that 75.5\% of adversarial poses generated for Inception-v3 are also misclassified by YOLOv3 (see Sec.~\ref{sec:yolo} for more details).
%These results suggest the adversarial pose problem transfers across datasets, models, and tasks.

%\subsec{ImageNet and MS COCO datasets}

%\an{Need to summarize the Table~\ref{tab:yolo_transfer_stats}here}

%The Google Inceptions v3 neural network was trained on ImageNet datasets while the YOLOv3 was trained on MS COCO and the bounding box data for YOLOv3 training is unavailable on ImageNet.
%\anremoved{
%To study the transferability between two neural networks trained on different datasets, we choose those objects with the same or similar labels on both datasets (Table~\ref{tab:yolo_transfer_stats} ``Label'' column).
%We use the 30 3D objects (Fig.~\ref{fig:dataset_A}) as our candidates and then pick the 13 3D objects with the highest accuracy when testing rendered images by YOLOv3 (with NMS threshold 0.4 and confidence threshold 0.5) on both the non-differentiable render (NR) and the differentiable renderer (DR).
%%
%\subsec{Correctly classified image examples from the differentiable renderer (DR)}
%For this set of images, each object rendered 36 images by the DR~\cite{kato2018neural}. 
%The images examples were generated in the same way of evaluation of Datasets A as mentioned above in Sec.~\ref{sec:evaluation}.
%The difference is that we only picked 13 of them due to inconsistency of MS COCO and ImageNet.
%%
%Those generated images are mostly correctly classified by Inception v3 (top 1 accuracy 76.92\%).
%The images of the same object have average accuracy 93.80\% when tested on YOLO v3, which shows the good transferability of correct classification examples. 
%Statics for each objects see Table~\ref{tab:yolo_transfer_stats} ``DR'' column.
%%
%\subsec{Correctly classified image examples from the non-differentiable renderer (NR) }
%For this set of images, each object rendered 30 images with a random posed camera by the NR.
%All those images correctly classified by Inception v3 with top-1 accuracy 100\% and confidence $>$90\% were collected.
%When tested by YOLO v3, the average accuracy is 81.03\%.
%Statics for each objects see Table~\ref{tab:yolo_transfer_stats} ``NR'' column.
%%
%\subsec{Misclassified image examples from the non-differentiable renderer (NR)}
%We sampled with random camera poses and collected 1350 adversarial posed images for each 3D object by the NR.
%Although the same objects can generate normal posed images with high accuracy on both DR and NR,
%the average accuracy now dramatically drops to 24.50\%, which is 56.53\% reduced by average in this set.
%Statics for each objects see Table~\ref{tab:yolo_transfer_stats} ``NR AX'' column.
%}

\begin{table*}
	\centering
	\begin{tabular}{rllrrrrrrr}
		\toprule
		& \multicolumn{2}{c}{(a) Label mapping} & \multicolumn{2}{c}{(b) Accuracy on} & \multicolumn{2}{c}{(c) Accuracy on} & \multicolumn{3}{c}{(d) Accuracy on} \\
		& \multicolumn{2}{c}{} & \multicolumn{2}{c}{yaw-rotation poses} & \multicolumn{2}{c}{random poses} & \multicolumn{3}{c}{adversarial poses} \\
		\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-10}
		& ImageNet & MS COCO & \#/36 & acc (\%) & \#/30 & acc (\%) & \#/1350 & acc (\%) & $\Delta$acc (\%) \\
		\midrule
		1 & park bench & bench & 31 & 86.11 & 22 & 73.33 & 211 & 15.63 & 57.70 \\
		2 & bald eagle & bird & 34 & 94.11 & 24 & 80.00 & 597 & 44.22 & 35.78 \\
		3 & school bus & bus & 36 & 100.00 & 18 & 60.00 & 4 & 0.30 & 69.70 \\
		4 & beach wagon & car & 34 & 94.44 & 30 & 100.00 & 232 & 17.19 & 82.81 \\
		5 & tiger cat & cat & 26 & 72.22 & 25 & 83.33 & 181 & 13.41 & 69.93 \\
		6 & German shepherd & dog & 32 & 88.89 & 28 & 93.33 & 406 & 30.07 & 63.26 \\
		7 & motor scooter & motorcycle & 36 & 100.00 & 18 & 60.00 & 384 & 28.44 & 31.56 \\
		8 & jean & person & 36 & 100.00 & 29 & 96.67 & 943 & 69.85 & 26.81 \\
		9 & street sign & stop sign & 31 & 86.11 & 26 & 86.67 & 338 & 25.04 & 61.15 \\
		10 & moving van & truck & 36 & 100.00 & 24 & 80.00 & 15 & 1.11 & 78.89 \\
		11 & umbrella & umbrella & 35 & 97.22 & 25 & 83.33 & 907 & 67.19 & 16.15 \\
		12 & police van & car & 36 & 100.00 & 25 & 83.33 & 55 & 4.07 & 79.26 \\
		13 & trailer truck & truck & 36 & 100.00 & 22 & 73.33 & 26 & 1.93 & 71.41 \\
		\midrule
		& &Average & & 93.80 & & 81.03 & &24.50 & 56.53 \\ \bottomrule
	\end{tabular}
	\caption{
		Adversarial poses generated for a state-of-the-art ImageNet image classifier (here, Inception-v3) transfer well to an MS COCO detector (here, YOLO-v3).
		The table shows the YOLO-v3 detector's accuracy on: (b) object poses generated by a standard process of yaw-rotating the object; (c) random poses that are $100\%$ correctly classified by Inception-v3 with high confidence ($p \geq 0.9$); and (d) adversarial poses, \ie, $100\%$ misclassified by Inception-v3.\\\\
		(a) The mappings of 13 ImageNet classes onto 12 MS COCO classes.\\
		(b) The accuracy (``acc (\%)'') of the YOLO-v3 detector on 36 yaw-rotation poses per object.\\
		(c) The accuracy of YOLO-v3 on 30 random poses per object that were correctly classified by Inception-v3.\\
		(d) The accuracy of YOLO-v3 on 1,350 adversarial poses (``acc (\%)'') and the differences between c and d (``$\Delta$acc (\%)'').
		%
%		\anremoved{
%		The transferability of 13 3D objects classification results from Google Inception v3 to YOLO v3.
%		(1) \textbf{Label}:  Labels in ImageNet and MS COCO datasets defined as correct for each 3D object. Note that the Inception v3 model was trained and tested on ImageNet labels and the YOLO v3 on MS COCO.
%		(2) \textbf{DR}: YOLO v3 classification results of images which previously were correctly classified on Inception v3 (top-1 accuracy 76.92\%). Images were generated from the DR.
%		(3) \textbf{NR}: YOLO v3 classification results of images which previously were correctly classified on Inception v3 (top-1 accuracy 100.00\% confidence $>$90\%). Images were generated from the NR via random sampling.
%		(4) \textbf{NR AX}: YOLO v3 classification results of images which previously were uniquely misclassified on Inception v3. Images were generated by the NR via random sampling.
%		(5) \textbf{\#/36}, \textbf{\#/30}, and \textbf{\#/1350}: Numbers of images correctly classified by YOLO v3 over total 36, 30 and 1350 images for each 3D object.
%		(6) \textbf{acc\%}: Accuracy defined as the percentage of correctly classified images by YOLO v3.
%		(7) \textbf{$\Delta$acc\%}: Accuracy drop when comparing the misclassified ones by YOLO v3 (``NR'' column  \textbf{acc\%} ) with the correctly classified ones by YOLO v3 (``NR AX'' column \textbf{acc\%}).
%		}
	}
	\label{tab:yolo_transfer_stats}
\end{table*}

\clearpage
%%%%% chengfei part end

% \section{Original Random Sampling by MA}

% %\subsection{Random Sampling}
% \subsec{Random search}
% \label{sec:sampling}
% To assess Inception-v3's vulnerability to adversarial poses, we generated predictions for renders of objects that had been randomly rotated and translated in the camera view.
% The 3D scene was initialized with the object centered at the origin and scaled such that it was bound in a unit cube.
% For each random sample, the object was rotated by applying the linear transformation in equation~\eqref{eq:rot} to each vertex $v_{i} \in \R^3$.

% \begin{equation} \label{eq:rot}
% v_{i}^{R} = R_{y}R_{p}R_{r}v_{i}
% \end{equation}

% \noindent
% where $v_{i}$ is the vector $(x_{i}, y_{i}, z_{i})$ containing the \textit{initial} world coordinates of the vertex indexed by $i$.
% The rotation matrices, $R_{y}$, $R_{p}$, and $R_{r}$, corresponds to the object's yaw, pitch, and roll, respectively\footnote{See Section~\ref{sec:trans-mat} for the derivation}. The rotation angles are uniformly sampled from $(0, 2\pi)$.

% The object was then translated by adding a vector $t$ to each rotated vertex $v_{i}^{R}$.

% \begin{equation}
% v_{i}^{R,t} = t + v_{i}^{R}
% \end{equation}

% \noindent
% where $t$ is the vector $(x_{\delta}, y_{\delta}, z_{\delta})$. $z_{\delta}$ was drawn from $\mathcal{U}(-28, 0)$ and $x_{\delta}$ and $y_{\delta}$ were drawn from $\mathcal{U}(-s, s)$ where $s$ was the maximum possible translation in the $x$ and $y$ directions that would keep the center of the object within the frame of the image.
% Specifically, $s$ was defined as:

% \begin{equation}
% s = d \cdot\tan(\theta_{v})
% \end{equation}

% \noindent
% where $\theta_{v}$ is one half the camera's angle of view (\ie, $8.213\degree$ in our experiments) and $d$ is the absolute value of the difference between the camera's $z$-coordinate and $z_{\delta}$.


% Following the vertex transformations, the scene (consisting of the vertices, texture map, and lighting configuration) was rendered into an image, which was then fed into the neural network to generate class probabilities.
% One million samples were generated per class.

\section{Experimental setup for the differentiable renderer}\label{sec:dr-expr}

For the gradient descent method (DR-G) that uses the approximate gradients provided by the differentiable renderer \cite{kato2018neural} (DR), we set up the rendering parameters in the DR to closely match those in the NR.
However, there were still subtle discrepancies between the DR and the NR that made the results (DR-G vs. FD-G in Sec.~\ref{sec:comparing_methods}) not directly comparable.
Despite these discrepancies (described below), we still believe the FD gradients are more stable and informative than the DR gradients (\ie, FD-G outperformed DR-G).\footnote{In preliminary experiments with only the DR (not the NR), we also empirically found FD-G to be more stable and effective than DR-G (data not shown).}\\

\subsec{DR setup} 
For all experiments with the DR, the camera was centered at \((0, 0, 16)\) with an up direction \((0, 1, 0)\). 
The object's spatial location was constrained such that the object center was always within the frame.
The depth values were constrained to be within \([-14, 14]\).
Similar to experiments with the NR, we used the \medium lighting setting.
%The DR supports two types of lighting, i.e.,
%ambient light and directional light. 
The ambient light color was set to white with an intensity 1.0, while the directional light was set to white with an intensity 0.4.  
Fig.~\ref{fig:dr-demo} shows an example school bus rendered under this \medium lighting at different distances.

\begin{figure}[ht]
  \centering
  \subcaptionbox{School bus at \((0, 0, -14)\)}{{\includegraphics[width=.32\textwidth]{dr_demo_bus2.png}}}
  \subcaptionbox{School bus at \((0, 0, 0)\)}{{\includegraphics[width=.32\textwidth]{dr_demo_bus0.png}}}
  \subcaptionbox{School bus at \((0, 0, 14)\)}{{\includegraphics[width=.32\textwidth]{dr_demo_bus1.png}}}
  \caption{School bus rendered by the DR at different distances.}\label{fig:dr-demo}
\end{figure}


The known discrepancies between the experimental setups of FD-G (with the NR) vs. DR-G (with the DR) are:

\begin{enumerate}
	\item The exact \medium lighting parameters for the NR described in the main text (Sec.~\ref{sec:easily_confused}) did not produce similar lighting effects in the DR.
	Therefore, the DR lighting parameters described above were the result of manually tuning to qualitatively match the effect produced by the NR \medium lighting parameters.
	\item While the NR uses a built-in tessellation procedure that automatically tessellates input objects before rendering, we had to perform an extra pre-processing step of manually tessellating each object for the DR.
	While small, a discrepancy still exists between the two rendering results (Fig.~\ref{fig:compare_tessellation}b vs. c).
\end{enumerate}

%\an{Editing here}
%Note that the experiment results of using differentiable renderer and
%non-differential renderer may not directly comparable, mainly due to some
%limitations of the differentiable renderer.

%\begin{enumerate}
%\item Different objects input.  As shown in Table~\ref{tab:num_triangles}, the
%  differentiable renderer takes in further tessellated objects due to its
%  implementation, while the non-differentiable renderer takes in the original
%  object file.  The reason the differentiable renderer does not handle textures
%  when the triangle faces are too large.
% \an{Removed the below because we never compared the efficiency. That is, both DR-G, ZRS, and FD-G are given the same budget of 100 steps.}   
%\item Optimization steps.  For differentiable renderer, we stop the optimization
%  once we find an adversarial, while for the non-differentiable render, we let
%  the optimization run for full 100 step.  This is partially due to the
%  computation cost of differentiable renderer.

% an{Removed because we are not optimizing light, instead we only used a fixed light.}
%\item Lighting conditions.  The differentiable renderer has a weird lighting
%  implementation where it allows the light intensity to go over 1.  We fixed
%  this issue, but it still renders different images when we have different light
%  intensities over 1.
%\end{enumerate}

%\section{Notes on optimization speed between using differentiable renderer and non-differentiable renderer}
%\label{sec:finite_diff}
%
%\subsec{Finite differences}
%Although several differentiable renderers exist that allow backpropagation to a 3D scene from an image, we found a simple finite-difference approach to be more effective for this task.
%The spherical harmonics approach used by~\cite{Liu2018} precludes the types of joint vertex transformations we focus on here.
%While the approximate gradients defined in~\cite{kato2018neural} were adequate for texture and silhouette optimization, we found them to be somewhat erratic in our experiments (see Section~\ref{sec:kr}).
%Further, rendering images with the neural mesh renderer was several orders of magnitude slower than using ModernGL;\@ partly because the 3D models had to first be highly tessellated to be rendered with adequate quality when using the neural mesh renderer, but primarily because OpenGL has been highly optimized over its nearly 30 years of existence.
%So, even with the added overhead of computing two losses per parameter for the finite-difference method, each iteration was still much faster than an iteration using the neural mesh renderer.
%Our experiments on Nvidia DGX-1 shows that actual rendering time is around 1 ms.
%%\todo{An actual rendering speed of ModernGL: e.g. 0.001s?}

\section{Gradient descent with the DR gradients}\label{sec:kr}

In preliminary experiments (data not shown), we found the DR gradients to be relatively noisy when using gradient descent to find targeted adversarial poses (\ie, DR-G experiments).
To mitigate this problem, we experimented with (1) parameter augmentation (Sec.~\ref{sec:param-augm}); and (2) multi-view optimization (Sec.~\ref{sec:multi-camera-optim}).
In short, we found parameter augmentation helped and used it in DR-G.
However, when using the DR, we did not find multiple cameras improved optimization performance and thus only performed regular single-view optimization for DR-G.

\subsection{Parameter augmentation}%
\label{sec:param-augm}

We performed gradient descent using the DR gradients (DR-G) in an augmented parameter space corresponding to 50 rotations and one translation to be applied to the original object vertices.
That is, we backpropagated the DR gradients into the parameters of these pre-defined transformation matrices.
Note that DR-G is given the same budget of $100$ steps per optimization run as FD-G and ZRS for comparison in Sec.~\ref{sec:comparing_methods}.

The final transformation matrix is constructed by a series of rotations followed
by one translation, i.e., 

\begin{align*}
M = T\cdot R_{n-1}R_{n-2}\cdots R_0
\end{align*}

\noindent where \(M\) is the final transformation matrix, \(R_i\) the rotation matrices, and \(T\) the translation matrix.  

We empirically found that increasing the number of rotations per step helped (a) improve the success rate of hitting the target labels; (b) increase the maximum confidence score of the found AXs; and (c) reduce the number of steps, \ie, led to faster convergence (see Fig.~\ref{fig:param-augm}).
Therefore, we empirically chose $n=50$ for all DR-G experiments reported in the main text.

%Fig.~\ref{fig:param-augm} shows the effect of increasing the number of parameters.

\begin{figure}[h]
  \centering
  \subcaptionbox{$y$-axis: success rate}{\includegraphics[width=0.25\linewidth]{n_rots_success.pdf}}%
  \subcaptionbox{$y$-axis: max confidence}{\includegraphics[width=0.25\linewidth]{n_rots_max_prob.pdf}}%
  \subcaptionbox{$y$-axis: mean number of steps}{\includegraphics[width=0.25\linewidth]{n_rots_expense.pdf}}
	\caption{
  	We found that increasing the number of rotations (displayed in $x$-axes) per step helped:\\
  	(a) improve the success rate of hitting the target labels;\\
  	(b) increase the maximum confidence score of the found adversarial examples;\\
  	(c) reduce the average number of steps required to find an AX, \ie, led to faster convergence.\\
%  	The benefits of using more parameters when conducting targeted
%    attacks with the DR.\ 
%    The X-axes are number of rotations, 
%    $y$-axes shows
%    success rates (a), probabilities (b), step counts (c).
}
\label{fig:param-augm}
\end{figure}

\subsection{Multi-view optimization}%
\label{sec:multi-camera-optim}

Additionally, we attempted to harness multiple views (from multiple cameras) to increase the chance of finding a target adversarial pose.
Multi-view optimization did not outperform single-view optimization using the DR in our experiments.
Therefore, we only performed regular single-view optimization for DR-G.
We briefly document our negative results below.

%Instead of having one single camera in the 3D scene and 
Instead of backpropagating the DR gradient to a single camera looking at the object in the 3D scene, one may set up multiple cameras, each looking at the object from a different angle.
This strategy intuitively allows gradients to still be backpropagated into the vertices that may be occluded in one view but visible in some other view.
We experimented with six cameras and backpropagating to all cameras in each step.
However, we only updated the object following the gradient from the view that yielded the lowest loss among all views.
One hypothesis is that having multiple cameras might improve the chance of hitting the target.

In our experiments with the DR using 100 steps per optimization run, multi-view optimization performed worse than single-view in terms of both the success rate and the number of steps to converge.
We did not compare all 30 objects due to the expensive computational cost, and only report the results from optimizing two objects \class{bald eagle} and \class{tiger cat} in Table~\ref{tab:v1v6}.
Intuitively, multi-view optimization might outperform single-view optimization given a large enough number of steps.
%Table~\ref{tab:v1v6} gives an example comparison between the two setup.

\begin{table}[ht]
  \centering
  \begin{tabular}{l*{4}{r}}
    \toprule
    & \multicolumn{2}{c}{
    	\class{bald eagle}} & \multicolumn{2}{c}{\class{tiger cat}} \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}
    & Steps & Success rate & Steps & Success rate \\
    \midrule
    Single-view  & 71.80 & 0.44 & 90.70 & 0.15 \\
    Multi-view & 81.28 & 0.23 & 96.84 & 0.04 \\
    \bottomrule
  \end{tabular}
  \caption{
  	Multi-view optimization performed worse than single-view optimization in both (a) the number of steps to converge and (b) success rates.
  	We show here the results of two runs of optimizing with the \class{bald eagle} and \class{tiger cat} objects.
  	The results are averaged over $50$ target labels $\times 50$ trials = $2,500$ trials.
  	Each optimization trial for both single- and multi-view settings is given the budget of $100$ steps.
%  	Muti-/Single- camera setup
  	\label{tab:v1v6}}
\end{table}

\newpage
\section{3D Transformation Matrix}\label{sec:trans-mat}

A rotation of \(\theta\) around an arbitrary axis \((x, y, z)\) is given by the
following homogeneous transformation matrix.

\begin{equation}
  \label{eq:rotation-matrix}
  R =
  \begin{vmatrix*}[l]
    x x(1 - c) + c & x y(1 - c) - z s & x z(1 - c) + y s & 0\\
    x y(1 - c) + z s & y y(1 - c) + c & y z(1 - c) - x s & 0\\
    x z(1 - c) - y s & y z(1 - c) + x s & y z(1 - c) + c & 0\\
    0 & 0 & 0 & 1
  \end{vmatrix*}
\end{equation}

\noindent
where \(s = \sin\theta\), \(c = \cos\theta\), and the axis is normalized, \ie, \(x^2 + y^2 + z^2 = 1\).
Translation by a vector \((x, y, z)\) is given by the following homogeneous
transformation matrix.

\begin{equation}
  \label{eq:translation-matrix}
  T =
  \begin{vmatrix*}[l]
    1 & 0 & 0 & x\\
    0 & 1 & 0 & y\\
    0 & 0 & 1 & z\\
    0 & 0 & 0 & 1
  \end{vmatrix*}
\end{equation}

Note that in the optimization experiments with random search (RS) and finite-difference gradients (FD-G), we dropped the homogeneous component for simplicity, \ie, the rotation matrices of yaw, pitch, and roll are all $3 \times 3$.  
The homogeneous component is only necessary for translation, which can be achieved via simple vector addition. 
However, in DR-G, we used the homogeneous component because we had some experiments interweaving translation and rotation.  
The matrix representation was more convenient for the DR-G experiments.  
As they are mathematically equivalent, this arbitrary implementation choice should not alter our results.

\newpage

\begin{table*}[h]
	\begin{center}
		\begin{tabular}{lr}
			\toprule
			Object          & Accuracy (\%) \\
			\midrule
			ambulance       & 3.64       \\
			backpack        & 8.63       \\
			bald eagle      & 13.26      \\
			beach wagon     & 0.60       \\
			cab             & 2.64       \\
			cell phone      & 14.97      \\
			fire engine     & 4.31       \\
			forklift        & 5.20       \\
			garbage truck   & 4.88       \\
			German shepherd & 9.61       \\
			\bottomrule
		\end{tabular}
		\begin{tabular}{lr}
			\toprule
			Object        & Accuracy (\%) \\
			\midrule
			golfcart      & 2.14       \\
			jean          & 2.71       \\
			jeep          & 0.29       \\
			minibus       & 0.83       \\
			minivan       & 0.66       \\
			motor scooter & 20.49      \\
			moving van    & 0.45       \\
			park bench    & 5.72       \\
			parking meter & 1.27       \\
			pickup        & 0.86       \\
			\bottomrule
		\end{tabular}
		\begin{tabular}{lr}
			\toprule
			Object               & Accuracy (\%) \\
			\midrule
			police van           & 0.95       \\
			recreational vehicle & 2.05       \\
			school bus           & 3.48       \\
			sports car           & 2.50       \\
			street sign          & 26.32      \\
			tiger cat            & 7.36       \\
			tow truck            & 0.87       \\
			traffic light        & 14.95      \\
			trailer truck        & 1.27       \\
			umbrella             & 49.88      \\
			\bottomrule
		\end{tabular}
	\end{center}
	\caption{The percent of three million random samples
		that were correctly
		classified by Inception-v3 \cite{szegedy2016rethinking} for each object.
		That is, for each lighting setting in $\{ \bright, \medium, \dark\}$, we generated $10^6$ samples.
		See Sec.~\ref{sec:random_search}
		for details on the sampling procedure.
		% \an{I wonder if we could also add a plot the stats in \url{https://docs.google.com/spreadsheets/d/1NKCWxDcz2Of6JLsgG7JZkIMy54KVlGd5mPBQRPuCloo} (email with Long).}
	}
	\label{tab:sampling_stats}
\end{table*}

% \an{Qi: Report
%     \begin{itemize}
%         \item where do models come from
%         \item number of faces
%         \item how you generate the views: distances and degrees
%         \item how you get the accuracy and average confidence scores
%     \end{itemize}
% }


\begin{figure}[ht]
	\centering
	\subcaptionbox{\bright}{{\includegraphics[width=.32\textwidth]{bright_final.jpg}}}
	\subcaptionbox{\medium}{{\includegraphics[width=.32\textwidth]{medium_final.jpg}}}
	\subcaptionbox{\dark}{{\includegraphics[width=.32\textwidth]{dark_final.jpg}}}
	\caption{Renders of the \class{school bus} object using the NR \cite{modernGL} at three different lighting settings. 
	The directional light intensities and ambient light intensities were $(1.2, 1.6)$, $(0.4, 1.0)$, and $(0.2, 0.5)$ for the \bright, \medium, and \dark settings, respectively.}\label{fig:light_intensity}
\end{figure}

\section{Adversarial poses were not found in ImageNet classes via a nearest-neighbor search}
\label{sec:nearest_neighbors}

We performed a nearest-neighbor search to check whether adversarial poses generated (in Sec.~\ref{sec:easily_confused}) can be found in the ImageNet dataset.

~\\\subsec{Retrieving nearest neighbors from a single class corresponding to the 3D object} 
We retrieved the five nearest training-set images for each adversarial pose (taken from a random selection of adversarial poses) using the \layer{fc7} feature space from a pre-trained AlexNet \cite{Krizhevsky2012}.
The Euclidean distance was used to measure the distance between two \layer{fc7} feature vectors.
We did not find qualitatively similar images despite comparing all $\sim$1,300 class images corresponding to the 3D object used to generate the adversarial poses (\eg, \class{cellphone}, \class{school bus}, and \class{garbage truck} in Figs.~\ref{fig:nearest_cellphone},~\ref{fig:nearest_schoolbus}, and~\ref{fig:nearest_garbagetruck}).
This result supports the hypothesis that the generated adversarial poses are out-of-distribution.

~\\\subsec{Searching from the validation set}
We also searched the entire 50,000-image validation set of ImageNet.
Interestingly, we found the top-5 nearest images were sometimes from the same class as the \emph{targeted} misclassification label (see Fig.~\ref{fig:nearest_val_images}).

%
%
%To verify the OoD examples are out of the distribution of the images in ImageNet, we retrieved 5 most similar images in the training set of ImageNet. We used the euclidean distance between the image $fc7$ features extracted from a pre-trained AlexNet in equation \ref{eq:euclidean_distance} to measure the similarity of the images. We randomly picked 5 OoD examples with high confidence score of misclassified labels to compute the distance between each example and 1300 images in training set to get the nearest images. The results are shown in Fig.~\ref{fig:nearest_cellphone}, Fig.~\ref{fig:nearest_schoolbus}, Fig.~\ref{fig:nearest_garbagetruck}

%\begin{equation} \label{eq:euclidean_distance}
%d( \vec{x}, \vec{t} ) = \sqrt{\sum_{i=1}^{n} (x_i-t_i)^2}
%\end{equation}

%where $x$ is the $fc7$ feature vector of OoD example and $t$ is the  $fc7$ feature vector of image in training set of ImageNet, n is the dimension of the $fc7$ feature.

%%%% nearest neighbor images of 3 objects %%%%
\begin{figure*}
    \centering
    \includegraphics[width=0.9\columnwidth]{cellular_telephone_neighbors.jpg}
    \caption{
    For each adversarial example (leftmost), we retrieved the five nearest neighbors (five rightmost photos) from all $\sim$1,300 images in the \class{cellular phone} class.
    The Euclidean distance between a pair of images was computed in the \layer{fc7} feature space of a pre-trained AlexNet \cite{Krizhevsky2012}.
%    The ImageNet nearest neighbors to OoD poses of the \class{cellular telephone} object. 
   	The nearest photos from the class are mostly different from the adversarial poses.
    This result supports the hypothesis that the generated adversarial poses are out-of-distribution.
    The original, high-resolution figure is available at \url{https://goo.gl/X31VXh}.}
    \label{fig:nearest_cellphone}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.9\columnwidth]{school_bus_neighbors.jpg}
    \caption{
	For each adversarial example (leftmost), we retrieved the five nearest neighbors (five rightmost photos) from all $\sim$1,300 images in the \class{school bus} class.
	The Euclidean distance between a pair of images was computed in the \layer{fc7} feature space of a pre-trained AlexNet \cite{Krizhevsky2012}.
	The nearest photos from the class are mostly different from the adversarial poses.
	This result supports the hypothesis that the generated adversarial poses are out-of-distribution.
	The original, high-resolution figure is available at \url{https://goo.gl/X31VXh}.}
    \label{fig:nearest_schoolbus}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.9\columnwidth]{garbage_truck_neighbors.jpg}
    \caption{
    For each adversarial example (leftmost), we retrieved the five nearest neighbors (five rightmost photos) from all $\sim$1,300 images in the \class{garbage truck} class.
    The Euclidean distance between a pair of images was computed in the \layer{fc7} feature space of a pre-trained AlexNet \cite{Krizhevsky2012}.
%    The ImageNet nearest neighbors to OoD poses of the \class{garbage truck} object. 
	The nearest photos from the class are mostly different from the adversarial poses.
	This result supports the hypothesis that the generated adversarial poses are out-of-distribution.
    The original, high-resolution image is available at \url{https://goo.gl/X31VXh}.}
    \label{fig:nearest_garbagetruck}
\end{figure*}

%%%% comparison of 14 models %%%%
\begin{figure*}
	\centering
	\includegraphics[width=0.76\columnwidth]{7_pairs.jpg}
	\caption{
	In Sec.~\ref{sec:adversarial_training}, we trained an AlexNet classifier on the 1000-class ImageNet dataset augmented with 30 additional classes that contain adversarial poses corresponding to the 30 \emph{known} objects used in the main experiments.
	We also tested this model on 7 \emph{held-out} objects.
	Here, we show the renders of 7 pairs of (training-set object, held-out object).
%	7 pairs of renders of 3D objects are recognized well by Inception-v3. 
%	We chose 7 classes and paired 14 objects within same class from our 3D objects training-set and held-out 3D objects into 7 pairs. 
	The 3D objects are rendered by the NR \cite{modernGL} at a distance of $(0,0,4)$. 
%	against a white background. 
	Below each image is its top-5 predictions by Inception-v3 \cite{szegedy2016rethinking}.
	The original, high-resolution figure is available at \url{https://goo.gl/Li1eKU}.}
	\label{fig:7_pairs}
\end{figure*}


\begin{figure*}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=1.0\columnwidth]{ambulance_30_AXs.jpg}
        \caption{\class{ambulance}}\label{fig:30_ax_ambulance}
    \end{subfigure}
   \begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=1.0\columnwidth]{school_bus_30_AXs.jpg}
    \caption{\class{school bus}}\label{fig:30_ax_schoolbus}
   \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=1.0\columnwidth]{street_sign_30_AXs.jpg}
        \caption{\class{street sign}}\label{fig:30_ax_stopsign}
    \end{subfigure}
    \caption{
    30 random adversarial examples misclassified by Inception-v3 \cite{szegedy2016rethinking} with high confidence ($p \geq 0.9$) generated from 3 objects: \class{ambulance}, \class{school bus}, and \class{street sign}. 
    Below each image is the top-1 prediction label and confidence score.
%    The correct labels are (a)ambulance (b)school bus (c)street sign respectively. 
    The original, high-resolution figures for all 30 objects are available at \url{https://goo.gl/rvDzjy}.}\label{fig:30_ax}
\end{figure*}

%%%% 5 unique objects per class %%%%

\begin{figure*}
    \centering
    \includegraphics[width=1.0\columnwidth]{5_unique_objects_per_class.jpg}
    \caption{
    For each target class (\eg, \class{accordion piano}), we show five adversarial poses generated from five unique 3D objects.
    Adversarial poses are interestingly found to be homogeneous for some classes, \eg, \class{safety pin}.
    However, for most classes, the failure modes are heterogeneous.
%    The renders of 3D objects are misclassified into various classes and have heterogeneous poses even with the same label. We randomly picked some misclassified labels and listed the 5 renders of unique object with highest confidence score by Inception-v3. 
	The original, high-resolution figure is available at \url{https://goo.gl/37HYcE}.
    }\label{fig:common_failures_per_label}
\end{figure*}


%%%real AXs%%%%
\begin{figure*}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.9\columnwidth]{cellphone_real_AXs.jpg}
        \caption{\class{cellular phone}}\label{fig:real_ax_cellphone}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.9\columnwidth]{jeans_real_AXs.jpg}
        \caption{\class{jeans}}\label{fig:real_ax_jeans}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.9\columnwidth]{stop_sign_real_AXs.jpg}
        \caption{\class{street sign}}\label{fig:real_ax_stopsign}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.9\columnwidth]{umbrella_real_AXs.jpg}
        \caption{\class{umbrella}}\label{fig:real_ax_umbrella}
    \end{subfigure}
\caption{
     Real-world, high-confidence adversarial poses can be found by taking photos from strange angles of a familiar object, here, \class{cellular phone}, \class{jeans}, \class{street sign}, and \class{umbrella}.
     While Inception-v3 \cite{szegedy2016rethinking} can correctly predict the object in canonical poses (the top-left image in each panel), the model misclassified the same objects in unusual poses.
     Below each image is its top-1 prediction label and confidence score.
%     The first image(green label) is predicted correctly and all the others(red labels) are predicted incorrectly by Inception-v3. 
%     We picked 4 classes that are easier to take photos without safety risk as our targets: (a) \class{cellular telephone} (b) \class{jeans} (c) \class{street sign} (d) \class{umbrella}. 
     We took real-world videos of these four objects and extracted these misclassified poses from the videos.
%     To find real world OoD examples, we took some short videos at some certain views according to the poses of 3D objects that misclassified(shown in Fig. \ref{fig:common_failures_per_label}) and listed the images with high confidence score. 
	The original, high-resolution figures are available at \url{https://goo.gl/zDWcjG}.
}\label{fig:real_ax}
\end{figure*}




\newpage
\begin{figure*}[t]
    \begin{center}
        \includegraphics[width=\textwidth]{sensitivity.pdf}
    \end{center}
    \caption{
        Inception-v3 \cite{szegedy2016rethinking} is sensitive to single parameter disturbances of object poses that had originally been correctly classified.
        % Each point for a given class corresponds to a different correctly classified starting pose (\ie, rotation and translation).
        % For each subplot, the title parameter was randomly sampled 100 times using the sampling procedure described in Section \ref{sec:landscape} while holding the other parameters constant.
        For each object, we found 100 correctly classified 6D poses via a random sampling procedure (Sec.~\ref{sec:landscape}).
        Given each such pose, we re-sampled one parameter (shown on top of each panel, \eg, yaw) 100 times, yielding 100 classifications, while holding the other five pose parameters constant.
        In each panel, for each object (\eg, \class{ambulance}), we show an error plot for all resultant $100 \times 100 = 10,000$ classifications.
%        For each panel, we repeated that random sampling procedure (detailed in Sec.~\ref{sec:landscape}) 100 times for each object.
%        For each subplot, the title parameter was randomly sampled 100 times using the sampling procedure described in Sec.~\ref{sec:landscape} while holding the other parameters constant.
%        This process was repeated for 100 different correctly classified starting poses for each object.
%        ``Fail Rate'' is the percent of single parameter samples that resulted in misclassification.
        Each circle denotes the mean misclassification rate (``Fail Rate'') for each object, while the bars enclose one standard deviation.
        Across all objects, Inception-v3 is more sensitive to changes in yaw, pitch, roll, and depth (``z\_delta'') than spatial changes (``x\_delta'' and ``y\_delta'').
    }
    \label{fig:sensitivity}
\end{figure*}


\newpage
\begin{figure*}[t]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{nearest_val_images.jpg}
	\end{center}
	\caption{
		For each adversarial example (leftmost), we retrieved the five nearest neighbors (five rightmost photos) from the 50,000-image ImageNet validation set.
		The Euclidean distance between a pair of images was computed in the \layer{fc7} feature space of a pre-trained AlexNet \cite{Krizhevsky2012}.
		Below each adversarial example (AX) is its Inception-v3 \cite{szegedy2016rethinking} top-1 prediction label and confidence score.
		The associated ground-truth ImageNet label is beneath each retrieved photo.
		Here, we show an interesting, cherry-picked collection of cases where the nearest photos (in the \layer{fc7} feature space) are also qualitatively similar to the reference AX and sometimes come from the exact same class as the AX's predicted label.
%		While for most AXs, the nearest images often come from diverse classes, here we show a cherry-picked collection of cases where the nearest photos come from the same class as the AX's predicted label.
%		The nearest photos from the class are mostly different from the adversarial poses.
%		This result supports the hypothesis that the generated adversarial poses are out-of-distribution.
%		For each adversarial example, we retrieve the five nearest images in the \layer{fc7} feature space of a pre-trained AlexNet \cite{Krizhevsky2012}.
%		We retrieve nearest images from the 50K validation set.
%		Leftmost shows the AXs.
		More examples are available at \url{https://goo.gl/8ib2PR}.
		}
	\label{fig:nearest_val_images}
\end{figure*}

%\newpage
%\section{Generalization on adversarial examples with non-empty background}
%\label{sec:background_test}
%
%We generated 1,000,000 random samples for both the PT and our AT using the objects and backgrounds from Fig.~\ref{fig:teaser} (all of which were in the training set).
%For each object, we calculated both the percent of correct classifications and the percent of high confidence misclassifications.
%A summary of the results can be found in Table~\ref{tab:ax_bg_stats}.
%While our AT improved the number of correct classifications for the school bus and fire truck objects, the improvement is much less dramatic than what was observed when backgrounds were omitted.
%
%\begin{table}[h]
%	\begin{center}
%		\begin{tabular}{lccc}
%			\toprule
%			& school bus & motor scooter & fire truck \\
%			\midrule
%			PT cor. \% & 4.5 & 3.8 & 2.3 \\
%			AT cor. \% & 7.8 & 3.5 & 7.9 \\
%			% \begin{tabular}[b]{@{}l@{}}PT high\\confidence\\misclassifications (\%)\end{tabular} & 19.5 & 7.6 & 1.0 \\
%			PT incor. \% & 19.5 & 7.6 & 1.0 \\
%			% \begin{tabular}[b]{@{}l@{}}AT high\\confidence\\misclassifications (\%)\end{tabular} & 19.0 & 7.4 & 0.9 \\
%			AT incor. \% & 19.0 & 7.4 & 0.9 \\
%			\bottomrule
%		\end{tabular}
%	\end{center}
%	\caption{The percent of correct classifications (``cor.'') and high confidence (\ie, $p > 0.7$) misclassifications (``incor.'') by the pre-trained AlexNet (PT) and our re-trained AlexNet (AT) on random samples of objects from Fig.~\ref{fig:teaser} when using the associated backgrounds.}
%	\label{tab:ax_bg_stats}
%\end{table}


\end{document}
% test chengfei editing
