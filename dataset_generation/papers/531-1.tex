%% 
%% Copyright 2007, 2008, 2009 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references
%% SP 2008/03/01

%\documentclass[preprint,12pt,authoryear]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times,authoryear]{elsarticle}
%% \documentclass[final,1p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,3p,times,authoryear]{elsarticle}
\documentclass[final,3p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,5p,times,authoryear]{elsarticle}
%% \documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}

%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%===== MXY - Packages ====
\usepackage{color}
\newcommand{\mxy}[1]{ \color{red}{#1}}


%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\journal{ISPRS Journal of Photogrammetry and Remote Sensing}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{A Fully Convolutional Network for 3D Semantic Labeling of LiDAR Point Clouds}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \address[label1]{}
%% \address[label2]{}

\author{Mohammed Yousefhussien}

\address{Rochester Institute of Technology}

\begin{abstract}
%% Text of abstract
When classifying LiDAR point clouds, a large amount of time is devoted  to the process of engineering a reliable set of features which later is passed to a classifier of choice. Generally, such features - usually derived from the 3D covariance matrix - are computed using the surrounding neighboring points. The size of the neighborhood is either predefined or found automatically per point before estimating the features. While the features capture different aspects of the point and the corresponding neighborhood, the process is usually time consuming and requires the application at multiple scales combined with contextual methods in order to capture complete objects.  In this paper we present a 1D fully convolutional network that consumes terrain-normalized LiDAR points directly with the corresponding spectral data to generate point-wise labeling without using contextual methods. Our method only uses the 3D-coordinates along with three spectral features extracted from 2D georeferenced IR-R-G images. We train our network by splitting the data into blocks, and we show that our method doesn't depend on the order of the points within the block due to the nature of used pooling layer. We train and evaluate our model using the ISRPS 3D Semantic Labeling Challange. Our method scored second place with an overall accuracy of 81.6\%. We ranked third place with a mean F1-score of 63.32\%, surpassing the F1-score of the method with highest accuracy by 1.69\%. In addition to labeling LiDAR points, we also show that our method can be easily extended and generates promising results when applied to 2D semantic segmentation tasks.

\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
LiDAR \sep 3D Labeling Challenge \sep Deep Learning.
%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%% \linenumbers

%% main text
\section{Introduction}
\label{}{\mxy Dave, if you can add the story part here that would be great!}

\subsection{related work}
In order to label point clouds, many methods have been proposed which can be grouped into two main categories. The first category is the set of algorithms that are applied directly to the point clouds without changing the 3D nature of the data. We will refer to such methods as "Direct Methods". On the other hand, the second set of methods transform the input point cloud into a different representation such as images or volumes in which various semantic segmentation methods are applied. We will refer to the second set as "Transformation Methods". In this section, we will review methods correspond to both categories from the remote sensing as well as the computer vision communities. 
\subsubsection{Direct Methods}
In these methods, algorithms perform semantic labeling by directly consuming the point clouds whether by using simple point-wise discriminative models or by incorporating contextual information for better results. For example, \citet{Blomley16} used covariance features at multiple scales found using the eigenentropy-based scale selection method (\cite{weinnman2014}). Then, they evaluated four different classifiers using the ISPRS benchmark on 3D semantic labeling without contextual relationships. Their best performing model was using a Linear Discriminant Analysis (LDA) classifier combined with a variety of geometric features. While their method depends on various handcrafted 3D features, in our work we show that only using the location information along with three spectral bands is sufficient to obtain a high classification accuracy.
\\In (\cite{ramiya2014semantic}), the authors proposed the use of spectral data and points coordinates directly forming a per point vector of ($X$,$Y$,$Z$,$R$,$G$,$B$) components. To label every point, first, they filtered the scene into a ground and non-ground points using the method presented in (\cite{axelsson2000generation}). Then, a 3D region growing segmentation method applied to both sets of points is used to generate object proposal. By using several geometrical and spectral features, each segment is then classified into a set of five classes by combining or omitting classes from the main ISPRS 3D labeling challenge. However, while the authors stated the use of several features, details about such features were not provided. Also, even though their method combines spectral information with LiDAR, their goal was to improve the results by introducing more features. On the other hand, by only using spectral data along with the 3D point coordinates, we aim to show that without additional handcrafted features or 3D segmentation, we were able to achieve near state-of-the-art results when using all of nine classes presented in the main challenge. Several other non-contextual methods were also reported in the literature such as the work by (\cite{mallet2010}) which classified full-waveform LiDAR data using point-wise multiclass SVM, and (\cite{Chehata09}) who used Random Forests (RF) for airborne feature selection and urban classification.
\\To allow for spatial dependencies between object classes by considering the labels of the neighborhood, (\cite{NIEMEYER2014152}), proposed a contextual classification method based on Conditional Random Fields (CRF) to label each 3D point into one of seven categories. In the first part of their work, the authors implemented and compared two CRF models, where the first CRF is based on a linear model, while the second CRF is based on Random Forests (RF) for both the unary and the pairwise potentials. In the second part of their work, they focused on building detection, where another CRF model was introduced. During the training process in the first part, they randomly sampled 2000 point per class for training, while testing was carried out using the remaining points. While the authors reported promising results, they had to use multiple complex contextual models to capture the interactions of individual points with their surroundings. Such complex interactions resulted in promising results at the cost of test time speed of 81 minutes using linear models for the CRF potentials, and 3.4 minutes when using RF adding to that the time needed to estimate the 131 dimensional feature vector per point before testing. {\mxy Compared to our work, we don't explicitly use contextual model, instead we learn global features per region during the training process using only simple input features}. In their recent work, the authors extended their contextual classification model to use hierarchical high order CRF (\cite{NIEMEYER2016}). They introduced a two-layer CRF in which spatial and semantic context is incorporated. First, segments are generated using the first layer which operates on a point level and utilizes high order cliques and a Robust $P^{n}$ Potts model known as the point-layer. Features at this stage are derived directly from the points and their neighborhood using the eigenentropy-based scale selection method that is known to be time consuming. The second CRF layer operates on the generated segments which incorporates a larger spatial scale. Features at this stage included intensity-based and normal-based features in addition to two road-related features namely the {\it distance to a road} and the {\it orientation with respect to the closest road} (\cite{Golovinskiy}). By iteratively propagating the information between they layers, the method allowed for revision of wrong classifications at later stages. This method showed promising results on the 3D ISPRS challenge. However, this method had two major disadvantages. First, the method consists of multiple algorithms where each is designed separately. For example, the feature estimation process, the probabilistic classifier of the unary potential, and the minimization of the CRF energy function are separate algorithms that are learned or trained individually. Having multiple algorithms introduces the difficulty of optimizing the whole method at once. Second, the use of inference methods such as graph-cut and Loopy Belief Propagation (LBP) at different stages, in addition to the time needed for estimating the features, increases the inference time during testing. Such disadvantages call for faster test time methods with an end-to-end learning mechanism instead of relying on multiple individually trained components. 
%%===========================================================================================
\subsubsection{Transformation Methods}
Most of the transformation methods are focused on the use of deep learning. Recently, the computer vision field noticed a substantial improvement in various domains such as image labeling (\cite{Alex_NIPS2012}), object detection (\cite{RCNN2014}), semantic segmentation (\cite{Segnet_PAMI,FCN}), and target tracking (\cite{DLT,SPT}) to name a few. Such advances were possible due to the reintroduction of the Convolutional Neural Networks (CNNs) (\cite{LeCun1989b}), the availability of large scale datasets (\cite{imagenet}), in addition to the affordable high performance compute resources such as GPUs. With the recent improvements of CNNs (\cite{resnet}) and their ability to learn local and global features in an end-to-end fashion (\cite{deepvis}), an interest to apply such successful frameworks to large 3D datasets emerged. However, the nature of 3D point clouds being nonuniform and irregular created challenges as how to extend the 2D CNN architectures to handle such data. Several recent attempts by the computer vision community were proposed. In (\cite{MVCNN}), they generate 12 rendered views by placing 12 virtual cameras around the 3D shape every $30^{\circ}$. Then, each rendered view is passed though a replica of a common CNN separately. Next, the resulted representation are aggregated using a view-pooling layer. The pooled representations are then passed to another CNN where the categories are learned. During training, the weights are initialized using VGG-M network (\cite{vggm}). Several other methods use the multiview approach with various modification to the rendered views. For example, in (\cite{GIFT}) instead of just generating regular 2D images, they generate depth view images in which each pixel represents the depth of the shape at that location. Other methods include generating a  common signature from multiple view features found using a fine-tuned VGG-19 (\cite{Simonyan15}) model, or projecting a 3D shape into 36 channels and modify AlexNet (\cite{Alex_NIPS2012}) to handle such input. For more details regarding the specific implementations of the previous algorithms, we refer the reader to (\cite{shrec}). %=====================================================================================
\\While previous methods apply multiview approaches to 3D CAD shapes, other researches took the same approach to ground-based LiDAR. (\cite{Boulch_3D}) applied the multiview approach to label points from the large scale point cloud classification benchmark (\cite{hackel2017semantic3d}). First, they decimated the data and generated point features such as normals and local noise. Then, they generated a mesh to allow for multiview generation using a 3D mesh viewer. Using the mesh and the point attributes, they generated two types of views, RGB and a 3-channel depth composite. Given multiple 3-channel images, a two stream SegNet (\cite{Segnet_PAMI}) network fused with residual correction (\cite{Audebert2016SemanticSO}) is used to label corresponding pixels in both streams. Finally, they back project the 2D labels to the corresponding mesh area then to the point cloud. (\cite{Luca}) proposed a deep learning approach for road detection using unstructured LiDAR data. First, multiple top-view images encoding several features such as elevation and density are created to form a six-channel image representing the scene. Then, using a Fully Convolutional Network (FCN) (\cite{FCN}) for a single scale binary semantic segmentation trained on the KITTI dataset (\cite{KITTI}), the multi-channel image is classified into \{road,not-road\} classes. This work requires rendering the data unnecessarily 2D, in which it has to be gridded to form a 2D map. Such gridding introduces void location that have to be replaced with values different from the data itself. This forces the network to learn implicitly an additional task to understand that such points don't belong to any of the classes. While one can get away by assigning them to the negative class in a binary task, in multi-class problems, such points will have to be assigned a separate class which in turns increases the complexity and may reduce the performance of the network. %====================================================================================
\\While the previous algorithms share the common theme of generating multiple views of the 3D shape, others took the volumetric approach to handle points clouds while using deep learning methods. In (\cite{Bo_Li_iros}), the author presents a method for vehicle detection in ground-based point clouds. First he discretized the point cloud using square grids, then represented the data by a 4D-array with dimensions correspond to length, width, height, and channels. By using only a single channel, binary values were used to represent the availability or the absence of a point. Then, a 3D FCN is trained to produce two maps representing the objectness and the bounding box scores.The method was evaluated on the KITTI dataset. Similar approach to label ground based LiDAR is taken by (\cite{huang2016point}), where the data is passed through a voxelization process that generates occupancy voxel grids. Then, the occupancy voxels are labeled by the label that corresponds to the center point within a voxel. Next, a 3D CNN is trained to semantically label each voxel into one of seven classes. The individual points are then labeled by back propagating the labels of each voxel to the points that formed the voxel. In VoxNet (\cite{maturana2015voxnet}), the authors explore three types of occupancy grids, binary, density, and hit grid. In a binary grid, each voxel is assumed to have a binary state, occupied or unoccupied. In the density grid model, each voxel is assumed to have a continuous density corresponding to the probability that the voxel would block a sensor beam. Finally, the hit grid only consider hits and ignore the difference between unknown and free space. Using each occupancy model individually, a 3D CNN with 32x32x32 grid inputs was trained. To handle multi-resolution inputs, they trained two separate networks each receiving a occupancy grid with different resolution. (\cite{qi2016volumetric}), the authors noticed a gap in performance between multiview and volumetric methods. To close such a gap, they proposed two volumetric CNN architectures which improved the results on the 3D classification task. The first network introduced auxiliary learning tasks by classifying parts of the object, which in turn helped to implicitly learn deeper details od the 3D shape. The second 3D CNN used long anisotropic kernels that projects the 3D volume to a 2D representation that can be processed later using an image-based CNN. For the 2D CNN part they adapted the Network In Network (NIN) architecture {\mxy NIN}. To combine the multiview approach with propsed volumetric methods, they rotated the object in 3D to create various orientations. Each individual orientation is processed individually by the same network to generate 2D representations that are pooled together and passed later to the image-based CNN. \\{\mxy say this another approach,and talk about the multiview ground-based method.}In ({\cite{Yansong}}), a semantic segmentation method using a combination of spectral and LiDAR data is presented. Instead of working directly on the LiDAR data by extracting the spectral data from georeferenced images, the authors decided to generate DSM images encoding the height per pixel as a separate channel and tackle the problem as a 2D semantic classification task. In order to fuse the information from the LiDAR and the spectral modalities, they estimated two probability maps suing two different methods. First, they used a pre-trained FCN to estimate the first probability map using the spectral data. Then, by handcrafting another set of features from both the spectral and the LiDAR data, a logistic regression is then applied to generate a second set of probability maps. At the end of the two stream process, they combined the two probability maps using a high order Conditional Random Fields (CRF) to label every pixel into one of six categories. {\mxy In contrast, we show that instead of creating two streams to account for the additional depth information, we can extend our single 3D method to work with 2D semantic segmentation with different modalities by a simple modification to the data preparation process}.
\\By closely following the previous methods, one can notice the following: \begin{itemize}
\item intro pointnet
\item datasets
\item blah
\item blah

\end{itemize}
%%===========================================================================================
%%===========================================================================================

\subsection{contribution}


\section{Methodology}
\label{}

\section{Evaluation}
\label{}

\section{Conclusions}
\label{}

\section*{Acknowledgments}
\section*{References}

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
\bibliographystyle{elsarticle-harv} 
\bibliography{library}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

\begin{thebibliography}{00}

%% \bibitem[Author(year)]{label}
%% Text of bibliographic item

\bibitem[ ()]{}

\end{thebibliography}
\end{document}

\endinput
%%
%% End of file `elsarticle-template-harv.tex'.

