\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

%%% colors\usepackage{color}\definecolor{ChrisCol}{RGB}{255,50,50}\definecolor{MichaelCol}{RGB}{100,120,240}\newcommand{\michael}[1]{{\color{MichaelCol}{ [michael] #1}}}\newcommand{\mleft}[1]{{\color{MichaelCol}{$\leftarrow$ #1}}}\newcommand{\chris}[1]{{\color{ChrisCol}{ [chris] #1}}}%\newcommand{\chris}\newcommand{\todo}[1]{{\textbf{\color{red}{#1}}}}%%% notation\usepackage{amsmath}\usepackage{amsfonts}\usepackage{amssymb}\usepackage{algorithm}\usepackage[noend]{algpseudocode}\newcommand{\domain}{\mathcal{I}}\newcommand{\reals}{\mathbb{R}}\newcommand{\bv}[1]{\mathbf{#1}}\newcommand{\vecx}{\bv{x}}\newcommand{\vecy}{\bv{y}}\newcommand{\vecp}{\bv{p}}\newcommand{\vecq}{\bv{q}}\newcommand{\vecr}{\bv{r}}\newcommand{\vecg}{\bv{g}}\newcommand{\vecz}{\bv{z}}\newcommand{\vech}{\bv{h}}\newcommand{\transfT}{\bv{T}}\newcommand{\iwidth}{w}\newcommand{\iheight}{h}\newcommand{\imgFun}{f_{img}}\newcommand{\RGB}{[0,1]^3}\newcommand{\allTransf}{\mathcal{T}}\newcommand{\matchFun}{m}\newcommand{\matchFunExplain}{$m$ for \emph{``matching''}}\newcommand{\corrMap}{m}%%% misc\usepackage{lineno}\usepackage{multirow}\usepackage{array}\usepackage{stackrel}\usepackage{tabularx}\usepackage{subfig}%\usepackage[none]{hyphenat}\usepackage{url}\urlstyle{same}
\newcommand{\argmax}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{max}}\;}
\newcommand{\argmin}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{min}}\;}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\definecolor{darkgreen}{rgb}{0.25,0.65,0.3}
\definecolor{darkred}{rgb}{0.85,0.25,0.3}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false,citecolor=darkgreen,linkcolor=darkred]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{751} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
%\title{Markov Random Fields Prior for Image Synthesis Using Deep Neural Network}
\title{Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis}
% Michael: Maybe make MRF and DCNN equal contributors?

\author{Chuan Li\\
Johannes Gutenberg University\\
Mainz, Germany\\
{\tt\small chuli@uni-mainz.de}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Michael Wand\\
Johannes Gutenberg University\\
Mainz, Germany\\
{\tt\small wandm@uni-mainz.de}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
This paper studies a combination of generative Markov random field (MRF) models and discriminatively trained deep convolutional neural networks (dCNNs) for synthesizing 2D images. The generative MRF acts on higher-levels of a dCNN feature pyramid, controling the image layout at an abstract level.
We apply the method to both photographic and non-photo-realistic (artwork) synthesis tasks. The MRF regularizer prevents over-excitation artifacts and reduces implausible feature mixtures common to previous dCNN inversion approaches, permitting synthezing photographic content with increased visual plausibility. Unlike standard MRF-based texture synthesis, the combined system can both match and adapt local features with considerable variability, yielding results far out of reach of classic generative MRF methods.
%
% In our experiments, the MRF regularized dCNN frequently avoids structural confusion and over-excitation artifacts of previous dCNN inversion methods. In comparison to standard MRF-based texture synthesis methods, we obtain better generalization performance, both in terms of adapting example patterns to new image compositions as well as relating semantically similar but pixel-level unrelated content, thereby permitting synthesis results inaccessible to previous texture synthesis methods.
%
%This paper explores MRF prior for synthesizing images using deep neural networks.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

The problem of synthesizing content by example is a classic problem in computer vision and graphics. It is of fundamental importance to many applications, including creative tools such as high-level interactive photo editing \cite{Agarwala04,Barnes09,Hertzmann01}, as well as scientific applications, such as generating stimuli in psycho-physical experiments~\cite{Gatys2015b}.

\begin{figure}
	\centering
		\includegraphics[width=1\linewidth]{teaser2.png}
		\caption{By combining deep convolutional neural network with MRF prior, our method can transfer both photorealistic and non-photorealistic styles to new images. Images credited to flickr users \emph{mricon} (A) and \emph{Peter Dahlgren} (B).}
	\label{fig:teaser}
\end{figure}

In this paper, we specifically consider the problem of data-driven image synthesis: Given an example image, we want to fully automatically create a variant of the example image that looks similar but differs in structure. The intended deviation is controlled by additional constraints provided by the user, ranging from just changing image dimensions to detailed layout specifications. Concretely, we implement this by splitting up the input into a \emph{``style''} image and a \emph{``content''} image~\cite{Gatys2015b,Hertzmann01}. The first describes the building blocks the image should be made of, the second constrains their layout. Figure~\ref{fig:teaser} shows an example of style transferred images, where the input images are shown on the left. Our results are are shown on the right. Notice our method produces plausible results for both art to photo and photo to art transfers. In particular, see how meso-structures in the style images, such as the mouth and eyes, are intentionally reused in the synthesized images.

The classic data-driven approach to generative image modeling is based on Markov random fields (MRFs): We assume that the most relevant statistical dependencies in an image are present at a local level and learn a distribution over the likelihood of local image patches by considering all local $k \times k$ pixel patches in the example image(s). Usually, this is done using a simple nearest-neighbor scheme \cite{Efros99}, and inference is performed by approximate MRF inference \cite{Efros01,Kwatra05,Kwatra03} or greedy approximations \cite{Barnes09,Efros99,Hertzmann01,Wei00}.

\begin{figure*}[t]
	\centering
		%\includegraphics[width=0.85\columnwidth]{Figure/netMRFVisMed.png}
		\includegraphics[width=1\linewidth]{FigurePCA.png}
		\caption{The input image is encoded by the VGG network (pixel colors show a 3D PCA embedding of the high-dimensiontal feature space). Related image content is mapped to semi-distributed, approximately spatially coherent feature constellations of increasing invariance \protect\cite{Zeiler14}. Input image credited to flickr user \emph{Emery Way}.}
	\label{fig:PCAface}
\end{figure*}

A critical limitation of MRFs texture synthesis is the difficulty of learning the distribution of plausible image patches from example data. Even the space of local $k \times k$ image patches (typically: $k \approx 5...31$) is already way too high-dimensional to be covered with simple sampling and nearest-neighbor estimation. The results are mismatched local pieces, which are subsequently stitched~\cite{Kwatra03} or blended~\cite{Kwatra05} together in order to minimize the perceptual impact of the lack of ability to generalize. The missing ingredient is a strong scheme for interpolating and adapting images from very sparse example sets of sample patches. 

In terms of invariance and ability to generalize, discriminatively trained deep convolutional neural networks have recently made dramatic impact~\cite{Krizhevsky2012,Simonyan14c}. They are able to recognize complex classes of image features, modeling non-linear deformations and appearance variations far beyond the abilities of simple nearest neighbors search.

However, the discriminative design poses a problem: The corresponding dCNNs compress image information progressively over multiple pooling layers to a very coarse representation, which encodes semantically relevant feature descriptors in a semi-distributed (not spatially localized) fashion (Figure~\ref{fig:PCAface}). While an inverse process can be defined \cite{Denton15,Dong15,Gatys15,Gatys2015b,Gauthier15,Mahendran15,Xu15,Zeiler14}, it reamins difficult to control: For example, simply maximizing the class-excitation of the network leads to hallucinatory patterns~\cite{Mordvintsev15}. Rather than that, we need to reproduce the correct statistics for neural encoding in the synthesis images. 

%distributed encoding of the constellations of multiple features (higher layers such as \textit{relu5\_1} in Figure~\ref{fig:PCAface}).
%In recent work~\cite{Zeiler14,Xu15,Denton15,Dong15,Gatys15,Gatys2015b,Gauthier15,Mahendran15}, methods have been developed to \emph{invert} the neural encoding, i.e., to predict pixel-level images from high-level semantic information. However, the problem at this point is data amplification --- a lot of additional information has to be invented from scratch. Simply maximizing the class-excitation of the network leads to hallucinatory patterns~\cite{Mordvintsev15}, where features relevant to the specified class fill and clutter the image. An alternative is to use adversary generative networks, which train a second network that is sensitive to implausible features. However, resolution remains limited and photo-realistic results are not achieved \todo{Chris - check comparison}. The problem can also be avoided altogether by using generative neural networks without information loss~\cite{Bengio2014}; however, costs are still prohibitive for high-resolution images.

Addressing this problem, Gatys et al.~\cite{Gatys15,Gatys2015b} have recently demonstrated remarkable results for transferring styles to guiding ``content'' images: Their method uses the filter pyramid of the VGG network~\cite{Simonyan14c} as a higher-level representation of images, benefitting from the vast knowledge acquired through training the dCNN on millions of photographs. Then, feature layout is simply controlled by penalizing the difference of the high-level neural encoding of the newly synthesized image to that of the content image. Further, the process is regularized by matching feature statistics of the ``style'' image and the newly synthesized picture by matching the correlations across the various filter channels, captured by a Gram matrix. The method yields very impressive results for applying artistic styles of paintings to photographs~\cite{Gatys15}. However, strict local plausibility remains difficult. In particular, using photographs as styles does not yield plausible results because only \emph{per-pixel} feature correlations are captured at different layers and the spatial layout is constrained too weakly.
%it is still difficult to generate photo-realistic results, as the method captures only feature statistics, but no additional constrains the spatial layout of image features.

Our paper augments their framework by replacing the bag-of-feature-like statistics of Gram-matrix-matching by an MRF regularizer that maintains local patterns of the ``style'' exemplar: MRFs and dCNNs are a canonical combination --- both models crucially rely on the assumption of locally correlated information and translational invariance. This equips the encoding of features in a dCNN with approximate Markovian consistency properties: Local patches have characteristic arrangements of feature activations to describe objects, and higher-up encoding becomes more invariant under in-class variation (Figure~\ref{fig:PCAface}).
We therefore use the generative MRF model on such higher levels of the network (\textit{relu3\_1} and \textit{relu4\_1} of the 19-layer VGG network\footnote{Notice that in Figure~\ref{fig:PCAface} layer \textit{relu5\_1} shows the most discriminative encoding for single pixels. However, in practice we found using $3 \times 3$ patches at layer \textit{relu4\_1} produce the best synthesis results. Intuitively, using patches from a slightly lower layer has similar matching performance, but permits overlapped MRFs and increased details in synthesis.}). This prescribes a plausible local layout of objects in terms of more abstract categories and, importantly, tries to ensure a consistent encoding of these higher-level features. The actual task of generalizing within object categories and plausible blending is then performed by the dCNN's lower levels via inversion~\cite{Mahendran15}.

Technically, we implement the additional MRF prior by an additional energy term that models Markovian consistency of the upper layers of the dCNN feature pyramid. We then utilize the EM algorithm of Kwatra et al.~\cite{Kwatra05} for MRF optimization: It easily integrates in the variational framework. Further, we will show that higher-level neural encodings are more perceptually linear, which matches well with the linear blending approach of the M-step.

We apply our method to a number of photo-realistic and non-photo-realistic image synthesis tasks, and show it is able to generalize among image patches far beyond the abilities of classic MRFs. In style-transfer scenarios, the combined method additionally benefits from the abilities of the dCNN to match semantically related image portions automatically, without user annotations. In comparison to previous methods that invert dCNNs, the MRF prior improves local plausibility of the feature layouts, avoiding hallucinatory artifacts and usually providing a more plausible meso-structure than the statistical approach of Gatys et al.~\cite{Gatys15}. In particular, we can strongly improve the plausibility of synthesizing photographs, which was not possible with the previous methods.

%@misc{Mordvintsev2015,
	%authors={Alexander Mordvintsev and Christopher Olah and Mike Tyka},
	%title={Inceptionism: Going Deeper into Neural Networks},
	%howpublished={http://googleresearch.blogspot.com/2015/06/inceptionism-going-deeper-into-neural.html},
	%year={2015},
%}

%Deep neural networks is trendy. Generative applications have been found. Strength in deforming object, applications in NPR. Limitation.

%MRF has been widely used in traditional image and texture synthesis. It is complementary to Deep neural network.


%Parametric and non-parametric view. The deep neural network is so nonlinear and rich, so is able to bridge these two aspect and always asking for sapping the parametric result to the non-parametric reference data.

\section{Related Work}
%Here we briefly review two categories of related work.

%In the following, we review additional related work in the domain of generative neural networks and texture synthesis.

\textbf{Image synthesis with neural networks:} 
The success of dCNNs in discriminative tasks~\cite{ILSVRC15} has also raised interest in generative variants. Zeiler et al.~\cite{Zeiler14} introduce a deconvolutional network to back-project neuron activations to pixels. Similarly, Mahendran and  Vedaldi~\cite{Mahendran15} reconstruct images from the neural encoding in intermediate layers. The work of Gatys et al~\cite{Gatys15}, detailed above, can also be employed in unguided settings~\cite{Gatys2015b}, outperforming traditional parametric texture synthesis which only uses a linear feature bank and no statistical priors~\cite{Portilla00}. %In both cases the authors show that lower layers retain photographically accurate information, and higher layers capture stronger degrees of geometric and photometric invariance.
%
%%%
%
%Nguyen et al.~\cite{Nguyen14} demonstrate that images unrecognizable to humans can be identified as recognizable objects by the network with high confidence~\cite{Nguyen14}, and Szegedy et al.~\cite{Szegedy13} show how to change a photograph in ways imperceptible to humans that lead to completely different classification result~\cite{Szegedy13}. These results raise some caution about the suitability of discriminatively trained network for generative tasks.
%
%The strong invariance at higher layers of a network is crucially important for discrimination, but may be counter-productive for generative tasks. This is due to the natural of cascaded abstraction and information lost through the forward propagation. In fact, it is possible to generate images that easily fool the state-of-the-art deep neural networks: images completely unrecognizable to humans can be identified as recognizable objects by the network with high confidence~\cite{Nguyen14}; changing a real photo image in a way imperceptible to humans can lead to completely different classification result~\cite{Szegedy13}. These evidences imply that directly applying discriminatively trained network for generative tasks is sub-optimal.
%
%Great efforts have been made to achieve better control over the inversion of encodings in discriminatively trained network.
%Mordvintsev et al.~\cite{Mordvintsev15} produce hallucinogenic images by amplifying neurons that encode complex features or even whole objects.% However, due to the lack of prior on natural image statistics, the results always appear over-processed and non-photorealistics. 

 %The style transfer application also compares favorably to previous non-photorealistic rendering~\cite{Collomosse12}, which 
%
%Mordvintsev et al.~\cite{Mordvintsev15} produce hallucinogenic images by amplifying neurons that encode complex features or even whole objects.
%
% However, due to the lack of prior on natural image statistics, the results always appear over-processed and non-photorealistics. 
%
%The work of Gatys et al~\cite{Gatys15}, discussed in detail above, can has also been employed in unguided settings~\cite{Gatys2015b}, outperforming traditional parametric texture synthesis which can only rely on a linear feature bank and no statistical priors~\cite{Portilla00}. %The style transfer application also compares favorably to previous non-photorealistic rendering~\cite{Collomosse12}, which 
%
%utilized the discriminatively pretrained VGG deep neural network~\cite{Simonyan14c} as a non-linear filter bank and use correlations between feature responses as a statistical prior for texture synthesis. The method outperforms traditional parametric texture synthesis which can only rely on a linear feature bank and no statistical priors~\cite{Portilla00}. The same technique~\cite{Gatys15} also works remarkably well for example-driven artistic rendering. Learning styles from ``style images'' offers greater versatility than traditional non-photorealisic rendering that requires specially designed procedure for each individual style~\cite{Collomosse12}. 
%However, the structure model is too flexible for photo-realistic synthesis.
%
% Notice~\cite{Gatys2015b,Gatys15} only used a global prior to control look and feel of the synthesis. There is no local prior to prevent the synthesized image from locally deviating from natural images. And the nature of parametric sampling (backpropogation) tends to generate distortions. This is particularly problematic when using their method to synthesize artwork of clear structures, or to synthesis photorealisic images~\cite{Gatys2015b}. In contrast, our method snaps the synthesis image locally towards the exemplar data, and only allow deformation when it is necessary. 
%
%In this way, the global look and feel of the synthesis image is kept in consistent with the reference painting. 
%
%Their method uses neural representations to separate the content and style of images, and combine the style of artistic paintings with the content of photos. Importantly, they encode the style of a painting at different scales, each represent the correlation between the neuron activations on a specific intermediate layer of the network. 
%A generator network creates artificial images and a discriminator network distinguishes between the generated images and real training images. At each iteration, the generator learns to produce images that look real to the discriminator, and the discriminator learns to not be tricked by the generator. 

%A further alternative to image synthesis is the framework of generative adversarial networks~\cite{Goodfellow2014}. Here, two networks iteratively improve each other. A generator network creates artificial images and a discriminator network distinguishes between the generated images and real training images. At each iteration, the generator learns to produce images that look real to the discriminator, and the discriminator learns to not be tricked by the generator. In the end the generator is able to produces more natural images than naive image synthesis. However, in many cases the output quality is still rather limited. ~\cite{Gauthier15} extend this model to a Laplacian pyramid setting. This leads to clear improvement on output quality. Nonetheless, it is expensive to train such model for synthesizing large images, and the results often still lack structure.~\cite{Denton15} extend the model to a conditional setting, but the model is limited for generating face images. 
%It is also possible to re-train networks for specific generative tasks, such as image deblur~\cite{Xu15}, super-resolution ~\cite{Dong15}, and class visualization~\cite{Lu15}. 

A further approach is the framework of generative adversarial networks~\cite{Goodfellow2014}. Here, two networks, one as the discriminator and other as the generator iteratively improve each other by playing a minnimax game. In the end the generator is able to produces more natural images than naive image synthesis. However, in many cases the output quality is still rather limited. Gauthier et al.~\cite{Gauthier15} extend this model by a Laplacian pyramid. This leads to clear improvement on output quality. Nonetheless, training for large images remains expensive and the results often still lack structure. Denton et al.~\cite{Denton15} extend the model to a conditional setting, limited to generating faces. It is also possible to re-train networks for specific generative tasks, such as image deblur~\cite{Xu15}, super-resolution ~\cite{Dong15}, and class visualization~\cite{Lu15}. 

 %By locally snapping the portions of the synthesized image to example image patches, convincing results are obtained for regular textures.
\textbf{MRF-based image synthesis:}
MRFs are the classic framework for for non-parametric image synthesis~\cite{Efros99}.
As explained in the introduction, a key issue is adapting local patches beyond simple stitching~\cite{Kwatra03} or blending~\cite{Kwatra05}, and our paper focuses on this issue. Aside from this, MRF models suffer from a second, significant limitation: Local image statistics is usually not sufficient for capturing complex image layouts at a global scale. While local details appear plausible, the global arrangement often merely resembles an unstructured ``texture soup''. Multi-resolution synthesis~\cite{Hertzmann01,Kwatra05,Wei00} provides some improvement here (and we adapt this in our method, too), but a principled solution requires additional high-level constraints. These can be either explicitly provided by the user~\cite{Agarwala04,Barnes09,Efros01,Gatys15,Hertzmann01,Lee10,Xie07}, or learned from non-local image statistics~\cite{He12,Li15,Zhang13}. Long range correlations have also been modeled by spatial LTSM neural networks; results so far are still limited to semi-regular textures~\cite{Theis2015}.
 Our paper opts for the first, simpler solution of explicit layout constraints through a ``content'' image~\cite{Gatys15,Hertzmann01} --- in principle, learning of global layout constraints is mostly orthogonal to our approach.



 %By locally snapping the portions of the synthesized image to example image patches, convincing results are obtained for regular textures.
%
%This is usually sped-up through accelerated matching algorithms~\cite{Wei00,Barnes09}.
%
%can further improve the quality and efficiency of the synthesis process. In practice MRFs are required to overlap so a coherent transition can be achieved using either blending~\cite{Kwatra05} or stitching~\cite{Kwatra03}. The former uses a least squared optimization so is more versatile, the later solves a labeling problem so the result is often visually sharper. However, the non-parametric character of this stream of methods means there are only very limited number of examples to choose. This leads to artifact when adjacent MRFs do not fit. For example, blurry pixels and visually implausible seams at the overlap.
%Artifacts can be reduced by improved inference algorithms based on graph-cut~\cite{Kwatra03} (which reduces artifacts by stitching) or EM-based optimization \cite{Kwatra05} (which merges patches through blending).
%
%\textbf{MRF-based image synthesis:} MRF models are the classic tool for for non-parametric image synthesis~\cite{Efros99}. As explained in the introduction, a key issue is adapting local patches beyond simple stitching~\cite{Kwatra03} or blending~\cite{Kwatra05} to archive stronger generalization from limited example data, and our paper focuses on addressing this issue. However, aside from this, MRF models suffer from a second very significant limitation: Local image statistics is usually not sufficient for capturing complex image layouts at a global scale. While local details remain plausible in synthesis results, the global arrangement often merely resembles an unstructured ``texture soup''. Multi-resolution synthesis~\cite{Hertzmann01,Kwatra05,Wei00} provides some improvement here (and we adapt this in our method, too), but a principled solution requires additional high-level constraints. These can be either explicitly provided by the user~\cite{Agarwala04,Barnes09,Efros01,Gatys15,Hertzmann01,Lee10,Xie07}, or learned from non-local image statistics~\cite{He12,Li15,Zhang13}. Our paper opts for the first,  for the simpler solution of explicit layout constraints through a ``content'' image~\cite{Gatys15,Hertzmann01} --- in principle, learning of global layout constraints is mostly orthogonal to our approach.
%As explained in the introduction, a key issue is adapting local patches beyond simple stitching~\cite{Kwatra03} or blending~\cite{Kwatra05}, and our paper focuses on this issue. Nonetheless, aside from this, MRF models suffer from a second very significant limitation: Local image statistics is usually not sufficient for capturing complex image layouts at a global scale. While local details remain plausible in synthesis results, the global arrangement often merely resembles an unstructured ``texture soup''. Multi-resolution synthesis~\cite{Wei00,Hertzmann01,Kwatra05} provides some improvement here (and we adapt this in our method, too), but a principled solution requires additional high-level constraints. These can be either explicitly provided by the user~\cite{Hertzmann01,Efros01,Agarwala04,Xie07,Barnes09,Lee10,Gatys15}, or learned from non-local image statistics~\cite{He12,Zhang13,Li15}. Our paper opts for the first,  for the simpler solution of explicit layout constraints through a ``content'' image~\cite{Hertzmann01,Gatys15} --- in principle, learning of global layout constraints is mostly orthogonal to our approach.

% For example ``texture soup'' will be created if the size of the MRFs are too small. Simply increasing the size often makes the model too rigid to create useful results. For texture images, a common strategy is to use a multi-resolution approach~\cite{Kwatra05} that synthesizes the image in a coarse-to-detail fashion. However this only has limited impact on more complex, non-texture images. In these cases, additional structure constraints such as statistics of regularity~\cite{He12,Zhang13}, semantics~\cite{Li15}, reference structure~\cite{Hertzmann01,Efros01,Xie07,Lee10} or even user interaction~\cite{Agarwala04} are required. In this paper we use multi-resolution for texture synthesis, and a data-driven approach for style transfer. 


\section{Model}
We now discuss our combined MRFs and dCNNs model for image synthesis. We assume that we are given a style image, denoted by $\bv{x}_s \in \reals^{w_s \times h_s}$, and a content image $\bv{x}_c \in \reals^{w_c \times h_c}$ for guidance. The (yet unknown) synthesized image is denoted by $\bv{x} \in \reals^{w_c \times h_c}$. We transfer the style of $\bv{x}_s$ into the layout of $\bv{x}_c$ by making the high-level neural encoding of $\bv{x}$ similar to $\bv{x}_c$, but using local patches similar to those of $\bv{x}_s$. The latter is the MRF prior that maintains the encoding of the style.
%
%We formulate style transfer as the problem of generating an image that resembles the content image using Markov random fields from a style image. Importantly, we perform such a process at a number of layers in a deep convolutional neural network instead of using image pixels. 
%
Formally, $\bv{x}$ minimizes the following energy function:
%
%\footnotesize
\begin{eqnarray}
\bv{x}&= &\argmin{x}E_{s}(\Phi(\bv{x}), \Phi(\bv{x}_{s})) + \nonumber \\&& \alpha_{1}E_{c}(\Phi(\bv{x}), \Phi(\bv{x}_{c})) + \alpha_{2}\Upsilon(\bv{x})
\label{eq:energy}
\end{eqnarray}
%
% that resembles the appearance of a style image $\bv{x}_{s}$,
$E_{s}$ denotes the style loss function (MRFs constraint), where $\Phi(\bv{x})$ is $\bv{x}$'s feature map from some layer in the network. 
%We use  $\Psi(\Phi(\bv{x}))$ to represent the Markov random fields in that layer.
%Minizing $E_{s}$ ensures the MRFs property of the style image is preserved in the synthesized image.
%
$E_{c}$ is the content loss function. It computes the squared distance between the feature map of the synthesis image and that of the content guidance image $\bv{x}_{c}$. As shown in~\cite{Gatys15,Mahendran15}, minimizing $E_{c}$ generates an image that is contextually related to $\bv{x}_{c}$. The additional regularizer $\Upsilon(\bv{x})$ is a smoothness prior on the reconstruction. Next, we explain how to define these terms in details.
%\textbf{MRFs loss function}: Let, $\Psi( \cdot )$ denote the set of local $k \times k$ image patches. Formally, we select one or more layers in the neural encoding $\Phi(\bv{x})$ and stack all local $k \times k$ patches into a matrix $\Psi(\Phi(\bv{x}))$, where each column vector lists the entries of each patch.
%
%a matrix, consisting of all local $k \times k$ patches of the neural encoding $\Phi(\bv{x})$ (at one or more levels, to be selected) as its column vectors. For each patch, we compute the closest matching patch in $\Psi(\Phi(\bv{x}_{s}))$ and denote the result by $$
%
%are the ``neural'' patches in $\Phi(\bv{x})$, and $\Psi(\Phi(\bv{x}_{s}))$ are the neural patches in the style image. We define $E_{s}$ as sum of the Euclidean distance between every patch in $\Psi(\Phi(\bv{x}))$ to its best match in $\Psi(\Phi(\bv{x}_{s}))$:
%
%\begin{equation}
%E_{t}(\Psi(\Phi(\bv{x})), \Psi(\Phi(\bv{x}_{s}))) = \sum_{i = 1}^{m}||\Psi(\Phi(\bv{x}), i) - \Psi(\Phi(\bv{x}_{s}), j)||^{2}
%\label{eq:mrf}
%\end{equation}
%

%\textbf{MRFs loss function}: Intuitively, $\Psi(\Phi(\bv{x}))$ are the ``neural'' patches in $\Phi(\bv{x})$, and $\Psi(\Phi(\bv{x}_{s}))$ are the neural patches in the style image. We define $E_{s}$ as sum of the Euclidean distance between every patch in $\Psi(\Phi(\bv{x}))$ to its best match in $\Psi(\Phi(\bv{x}_{s}))$:
\textbf{MRFs loss function}: Let $\Psi(\Phi(\bv{x}))$ denote the list of all local patches extracted from $\Phi(\bv{x})$ -- a specified set of feature maps of $\bv{x}$. Each ``neural patch'' is indexed as $\Psi_i( \Phi(\bv{x}) )$ and of the size $k \times k \times C$, where $k$ is the width and height of the patch, and $C$ is the number of channels for the layer where the patch is extracted from.
% in the neural encoding $\Phi(\bv{x})$. Formally, we select one or more layers in the neural encoding $\Phi(\bv{x})$ and stack all local $k \times k$ patches of into a matrix $\Psi(\Phi(\bv{x}))$, where each column vector lists the entries of each patch.
We set the energy function to
%
\begin{equation}
E_{s}(\Phi(\bv{x}), \Phi(\bv{x}_{s})) = \sum_{i = 1}^{m}||\Psi_i(\Phi(\bv{x})) - \Psi_{NN(i)}(\Phi(\bv{x}_{s}))||^{2}
\label{eq:mrf}
\end{equation}
%
Here $m$ is the cardinality of $\Psi(\Phi(\bv{x}))$.
% We use $\Psi(\Phi(\bv{x}), i)$ to represent the $i$-th patch in $\Psi(\Phi(\bv{x}))$, and $\Psi(\Phi(\bv{x}_{s}), j)$ to represent its best match in $\Psi(\Phi(\bv{x}_{s}))$. Let $\psi$ denotes the vectorized form of the patch,
For each patch $\Psi_i(\Phi(\bv{x}))$ we find its best matching patch $\Psi_{NN(i)}(\Phi(\bv{x}_{s}))$ using normalized cross-correlation over all $m_s$ example patches in $\Psi(\Phi(\bv{x}_{s}))$: 
\begin{equation}
NN(i) := \argmin{j=1,...,m_s}\frac{\Psi_i(\Phi(\bv{x}))\cdot\Psi_j(\Phi(\bv{x}_{s}))}{|\Psi_i(\Phi(\bv{x}))|\cdot|\Psi_j(\Phi(\bv{x}_{s}))|}
\label{eq:ncorr}
\end{equation}
%
We use normalized cross-correlation to achieves stronger invariance. The matching process can be efficiently executed by an additional convolutional layer (explained in the implement details). Notice although we use normalized cross-correlation to find the best match, their Euclidean distance is minimized in Equation~\ref{eq:mrf} for producing an image that is visually close to the reference style.  

\textbf{Content loss function}: $E_{c}$ guides the content of the synthesized image by minimizing the squared Euclidean distance between $\Phi(\bv{x})$ and $\Phi(\bv{x}_{c})$:
\begin{equation}
E_{c}(\Phi(\bv{x}), \Phi(\bv{x}_{c})) = ||\Phi(\bv{x}) - \Phi(\bv{x}_{c})||^{2}
\label{eq:content}
\end{equation}

\textbf{Regularizer}: There is significant amount of low-level image information discarded during the discriminative training of the network. In consequence, reconstructing an image from its neural encoding can be noisy and unnatural. For this reason, we penalize the squared gradient norm~\cite{Mahendran15} to encourage smoothness in the synthesized image:
%
\begin{equation}
\Upsilon(\bv{x}) = \sum_{i, j}\left((x_{i, j + 1} - x_{i, j})^{2} + (x_{i + 1, j} - x_{i, j})^{2}\right)
\label{eq:tv}
\end{equation}
%
\textbf{Minimization}: We minimize Equation~\ref{eq:energy} using back-propagation with Limited-memory BFGS. In particular, the gradient of $E_{s}$ with respect to the feature maps is the element-wise difference between $\Phi(\bv{x})$ and their MRFs based reconstruction using patches from $\Phi(\bv{x}_{s})$. Such a reconstruction is essentially a texture optimization process~\cite{Kwatra05} that uses neural patches instead of pixel patches. It is crucially important to optimize this MRF energy at the neural level, as the traditional pixel based texture optimization will not be able produce results of comparable quality.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.75\linewidth]{analysis_matching2.png}
	\caption{Comparison of patch matching at different layers of a VGG network.}\label{fig:analysis_matching}
\end{figure}

\textbf{Weight} The $\alpha_{1}$ and $\alpha_{2}$ are weights for the content constraint and the natural image prior, respectively. We set $\alpha_{1} = 0$ for non-guided synthesis. By default we set $\alpha_{1} = 1$ for style transfer, while user can fine tune this value to interpolate between the content and the style. $\alpha_{2}$ is fixed to $0.001$ for all cases.

\section{Analysis}
Our key insight is that combining MRF priors with dCNN can significantly improve synthesis quality. This section provides a detailed analysis of our method from three perspectives: we first show compared to pixel values, neural activation leads to better patch matching and blending. We then show how MRFs can further improve the results.

\subsection{Neural Matching}
A key component of non-parametric image synthesis is to match the synthesized data with the example. (Figure~\ref{fig:analysis_matching} is a toy example that shows neural activation gives better matching than pixels. The task is to match two different car images. The first column contains the query patches from one car; every other column shows the best matching in the other car, found at different feature maps (including the pixel layer).

It is clear that patch matching using pixels or neural activation at the lower layers (such as \textit{relu2\_1}) is sensitive to appearance variation. The neural activations at layers \textit{relu3\_1} and \textit{relu4\_1} give better results. The top layers (\textit{relu5\_1}) seem to decrease the match quality due to the increasing invariance against appearance variation. This is not surprising because the features at middle layers are usually trained for recognizing object parts, as discussed in~\cite{Zeiler14}. For these reason, we use neural patches at layers \textit{relu3\_1} and \textit{relu4\_1} as MRFs to constrain the synthesis process.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{analysis_blending.png}
	\caption{Linear blending behave differently in pixel space and in neural space. Input photos credited to: \emph{rockysretreat.com} (dog A), \emph{wall321.com} (dog B), flickr user \emph{Jsome1} (cat A), flickr user \emph{MendocinoAnimalCare} (cat B).}\label{fig:analysis_blending}
\end{figure}

\subsection{Neural Blending}
The least-squared optimization for minimizing the texture term ($E_{s}$ in Equation~\ref{eq:mrf}) leads to a linear blending operation for overlapping patches. Here we show that blending neural patches often works better than directly blending pixel patches. Specifically, we compare two groups of blending results: The first method is to directly blend the pixels of two input patches. The second method passes these patches through the network and blend their neural activations at different layers. For each layer, we then reconstruct the blending result back into the pixel space using the method described in~\cite{Mahendran15}.

Figure~\ref{fig:analysis_blending} compares the results of these two methods. The first two columns are the input patches A and B for blending. They are intentionally chosen to be semantically related and structurally similar, but are significantly different in pixel values. The third column shows the average of these two patches. Each of the remaining column shows the reconstruction of the blending at a different layer. It is clear that pixel based and neural based blending give very different results: averaging the pixels often gives strong ghost artifacts. This can be reduced as we dive deeper into the network: through experiments we observed that the middle level layers such as \textit{relu3\_1} and \textit{relu4\_1} often give more meaningful blendings. Lower layers such as \textit{relu2\_1} behave similarly to pixels; reconstruction from layers beyond \textit{relu4\_1} tends to be too fuzzy to be used for synthesis. This also matches our previous observation of middle layers' privilege for patch matching -- because a representation that gives better discriminative performance is more robust to noise and enables better interpolation.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{analysis_mrf.png}
	\caption{Effect of MRFs prior in neural based synthesis. We do not show the example patches here because they are almost visually identical to the synthesized patches cropped from our results (top). In contrast, patches cropped from~\cite{Gatys15}'s synthesis results have severe distortion and smear (bottom).}\label{fig:analysis_mrf}
\end{figure}

\subsection{Effect of the MRF Prior}
Despite the advantages in patch matching and blending, it is still possible for a dCNN to generate implausible results. For example, the matched patches from different layers might not always fire at the same place in the image space, indicated by the offsets between the patches found at layer 
\textit{relu3\_1} and \textit{relu4\_1} (Figure~\ref{fig:analysis_matching}). The blending may also produce artifacts, for example, the destructed face of the dog (\textit{relu4\_1}, figure~\ref{fig:analysis_blending}) and the ghosting eyes of the cat (\textit{relu3\_1}, figure~\ref{fig:analysis_blending}). Extreme cases can be found at~\cite{Mordvintsev15} which produces hallucinogenic images by simply stimulating neural activations without any constraint of the natural image statistics.

More natural results can be obtained through additional regularizer such as smothness (total variation) or the Gram matrix of pixel-wise filter responses~\cite{Mahendran15,Gatys15}.
Our approach adds an MRF regularizer to the middle / upper levels of the network.
% Nonetheless, none of these methods uses MRFs in a non-parametric way to regularize middle level statistics. In the context of image synthesis, preserving such non-parametric middle level information often gives better mesostructures, while still be flexible enough for many higher level structure manipulations. 
%
To show the benefits of the MRFs prior, we compare the synthesis results with and without the constraint. We compare against the ``style constraint'' based on matching Gram matrices from~\cite{Gatys15}. Figure~\ref{fig:analysis_mrf} validates the intended improvements in local consistency. The first row shows image patches cropped from our results. They are visually consistent to the patches in the original style images. In contrast,~\cite{Gatys15} produces artifacts such as distortions and smears. The new MRF prior reduces flexibility in local adaptation in favor of reproducing meso-scale features more faithfully.

\section{Implementation Details}
This section describes implementation details of our algorithm~\footnote{We release code at: https://github.com/chuanli11/CNNMRF}. We use the pre-trained 19-layer VGG-Network from~\cite{Simonyan14c}. The synthesis $\bv{x}$ is initialized as random noise, and iteratively updated by minimizing Equation~\ref{eq:energy} using back-propagation. We use layer \textit{relu3\_1} and \textit{relu4\_1} for MRFs prior, and layer \textit{relu4\_2} for content constraint. 

For both layer \textit{relu3\_1} and \textit{relu4\_1} we use $3 \times 3$ patches. To achieve the best synthesis quality we set the stride to one so patches are very densely sampled. The patch matching (Equation~\ref{eq:ncorr}) is implemented as an additional convolutional layer for fast computation. In this case patches sampled from the style image are treated as the filters. The best matching of a query patch is the filter that gives the maximum response. We can pre-computed the magnitude of the filters ($||\Psi_i(\Phi(\bv{x}_{t}))||$ in Equation~\ref{eq:ncorr}) as they will not change during the synthesis. Unfortunately the magnitude of patches in the synthesized image ($||\Psi_i(\Phi(\bv{x}))||$) needs to be computed on the fly.

In practice we use a multi-resolution process: We built a image pyramid using the scaling factor of two, and stop when the longest dimension of the synthesized image is less than 64 pixels. The reference texture and reference content images are scaled accordingly. We perform 200 iterations for each resolution, and the output of the previous resolution is bi-linearly up-sampled as the initialization for the next resolution. We implement our algorithm under the Torch framework. Our algorithm take about three minutes to synthesis an image of size $384 \times 384$ with a Titan X GPU. 

To partially overcome the perspective and scale difference between the style and the content images, we sample patches from a number of copies of the style image with different rotations and scales: we use seven scales $\{0.85, 0.9, 0.95, 1, 1.05, 1.1, 1.15\}$, and for each scale we create five rotational copies $\{-\frac{\pi}{12}, -\frac{\pi}{24}, 0, \frac{\pi}{24}, \frac{\pi}{12}\}$. Since increasing the number of patches is computational expensive, in practice we only use the rotational copies for objects that can deform -- for example faces.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{qlt_compare1.png}
	\caption{Comparison with Gatys et al.~\cite{Gatys15} for artistic synthesis. Content images credited to flickr users \emph{Christopher Michel} (top) and \emph{Peter Dahlgren} (bottom).}\label{fig:qlt_compare1}
\end{figure}

\section{Results}
This section discusses our results. Here we focus on style transfer and refer readers to our supplementary material for the results of un-guided synthesis. As discussed later in this section, our method has strong restriction with the input data, so a formal user study is less meaningful. For this reason, our evaluation is performed by demonstrating our method with a number of typical successful and failure cases, with comparisons and discussions related to the state of the art neural based image stylization method~\cite{Gatys15}. We refer readers to our supplementary report for more results.

Figure~\ref{fig:qlt_compare1} shows two examples of stylizing photos by artwork. The input photos (left) are stylized using Pablo Picasso's ``Self-portrait 1907'' and Wassily Kandinsky's ``Composition VIII'', respectively. In both cases, Gatys et al.'s~\cite{Gatys15} method (middle) achieves interesting results, preserving the content of the photos and the overall look and feel of the artworks. However, there are weaknesses on closer inspection: The first result (top row) contains many unnecessary details, and the eyes look unnatural. Their second result lost the characteristic shapes in the original painting and partially blends with the content exemplar. In contrast, our first result synthesized more plausible facial features. In the second result, our method also resembles the style better: notice the important facial features such as eyes and the mouth are synthesized as simple shapes, and the hair as regions of dark color.

Figure~\ref{fig:qlt_compare2} shows two examples of photorealsitic synthesis. We transfer the style of a vintage car to two different modern vehicles. Notice the lack of photo-realistic details and strong smears in~\cite{Gatys15}'s results. With the MRFs constraint (right) our results are closer to photorealistic.

From an informal user study, we observed that \cite{Gatys15} usually keeps the content better, and our method produces more accurate styles. Figure~\ref{fig:qlt_compare3} gives detailed analysis between these two different characters. Here we show three patches from the content image (red boxes), and their closet matches from the style image and the synthesis images (using neural level matching). Notice our method produces more plausible results when a good match can be found between the content and the style images (the first patch), and performs less well when mis-matching happens (the second patch, where our synthesis deviates from the content image). For the third patch, there is no matching can be found for the car. In this case, our method replaces the car with texture synthesis, while \cite{Gatys15} keeps the car and renders it with artifacts. In general, our method creates more stylish images, but may contain artifacts when the MRFs do not fit the content. In contrast, the parametric method \cite{Gatys15} is more adaptable to the content, but at the cost of deviating from the style.

\begin{figure*}[ht]
	\centering
	\includegraphics[width=0.95\linewidth]{qlt_compare6.png}
	\caption{Comparison with Gatys et al.~\cite{Gatys15} for photo-realistic synthesis. Input images credited to flickr users \emph{Brett Levin}, \emph{Axion23} and \emph{Tim Dobbelaere}.}\label{fig:qlt_compare2}
\end{figure*}

%\begin{figure*}[ht]
	%\centering
	%\includegraphics[width=1\linewidth]{Figure/qlt_compare/qlt_compare4.png}
	%\caption{Comparisons with~\cite{Gatys15} for bi-directional style transfer.}\label{fig:qlt_compare4}
%\end{figure*}

\begin{figure*}[ht]
	\centering
	\includegraphics[width=0.95\linewidth]{qlt_compare5.jpg}
	\caption{Detailed analysis of the difference between our results and Gatys et al.'s~\cite{Gatys15}'s results. Input images credited to flickr users \emph{Eden, Janine and Jim} and \emph{Garry Knight}.}\label{fig:qlt_compare3}
\end{figure*}


%Compare with Neural Style and Texture Optimization
%
%\subsection{Texture Synthesis}
%maybe include regluar structure and offset statistics too
%\subsection{Style Transfer}

\subsection{Limitations}
Our method is an interesting extension for image based style transfer, especially for photorealistic styles. Nonetheless, it has many limitations. First and for most, it is more restricted to the input data: it only works if the content image can be re-assembled by the MRFs in the style image. For example, images of strong perspective or structure difference are not suitable for our method. 

Figure~\ref{fig:limit} shows two further example cases where~\cite{Gatys15} works better: The first example aim to transfer the style of a white dog according to the content of a yellow dog. Notice how the dogs' facial expression differ in these images. In this case our result (right) failed to reproduce the open mouth in the content image. The second example shows a artistic style that can be more easily transferred by the parametric method: Notice how the textures adapt to the content more naturally in~\cite{Gatys15}'s method. In contrast, our method tries to ``reshuffle'' building blocks in the style image and lost important features in the content image. In general, our method works better for subjects that allows structural deformation, such as faces and cars. For subjects that have strict symmetry properties such as architectures, it is often that our method will generate structural artifacts. In this case, structural statistics such as~\cite{He12} may be used for regularizing the synthesized image.

Although our method achieved improvement for photorealistic synthesis, it is still not as sharp as the original photos. This is due to the loss of non-discriminative image details during the training of the network. This opens an interesting future work that how the dCNN can be retrained, or incorporated with stitching based texture synthesis such as~\cite{Kwatra03} for achieving pixel-level photorealism.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{limit.png}
	\caption{Cases where Gatys et al ~\cite{Gatys15} works better.}\label{fig:limit}
\end{figure}

\section{Conclusions}
The key insight of this paper is that we can combine the discriminative power of a deep neural network with classical MRFs based texture synthesis. We developed a simple method that is able to produce encouraging new results for style transfer between images. We analyzed our results with a number of typical successful and failure cases, and discussed its pros and cons compared to the state of the art neural based method for transferring image styles. Importantly, our results often preserve better mesostructures in the synthesized image. For this first time, this permits transferring photo-realistic styles with some plausibility. The stricter control of the mesostructure is also the biggest limitation at this point: The MRF prior only offers advantages when style and content images consists of similarly shaped elements without strong changes in perspective, size, or shape, as covered by the invariance of the high-level neural encoding. Otherwise, artifacts might occur. For pure artistic styles, the increased rigidity can then be a disadvantage.

Our work is only one step in the direction of leveraging deep convolutional neural networks for improving image synthesis. It opens many interesting questions and future work such as how to resolve the incompatibility between the structure guidance and the MRFs~\cite{He12}; how to more efficiently study and transfer the middle level style across a big dataset; and how to generate pixel-level photorealistic images by incorporating with stitching based texture synthesis~\cite{Kwatra03}, or with generative training of the network.


\section*{Acknowledgments}
This work has been partially supported by the Intel Visual Computing Institute and by the International Max Planck Research School for Computer Science. We thank Hao Su and Daniel Franzen for inspiring discussions.

{\small
\bibliographystyle{ieee}
\bibliography{paper}
}

\end{document}

%
%%-------------------------------------------------------------------------
%\subsection{Language}
%
%All manuscripts must be in English.
%
%\subsection{Dual submission}
%
%Please refer to the author guidelines on the CVPR 2016 web page for a
%discussion of the policy on dual submissions.
%
%\subsection{Paper length}
%Papers, excluding the references section,
%must be no longer than eight pages in length. The references section
%will not be included in the page count, and there is no limit on the
%length of the references section. For example, a paper of eight pages
%with two pages of references would have a total length of 10 pages.
%{\bf There will be no extra page charges for CVPR 2016.}
%
%Overlength papers will simply not be reviewed.  This includes papers
%where the margins and formatting are deemed to have been significantly
%altered from those laid down by this style guide.  Note that this
%\LaTeX\ guide already sets figure captions and references in a smaller font.
%The reason such papers will not be reviewed is that there is no provision for
%supervised revisions of manuscripts.  The reviewing process cannot determine
%the suitability of the paper for presentation in eight pages if it is
%reviewed in eleven.  
%
%%-------------------------------------------------------------------------
%\subsection{The ruler}
%The \LaTeX\ style defines a printed ruler which should be present in the
%version submitted for review.  The ruler is provided in order that
%reviewers may comment on particular lines in the paper without
%circumlocution.  If you are preparing a document using a non-\LaTeX\
%document preparation system, please arrange for an equivalent ruler to
%appear on the final output pages.  The presence or absence of the ruler
%should not change the appearance of any other content on the page.  The
%camera ready copy should not contain a ruler. (\LaTeX\ users may uncomment
%the \verb'\cvprfinalcopy' command in the document preamble.)  Reviewers:
%note that the ruler measurements do not align well with lines in the paper
%--- this turns out to be very difficult to do well when the paper contains
%many figures and equations, and, when done, looks ugly.  Just use fractional
%references (e.g.\ this line is $095.5$), although in most cases one would
%expect that the approximate location will be adequate.
%
%\subsection{Mathematics}
%
%Please number all of your sections and displayed equations.  It is
%important for readers to be able to refer to any particular equation.  Just
%because you didn't refer to it in the text doesn't mean some future reader
%might not need to refer to it.  It is cumbersome to have to use
%circumlocutions like ``the equation second from the top of page 3 column
%1''.  (Note that the ruler will not be present in the final copy, so is not
%an alternative to equation numbers).  All authors will benefit from reading
%Mermin's description of how to write mathematics:
%\url{http://www.pamitc.org/documents/mermin.pdf}.
%
%
%\subsection{Blind review}
%
%Many authors misunderstand the concept of anonymizing for blind
%review.  Blind review does not mean that one must remove
%citations to one's own work---in fact it is often impossible to
%review a paper unless the previous citations are known and
%available.
%
%Blind review means that you do not use the words ``my'' or ``our''
%when citing previous work.  That is all.  (But see below for
%techreports.)
%
%Saying ``this builds on the work of Lucy Smith [1]'' does not say
%that you are Lucy Smith; it says that you are building on her
%work.  If you are Smith and Jones, do not say ``as we show in
%[7]'', say ``as Smith and Jones show in [7]'' and at the end of the
%paper, include reference 7 as you would any other cited work.
%
%An example of a bad paper just asking to be rejected:
%\begin{quote}
%\begin{center}
%    An analysis of the frobnicatable foo filter.
%\end{center}
%
%   In this paper we present a performance analysis of our
%   previous paper [1], and show it to be inferior to all
%   previously known methods.  Why the previous paper was
%   accepted without this analysis is beyond me.
%
%   [1] Removed for blind review
%\end{quote}
%
%
%An example of an acceptable paper:
%
%\begin{quote}
%\begin{center}
%     An analysis of the frobnicatable foo filter.
%\end{center}
%
%   In this paper we present a performance analysis of the
%   paper of Smith \etal [1], and show it to be inferior to
%   all previously known methods.  Why the previous paper
%   was accepted without this analysis is beyond me.
%
%   [1] Smith, L and Jones, C. ``The frobnicatable foo
%   filter, a fundamental contribution to human knowledge''.
%   Nature 381(12), 1-213.
%\end{quote}
%
%If you are making a submission to another conference at the same time,
%which covers similar or overlapping material, you may need to refer to that
%submission in order to explain the differences, just as you would if you
%had previously published related work.  In such cases, include the
%anonymized parallel submission~\cite{Authors14} as additional material and
%cite it as
%\begin{quote}
%[1] Authors. ``The frobnicatable foo filter'', F\&G 2014 Submission ID 324,
%Supplied as additional material {\tt fg324.pdf}.
%\end{quote}
%
%Finally, you may feel you need to tell the reader that more details can be
%found elsewhere, and refer them to a technical report.  For conference
%submissions, the paper must stand on its own, and not {\em require} the
%reviewer to go to a techreport for further details.  Thus, you may say in
%the body of the paper ``further details may be found
%in~\cite{Authors14b}''.  Then submit the techreport as additional material.
%Again, you may not assume the reviewers will read this material.
%
%Sometimes your paper is about a problem which you tested using a tool which
%is widely known to be restricted to a single institution.  For example,
%let's say it's 1969, you have solved a key problem on the Apollo lander,
%and you believe that the CVPR70 audience would like to hear about your
%solution.  The work is a development of your celebrated 1968 paper entitled
%``Zero-g frobnication: How being the only people in the world with access to
%the Apollo lander source code makes us a wow at parties'', by Zeus \etal.
%
%You can handle this paper like any other.  Don't write ``We show how to
%improve our previous work [Anonymous, 1968].  This time we tested the
%algorithm on a lunar lander [name of lander removed for blind review]''.
%That would be silly, and would immediately identify the authors. Instead
%write the following:
%\begin{quotation}
%\noindent
%   We describe a system for zero-g frobnication.  This
%   system is new because it handles the following cases:
%   A, B.  Previous systems [Zeus et al. 1968] didn't
%   handle case B properly.  Ours handles it by including
%   a foo term in the bar integral.
%
%   ...
%
%   The proposed system was integrated with the Apollo
%   lunar lander, and went all the way to the moon, don't
%   you know.  It displayed the following behaviours
%   which show how well we solved cases A and B: ...
%\end{quotation}
%As you can see, the above text follows standard scientific convention,
%reads better than the first version, and does not explicitly name you as
%the authors.  A reviewer might think it likely that the new paper was
%written by Zeus \etal, but cannot make any decision based on that guess.
%He or she would have to be sure that no other authors could have been
%contracted to solve problem B.
%
%FAQ: Are acknowledgements OK?  No.  Leave them for the final copy.
%
%
%\begin{figure}[t]
%\begin{center}
%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
%\end{center}
%   \caption{Example of caption.  It is set in Roman so that mathematics
%   (always set in Roman: $B \sin A = A \sin B$) may be included without an
%   ugly clash.}
%\label{fig:long}
%\label{fig:onecol}
%\end{figure}
%
%\subsection{Miscellaneous}
%
%\noindent
%Compare the following:\\
%\begin{tabular}{ll}
% \verb'$conf_a$' &  $conf_a$ \\
% \verb'$\mathit{conf}_a$' & $\mathit{conf}_a$
%\end{tabular}\\
%See The \TeX book, p165.
%
%The space after \eg, meaning ``for example'', should not be a
%sentence-ending space. So \eg is correct, {\em e.g.} is not.  The provided
%\verb'\eg' macro takes care of this.
%
%When citing a multi-author paper, you may save space by using ``et alia'',
%shortened to ``\etal'' (not ``{\em et.\ al.}'' as ``{\em et}'' is a complete word.)
%However, use it only when there are three or more authors.  Thus, the
%following is correct: ``
%   Frobnication has been trendy lately.
%   It was introduced by Alpher~\cite{Alpher02}, and subsequently developed by
%   Alpher and Fotheringham-Smythe~\cite{Alpher03}, and Alpher \etal~\cite{Alpher04}.''
%
%This is incorrect: ``... subsequently developed by Alpher \etal~\cite{Alpher03} ...''
%because reference~\cite{Alpher03} has just two authors.  If you use the
%\verb'\etal' macro provided, then you need not worry about double periods
%when used at the end of a sentence as in Alpher \etal.
%
%For this citation style, keep multiple citations in numerical (not
%chronological) order, so prefer \cite{Alpher03,Alpher02,Authors14} to
%\cite{Alpher02,Alpher03,Authors14}.
%
%
%\begin{figure*}
%\begin{center}
%\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%\end{center}
%   \caption{Example of a short caption, which should be centered.}
%\label{fig:short}
%\end{figure*}
%
%%------------------------------------------------------------------------
%\section{Formatting your paper}
%
%All text must be in a two-column format. The total allowable width of the
%text area is $6\frac78$ inches (17.5 cm) wide by $8\frac78$ inches (22.54
%cm) high. Columns are to be $3\frac14$ inches (8.25 cm) wide, with a
%$\frac{5}{16}$ inch (0.8 cm) space between them. The main title (on the
%first page) should begin 1.0 inch (2.54 cm) from the top edge of the
%page. The second and following pages should begin 1.0 inch (2.54 cm) from
%the top edge. On all pages, the bottom margin should be 1-1/8 inches (2.86
%cm) from the bottom edge of the page for $8.5 \times 11$-inch paper; for A4
%paper, approximately 1-5/8 inches (4.13 cm) from the bottom edge of the
%page.
%
%%-------------------------------------------------------------------------
%\subsection{Margins and page numbering}
%
%All printed material, including text, illustrations, and charts, must be kept
%within a print area 6-7/8 inches (17.5 cm) wide by 8-7/8 inches (22.54 cm)
%high.
%
%
%
%%-------------------------------------------------------------------------
%\subsection{Type-style and fonts}
%
%Wherever Times is specified, Times Roman may also be used. If neither is
%available on your word processor, please use the font closest in
%appearance to Times to which you have access.
%
%MAIN TITLE. Center the title 1-3/8 inches (3.49 cm) from the top edge of
%the first page. The title should be in Times 14-point, boldface type.
%Capitalize the first letter of nouns, pronouns, verbs, adjectives, and
%adverbs; do not capitalize articles, coordinate conjunctions, or
%prepositions (unless the title begins with such a word). Leave two blank
%lines after the title.
%
%AUTHOR NAME(s) and AFFILIATION(s) are to be centered beneath the title
%and printed in Times 12-point, non-boldface type. This information is to
%be followed by two blank lines.
%
%The ABSTRACT and MAIN TEXT are to be in a two-column format.
%
%MAIN TEXT. Type main text in 10-point Times, single-spaced. Do NOT use
%double-spacing. All paragraphs should be indented 1 pica (approx. 1/6
%inch or 0.422 cm). Make sure your text is fully justified---that is,
%flush left and flush right. Please do not place any additional blank
%lines between paragraphs.
%
%Figure and table captions should be 9-point Roman type as in
%Figures~\ref{fig:onecol} and~\ref{fig:short}.  Short captions should be centred.
%
%\noindent Callouts should be 9-point Helvetica, non-boldface type.
%Initially capitalize only the first word of section titles and first-,
%second-, and third-order headings.
%
%FIRST-ORDER HEADINGS. (For example, {\large \bf 1. Introduction})
%should be Times 12-point boldface, initially capitalized, flush left,
%with one blank line before, and one blank line after.
%
%SECOND-ORDER HEADINGS. (For example, { \bf 1.1. Database elements})
%should be Times 11-point boldface, initially capitalized, flush left,
%with one blank line before, and one after. If you require a third-order
%heading (we discourage it), use 10-point Times, boldface, initially
%capitalized, flush left, preceded by one blank line, followed by a period
%and your text on the same line.
%
%%-------------------------------------------------------------------------
%\subsection{Footnotes}
%
%Please use footnotes\footnote {This is what a footnote looks like.  It
%often distracts the reader from the main flow of the argument.} sparingly.
%Indeed, try to avoid footnotes altogether and include necessary peripheral
%observations in
%the text (within parentheses, if you prefer, as in this sentence).  If you
%wish to use a footnote, place it at the bottom of the column on the page on
%which it is referenced. Use Times 8-point type, single-spaced.
%
%
%%-------------------------------------------------------------------------
%\subsection{References}
%
%List and number all bibliographical references in 9-point Times,
%single-spaced, at the end of your paper. When referenced in the text,
%enclose the citation number in square brackets, for
%example~\cite{Authors14}.  Where appropriate, include the name(s) of
%editors of referenced books.
%
%\begin{table}
%\begin{center}
%\begin{tabular}{|l|c|}
%\hline
%Method & Frobnability \\
%\hline\hline
%Theirs & Frumpy \\
%Yours & Frobbly \\
%Ours & Makes one's heart Frob\\
%\hline
%\end{tabular}
%\end{center}
%\caption{Results.   Ours is better.}
%\end{table}
%
%%-------------------------------------------------------------------------
%\subsection{Illustrations, graphs, and photographs}
%
%All graphics should be centered.  Please ensure that any point you wish to
%make is resolvable in a printed copy of the paper.  Resize fonts in figures
%to match the font in the body text, and choose line widths which render
%effectively in print.  Many readers (and reviewers), even of an electronic
%copy, will choose to print your paper in order to read it.  You cannot
%insist that they do otherwise, and therefore must not assume that they can
%zoom in to see tiny details on a graphic.
%
%When placing figures in \LaTeX, it's almost always best to use
%\verb+\includegraphics+, and to specify the  figure width as a multiple of
%the line width as in the example below
%{\small\begin{verbatim}
%   \usepackage[dvips]{graphicx} ...
%   \includegraphics[width=0.8\linewidth]
%                   {myfile.eps}
%\end{verbatim}
%}
%
%
%%-------------------------------------------------------------------------
%\subsection{Color}
%
%Please refer to the author guidelines on the CVPR 2016 web page for a discussion
%of the use of color in your document.
%
%%------------------------------------------------------------------------
%\section{Final copy}
%
%You must include your signed IEEE copyright release form when you submit
%your finished paper. We MUST have this form before your paper can be
%published in the proceedings.
%
%Please direct any questions to the production editor in charge of these
%proceedings at the IEEE Computer Society Press: Phone (714) 821-8380, or
%Fax (714) 761-1784.

