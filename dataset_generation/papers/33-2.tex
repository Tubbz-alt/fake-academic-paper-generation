\documentclass[final,t]{beamer}

\usepackage{graphicx}

\usepackage{wrapfig}
\usepackage{resizegather}

\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\EE}[1]{\mathbb{E} \left[ #1 \right]}
\newcommand{\VV}[1]{\mathbb{V} \left[ #1 \right]}
\newcommand{\Cov}[1]{\mathrm{Cov} \left( #1 \right)}

\hyphenpenalty=1000

% poster template
% \usepackage[orientation=landscape,size=a0,scale=1.4,debug]{beamerposter}
% \usepackage[orientation=portrait,size=a0,scale=1.4,debug]{beamerposter}
\usepackage[orientation=landscape,size=custom,width=141.4,height=100,scale=1.7,debug]{beamerposter}
\usepackage[ruled, vlined]{algorithm2e}
\usetheme{zurichposter}

% references
\usepackage[bibstyle=authoryear, citestyle=authoryear-comp,%
hyperref=auto]{biblatex}
\bibliography{references}

% document properties
\title{Graph Hypernetworks for Neural Architecture Search}
\author{Chris J. Zhang$^{2, 3}$, Mengye Ren$^{1, 3}$, Raquel Urtasun$^{1,3}$
}
\institute{
$^1$University of Toronto, 
$^2$Unversity of Waterloo, 
$^3$Uber Advanced Technologies Group}


%------------------------------------------------------------------------------
\begin{document}

\begin{frame}{}
\begin{columns}[t]

%-----------------------------------------------------------------------------
%                                                                     COLUMN 1
% ----------------------------------------------------------------------------
\begin{column}{.32\linewidth}
    % Summary
    \vskip -0.5cm
    \begin{exampleblock}{Summary}
        \begin{itemize}
            \item Learning rate and momentum schedules can be crucial to effective training.
            %\item Currently relying on a hand-tuned schedule, even adaptive gradient methods.
            \item Goal: understand the \textbf{difficulty} of automatic schedules by optimizing a \textbf{truncated-horizon} model.
            
            %\item In a \textbf{deterministic} quadratic model, we show greedy look ahead solves the 
            %optimal learning rate and momentum.
            \item \textbf{Short-horizon bias:} optimizing for an \textbf{immediate} reward; reducing the loss on high curvature, via a \textbf{too conservative} learning rate.
            \item The bias stems from the combination of \textbf{stochasticity} and \textbf{ill-conditioning}.
            \item \textbf{Stochastic Meta-Descent} (SMD) to validate our hypotheses.
            \item \textbf{Greedy} learning rate is provably \textbf{optimal} with a perfect preconditioner. 
        \end{itemize}
    \end{exampleblock}
    
    \vspace{0.75in}
	\begin{exampleblock}{A Cartoon Picture}
     \begin{tabular}{p{0.45\textwidth}p{0.45\textwidth}}
	\begin{figure}
	\centering
	\includegraphics[width=20cm,trim={0cm 0.1cm 0cm 0.5cm},clip]{figs/quadratics/cartoon.pdf}
	\end{figure}
    &\vspace{0.4in}
    Aggressive learning rate (red) 
    
    followed by a decay schedule
    (yellow) wins over conservative learning rate (blue) by making more progress along the low curvature.
    \end{tabular}
    \vspace{-0.8in}
	\end{exampleblock}
    
    \vspace{0.8in}
    \begin{exampleblock}{Quadratic Experiment}
    \begin{itemize}
    \item In noisy quadratic case, greedy learning rate and momentum first eliminate high curvature losses, whereas the optimal learning rate and momentum first eliminate low curvature losses.
    \item In deterministic quadratic case, greedy aligned with optimal.
    \end{itemize}
        \begin{figure}[t!]
        \begin{center}
        \begin{tabular}{p{0.45\textwidth}p{0.45\textwidth}}%p{5cm}}
        \hspace{0pt}\includegraphics[width=0.45\textwidth]{figs/quadratics/noisy_losses}&\includegraphics[width=0.45\textwidth]{figs/quadratics/deter_losses} \\
        \vspace{-0.2in}
        \hspace{3pt}\includegraphics[width=0.445\textwidth]{figs/quadratics/noisy_lr}&\vspace{-0.2in}\hspace{5pt}\includegraphics[width=0.45\textwidth]{figs/quadratics/deter_lr}\\
        \vspace{-0.2in}\hspace{1pt}
        \includegraphics[width=0.445\textwidth]{figs/quadratics/noisy_mom}&\vspace{-0.2in}\hspace{7pt}\includegraphics[width=0.44\textwidth]{figs/quadratics/deter_mom}%\\
        %\small\hspace{1.3in}(a)&\small\hspace{1.3in}(b)
        \end{tabular}
        \vspace{-0.1in}
        %\caption{\small Quadratic experiment}
        \label{fig:noisy_quadratic}
        \end{center}
        \vspace{-0.7cm}
        \end{figure}
    \end{exampleblock}

\end{column}
\vspace{0pt}


%-----------------------------------------------------------------------------
%                                                                     COLUMN 2
% ----------------------------------------------------------------------------

\begin{column}{.32\linewidth}
    \vskip -0.5cm
    
    \begin{exampleblock}{Optimization intuition}
    \begin{itemize}
            \item In general, it is better to use \textbf{small} learning rate for \textbf{high} curvature; \textbf{large} learning rate for \textbf{low} curvature.
            \item In high dimension, which direction of the loss should be eliminated first?
            	\begin{itemize}
                \item \textbf{Deterministic} case: It does not matter.
                \item \textbf{Noisy} case:  Eliminate the losses on the \textbf{low} curvature directions \textbf{first}. Why?
                \end{itemize}
            \item \textbf{Noise} in the objective does not allow the loss go down to exactly zero. 
            \item If we optimize \textbf{high} curvature direction first, we will use \textbf{small} learning rate followed by \textbf{large} learning rate. However, when we increase the learning rate, the loss along high curvature direction will \textbf{rise} again.
            \item One should \textbf{keep at a high} learning rate \textbf{first} and then \textbf{decay}.
        \end{itemize}
    \end{exampleblock}
    
    \vspace{0.75in}
    
    \begin{exampleblock}{Analysis}
    \textbf{Noisy quadratic problem}:
    \begin{equation*}
    J(\boldsymbol{\theta}) = \mathbb{E}_{\mathbf{c}\sim\mathcal{P}}\left[\sum_i \frac{1}{2}h_{i}(\theta_{i}-c_{i})^2\right] = \frac{1}{2}\sum_ih_{i}((\theta_{i}-\theta_i^*)^2+\sigma_{i}^2).
    \end{equation*}
    \vskip -0.5cm
	\begin{itemize}
        \item We derived the mean and variance following SGD with momentum dynamics. 
        \item We derived the greedy learning rate and momentum.
        \item Greedy learning rate is optimal in the univariate vanilla SGD case.
    \end{itemize}
    
    \vspace{0.6in}
\textbf{Theorem 1} (\emph{Mean and variance dynamics})
    The expectations and the variances of the parameter $\theta$ and the velocity $v$ at each time step can be computed analytically using dynamic programming.
    
    \vspace{0.6in}
    \textbf{Theorem 2} (\emph{Greedy learning rate and momentum}) The greedy learning rate and momentum is given by, $(A(\cdot) := \mathbb{E}(\cdot)^2+\mathbb{V}[\cdot])$
    \begin{footnotesize}
  \begin{align*}
  &\alpha^{(t)*} =\frac{\sum_i \left(h_{i}^2A(\theta^{(t)}_{i})\Big(\sum_j h_{j}A(v^{(t)}_{j})\Big)-\Big(\sum_jh_{j}\mathbb{E}[\theta^{(t)}_{j}v^{(t)}_{j}]\Big) h_{i}^2\mathbb{E}[\theta^{(t)}_{i}v^{(t)}_{i}]\right)}{\sum_i\left( \left(h_{i}^3(A(\theta^{(t)}_{i})+(\sigma_{i})^2)\right)\Big(\sum_j h_{j}A(v^{(t)}_{j})\Big)-\Big(\sum_j(h_{j})^2\mathbb{E}[\theta^{(t)}_{j}v^{(t)}_{j}]\Big) h_{i}^2\mathbb{E}[\theta^{(t)}_{i}v^{(t)}_{i}]\right)}\\
  &\mu^{(t)*} = -\frac{\sum_i h_{i}(1-\alpha^{(t)*}  h_{i})\mathbb{E}[\theta^{(t)}_{i}v^{(t)}_{i}]}{\sum_i h_{i}A(v^{(t)}_{i})}\\
  \end{align*}
    \end{footnotesize}
    \hspace{-0.5cm}\textbf{Theorem 3} In 1-D, via vanilla SGD, for all $T\in\mathbb{N}$, the sequence of learning rate $\{\alpha^{(t)*}\}_{t=1}^{T-1}$ that minimizes the loss at time step $T$, $\mathcal{L}(\theta^T)$ is given by,
    \begin{equation*}
        \alpha^{(t)*} = \frac{A(\theta^{(t)})}{h(A(\theta^{(t)})+\sigma^2)}.
    \end{equation*}
    Moreover, this agrees with the greedy optimal learning rate.
    
    \vspace{0.5in}
    \textbf{Remark} If the Hessian and the gradient covariance are both spherical, then each dimension evolves identically and independently according to the 1-D dynamics. In principle, with a good enough preconditioner, the Hessian and the gradient covariance would be close enough to spherical that a greedy choice of $\alpha$ and $\mu$ would perform well. 
    % \end{theorem}
    \end{exampleblock}
    
\end{column}
\vspace{0pt}

\begin{column}{.32\linewidth}
    \vskip -0.5cm
    
    \begin{exampleblock}{Background: Stochastic Meta-Descent (SMD)}
    \begin{itemize}
        \item Idea: adaptively tune hyper-parameters by taking their gradients.
        \item Objective: tune learning rate to minimize the training loss after $T$ steps.
%         \item Original SMD: 1) learning rate on each dim., 2) approx. hyper-parameter gradients.
%         \item We study: 1) global learning rate, 2) exact hyper-parameter gradient.
        \item Use forward-mode autodiff to take gradients wrt. the learning rate.
%         \item Update on hyper-parameter on the log space.
    \end{itemize}
    \vskip 0.5cm
    \end{exampleblock}
    
    \begin{exampleblock}{Offline SMD Experiment}
    \begin{itemize}
	\item Meta-optimization by looking ahead $T$ steps from the very start.
    \item We look ahead 100, 1000, 5000, and 20,000 steps
    \end{itemize}
        \begin{figure}[t!]
        \centering
     \includegraphics[width=0.9\textwidth]{figs/surface/offline_pt50_surface_traj.pdf}
        \label{fig:sgd_best_decay}
        \end{figure}
        \begin{itemize}
        	\item Learning rate schedule:
            $\alpha_t = \frac{\alpha_0}{(1 + \frac{t}{K})^\beta}$. Optimize $\alpha_0$ and $\beta$.
            \item Training an MLP on MNIST.
            \item 2.5k random samples of hyper-parameters to visualize the surface.
            \item Minimize the loss after $T$ steps, for $T$ ranging from 100 to 20,000.
            \item More aggressive solution of $\alpha_0$ and $\beta$ if looking farther ahead.
        \end{itemize}
    \end{exampleblock}
    \begin{exampleblock}{Online SMD Experiment}
    \begin{itemize}
	\item Adapt learning rate along with training, by looking ahead $T$ steps
    \end{itemize}
        \begin{figure}
        \centering\includegraphics[width=0.42\textwidth]{figs/smd_init_lr_m100_mnist/combined.pdf}
	\quad
  \includegraphics[width=0.42\textwidth]{figs/smd_init_lr_m100_cifar/combined.pdf}
        \label{fig:smd_init_lr}
        \end{figure}
        \vspace{-0.2in}
        \begin{itemize}
        	\item Minimize loss 5 steps ahead.
            \item 100 Adam steps on log learning rate for every 10 training steps.
            \item Different initial learning rate: converge to similar solution.
            \item SMD learns too small learning rate: little progress in the long run.
		\end{itemize}
	\end{exampleblock}
    \begin{exampleblock}{Conclusion}
    \begin{itemize}
    \item Our analysis on the noisy quadratics helps explain the short-horizon bias, when the problem is both ill-conditioned and stochastic.
    \item The experiments verify that short-horizon bias exists in practice.
    \item Code: \url{https://github.com/renmengye/meta-optim-public}
    \end{itemize}
    \end{exampleblock}
\end{column}

\end{columns}
\end{frame}
\end{document}
