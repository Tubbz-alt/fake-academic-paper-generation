%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

\usepackage[cmex10]{amsmath}
%\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{float}
\usepackage{array}
\usepackage{epstopdf}
\usepackage{multirow}
\newcommand{\argmin}{\arg\!\min}
\newcommand{\argmax}{\arg\!\max}

\def\x{{\mathbf x}}
\def\L{{\cal L}}

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed




\title{\LARGE \bf
A Deep Unsupervised Learning Approach Toward MTBI Identification Using Diffusion MRI}


\author{Shervin~Minaee$^{1}$,  Yao~Wang$^1$, Anna Choromanska$^1$, Sohae Chung$^2$, Xiuyuan Wang$^2$, Els Fieremans$^2$ \\
Steven Flanagan$^3$, Joseph Rath$^3$, Yvonne W. Lui$^2$% <-this % stops a space
\thanks{*This work was supported by  National Institutes of Health (NIH) under award number  R21NS090349.}%
\thanks{$^{1}$Shervin Minaee, Yao Wang and Anna Choromanska are with the department of Electrical and Computer Engineering, New York University, New York, NY, 11201, USA
        {\tt\small (email:\ shervin.minaee@nyu.edu)}.} %
\thanks{$^{2}$Sohae Chung, Xiuyuan Wang, Els Fieremans, and Yvonne W. Lui are with the Department of Radiology, New York University
        New York, NY, 10016, USA.
       }
\thanks{$^{3}$Steven Flanagan and Joseph Rath are with the Department of Rehabilitation Medicine, New York University, New York, NY, 10016, USA.
       }
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Mild traumatic brain injury is a growing public health problem with an estimated incidence of over 1.7 million people annually in US. 
Diagnosis is based on clinical history and symptoms, and accurate, concrete measures of injury are lacking.
This work aims to directly use diffusion MR images obtained within one month of trauma to detect injury, by incorporating deep learning techniques.
To overcome the challenge due to limited training data,  we describe each brain region using the bag of word representation, which specifies  the distribution of representative patch patterns. 
We apply a convolutional auto-encoder to learn  the patch-level features, from overlapping image patches extracted from the MR images,  to learn features from  diffusion MR images of brain using an unsupervised approach.
Our experimental results show that the bag of word representation using  patch level features learnt by the auto encoder provides similar performance as that using the raw patch patterns, both significantly outperform earlier work relying on the mean values of MR metrics in selected brain regions.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
Mild traumatic brain injury (mTBI) is a growing public health problem, which can lead to a variety of problems including persistent headache, memory and attention deficits, as well as affective symptoms.
%In addition to civilian head trauma, we are now faced with sport-related, and U.S. military-related brain injuries \cite{mtbi1}.
%Up to 20-30\% of patients with mTBI develop persistent symptoms months to years after the initial injury, referred to as post-concussive syndrome (PCS).
%Currently, several different definitions of mTBI exist \cite{acrm}-\cite{dva}. 
%including World Health Organization, American Congress of Rehabilitation Medicine [ACRM] \cite{acrm},  Centers for Disease Control and Prevention \cite{cdcp}, Department of Defense \cite{dod}, and Department of Veteran Affairs \cite{dva}. 
%There is universal agreement that a unified, objective definition is needed. 
%Furthermore, most identification schemes rely on Glasgow Coma Scale score, which was recently deemed insufficient for diagnosing  traumatic brain injury by the National Institute for Neurological Disorders and Stroke, which proposed that neuroimaging have a larger role in the classification scheme for mTBI.
A building body of works show that diffusion MRI can reveal subtle brain injury; however, no single imaging metric has been shown to be sufficient as an independent biomarker \cite{lui3}-\cite{lui5}.


While diffusion MRI is extremely promising in the study of mTBI, a definitive method of diagnosing patients with recent mTBI remains a challenge.
Both gray matter such as the thalamus, and white matter such as the corpus callosum (CC) and frontal deep white matter have been repeatedly implicated as areas at high risk for injury in the literature.
While there have been a few previous studies using machine learning for mTBI identification from MR images \cite{yuanyi}-\cite{minaee2},  the features used in those works are mainly hand-crafted and may  not be the most discriminative features for this task.
%It is worth to mention that one could also use the mean value of (some of ) the above metrics as the features from MR images. The mean values can be calculated over different regions such as: thalamus, pre-frontal white-matter, corpus callosum (CC) Body, CC-Genu, and CC-Splenium (which are focused in our work). As one may guess, the mean value may not be the best way to extract features from a brain MRI.


In this work, we develop a machine learning framework to classify mTBI patients and controls using features extracted from diffusion MRI in the thalamus and corpus callosum, two regions highly implicated in imaging based on previous works \cite{thal}-\cite{cc}.
The main challenge for using a machine learning approach is that we only have limited samples (114 subjects), and each sample has a very high dimensional raw representation (multiple 3D volumes). Therefore, it is not possible to directly train a classification network using the raw MRI volume data as the input.
We  propose a new approach for feature extraction from MR images, where  we first learn the feature representation of patches using a deep unsupervised learning approach \cite{dnn}, and then aggregate the features from different patches through a bag of word representation, and use them along with demographic and neuro-cognitive test features as the overall feature vector.
We then use feature selection followed by a classification algorithm to identify mTBI patients.
The block diagram of the overall algorithm is shown in Fig. 1.
Through experimental study, we show that by learning patch level deep features and aggregating them through
a bag of word representation for each brain region, we can achieve much higher accuracy compared to using mean values of the various MR metrics in each region.
\begin{figure}[h]
\begin{center}
    \includegraphics[scale=0.42]{overall2.png}
\end{center}
  \caption{The block-diagram of the proposed mTBI identification algorithm}
\end{figure}


%This approach provides a powerful scheme to learn a global representation by aggregating deep features from local regions, and is especially useful for datasets with limited number of samples but high dimensional input data.
%and would be very useful for medical image analysis. 
%Through experimental study, we show that by learning deep features through a bag of word representation, we get much higher accuracy compared to using hand-crafted features. 


The rest of the paper is organized as follows. 
Section II provides the description of the proposed framework. 
Section III provides the experimental studies and comparison to other works. 
And finally the paper is concluded in Section IV.

\section{The Proposed Framework}
There have been some previous studies on mTBI classification using various sets of features, from demographic (such as age and gender) and neurocognitive  to hand-crafted features from medical images.
%So far, none of these features alone have been able to classify a person as mTBI patient.
In this work we propose a framework for mTBI identification that incorporates all the imaging, demographics, and neuro-cognitive features.
We derive the imaging features from multi-shell diffusion MR imaging, and bicompartment modeling based on white matter tract integrity (WMTI) metrics derived from diffusion kurtosis imaging (DKI) \cite{els}, that are shown to be promising for assessment of mTBI patients against controls \cite{lui3}-\cite{lui5}.
These metrics are summarized in Table I.
\begin{table}[h]
\centering
\caption{MRI metrics description}
\begin{tabular}{| m{2cm} | m{5.5cm} |}
\hline
MRI Metric & Metric Description  \\
\hline
AWF & Axonal Water Fraction  \\
\hline
Da & Diffusivity within Axons  \\
\hline
De-par & Diffusion parallel to the axonal tracts in the extra-axonal  \\
\hline
De-perp & Diffusion perpendicular to the axonal tracts in the extra-axonal  \\
\hline
FA & Fractional Anisotropy  \\
\hline
MD & Mean Diffusion  \\
\hline
AK,\MK,\RK & Axial Kurtosis, Mean Kurtosis, Radial Kurtosis  \\
\hline
\end{tabular}
  \label{tab:1}
\end{table}


%Demographic and neurocognitive features are easy to derive, but for imaging features, it is not clear what is the best feature set to use.
%For demographic features, age and sex are used in this paper.
%Many of the previous works used hand-crafted features for image representation, but since it is not clear beforehand which imaging features are the best for mTBI identification, we learn the features from MR images using a deep learning framework.

Because of the limitation of the number of samples, it is not possible to train a deep convolutional network to directly classify the brain images. 
To tackle this problem, and also based on the assumption that mTBI may impact only certain regions in the brain, we propose to represent each brain region by a bag of words (BoW) representation, which is the histogram of different clustered  patch (i.e. a small 2D slice in brain) patterns among all patches in the region. 

A main challenge in using such a representation is how to describe each local patch. 
Because mTBI does not necessarily impact all patches, we cannot infer the patch-level  labels from subject-level labels. 
Therefore we cannot learn patch-level features through supervised learning. To overcome this problem, we apply an unsupervised learning approach at the patch level, and train an auto-encoder to learn features that can be used to reconstruct patches.
In the following,  we first explain how we learn patch level features through training an auto-encoder, and then describe how we aggregate the patch-level features using a bag of words representation for each region, and finally how to combine the region representations and other information for patient-level classification.


\subsection{Patch Feature Learning Using an Auto-Encoder}
In order to learn patch-level features without having labels, we employ a convolutional auto-encoder \cite{CAE}-\cite{cae2},  an unsupervised feature learning approach.
%But on the other hand we would no longer have the label for these patches, as an mTBI patient does not mean that all patches from different parts of the brain would be impacted. Therefore we cannot use supervised learning approaches and need to go toward unsupervised feature learning.
Convolutional auto-encoder receives an image patch as the input and performs multiple convolution plus downsampling layers to encode the image into some latent representation, and then uses these features to reconstruct the original patch  through deconvolution.
By doing so,  the network is forced to learn some representative information that is sufficient to recover the image.
The overall architecture of a convolutional auto-encoder is shown in Figure 2.
%It can be seen that the network is consist of two main parts, an encoder and a decoder.
Here, each layer of the network performs three operations: convolution, nonlinear transformation, and pooling (downsampling).
After training this model, the latent representation in the mid-layer is used as patch feature representation.
\begin{figure}[h]
\begin{center}
    \includegraphics[scale=0.34]{CAE.png}
\end{center}
  \caption{The block-diagram of the proposed convolutional auto-encoder}
\end{figure}



In our study we consider two scenarios when training the convolutional auto-encoder.
In one scenario, we train one model for each metric (such as FA, MK, RK, etc.).
In the other scenario, we concatenate all metrics together (treat them as different channels) to form a 3D patch and train one convolutional auto-encoder.
We use the same model for all regions (Thalamus and CC).

 
\subsection{Bag of Visual Words}
Once the features are extracted from each patch, 
%we need to somehow aggregate the feature representation of different patches to get a global imaging feature for each patient.
%One simple way could be to get the average representation of patch features as the overall representation. But that would be over simplification. 
we use the bag of words (BoW) representation  \cite{bow1} to describe each brain region, which calculates the histogram of representative patterns (or visual words) over all patches in this region.
%Bag of visual words is a popular approach in computer vision, which is used for various applications \cite{bow2}-\cite{bow3}.
%The idea of bag of visual words in computer vision is inspired by bag of word representation in text analysis, where a document is represented as a histogram of words, and those histograms are used to analyze the text documents. In the same way, one can represent an image (or video) as a histogram of visual words. Since there is no intrinsic word defined for images, we need to first create the visual words.  A popular approach is to use such as k-means clustering algorithms, to cluster these patches into $K$ clusters, and use their centroids as the visual words. Here we apply BoW approach on the deep features extracted from each patch, and use the histogram of deep features as the brain representation.
To find the visual words, we apply the K-means clustering algorithm to the patch features obtained for all training patches. Given the MR images of a subject, we extract overlapping patches from each of the two designated brain regions (Thalamus and CC) Each patch is quantized to its closest visual words, and each metric in each region is described by a histogram of different visual words among all patches in this region. In the case where we trained a single auto-encoder to generate a single feature representation for all metrics together, each region is represented by a single histogram.
The block diagram of the BoW approach is shown in Figure 3.
In this figure, for better visualization we show patches from different parts of brain, but in our work we only focused on Thalamus and CC.
\begin{figure}[h]
\begin{center}
    \includegraphics[scale=0.27]{BoW_blk.png}
\end{center}
  \caption{The block-diagram of the proposed BoW approach. }
\end{figure}



\subsection{Feature Selection and Classification}
After deriving deep-bag-of-words features from diffusion MR images, we will get two feature vectors, one for Thalamus region and the other for CC.
We concatenate the features from both regions (and all metrics), with demographic and neuro-cognitive test features, to form the final feature vector.
%Depending on the number of words in BoW features the final feature vector dimensionality could be much larger than the number of samples.
We then perform feature selection \cite{fs1}-\cite{fs2} to minimize the risk of over-fitting before classification.
We tried multiple feature selection approaches such as max-relevance and min-redundancy (MRMR) \cite{mrmr}, maximum correlation, greedy forward selection, and it turns out that the greedy forward feature selection works best for our problem.
This approach selects the best features one at a time  with a given classifier, through a cross-validation approach. 
%Assuming $S_k$ denotes the best subset of features of size $k$, the $(k+1)$-th feature is selected as the one which results in the highest cross-validation accuracy rate along with the features already chosen (in $S_k$).
%One can stop adding feature, either by setting a maximum size for the feature set, or when adding more features does not increase the accuracy rate.
%After selecting the feature subset, we use support vector machine to classify samples into patient and control. 
For classification, we tried different classifiers (such as SVM, neural network, and random forest) and SVM was chosen because it generally gave better performance.



\section{Experimental Results}
We evaluate the performance of the proposed approach on our dataset of 114 subjects. 
This dataset contains 70 mTBI subjects between 18 and 64 years old, within 1 month of mTBI as defined by the American College of Rehabilitation Medicine (ACRM) criteria for head injury and 44 healthy age and sex-matched controls. 
%Imaging was performed on a 3.0 Tesla Siemens Trio (Erlangen, Germany) magnet including multi-shell diffusion MRI at 5 b-values (250, 1000, 1500, 2000, 2500 s/mm2) in a total of 136 directions using multiband 2 at isotropic 2.5mm image resolution.

To evaluate the model performance, we use  a cross validation approach, where each time we randomly take 20\% of the samples for validation, and the rest for training. 
We repeat this procedure 50 times (to decrease sampling bias), and report the average validation error as the model performance.
%For the forward feature selection process, for each candidate new feature, the training samples are used to train the SVM model, and the performance is evaluated on validation samples.

For the convolutional auto-encoder, we learn features on patches of 16x16 pixels in each image slice.
The encoder and decoder each have 4 layers, and the kernel size is always set to (3,3). 
The latent feature dimension is 32 for the networks which are trained on each metric, and 64 for the network which is trained on the stack of all metrics.
To train the model, the batch size is set to 500, and the model is trained for 10 epochs. The learning rate for the stochastic gradient descent is set to 0.0003.
The learnt auto-encoder is used to generate latent features on each overlapping patch in the training images. The resulting features are further clustered to $N$ words using K-means clustering. $N$ was varied among 20, 30, and 40. Each  MR metric in each region is represented by a histogram of $N$ dimension
For SVM, we use radial basis function (RBF) kernel. 
The hyper-parameters of SVM model (kernel width gamma, and the mis-classification penalty weight, C) are tuned based on the validation set.
%It is worth to mention that, we normalize all features before feeding as the input to SVM, by making them zero-mean and unit-variance.



%with best single feature, and optimum subset of features chosen by MRMR and greedy forward feature selection.
%Also, the best feature subset chosen by the greedy feature selection has an accuracy of 80\% with 8 features (De\_par in thalamus, De\_par and DA in pre-frontal white matter, FA in CC-Genu, AWF and De\_par in CC-Body, Stroop and SDMT).

In the first experiment, we compare the proposed approach with some previous works. 
For the proposed approach, the initial image feature representation is 260 dimensional, with 20 words for each of 8 MR metrics (AWF, DA, De\_par, FA, MD, AK, MK, RK) in CC, and 5 MR metrics in Thalamus (FA, MD, AK, MK, RK).
Together with additional 2 demographic and 4 clinical features (Stroop, SDMT, CVLT, FSS), the total feature dimension is 266.
We compare our approach with previous works that only used mean values of each MR metric in each region \cite{minaee1}, and also the work in \cite{yuanyi}, as well as BoW approach on raw patches \cite{minaee1} (that is, we find the visual words by applying the K-means clustering algorithm on the raw image patches directly).
It is worth  mentioning that the work in \cite{yuanyi} was using a different dataset, and an overlapping but different set of metrics.

With forward feature selection using the SVM classifier,  the  optimal feature subset contains 10 features FA in Thalamus, DA in CC, AWF in CC, AWF in CC, AWF in CC, FA in CC, RK in Thalamus, MK in Thalamus, RK in CC, FA in Thalamus. 
%The average cross-validation classification accuracy is 87.8\% for 20D BoW histogram and 90.1\% for 30D histograms.  
The comparison with previous works is provided in Table II.
As we can see, Deep-BoW approach achieves reasonable improvement over previously used features, except for BoW on raw patches where it performs slightly worse than using the raw pixel values. 
We believe the performance of Deep-BoW could be further improved by better tuning the deep network hyper-parameters.
\begin{table}[ht]
\centering
  \caption{Performance comparison of different approaches}
  \centering
\begin{tabular}{|m{5.3cm}|m{2.4cm}|}
\hline
The Algorithm  & Classification Rate on Validation Set\\
\hline 
Single best feature \cite{minaee1} &   \\ \\\\\\\72\% \\
\hline
The selected subset with 8 features \cite{minaee1}  &  \\\\ \\\ \\80\%\\
\hline
The algorithm in \cite{yuanyi} using selected features with MRMR, and Baysian Network   &  \\\ \\\\\\82\%\\
\hline
BoW on raw patches and 10 metrics \cite{minaee2}  &  \\\\\\\\ \85.5\%\\
\hline
The algorithm in \cite{yuanyi} using selected features with MRMR, and Neural Network  &  \\\\ \\\\\86\%\\
\hline
BoW on raw patches  with 20D histograms \cite{minaee1}  &  \\\\\\ \\  \92\%\\
\hline
The proposed Deep-BoW with 20D histograms  &  \\ \\ \\\\\87.8\%\\
\hline
The proposed Deep-BoW with 30D histograms  &  \\\ \ \\\\\90.1\%\\
\hline
\end{tabular}
\label{TblComp}
\end{table}

%One possible explanation is that BoW features are able to find more discriminative pattern between mTBI and control cohorts.

We have also evaluated the classification performance for feature subset of different size. 
Besides classification accuracy, we also report the sensitivity and specificity, which are important in the study of medical data analysis. 
The sensitivity and specificity are defined as in Eq. (1), where TP, FP, TN, and FN denote true positive, false positive, true negative, and false negative respectively. 
In our evaluation, we treat the mTBI subjects as positive. 
\begin{gather}
 \text{Sensitivity}= \frac{\text{TP}}{\text{TP+FN}} \ , 
\ \ \ \ \text{Specificity}= \frac{\text{TN}}{\text{TN+FP}} %%%% True Positive Rate 
\end{gather}
Figure 4 denotes the classification accuracies, sensitivities and specificities achieved by optimum subset of feature of dimension 1 to 10.
\begin{figure}[h]
\begin{center}
    \includegraphics[scale=0.21]{deepbow30d_diffsize.png}
\end{center}
\vspace{-0.2cm}
  \caption{The model performance for feature set of different size}
\end{figure}

In Table III, we provide a comparison between the two scenarios, where in one of them one network is trained per metrics, and in the other one a single network is trained over the stack of multiple metrics (for different number of words).

\begin{table}[ht]
\centering
  \caption{Performance comparison for different approaches}
  \centering
\begin{tabular}{|m{2.1cm}|m{1.25cm}|m{1.8cm}|m{1.5cm}|}
\hline
Auto-Encoder \\\Scenario  & Bow-Hist Dimension & Image Feature Dimension & Cross-Validation Accuracy\\
\hline
Multiple Network  & 20 & 260 & 87.8\% \\
\hline
Multiple Network  & 30 & 390  & 90.1\%  \\
\hline
%Multiple Network  & 40 & 520  & 90.3\% \\
%\hline
%Single Network  & 40 & 80  &  80.5\% \\
%\hline
%Single Network  & 80 & 160  & 83.6\% \\
%\hline
Single Network  & 130 & 260  & 89.4\% \\
\hline
Single Network  & 190 & 380  & 90\% \\
\hline
\end{tabular}
\label{TblComp}
\end{table}


To have a better generalization accuracy analysis, we also provide the comparison in terms of the accuracy on a heldout set.
In each run, we randomly pick 20 samples as the heldout set.
We then run cross validations 50 times within the remaining data, to generate 50 models, and use the ensemble of 50 models to make prediction on the held-out set and calculate the classification accuracy. 
We repeat this 6 times, each time with a different set of 20 heldout samples chosen randomly and report the average accuracy.
For this evaluation, we only compare two BoW based approaches, one derived by applying K-means on the raw patch images, and another by applying K-means on the patch level deep features. 
The average accuracy for the heldout sets for these two approaches are given in Table III. 
Although BoW using deep patch level features had lower cross validation accuracy in Table I, it shows equivalent heldout set accuracy.
For BoW on raw patches, the selected subset of features includes: FA in Thal (Thalamus), MK in Thal, FA in Thal,
Deperp in CC, MK in Thal, FA in CC, MD in
CC, AWF in CC, FA in CC.
For BoW using the deep features, the selected features include:  FA in Thal, DA in CC, AWF in CC, AWF in CC,
AWF in CC, FA in CC, RK in Thal, MK in Thal, RK in CC,
FA in Thal.
%For this comparison, we evaluate the model performance on a held-out set of 20 samples. In each run, we randomly pick 20 samples as  the heldout set. We then run cross validations 50 times within the remaining data, to generate 50 models, and use the ensemble of 50 models to make prediction on the held-out set  and calculate the classification accuracy. We repeat this 6 times, each time with a different set of 20 heldout samples chosen randomly and report the average accuracy.
This comparison is provided in Table IV.
\vspace{-0.3cm}
\begin{table}[ht]
\centering
  \caption{Performance comparison of different approaches}
  \centering
\begin{tabular}{|m{4cm}|m{2.5cm}|}
\hline
The Algorithm   &  Heldout Set Accuracy \\
\hline
BoW on raw patches \cite{minaee1} using 20 words for each metric and region  &  \\\\\\\ \86.2\%\\
\hline
The proposed Deep-BoW  &  \\\\\\\\ 86.4\%\\
\hline
\end{tabular}
\label{TblComp}
\end{table}

\iffalse
\begin{table}[ht]
\centering
  \caption{Performance comparison of different approaches}
  \centering
\begin{tabular}{|m{4cm}|m{1.7cm}|m{1.5cm}|}
\hline
The Algorithm  & Accuracy on Validation Set & Accuracy on Heldout Set \\
\hline
BoW on raw patches \cite{minaee1} using 20 words for each metric and region  &  \\\\ \\\92 &  \\\\ \85.9\%\\
\hline
The proposed Deep-BoW  & \\\\ \\\90.1  &  \\\\\ 87.1\%\\
\hline
\end{tabular}
\label{TblComp}
\end{table}
\fi





Finally, we  present the the average histograms  of patients, and control subjects. 
These histograms and their difference are shown in Fig. 5.
As we can see mTBI and control subjects have clear differences in some part of these representations.
\begin{figure}[h]
\begin{center}
    \includegraphics[scale=0.18]{hist_14metrics_template_dec6.png}
\end{center}
\vspace{-0.2cm}
  \caption{Deep-BoW histograms of patients and controls}
\end{figure}




\section{Conclusion}
In this work, we propose a machine learning framework for mTBI identification from diffusion MRI using a relatively small dataset of 114 subjects.
To approach this problem without overfitting, we employ a deep unsupervised learning approach to learn feature representation for image patches, followed by aggregating patch level features using bag of word representation to form the overall image feature. 
These features are used along with demographic and neuro-cognitive features for patient identification.
Then greedy forward feature selection and SVM are used to perform classification.
Through experimental studies, we show that by learning deep visual features, we obtain significant gain over using mean values of MR metrics in brain regions, and also hand-crafted features.
%The performance is also improved over the approach where the visual words are determined based on  the raw image patch representation.
 
%This methodology is especially useful for learning features from datasets with small number of samples, as in many of medical image analysis studies.





\section*{ACKNOWLEDGMENT}
Research reported in this paper is supported by National Institute of Neurological Disorders and Stroke of  National Institutes of Health (NIH) under award number  R21NS090349. The content is solely the responsibility of the authors and does not necessarily represent the official views of the NIH.




\begin{thebibliography}{99}
\footnotesize
\bibitem{lui3}
E Raz, JH Jensen, Y Ge,  et al. ``Brain iron quantification in mild traumatic brain injury: a magnetic field correlation study'', American journal of neuroradiology, pp.1851-1856, 2011.
%\bibitem{lui4}
%L Tang, Y Ge, DK Sodickson, L Miles, et al.  ``Thalamic resting-state functional networks: disruption in patients with mild traumatic brain injury'', Radiology, 260(3), pp.831-840, 2011.
\bibitem{lui5}
Zhou Y, Kierans A, Kenul D, Ge Y, Rath J, Reaume J, Grossman RI, Lui YW, ``Longitudinal Regional Brain Volume Changes in Mild Traumatic Brain Injury Patients'', Radiology, in press 2013.
\bibitem{els}
E Fieremans, JH Jensen, JA Helpern, ``White matter characterization with diffusional kurtosis imaging'', Neuroimage, Elsevier, 177-188, 2011.
\bibitem{yuanyi}
YW Lui, Y Xue, D Kenul, Y Ge, RI Grossman, Y Wang, ``Classification algorithms using multiple MRI features in mild traumatic brain injury'', Neurology 83.14: 1235-1240, 2014.
\bibitem{vergara}
VM Vergara, AR Mayer, E Damaraju, KA Kiehl, V Calhoun, ``Detection of mild traumatic brain injury by machine learning classification using resting state functional network connectivity and fractional anisotropy'', Journal of neurotrauma, 2017.
\bibitem{minaee1}
S Minaee, S Wang, Y Wang,  et al. ``Identifying Mild Traumatic Brain Injury Patients From MR Images Using Bag of Visual Words'', Signal Processing in Medicine and Biology Symposium, IEEE, 2017.
\bibitem{minaee2}
S Minaee, Y Wang, S Chung,  et al. ``A Machine Learning Approach For Identifying Patients with Mild Traumatic Brain Injury Using Diffusion MRI Modeling'', ASFNR 12th Annual Meeting, 2017.
\bibitem{thal}
EJ Grossman, M Inglese, ``The role of thalamic damage in mild traumatic brain injury'', Journal of neurotrauma: 163-167, 2016.
\bibitem{cc}
S Chung, E Fieremans, X Wang et al. ``White matter tract integrity: an indicator of axonal pathology after mild traumatic brain injury'', Journal of neurotrauma, 2017.
\bibitem{dnn}
I Goodfellow, Y Bengio, A Courville, ``Deep learning'',  Cambridge: MIT press, 2016.

%\bibitem{mtbi1}
%Faul M LW, Wald MM, Coronado VG, ``Traumatic Brain Injury in the United States: Emergency Department Visits, Hospitalizations and Deaths'', 2010.
%\bibitem{acrm}
%Carroll LJ, Cassidy JD, Peloso PM, et al. ``Prognosis for mild traumatic brain injury: results of the WHO Collaborating Centre Task Force on Mild Traumatic Brain Injury'' J Rehabil Med: 84?105, 2004.
%\bibitem{cdcp}
%Kay T, Harrington D, Adams R, et al. ``Definition of mild
%traumatic brain injury'', J Head Trauma Rehabil; 8:86?87, 1993.
%\bibitem{dod}
%Marr A, Corronado V, editors. ``Central Nervous System Injury Surveillance Data Submission Standards?2002. Centers for Disease Control and Prevention'', National Center for Injury Prevention and Control; 2004.
%\bibitem{dva}
%Veterans Administration Department of Defense Clinical Practice Guideline for Mangement of Concussion/Mild Traumatic Brain Injury, 2014.


%\bibitem{lui1}
%Cohen, B. A., M. Inglese, H. Rusinek, J. S. Babb, R. I. Grossman, and O. Gonen. ``Proton MR spectroscopy and MRI-volumetry in mild traumatic brain injury'', American Journal of Neuroradiology,  907-913, 2007.
%\bibitem{lui2}
%Miles, Laura, Robert I. Grossman, Glyn Johnson, James S. Babb, Leonard Diller, and Matilde Inglese. ``Short-term DTI predictors of cognitive dysfunction in mild traumatic brain injury'', Brain injury 22, no. 2: 115-122, 2008.

%\bibitem{sift}
%David Lowe, ``Distinctive image features from scale-invariant keypoints'', International journal of computer vision, 2004.
%\bibitem{surf}
%H Bay, A Ess, T Tuytelaars, L Van Gool, ``Surf: Speeded up robust features'', In European conference on computer vision, Springer, pp. 404-417, 2006.
%\bibitem{hog}
%N Dalal, B Triggs , ``Histograms of oriented gradients for human detection'', CVPR, IEEE, 2005.
%\bibitem{lbp}
%Z Guo, L Zhang, D Zhang, ``A completed modeling of local binary pattern operator for texture classification'', IEEE Transactions on Image Processing 19, no. 6, 2010.
%\bibitem{yann}
%Y LeCun, L Bottou, Y Bengio, and P Haffner, ``Gradient-based learning applied to document recognition'', Proceedings of the IEEE 86, no. 11: 2278-2324, 1998.
%\bibitem{alex}
%A Krizhevsky, I Sutskever, GE Hinton, ``Imagenet classification with deep convolutional neural networks'', In Advances in neural information processing systems, pp. 1097-1105, 2012.

%\bibitem{cnn1}
%R Girshick, J Donahue, T Darrell, J Malik, ``ich feature hierarchies for accurate object detection and semantic segmentation'', CVPR, IEEE, 2014.
%\bibitem{cnn3}
%O Ronneberger, P Fischer, T Brox, ``U-net: Convolutional networks for biomedical image segmentation'', In International Conference on Medical image computing and computer-assisted intervention, Springer, 2015.
%\bibitem{cnn4}
%K He, X Zhang, S Ren, J Sun, ``Deep residual learning for image recognition'', In Proceedings of the IEEE conference on computer vision and pattern recognition, 2016.
%\bibitem{cnn2}
%Q You, H Jin, Z Wang, C Fang, J Luo, ``Image captioning with semantic attention'', CVPR, IEEE, 2016.
%\bibitem{offshelf}
%AS Razavian, H Azizpour, J Sullivan, ``CNN features off-the-shelf: an astounding baseline for recognition'', Computer Vision and Pattern Recognition Workshops (CVPRW), IEEE, 2014.
%\bibitem{iris}
%S Minaee, AA Abdolrashidiy, Y Wang, ``An experimental study of deep convolutional features for iris recognition'', Signal Processing in Medicine and Biology Symposium, IEEE, 2016.

\bibitem{bow1}
J Yang, YG Jiang, AG Hauptmann, CW Ngo, ``Evaluating bag-of-visual-words representations in scene classification'', International workshop on multimedia information retrieval, ACM, 2007.
%\bibitem{bow2}
%Y Yang, S Newsam, ``Bag-of-visual-words and spatial extensions for land-use classification'', Proceedings of the 18th SIGSPATIAL international conference on advances in geographic information systems, ACM, 2010.
%\bibitem{bow3}
%X Peng, L Wang, X Wang, Y Qiao, ``Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice'', Computer Vision and Image Understanding 150: 109-125, 2016.

\bibitem{CAE}
J Masci, U Meier, D Ciresan, J Schmidhuber, ``Stacked convolutional auto-encoders for hierarchical feature extraction'', International Conference on Artificial Neural Networks, 2011.
\bibitem{cae2}
B Leng, S Guo, X Zhang, Z Xiong, ``3D object retrieval with stacked local convolutional auto-encoder'', Signal Processing 112, 2015.
\bibitem{fs1}
I Guyon, A Elisseeff, ``An introduction to variable and feature selection'', Journal of machine learning research, 3(Mar), 1157-1182, 2003.
\bibitem{fs2}
G Chandrashekar, F Sahin, ``A survey on feature selection methods'', Computers and Electrical Engineering, Elsevier,  2014.
\bibitem{mrmr}
H Peng, F Long, C Ding, ``Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy'', IEEE Transactions on pattern analysis and machine intelligence, 2005.
%\bibitem{dr1}
%CM, Bishop, ``PATTERN RECOGNITION AND MACHINE LEARNING'', Springer-Verlag New York, 2006.
%\bibitem{dr2}
%JP Cunningham, Z Ghahramani, ``Linear dimensionality reduction: survey, insights, and generalizations'', Journal of Machine Learning Research, 2859-900, 2015.
%\bibitem{svm}
%Cortes, Corinna, and Vladimir Vapnik. "Support-vector networks." Machine learning 20.3: 273-297, 1995.
%\bibitem{lr}
%Christopher Bishop, ``Pattern recognition and machine learning'', springer, 2006.
%\bibitem{rf}
%A  Liaw, M Wiener, ``Classification and regression by random forest'', R news 2.3: 18-22, 2002.
%\bibitem{nn}
%I Goodfellow, Y Bengio, A Courville, ``Deep learning'', MIT press, 2016.



\end{thebibliography}




\end{document}
